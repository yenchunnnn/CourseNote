# Introduction to Writing Functions

## Write a Function

### Why write function?

**Benefits of writing functions:**

-   Functions eliminate repetition from your code, which

    -   can reduce your workload, and

    -   help avoid errors.

-   Functions also allow code reuse and sharing.

#### Calling functions

Best practice for calling functions is to include them in the order shown by `args()`, and to only name rare arguments.

```{r}
args(rank)
```

### Convert scripts into functions

-   Make a template

-   Paste your script into the body

-   Choose the arguments

-   Replace specific values with arguments

-   Generalize variable names

-   Remove the final assignment

#### First function

```{r}
# Your script, from a previous step
# Sample from coin_sides once
coin_sides <- c("head", "tail")
sample(coin_sides, 1)

# Paste your script into the function body
toss_coin <- function() {
  coin_sides <- c("head", "tail")
  sample(coin_sides, 1)
}

# Call your function
toss_coin()
```

#### Inputs to functions

The inputs to functions are called arguments.

```{r}
# Update the function to return n coin tosses
toss_coin <- function(n_flips) {
  coin_sides <- c("head", "tail")
  sample(coin_sides, size = n_flips, replace = TRUE)
}

# Generate 10 coin tosses
toss_coin(10)
```

#### Multiple inputs to functions

If a function should have more than one argument, list them in the function signature, separated by commas.

Bias the coin by weighting the sampling. Specify the `prob` argument so that heads are sampled with probability `p_head` (and tails are sampled with probability `1 - p_head`).

```{r}
coin_sides <- c("head", "tail")
n_flips <- 10
p_head <- 0.8

# Define a vector of weights
weights <- c(p_head, 1 - p_head)

# Update so that heads are sampled with prob p_head
sample(coin_sides, n_flips, replace = TRUE, prob = weights)
```

Update the definition of `toss_coin()` so it accepts an argument, `p_head`, and weights the samples using the code you wrote in the previous step.

```{r}
# Update the function so heads have probability p_head
toss_coin <- function(n_flips, p_head) {
  coin_sides <- c("head", "tail")
  # Define a vector of weights
  weights <- c(p_head, 1 - p_head)
  # Modify the sampling to be weighted
  sample(coin_sides, n_flips, replace = TRUE, prob = weights)
}

# Generate 10 coin tosses with an 80% chance of each head
toss_coin(10, 0.8)
```

### Readable code

-   Function names should contain a verb

-   Readability vs. typeability

    -   Understanding code \>\> typing code

    -   Code editors have autocomplete

    -   You can alias common functions

-   Types of argument

    -   Data arguments: what you compute on

    -   Detail arguments: how you perform the computation

    -   Data args should precede detail args

**Renaming GLM**

R's generalized linear regression function, `glm()`, suffers the same usability problems as `lm()`: its name is an acronym, and its `formula` and `data` arguments are in the wrong order.

To solve this exercise, you need to know two things about generalized linear regression:

1.  `glm()` formulas are specified like `lm()` formulas: response is on the left, and explanatory variables are added on the right.

2.  To model count data, set `glm()`'s `family` argument to `poisson`, making it a Poisson regression.

```{r message=FALSE}
library(tidyverse)
snake_river_visits <- read_rds("data/snake_river_visits.rds")

# Run a generalized linear regression 
glm(
  # Model no. of visits vs. gender, income, travel
  n_visits ~ gender + income + travel, 
  # Use the snake_river_visits dataset
  data = snake_river_visits, 
  # Make it a Poisson regression
  family = poisson
)
```

Define a function, `run_poisson_regression()`, to run a Poisson regression. This should take two arguments: `data` and `formula`, and call `glm()` .

```{r}
# Write a function to run a Poisson regression
run_poisson_regression <- function(data, formula) {
    glm(formula, data, family = poisson)
}
```

```{r}
# Re-run the Poisson regression, using your function
model <- snake_river_visits %>%
  run_poisson_regression(n_visits ~ gender + income + travel)

# Run this to see the predictions
# snake_river_explanatory %>%
#  mutate(predicted_n_visits = predict(model, ., type = "response"))%>%
#  arrange(desc(predicted_n_visits))
```

## All About Arguments

### Default arguments

#### Numeric defaults

Only set defaults for numeric detail arguments, not data arguments.

`cut_by_quantile()` converts a numeric vector into a categorical variable where quantiles define the cut points.

By specifying default arguments, you can make it easier to use. Let's start with `n`, which specifies how many categories to cut `x` into.

```{r}
# A numeric vector of the number of visits to Snake River
n_visits <- snake_river_visits$n_visits

# Set the default for n to 5
cut_by_quantile <- function(x, n = 5, na.rm, labels, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the n argument from the call
cut_by_quantile(
  n_visits,
  na.rm = FALSE, 
  labels = c("very low", "low", "medium", "high", "very high"),
  interval_type = "(lo, hi]"
)
```

#### Logical defaults

`cut_by_quantile()` is now slightly easier to use, but you still always have to specify the `na.rm` argument. This removes missing values.

Where functions have an argument for removing missing values, the best practice is to **not** remove them by default (in case you hadn't spotted that you had missing values). That means that the default for `na.rm` should be `FALSE`.

```{r}
# Set the default for na.rm to FALSE
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the na.rm argument from the call
cut_by_quantile(
  n_visits,  
  labels = c("very low", "low", "medium", "high", "very high"),
  interval_type = "(lo, hi]"
)
```

#### NULL defaults

The `cut()` function used by `cut_by_quantile()` can automatically provide sensible labels for each category.

The code to generate these labels is [**pretty complicated**](https://github.com/wch/r-source/blob/29a9e663a2352843a6ea26b259725b0b97d0e4bd/src/library/base/R/cut.R#L42-L60), so rather than appearing in the function signature directly, its `labels` argument defaults to `NULL`, and the calculation details are shown on the [**`?cut`**](https://www.rdocumentation.org/packages/base/topics/cut) help page.

(If you use this capability, make sure to document how the argument behaves in the function's help page.)

```{r}
# Set the default for labels to NULL
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, interval_type) {
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the labels argument from the call
cut_by_quantile(
  n_visits,
  interval_type = "(lo, hi]"
)
```

#### Categorical defaults

-   Pass a character vector in the signature.

-   Call `match.arg()` in the body.

The pattern for categorical defaults is:

``` r
function(cat_arg = c("choice1", "choice2")) {
  cat_arg <- match.arg(cat_arg)
}
```

When cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom.

In mathematical terminology,

-   "open on the left, closed on the right" = `(lo, hi]`

-   "closed on the left, open on the right" = `[lo, hi)`

`cut_by_quantile()` should allow these two choices.

```{r}
# Take "rank()" for example, look at the ties.method argument
head(rank, 7)
```

```{r}
# Set the categories for interval_type to "(lo, hi]" and "[lo, hi)"
cut_by_quantile <- function(x, n = 5, na.rm = FALSE, labels = NULL, 
                            interval_type = c("(lo, hi]", "[lo, hi)")) {
  # Match the interval_type argument
  interval_type <- match.arg(interval_type)
  probs <- seq(0, 1, length.out = n + 1)
  qtiles <- quantile(x, probs, na.rm = na.rm, names = FALSE)
  right <- switch(interval_type, "(lo, hi]" = TRUE, "[lo, hi)" = FALSE)
  cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE)
}

# Remove the interval_type argument from the call
cut_by_quantile(n_visits)
```

`match.arg()` handles throwing an error if the user types a value that wasn't specified.

### Pass arguments between functions

#### Harmonic mean

The harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. The harmonic mean is often used to average ratio data.

![](image/harmonic%20mean.png)

You'll be using it on the price/earnings ratio of stocks in the Standard and Poor's 500 index, provided as `std_and_poor500`. Price/earnings ratio is a measure of how expensive a stock is.

```{r}
std_and_poor500 <- read_rds("data/std_and_poor500_with_pe_2019-06-21.rds")
glimpse(std_and_poor500)
```

```{r}
# Write a function to calculate the reciprocal
get_reciprocal <- function(x) {
  1/x
}

# Write a function to calculate the harmonic mean
calc_harmonic_mean <- function(x) {
  x %>%
    get_reciprocal() %>%
    mean() %>%
    get_reciprocal()
}

# Calculate each sector Price/earnings ratio
std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio
  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio))
```

It looks like we have a problem though: most sectors have missing values.

#### Handling missing values

It would be useful for your function to be able to remove missing values before calculating.

Rather than writing your own code for this, you can outsource this functionality to `mean()`.

```{r}
# Add an na.rm arg with a default, and pass it to mean()
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio, and remove missing value
  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))
```

#### Using `...`

The tradeoff

-   Benefits

    -   Less typing for you

    -   No need to match signatures

-   Drawbacks

    -   You need to trust the inner function

    -   The interface is not as obvious to users

Rather than explicitly giving `calc_harmonic_mean()` and `na.rm` argument, you can use `...` to simply "pass other arguments" to `mean()`.

```{r}
# Swap na.rm arg for ... in signature and body
calc_harmonic_mean <- function(x, ...) {
  x %>%
    get_reciprocal() %>%
    mean(...) %>%
    get_reciprocal()
}

std_and_poor500 %>% 
  # Group by sector
  group_by(sector) %>% 
  # Summarize, calculating harmonic mean of P/E ratio, and remove missing value
  summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE))
```

Using `...` doesn't change how people use your function; it just means the function is more flexible.

### Checking arguments

#### Throwing errors

If a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are

-   Throw the error message as soon as you realize there is a problem (typically at the start of the function).

-   Make the error message easily understandable.

You can use the `assert()` functions from `assert` package to check inputs and throw errors when they fail. (no output means no problem)

``` r
library(assert)

calc_harmonic_mean <- function(x, na.rm = FALSE) {
  # Assert that x is numeric
  assert(is.numeric(x))
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it strings
calc_harmonic_mean(std_and_poor500$sector)
```

``` {.r .output style="background: white; color: black"}
Error: in calc_harmonic_mean(x = std_and_poor500$sector)
 Failed checks:
 is.numeric(x)
```

#### Custom error logic

Sometimes the `assert()` functions in assert don't give the most informative error message. For example, won't say why that's a problem. In that case, you can use the `is_*()` functions in conjunction with messages, warnings, or errors to define custom feedback.

The harmonic mean only makes sense when x has all positive values.

``` r
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  assert(is.numeric(x))
  # Check if any values of x are non-positive
  if(any(x < 0, na.rm = TRUE)) {
    # Throw an error
    stop("x contains non-positive values, so the harmonic mean makes no sense.")
  }
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it negative numbers
calc_harmonic_mean(std_and_poor500$pe_ratio - 20)
```

``` {.r .output style="background: white; color: black"}
Error in calc_harmonic_mean(std_and_poor500$pe_ratio - 20) :
 x contains non-positive values, so the harmonic mean makes no sense.
```

Explaining what went wrong is helpful to users. Explaining why it is wrong is even better!

#### Fixing function arguments

You still need to provide some checks on the `na.rm` argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it.

`na.rm` should be a logical vector with one element.

Fix the `na.rm` argument by using `use_first()` to select the first `na.rm` element, and `coerce_to()` to change it to logical.

```{r}
library(assert)
library(assertive.base)

# Update the function definition to fix the na.rm argument
calc_harmonic_mean <- function(x, na.rm = FALSE) {
  assert(is.numeric(x))
  if(any(x < 0, na.rm = TRUE)) {
    stop("x contains non-positive values, so the harmonic mean makes no sense.")
  }
  # Use the first value of na.rm, and coerce to logical
  na.rm <- coerce_to(use_first(na.rm), target_class = "logical")
  x %>%
    get_reciprocal() %>%
    mean(na.rm = na.rm) %>%
    get_reciprocal()
}

# See what happens when you pass it malformed na.rm
calc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5)
```

## Return Values and Scope

### Returning values

Reasons for returning early

1.  You already know the answer.

2.  The input is an edge case.

#### Returning early

Sometimes, you don't need to run through the whole body of a function to get the answer. In that case you can return early from that function using `return()`.

Returning early can often improve the performance of your functions considerably for some input values.

You need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn't a century (like 1904 but not 1900 or 1905).

```{r}
# checking for the cases of year being divisible by 400, then 100, then 4, returning early from the function in each case.
is_leap_year <- function(year) {
  # If year is div. by 400 return TRUE
  if(year %% 400 == 0) {
    return(TRUE)
  }
  # If year is div. by 100 return FALSE
  if(year %% 100 == 0) {
    return(FALSE)
  }  
  # If year is div. by 4 return TRUE
  if(year %% 4 == 0) {
    return(TRUE)
  }
  
  
  # Otherwise return FALSE
  else {
    return(FALSE)
  }
}
```

#### Returning invisibly

When the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be printed as well. In that case, the value should be [**invisibly returned**](https://www.rdocumentation.org/packages/base/topics/invisible): `invisible()`

The base R plot function returns `NULL`, since its main purpose is to draw a plot. Recall that `plot()` has a [**formula interface**](https://www.rdocumentation.org/packages/graphics/topics/plot.formula): `plot(y ~ x, data = data)`

```r
# Using cars, draw a scatter plot of dist vs. speed
plt_dist_vs_speed <- base::plot(dist ~ speed, data = cars)

# Oh no! The plot object is NULL
plt_dist_vs_speed
```

This isn't helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step.

```r
# Define a pipeable plot fn with data and formula args
pipeable_plot <- function(data, formula) {
  # Call plot() with the formula interface
  base::plot(formula, data)
  # Invisibly return the input dataset
  invisible(data)
}

# Draw the scatter plot of dist vs. speed again
plt_dist_vs_speed <- cars %>% 
  pipeable_plot(dist ~ speed)

# Now the plot object has a value
plt_dist_vs_speed
```

### Return multiple values

R functions can only return a single value, but there are two ways to get around this rule:

1.  return several objects in a **list**

2.  store objects as **attributes**

When to use each technique:

-   If you need the result to have a particular type, add additional return values as attributes.

-   Otherwise, collect all return values into a list

#### Returning many things

Functions can only return one value. If you want to return multiple things, then you can store them all in a list.

**Multi-assignment**

If users want to have the list items as separate variables, they can assign each list element to its own variable.

Using `zeallot`'s multi-assignment operator, `%<-%`, e.g, `c(a, b) %<-% list(2 elements)`

`broom` package:

-   `glance()`: model

-   `tidy()`: coefficient

-   `augment()`: observation

```{r message=FALSE}
library(broom)
library(zeallot)

# Look at the structure of model (it's a mess!)
str(model)

# Use broom tools to get a list of 3 data frames
list(
  # Get model-level values
  model = glance(model),
  # Get coefficient-level values
  coefficients = tidy(model),
  # Get observation-level values
  observations = augment(model)
)
```

```{r}
# Wrap this code into a function, groom_model
groom_model <- function(model) {
  list(
    model = glance(model),
    coefficients = tidy(model),
    observations = augment(model)
  )
}

# Call groom_model on model, assigning to 3 variables
c(mdl, cff, obs) %<-% groom_model(model)

# See these individual variables
mdl; cff; obs
```

#### Returning metadata

Sometimes you want to return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn't appropriate.

This is common when you have a result plus metadata about the result. (Metadata is "data about the data". For example, it could be the file a dataset was loaded from)

In that case, you can store the metadata in attributes.

``` r
attr(object, "attribute_name") <- attribute_value
```

```r
pipeable_plot <- function(data, formula) {
  plot(formula, data)
  # Add a "formula" attribute to data
  attr(data, "formula") <- formula
  invisible(data)
}

# From previous exercise
plt_dist_vs_speed <- cars %>% 
  pipeable_plot(dist ~ speed)

# Examine the structure of the result
str(plt_dist_vs_speed)
```

### Environments

Environments are a type of variable that is used to store other variables. Mostly, you can think of them as lists.

Every environment has a parent environment (except the empty environment, at the root of the environment tree). This determines which variables R know about at different places in your code.

#### Create & explore environments

-   `ls.str(list)`: list the elements, give a compact summary of multiple variables at once.

-   `list2env(list)`: convert the list to an environment.

-   `environmentName(parent.env(env))`: find the parent of an environment, and display it.

Facts about the Republic of South Africa are contained in `capitals`, `national_parks`, and `population`.

```{r}
# create capitals
city <- c("Cape Town", "Bloemfontein", "Pretoria")
type_of_capital <- c("Legislative", "Judicial", "Administrative")
capitals <- data.frame(city, type_of_capital)

# create national_parks
national_parks <- c(
"Addo Elephant National Park",
"Agulhas National Park",
"Ai-Ais/Richtersveld Transfrontier Park",
"Augrabies Falls National Park",
"Bontebok National Park",
"Camdeboo National Park",
"Golden Gate Highlands National Park",
"Hluhluwe–Imfolozi Park",
"Karoo National Park",
"Kgalagadi Transfrontier Park",
"Knysna National Lake Area",
"Kruger National Park",
"Mapungubwe National Park",
"Marakele National Park",
"Mokala National Park",
"Mountain Zebra National Park",
"Namaqua National Park",
"Table Mountain National Park",
"Tankwa Karoo National Park",
"Tsitsikamma National Park",
"West Coast National Park",
"Wilderness National Park")

# create population
population <- ts(c(40583573, 44819778, 47390900, 51770560, 55908900), start = 1996, end = 2016, frequency = 0.2); population
```

```{r}
# Add capitals, national_parks, & population to a named list
rsa_lst <- list(
  capitals = capitals,
  national_parks = national_parks,
  population = population
)

# List the structure of each element of rsa_lst
ls.str(rsa_lst)
```

```{r}
# Convert the list to an environment
rsa_env <- list2env(rsa_lst)

# List the structure of each variable
ls.str(rsa_env)
```

```{r}
# Find the parent environment of rsa_env
parent <- parent.env(rsa_env)

# Print its name
environmentName(parent)
```

#### Do variables exist?

If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it.

-   `exists("variable", envir = env)`

You can force R to only look in the environment you asked for by setting `inherits` to `FALSE`.

-   `exists("variable", envir = env, inherits = FALSE)`

```{r}
# Compare the contents of the global environment and rsa_env
ls.str(globalenv())
ls.str(rsa_env)

# Does groom_model exist in rsa_env?
exists("groom_model", envir = rsa_env)

# Does groom_model exist in rsa_env, ignoring inheritance?
exists("groom_model", envir = rsa_env, inherits = FALSE)
```

R searches for variables in all the parent environments, unless you explicitly tell it not to (`inherits = FALSE`).

### Scope and precedence

When you call a function, R gives it an environment to store its variables.

**Accessing variables outside functions**

When R couldn't find variable inside the function's environment, it looked in the parent environment. And variable can still be used inside the function.

``` r
x_times_y <- function(x) {
    x * y
}

x_times_y(10)
```

``` {.r .print style="background: white; color: black"}
Error in x_times_y(10) : object 'y' not found
```

```{r}
x_times_y <- function(x) {
    x * y
}

y <- 4
x_times_y(10)
```

**Accessing function variables from outside**

But you can't look inside the function's environment from outside.

``` r
x_times_y <- function(x) {
    x * y
}

y <- 4
x_times_y(10)

print(x)
```

``` {.r .print style="background: white; color: black"}
Error: object 'x' not found
```

Although x is defined inside the function, you can't access it from outside the function.

**Variable precedence**

Variables inside functions take precedence over variables outside functions.

So, R will search for variables inside the function first. If it finds y there, it doesn't need to look in the parent environment.

```{r}
x_times_y <- function(x) {
    y <- 6
    x * y
}

y <- 4

x_times_y(10)
```

Values defined inside the function take precedence over values passed into the function.

```{r}
x_times_y <- function(x) {
    x <- 9
    y <- 6
    x * y
}

y <- 4

x_times_y(10)
```

## Case Study on Grain Yields

### Grain yields & unit conversion

#### Convert areas to metric 1

In this chapter, you'll be working with [**grain yield data**](https://www.rdocumentation.org/packages/agridat/topics/nass.corn) from the [**United States Department of Agriculture, National Agricultural Statistics Service**](https://quickstats.nass.usda.gov/). Unfortunately, they report all areas in acres.

So, the first thing you need to do is write some utility functions to *convert areas in acres to areas in hectares*.

To solve this exercise, you need to know the following:

1.  There are 4840 square yards in an acre.

2.  There are 36 inches in a yard and one inch is 0.0254 meters.

3.  There are 10000 square meters in a hectare.

```{r}
# convert areas in acres to areas in square yards.
# Write a function to convert acres to sq. yards
acres_to_sq_yards <- function(acres) {
  acres * 4840
}

# convert distances in yards to distances in meters.
# Write a function to convert yards to meters
yards_to_meters <- function(yards) {
    yards * 36 * 0.0254
}

# convert areas in square meters to areas in hectares.
# Write a function to convert sq. meters to hectares
sq_meters_to_hectares <- function(sq_meters) {
    sq_meters / 10000
}
```

#### Convert areas to metric 2

You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function.

magrittr's pipeable operator replacements

- `x * y` = `x %>% multiply_by(y)`

- `x ^ y` = `x %>% raise_to_power(y)`

- `x[y]` = `x %>% extract2("y")`

```{r message=FALSE}
library(magrittr)

# Write a function to convert distance in square yards to square meters
sq_yards_to_sq_meters <- function(sq_yards) {
  sq_yards %>%
    # Take the square root
    sqrt() %>%
    # Convert yards to meters
    yards_to_meters() %>%
    # Square it
    raise_to_power(2)
}

# Write a function to convert areas in acres to hectares
acres_to_hectares <- function(acres) {
  acres %>%
    # Convert acres to sq yards
    acres_to_sq_yards() %>%
    # Convert sq yards to sq meters
    sq_yards_to_sq_meters() %>%
    # Convert sq meters to hectares
    sq_meters_to_hectares()
}

# harmonically convert areas in acres to hectares.
# Define a harmonic acres to hectares function
harmonic_acres_to_hectares <- function(acres) {
  acres %>% 
    # Get the reciprocal
    get_reciprocal() %>%
    # Convert acres to hectares
    acres_to_hectares() %>% 
    # Get the reciprocal again
    get_reciprocal()
}
```

By breaking down this conversion into lots of simple functions, you have easy to read code, which helps guard against bugs.

#### Convert yields to metric

The yields in the NASS corn data are also given in US units, namely bushels per acre.

You'll need to write some more utility functions to convert this unit to the metric unit of kg per hectare.

Bushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts.

1. One pound (lb) is 0.45359237 kilograms (kg).
2. One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat.

```{r}
# Write a function to convert lb to kg
lbs_to_kgs <- function(lbs) {
    lbs * 0.45359237
}

# Write a function to convert bushels to lbs
bushels_to_lbs <- function(bushels, crop) {
  # Define a lookup table of scale factors
  c(barley = 48, corn = 56, wheat = 60) %>%
    # Extract the value for the crop
    extract2(crop) %>%
    # Multiply by the no. of bushels
    multiply_by(bushels)
}

# Write a function to convert bushels to kg
bushels_to_kgs <- function(bushels, crop) {
  bushels %>%
    # Convert bushels to lbs for this crop
    bushels_to_lbs(crop) %>%
    # Convert lbs to kgs
    lbs_to_kgs()
}

# Write a function to convert bushels/acre to kg/ha
bushels_per_acre_to_kgs_per_hectare <- function(bushels_per_acre, crop = c("barley", "corn", "wheat")) {
  # Match the crop argument
  crop <- match.arg(crop)
  bushels_per_acre %>%
    # Convert bushels to kgs for this crop
    bushels_to_kgs(crop) %>%
    # Convert harmonic acres to ha
    harmonic_acres_to_hectares()
}
```

You now have a full stack of functions for converting the units. 

#### Apply the unit conversion

Now that you've written some functions, it's time to apply them! The NASS `corn` dataset is available, and you can fortify it (jargon for "adding new columns") with metrics areas and yields.

This fortification process can also be turned into a function, so you'll define a function for this, and test it on the NASS `wheat` dataset.

```{r}
corn <- read_rds("data/nass.corn.rds")
wheat <- read_rds("data/nass.wheat.rds")

# View the corn dataset
glimpse(corn)
```

```{r}
corn <- corn %>%
  # Add some columns
  mutate(
    # Convert farmed area from acres to ha
    farmed_area_ha = acres_to_hectares(farmed_area_acres),
    # Convert yield from bushels/acre to kg/ha
    yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
      yield_bushels_per_acre,
      crop = "corn"
    )
  )

glimpse(corn)
```

```{r}
# Wrap this code into a function
fortify_with_metric_units <- function(data, crop) {
  data %>%
    mutate(
      farmed_area_ha = acres_to_hectares(farmed_area_acres),
      yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare(
        yield_bushels_per_acre, 
        crop = crop
      )
    )
}

# Try it on the wheat dataset
wheat <- fortify_with_metric_units(wheat, "wheat")
glimpse(wheat)
```

### Visualizing grain yields

#### Plot yields over time

An obvious question to ask about each crop is, "how do the yields change over time in each US state?"

```{r}
# Using corn, plot yield (kg/ha) vs. year
ggplot(corn, aes(year, yield_kg_per_ha)) +
  # Add a line layer, grouped by state
  geom_line(aes(group = state)) +
  # Add a smooth trend layer
  geom_smooth()
```

Turn the plotting code into a function.

```{r}
# Wrap this plotting code into a function
plot_yield_vs_year <- function(data) {
  ggplot(data, aes(year, yield_kg_per_ha)) +
    geom_line(aes(group = state)) +
    geom_smooth()
}

# Test it on the wheat dataset
plot_yield_vs_year(wheat)
```

Look at the huge increase in yields from the time of the Green Revolution in the 1950s.

#### A nation divided

The USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups.

The "Corn Belt", where most US corn is grown is in the "West North Central" and "East North Central" regions. The "Wheat Belt" is in the "West South Central" region.

```{r message=FALSE}
usa_census_regions <- read_delim("data/usa_census_regions.txt", delim = ",")
usa_census_regions
```

```{r}
# Inner join the corn dataset to usa_census_regions by state
corn <- corn %>%
  inner_join(usa_census_regions, by = "state")

glimpse(corn)
```

Turn the code into a function.

```{r}
# Wrap this code into a function
fortify_with_census_region <- function(data) {
  data %>%
    inner_join(usa_census_regions, by = "state")
}

# Try it on the wheat dataset
wheat <- fortify_with_census_region(wheat)
glimpse(wheat)
```

With the census data incorporated into the crop datasets, you can now look at yield differences between the regions.

#### Plot yields over time by region

Now you are ready to look at how the yields change over time in each region of the USA.

Use the function you wrote to plot yield versus year for the `corn` dataset, then facet the plot, wrapped by `census_region`.

```{r}
# Plot yield vs. year for the corn dataset
plot_yield_vs_year(corn) +
  # Facet, wrapped by census region
  facet_wrap(vars(census_region))
```

The corn yields are highest in the West North Central region, the heart of the Corn Belt. 

Turn the code into a function.

```{r}
# Wrap this code into a function
plot_yield_vs_year_by_region <- function(data) {
  plot_yield_vs_year(data) +
    facet_wrap(vars(census_region))
}

# Try it on the wheat dataset
plot_yield_vs_year_by_region(wheat)
```

For wheat, it looks like the yields are highest in the Wheat Belt (West South Central region) have been overtaken by some other regions.

### Modeling grain yields

#### Running a model

Notice that the lines on the plot aren't straight. This means that we need to choose a model that handles _nonlinear_ responses. `geom_smooth` creates the smooth trend lines by using _generalized additive models (gam)_.

**generalized additive model**

`mgcv` package GAM syntax:

`gam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset)`

Here, `s()` means "make the variable smooth", where smooth very roughly means nonlinear.

```{r message=FALSE}
library(mgcv)

# Run a generalized additive model of yield vs. smoothed year and census region
corn_model <- gam(yield_kg_per_ha ~ s(year) + census_region, data = corn)
corn_model
```

Wrap the modeling code into a function.

```{r}
# Wrap the model code into a function
run_gam_yield_vs_year_by_region <- function(data) {
  gam(yield_kg_per_ha ~ s(year) + census_region, data = data)
}

# Try it on the wheat dataset
wheat_model <- run_gam_yield_vs_year_by_region(wheat)
wheat_model
```

#### Making yield predictions

Using the models to make predictions. You can do this using a call to `predict()`, in the following form:

`predict(model, cases_to_predict, type = "response")`

GAMs of the `corn` and `wheat` datasets are available as `corn_model` and `wheat_model`. A character vector of census regions is stored as `census_regions`.

```{r}
census_regions <- c("New England", "Mid-Atlantic", "East North Central", "West North Central", "South Atlantic", "East South Central", "West South Central", "Mountain", "Pacific" )

# Make predictions in 2050  
predict_this <- data.frame(
  year = 2050,
  census_region = census_regions
) 

# Predict the yield
pred_yield_kg_per_ha <- predict(corn_model, predict_this, type = "response")

predict_this %>%
  # Add the prediction as a column of predict_this 
  mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha) %>%
  arrange(desc(pred_yield_kg_per_ha))
```

Wrap the script into a function.

```{r}
# Wrap this prediction code into a function
predict_yields <- function(model, year) {
  predict_this <- data.frame(
    year = year,
    census_region = census_regions
  ) 
  pred_yield_kg_per_ha <- predict(model, predict_this, type = "response")
  predict_this %>%
    mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha) %>%
    arrange(desc(pred_yield_kg_per_ha))
}

# Try it on the wheat dataset
predict_yields(wheat_model, 2050)
```

The models predict that in 2050, the highest yields will be in the Pacific region for both corn and wheat.

#### Do it all over again

Now you are going to rerun the whole analysis from this chapter on a new crop, barley. 

Barley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana.

Since all the infrastructure is in place, 

`fortify_with_metric_units()`, `fortify_with_census_region()`, `plot_yield_vs_year_by_region()`, `run_gam_yield_vs_year_by_region()`, and `predict_yields()`

that's less effort than it sounds!

```{r}
barley <- read_rds("data/nass.barley.rds")
head(barley)
```

Fortify the `barley` data with metric units, then with census regions.

```{r}
fortified_barley <- barley %>% 
  # Fortify with metric units
  fortify_with_metric_units("barley") %>%
  # Fortify with census regions
  fortify_with_census_region()

# See the result
glimpse(fortified_barley)
```

Plot the yield versus year by census region.
```{r}
# Plot yield vs. year by region
fortified_barley %>% plot_yield_vs_year_by_region()
```

Run a GAM of yield versus year by census region, then predict the yields in 2050.

```{r}
fortified_barley %>% 
  # Run a GAM of yield vs. year by region
  run_gam_yield_vs_year_by_region() %>% 
  # Make predictions of yields in 2050
  predict_yields(2050)
```

Since all your analysis code was contained in functions, it was really simple to apply it to another dataset. Here you can see that yields are highest in the Mountain region, and the model predicts that this will still be the case in 2050.



















