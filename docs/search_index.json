[["index.html", "Data Scientist with R About", " Data Scientist with R Yen Chun Chen 2024-01-16 About This is a notebook about the data science courses I took from the Datacamp. If you think it’s helpful, please do not hesitate to give me a star. ⭐ Cheers!  "],["introduction-to-r.html", "Chapter 1 Introduction to R 1.1 Basic arithmetic 1.2 Basic data types 1.3 Vector 1.4 Matrix 1.5 Factor 1.6 Data Frame 1.7 List 1.8 Bring it all together", " Chapter 1 Introduction to R 1.1 Basic arithmetic # An addition 5 + 5 ## [1] 10 # A subtraction 5 - 5 ## [1] 0 # A multiplication 3 * 5 ## [1] 15 # A division (5 + 5) / 2 ## [1] 5 # Exponentiation (次方) 2 ^ 5 ## [1] 32 # Modulo (餘數) 28 %% 5 ## [1] 3 1.2 Basic data types numerics decimal (小數), eg: 4.5 integer (整數), eg: 4 logical: boolean values, TRUE or FALSE characters: text value Check data type function: class() # Declare variables of different types my_numeric &lt;- 42 my_character &lt;- &quot;universe&quot; my_logical &lt;- FALSE # Check class of my_numeric class(my_numeric) ## [1] &quot;numeric&quot; # Check class of my_character class(my_character) ## [1] &quot;character&quot; # Check class of my_logical class(my_logical) ## [1] &quot;logical&quot; 1.3 Vector Vectors (one dimensional array): can hold numeric, character or logical values. The elements in a vector all have the same data type. 1.3.1 Useful function c(), names(), sum() Here the example # Poker and roulette winnings from Monday to Friday: poker_vector &lt;- c(140, -50, 20, -120, 240) roulette_vector &lt;- c(-24, -50, 100, -350, 10) days_vector &lt;- c(&quot;Monday&quot;, &quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;, &quot;Friday&quot;) names(poker_vector) &lt;- days_vector #names: columns names names(roulette_vector) &lt;- days_vector # Total winnings with poker total_poker &lt;- sum(poker_vector) total_poker ## [1] 230 # Total winnings with roulette total_roulette &lt;- sum(roulette_vector) total_roulette ## [1] -314 # Total winnings overall total_week &lt;- sum(total_poker, total_roulette) # Comparison total_poker &gt; total_roulette ## [1] TRUE # Print out total_week total_week ## [1] -84 1.3.2 Selection - by brackets or names To select elements of a vector (and later matrices, data frames, …), you can use square brackets, or using the names of the vector elements. # Define a new variable based on a selection poker_wednesday &lt;- poker_vector[3]; poker_wednesday ## Wednesday ## 20 # To select multiple elements from a vector, you can add square brackets at the end of it. You can indicate between the brackets what elements should be selected. # Define a new variable based on a selection # Select by brackets poker_two &lt;- poker_vector[c(1, 5)]; poker_two ## Monday Friday ## 140 240 poker_midweek &lt;- poker_vector[2:4]; poker_midweek ## Tuesday Wednesday Thursday ## -50 20 -120 # Select by names poker_two_name &lt;- poker_vector[c(&quot;Monday&quot;, &quot;Friday&quot;)]; poker_two_name ## Monday Friday ## 140 240 poker_midweek_name &lt;- poker_vector[c(&quot;Tuesday&quot;, &quot;Wednesday&quot;, &quot;Thursday&quot;)]; poker_midweek_name ## Tuesday Wednesday Thursday ## -50 20 -120 1.3.3 Selection - by comparison - operators The (logical) comparison operators known to R are: &lt; for less than &gt; for greater than &lt;= for less than or equal to &gt;= for greater than or equal to == for equal to each other != not equal to each other Return True or False. # Which days did you make money on poker? selection_vector &lt;- poker_vector &gt; 0 # Print out selection_vector selection_vector ## Monday Tuesday Wednesday Thursday Friday ## TRUE FALSE TRUE FALSE TRUE 1.3.4 Selection - by comparison - values When you pass a logical vector in square brackets: it will only select the elements that correspond to TRUE in vector. # Select from poker_vector these days poker_winning_days &lt;- poker_vector[selection_vector] poker_winning_days ## Monday Wednesday Friday ## 140 20 240 1.4 Matrix Matrices (two dimensional array): can hold numeric, character or logical values. The elements in a matrix all have the same data type. 1.4.1 Construct function In R, a matrix is a collection of elements of the same data type (numeric, character, or logical) arranged into a fixed number of rows and columns. Since you are only working with rows and columns, a matrix is called two-dimensional. You can construct a matrix in R with the matrix() function. Consider the following example: matrix(1:9, byrow = TRUE, nrow = 3) In the matrix() function: The first argument is the collection of elements that R will arrange into the rows and columns of the matrix. Here, we use 1:9 which is a shortcut for c(1, 2, 3, 4, 5, 6, 7, 8, 9). The argument byrow indicates that the matrix is filled by the rows. If we want the matrix to be filled by the columns, we just place byrow = FALSE. The third argument nrow indicates that the matrix should have three rows. # Construct a matrix with 3 rows that contain the numbers 1 up to 9, filled by rows matrix(1:9, byrow = T, nrow = 3) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 # Construct a matrix with 3 rows that contain the numbers 1 up to 9, but filled by the columns matrix(1:9, byrow = F, nrow = 3) ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 1.4.2 Construct by vector In the editor, three vectors are defined. Each one represents the box office numbers from the first three Star Wars movies. The first element of each vector indicates the US box office revenue, the second element refers to the Non-US box office (source: Wikipedia). In this exercise, you’ll combine all these figures into a single vector. Next, you’ll build a matrix from this vector. # Box office Star Wars (in millions!) new_hope &lt;- c(460.998, 314.4) empire_strikes &lt;- c(290.475, 247.900) return_jedi &lt;- c(309.306, 165.8) # Create box_office box_office &lt;- c(new_hope, empire_strikes, return_jedi) box_office ## [1] 460.998 314.400 290.475 247.900 309.306 165.800 # Construct a matrix with 3 rows, where each row represents a movie. star_wars_matrix &lt;- matrix(box_office, byrow = T, nrow = 3) star_wars_matrix ## [,1] [,2] ## [1,] 460.998 314.4 ## [2,] 290.475 247.9 ## [3,] 309.306 165.8 1.4.3 Naming a matrix Not only does this help you to read the data, but it is also useful to select certain elements from the matrix. rownames(my_matrix) &lt;- row_names_vector colnames(my_matrix) &lt;- col_names_vector matrix(vec, byrow, nrow, dimnames = list(rownames, columnnames)) # Vectors region and titles, used for naming region &lt;- c(&quot;US&quot;, &quot;non-US&quot;) titles &lt;- c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;) # Name the columns with region colnames(star_wars_matrix) &lt;- region # Name the rows with titles rownames(star_wars_matrix) &lt;- titles # Print out star_wars_matrix star_wars_matrix ## US non-US ## A New Hope 460.998 314.4 ## The Empire Strikes Back 290.475 247.9 ## Return of the Jedi 309.306 165.8 # Construct by matrix argument &quot;dimnames&quot; star_wars_matrix_dim &lt;- matrix(box_office, nrow = 3, byrow = TRUE, dimnames = list(titles, region)) star_wars_matrix_dim ## US non-US ## A New Hope 460.998 314.4 ## The Empire Strikes Back 290.475 247.9 ## Return of the Jedi 309.306 165.8 1.4.4 Manipulating - sum of each row &amp; column In R, the function rowSums() conveniently calculates the totals for each row of a matrix, colSums()calculates the totals for each column of a matrix. These function creates a new vector. # Calculate worldwide box office figures for each row worldwide_vector &lt;- rowSums(star_wars_matrix); worldwide_vector ## A New Hope The Empire Strikes Back Return of the Jedi ## 775.398 538.375 475.106 # Calculate worldwide box office figures for each column worldwide_vector_col &lt;- colSums(star_wars_matrix); worldwide_vector_col ## US non-US ## 1060.779 728.100 1.4.5 Manipulating - add columns You can add a column or multiple columns to a matrix with the cbind() function, which merges matrices and/or vectors together by column. For example: big_matrix &lt;- cbind(matrix1, matrix2, vector1 ...) # Bind the new variable worldwide_vector as a column to star_wars_matrix all_wars_matrix &lt;- cbind(star_wars_matrix, worldwide_vector) all_wars_matrix ## US non-US worldwide_vector ## A New Hope 460.998 314.4 775.398 ## The Empire Strikes Back 290.475 247.9 538.375 ## Return of the Jedi 309.306 165.8 475.106 1.4.6 Manipulating - add rows You can add a row or multiple rows to a matrix with the rbind() function, which merges matrices and/or vectors together by row. # Construct another matrix for merging. matrix2_rowname &lt;- c(&quot;The Phantom Menace&quot;, &quot;Attack of the Clones&quot;, &quot;Revenge of the Sith&quot;) star_wars_matrix2 &lt;- matrix(c(474.5, 552.5, 310.7, 338.7, 380.3, 468.5), byrow = T, nrow = 3, dimnames = list(matrix2_rowname, region)) star_wars_matrix2 ## US non-US ## The Phantom Menace 474.5 552.5 ## Attack of the Clones 310.7 338.7 ## Revenge of the Sith 380.3 468.5 # Combine both Star Wars trilogies in one matrix all_wars_matrix &lt;- rbind(star_wars_matrix, star_wars_matrix2) all_wars_matrix ## US non-US ## A New Hope 460.998 314.4 ## The Empire Strikes Back 290.475 247.9 ## Return of the Jedi 309.306 165.8 ## The Phantom Menace 474.500 552.5 ## Attack of the Clones 310.700 338.7 ## Revenge of the Sith 380.300 468.5 1.4.7 Selection of matrix elements You can use the square brackets [ ] to select one or multiple elements from a matrix. Whereas vectors have one dimension, matrices have two dimensions. You should therefore use a comma to separate the rows you want to select from the columns. For example: - my_matrix[1,2] selects the element at the first row and second column. - my_matrix[1:3,2:4] results in a matrix with the data on the rows 1, 2, 3 and columns 2, 3, 4. If you want to select all elements of a row or a column, no number is needed before or after the comma, respectively: - my_matrix[,1] selects all elements of the first column. - my_matrix[1,] selects all elements of the first row. # Select the non-US revenue for all movies non_us_all &lt;- all_wars_matrix[, 2]; non_us_all ## A New Hope The Empire Strikes Back Return of the Jedi ## 314.4 247.9 165.8 ## The Phantom Menace Attack of the Clones Revenge of the Sith ## 552.5 338.7 468.5 # Select the non-US revenue for first two movies non_us_some &lt;- all_wars_matrix[1:2, 2]; non_us_some ## A New Hope The Empire Strikes Back ## 314.4 247.9 1.4.8 Arithmetic - 1 Similar to what you have learned with vectors, the standard operators like +, -, /, *, etc. work in an element-wise way on matrices in R. For example, 2 * my_matrix multiplies each element of my_matrix by two. # Estimate the visitors.Assume that the price of a ticket was 5 dollars. Simply dividing the box office numbers by this ticket price gives you the number of visitors. visitors &lt;- all_wars_matrix / 5 visitors ## US non-US ## A New Hope 92.1996 62.88 ## The Empire Strikes Back 58.0950 49.58 ## Return of the Jedi 61.8612 33.16 ## The Phantom Menace 94.9000 110.50 ## Attack of the Clones 62.1400 67.74 ## Revenge of the Sith 76.0600 93.70 1.4.9 Arithmetic - 2 Just like 2 * my_matrix multiplied every element of my_matrix by two, my_matrix1 * my_matrix2 creates a matrix where each element is the product of the corresponding elements in my_matrix1 and my_matrix2. Note that this is not the standard matrix multiplication for which you should use %*% in R. # Construct another matrix ticket_rowname &lt;- c(&quot;A New Hope&quot;, &quot;The Empire Strikes Back&quot;, &quot;Return of the Jedi&quot;, &quot;The Phantom Menace&quot;, &quot;Attack of the Clones&quot;, &quot;Revenge of the Sith&quot;) ticket_prices_matrix &lt;- matrix(c(5,5,6,6,7,7,4,4,4.5,4.5,4.9,4.9), byrow = T, nrow = 6, dimnames = list(ticket_rowname, region)) ticket_prices_matrix ## US non-US ## A New Hope 5.0 5.0 ## The Empire Strikes Back 6.0 6.0 ## Return of the Jedi 7.0 7.0 ## The Phantom Menace 4.0 4.0 ## Attack of the Clones 4.5 4.5 ## Revenge of the Sith 4.9 4.9 # Estimated number of visitors visitors &lt;- all_wars_matrix / ticket_prices_matrix # US visitors us_visitors &lt;- visitors[, 1] # Average number of US visitors mean(us_visitors) ## [1] 75.01339 1.5 Factor The term factor refers to a statistical data type used to store categorical variables. The difference between a categorical variable and a continuous variable is that a categorical variable can belong to a limited number of categories. A continuous variable, on the other hand, can correspond to an infinite number of values. A good example of a categorical variable is sex(“Male” or “Female”). 1.5.1 Useful function The function factor() or as.factor() will encode the vector as a factor. # Sex vector sex_vector &lt;- c(&quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;) # Convert sex_vector to a factor factor_sex_vector &lt;-factor(sex_vector); factor_sex_vector ## [1] Male Female Female Male Male ## Levels: Female Male 1.5.2 Nominal &amp; Ordinal categorical variable A nominal variable is a categorical variable without an implied order. This means that it is impossible to say that ‘one is worth more than the other’. For example, think of the categorical variable animals_vector with the categories “Elephant”, “Giraffe”, “Donkey” and “Horse”. In contrast, ordinal variables do have a natural ordering. Consider for example the categorical variable temperature_vector with the categories: “Low”, “Medium” and “High”. Here it is obvious that “Medium” stands above “Low”, and “High” stands above “Medium”. # Animals animals_vector &lt;- c(&quot;Elephant&quot;, &quot;Giraffe&quot;, &quot;Donkey&quot;, &quot;Horse&quot;) factor_animals_vector &lt;- factor(animals_vector) factor_animals_vector ## [1] Elephant Giraffe Donkey Horse ## Levels: Donkey Elephant Giraffe Horse # Temperature, ordinal, order = T, levels temperature_vector &lt;- c(&quot;High&quot;, &quot;Low&quot;, &quot;High&quot;,&quot;Low&quot;, &quot;Medium&quot;) factor_temperature_vector &lt;- factor(temperature_vector, order = TRUE, levels = c(&quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)) factor_temperature_vector ## [1] High Low High Low Medium ## Levels: Low &lt; Medium &lt; High 1.5.3 Factor levels However, sometimes you will want to change the names of these levels for clarity or other reasons. R allows you to do this with the function levels(): levels(factor_vector) &lt;- c(\"name1\", \"name2\",...) # Code to build factor_survey_vector survey_vector &lt;- c(&quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;) factor_survey_vector &lt;- factor(survey_vector) factor_survey_vector ## [1] M F F M M ## Levels: F M # Specify the levels of factor_survey_vector levels(factor_survey_vector) &lt;- c(&quot;Female&quot;, &quot;Male&quot;) factor_survey_vector ## [1] Male Female Female Male Male ## Levels: Female Male 1.5.4 Summarizing a factor The function summary() will give you a quick overview of the contents of a variable. # Generate summary for survey_vector summary(survey_vector) ## Length Class Mode ## 5 character character # Generate summary for factor_survey_vector summary(factor_survey_vector) ## Female Male ## 2 3 See the difference of the outputs? So, make sure to add levels for factors. 1.5.5 Ordered factors R will return NA when you try to compare values in a factor, since the idea doesn’t make sense. Ordered factors, where more meaningful comparisons are possible. # Create speed_vector speed_vector &lt;- c(&quot;medium&quot;, &quot;slow&quot;, &quot;slow&quot;, &quot;medium&quot;, &quot;fast&quot;) By default, the function factor() transforms speed_vector into an unordered factor. To create an ordered factor, you have to add two additional arguments: ordered and levels. Function: factor(some_vector, ordered = TRUE, levels = c(&quot;lev1&quot;, &quot;lev2&quot; ...)) # Convert speed_vector to ordered factor vector factor_speed_vector &lt;- factor(speed_vector, order = T, levels = c(&quot;slow&quot;, &quot;medium&quot;, &quot;fast&quot;)) # Print factor_speed_vector factor_speed_vector ## [1] medium slow slow medium fast ## Levels: slow &lt; medium &lt; fast summary(factor_speed_vector) ## slow medium fast ## 2 2 1 Comparing ordered factors # Factor value for second data analyst da2 &lt;- factor_speed_vector[2]; da2 ## [1] slow ## Levels: slow &lt; medium &lt; fast # Factor value for fifth data analyst da5 &lt;- factor_speed_vector[5]; da5 ## [1] fast ## Levels: slow &lt; medium &lt; fast # Is data analyst 2 faster than data analyst 5? da2 &gt; da5 ## [1] FALSE 1.6 Data Frame Data frames (two-dimensional objects): can hold numeric, character or logical values. Within a column all elements have the same data type, but different columns can be of different data type. 1.6.1 Quick look at dataset The function head() enables you to show the first observations of a data frame. Similarly, the function tail() prints out the last observations in your dataset. head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 tail(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.7 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.9 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.5 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.5 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.6 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.6 1 1 4 2 ?mtcars ## starting httpd help server ... done The function str() shows you the structure of your dataset. str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 1.6.2 Creating a data frame Function: data.frame() # Definition of vectors name &lt;- c(&quot;Mercury&quot;, &quot;Venus&quot;, &quot;Earth&quot;, &quot;Mars&quot;, &quot;Jupiter&quot;, &quot;Saturn&quot;, &quot;Uranus&quot;, &quot;Neptune&quot;) type &lt;- c(&quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Terrestrial planet&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;, &quot;Gas giant&quot;) diameter &lt;- c(0.382, 0.949, 1, 0.532, 11.209, 9.449, 4.007, 3.883) rotation &lt;- c(58.64, -243.02, 1, 1.03, 0.41, 0.43, -0.72, 0.67) rings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE) # Create a data frame from the vectors planets_df &lt;- data.frame(name, type, diameter, rotation, rings) head(planets_df) ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 3 Earth Terrestrial planet 1.000 1.00 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE str(planets_df) ## &#39;data.frame&#39;: 8 obs. of 5 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ type : chr &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; ... ## $ diameter: num 0.382 0.949 1 0.532 11.209 ... ## $ rotation: num 58.64 -243.02 1 1.03 0.41 ... ## $ rings : logi FALSE FALSE FALSE FALSE TRUE TRUE ... 1.6.3 Selection - brackets You select elements from a data frame with the help of square brackets [ ]. By using a comma, you can indicate what to select from the rows and the columns respectively. For example: - my_df[1,2] selects the value at the first row and second column in my_df. - my_df[1:3,2:4] selects rows 1, 2, 3 and columns 2, 3, 4 in my_df. Sometimes you want to select all elements of a row or column. For example, my_df[1, ] selects all elements of the first row. # Print out diameter of Mercury (row 1, column 3) planets_df[1, 3] ## [1] 0.382 # Print out data for Mars (entire fourth row) planets_df[4, ] ## name type diameter rotation rings ## 4 Mars Terrestrial planet 0.532 1.03 FALSE 1.6.4 Selection - names You can also use the variable names to select columns of a data frame. # Select first 5 values of diameter column planets_df[1:5, &quot;diameter&quot;] ## [1] 0.382 0.949 1.000 0.532 11.209 1.6.5 Selection - $ There is a short-cut. If your columns have names, you can use the $ sign. # Select the rings variable from planets_df rings_vector &lt;- planets_df$rings; rings_vector ## [1] FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE # Select all columns for planets with rings (only output True value) planets_df[rings_vector, ] ## name type diameter rotation rings ## 5 Jupiter Gas giant 11.209 0.41 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 8 Neptune Gas giant 3.883 0.67 TRUE 1.6.6 Selection - subset The first argument of subset() specifies the dataset for which you want a subset. By adding the second argument, you give R the necessary information and conditions to select the correct subset. subset(my_df, subset = some_condition) You should see the subset() function as a short-cut. The code below will give the exact same result as you got in the previous exercise, but this time, you didn’t need the rings_vector! subset(planets_df, subset = rings) # Select planets with diameter &lt; 1 subset(planets_df, subset = diameter &lt; 1) ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE 1.6.7 Sorting order() is a function that gives you the ranked position of each element when it is applied on a variable, such as a vector for example: a &lt;- c(100, 10, 1000) order(a) ## [1] 2 1 3 a[order(a)] ## [1] 10 100 1000 You would like to rearrange your data frame such that it starts with the smallest planet and ends with the largest one. A sort on the diameter column. # Use order() to create positions positions &lt;- order(planets_df$diameter) # Use positions to sort planets_df planets_df[positions, ] ## name type diameter rotation rings ## 1 Mercury Terrestrial planet 0.382 58.64 FALSE ## 4 Mars Terrestrial planet 0.532 1.03 FALSE ## 2 Venus Terrestrial planet 0.949 -243.02 FALSE ## 3 Earth Terrestrial planet 1.000 1.00 FALSE ## 8 Neptune Gas giant 3.883 0.67 TRUE ## 7 Uranus Gas giant 4.007 -0.72 TRUE ## 6 Saturn Gas giant 9.449 0.43 TRUE ## 5 Jupiter Gas giant 11.209 0.41 TRUE 1.7 List Lists: a list is some kind super data type, you can store practically any piece of information in it! It allows you to gather a variety of objects under one name (that is, the name of the list) in an ordered way. These objects can be matrices, vectors, data frames, even other lists, etc. 1.7.1 Creating a list Function: list() my_list &lt;- list(comp1, comp2 ...) The arguments to the list function are the list components. These components can be matrices, vectors, other lists, etc. # Vector with numerics from 1 up to 10 my_vector &lt;- 1:10 # Matrix with numerics from 1 up to 9 my_matrix &lt;- matrix(1:9, ncol = 3) # First 10 elements of the built-in data frame mtcars my_df &lt;- mtcars[1:10,] # Construct list with these different elements: my_list &lt;- list(my_vector, my_matrix, my_df) my_list ## [[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## [[3]] ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 1.7.2 Creating a named list Just like on your to-do list, you want to avoid not knowing or remembering what the components of your list stand for. That is why you should give names to them. This creates a list with components that are named name1, name2, and so on. my_list &lt;- list(name1 = your_comp1, name2 = your_comp2) If you want to name your lists after you’ve created them, you can use the names() function as you did with vectors. The following commands are fully equivalent to the assignment above: my_list &lt;- list(your_comp1, your_comp2) names(my_list) &lt;- c(&quot;name1&quot;, &quot;name2&quot;) # Adapt list() call to give the components names-after names(my_list) &lt;- c(&quot;vec&quot;, &quot;mat&quot;, &quot;df&quot;) my_list ## $vec ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $mat ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## $df ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 # Adapt list() call to give the components names-before my_list &lt;- list(&quot;vec&quot; = my_vector, &quot;mat&quot; = my_matrix, &quot;df&quot; = my_df) my_list ## $vec ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $mat ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 ## ## $df ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 1.7.3 Selection You can also refer to the names of the components, with [[ ]] or with the $ sign. Note: to select elements from vectors, you use single square brackets: [ ]. Don’t mix them up! # Print the first element of the list, which is a vector my_list[[1]] ## [1] 1 2 3 4 5 6 7 8 9 10 my_list[[&quot;vec&quot;]] ## [1] 1 2 3 4 5 6 7 8 9 10 my_list$vec ## [1] 1 2 3 4 5 6 7 8 9 10 # Print the second element of the vector my_list[[1]][2] ## [1] 2 1.8 Bring it all together # Construct vectors movie_title &lt;- &quot;The Departed&quot; movie_actors &lt;- c(&quot;Leonardo DiCaprio&quot;, &quot;Matt Damon&quot;, &quot;Jack Nicholson&quot;, &quot;Mark Wahlberg&quot;, &quot;Vera Farmiga&quot;, &quot;Martin Sheen&quot;) scores &lt;- c(4.6, 5, 4.8, 5, 4.2) comments &lt;- c(&quot;I would watch it again&quot;, &quot;Amazing!&quot;, &quot;I liked it&quot;, &quot;One of the best movies&quot;, &quot;Fascinating plot&quot;) # Save the average of the scores vector as avg_review avg_review &lt;- mean(scores) # Combine scores and comments into the reviews_df data frame reviews_df &lt;- data.frame(scores, comments) # Sort reviews_df by scores reviews_df &lt;- reviews_df[order(reviews_df$scores), ] # Create and print out a list, called departed_list departed_list &lt;- list(&quot;title&quot; = movie_title, &quot;actors&quot; = movie_actors, &quot;reviews&quot; = reviews_df, &quot;average&quot; = avg_review) departed_list ## $title ## [1] &quot;The Departed&quot; ## ## $actors ## [1] &quot;Leonardo DiCaprio&quot; &quot;Matt Damon&quot; &quot;Jack Nicholson&quot; ## [4] &quot;Mark Wahlberg&quot; &quot;Vera Farmiga&quot; &quot;Martin Sheen&quot; ## ## $reviews ## scores comments ## 5 4.2 Fascinating plot ## 1 4.6 I would watch it again ## 3 4.8 I liked it ## 2 5.0 Amazing! ## 4 5.0 One of the best movies ## ## $average ## [1] 4.72 "],["intermediate-r.html", "Chapter 2 Intermediate R 2.1 Conditionals and Control Flow 2.2 Loops 2.3 Function 2.4 The apply family 2.5 Utilities", " Chapter 2 Intermediate R 2.1 Conditionals and Control Flow 2.1.1 Relational operators Equality, Greater and less than Remember that for string comparison, R determines the greater than relationship based on alphabetical order. Also, keep in mind that TRUE is treated as 1 for arithmetic, and FALSE is treated as 0. Therefore, FALSE &lt; TRUE is TRUE. #Alphabetical Order!字母順序排越後面越大 &quot;Hello&quot; &gt; &quot;Goodbye&quot; ## [1] TRUE # Comparison of character strings &quot;useR&quot; == &quot;user&quot; ## [1] FALSE # Comparison of character strings &quot;raining&quot; &lt;= &quot;raining dogs&quot; ## [1] TRUE # Comparison of logicals TRUE == FALSE ## [1] FALSE # Comparison of numerics -6 * 14 != 17 - 101 ## [1] FALSE # Compare a logical with a numeric TRUE == 1 ## [1] TRUE Compare vectors The sample code in the editor initializes the vectors linkedin and facebook. Each of the vectors contains the number of profile views your LinkedIn and Facebook profiles had over the last seven days. # The linkedin and facebook vectors have already been created for you linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) facebook &lt;- c(17, 7, 5, 16, 8, 13, 14) # Popular days linkedin &gt; 15 ## [1] TRUE FALSE FALSE FALSE FALSE TRUE FALSE # Quiet days linkedin &lt;= 5 ## [1] FALSE FALSE FALSE TRUE TRUE FALSE FALSE # LinkedIn more popular than Facebook linkedin &gt; facebook ## [1] FALSE TRUE TRUE FALSE FALSE TRUE FALSE Compare matrices Matrices and relational operators also work together. # The social data has been created for you views &lt;- matrix(c(linkedin, facebook), nrow = 2, byrow = TRUE); views ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 16 9 13 5 2 17 14 ## [2,] 17 7 5 16 8 13 14 # When does views equal 13? views == 13 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE FALSE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE FALSE # When is views less than or equal to 14? views &lt;= 14 ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE TRUE TRUE TRUE TRUE FALSE TRUE ## [2,] FALSE TRUE TRUE FALSE TRUE TRUE TRUE &amp; and | Watch out: 3 &lt; x &lt; 7 to check if x is between 3 and 7 will not work; you’ll need 3 &lt; x &amp; x &lt; 7 for that. linkedin ## [1] 16 9 13 5 2 17 14 # The last value of the linkedin vector last &lt;- tail(linkedin, 1); last ## [1] 14 # The last two values of the linkedin vector last2 &lt;- tail(linkedin, 2); last2 ## [1] 17 14 # Is last under 5 or above 10? last &lt; 5 | last &gt; 10 ## [1] TRUE # Is last between 15 (exclusive) and 20 (inclusive)? last &gt; 15 &amp; last &lt;= 20 ## [1] FALSE “&amp;” vs “&amp;&amp;”, “|” vs “||” “&amp;&amp;” and “||” will only return the first value. # Output True only c(TRUE, TRUE, FALSE) &amp;&amp; c(TRUE, FALSE, FALSE) # Output True only c(TRUE, TRUE, FALSE) || c(TRUE, FALSE, FALSE) Compare with vectors and matrices # linkedin exceeds 10 but facebook below 10 linkedin &gt; 10 &amp; facebook &lt; 10 ## [1] FALSE FALSE TRUE FALSE FALSE FALSE FALSE # When were one or both visited at least 12 times? linkedin &gt;= 12 | facebook &gt;= 12 ## [1] TRUE FALSE TRUE TRUE FALSE TRUE TRUE # When is views between 11 (exclusive) and 14 (inclusive)? view_true &lt;- views &gt; 11 &amp; views &lt;= 14; view_true ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] FALSE FALSE TRUE FALSE FALSE FALSE TRUE ## [2,] FALSE FALSE FALSE FALSE FALSE TRUE TRUE # Count the number of TRUEs in view_true sum(view_true) ## [1] 4 Not The ! operator, which negates a logical value. !TRUE ## [1] FALSE !(5 &gt; 3) ## [1] FALSE !!FALSE ## [1] FALSE is.numeric(5) ## [1] TRUE !is.numeric(5) ## [1] FALSE 2.1.2 Conditional statements The if statement syntax: if (condition) { expr } # Variables related to your last day of recordings medium &lt;- &quot;LinkedIn&quot; num_views &lt;- 14 # Examine the if statement for medium if (medium == &quot;LinkedIn&quot;) { print(&quot;Showing LinkedIn information&quot;) } ## [1] &quot;Showing LinkedIn information&quot; # Write the if statement for num_views if (num_views &gt; 15) { print(&quot;You are popular!&quot;) } The else statement You can only use an else statement in combination with an if statement. The else statement does not require a condition; its corresponding code is simply run if all of the preceding conditions in the control structure are FALSE. It’s important that the else keyword comes on the same line as the closing bracket of the if part! syntax: if (condition) { expr1 } else { expr2 } # Control structure for num_views if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) } else { print(&quot;Try to be more visible!&quot;) } ## [1] &quot;Try to be more visible!&quot; The else if statement The else if statement allows you to further customize your control structure. You can add as many else if statements as you like. Keep in mind that R ignores the remainder of the control structure once a condition has been found that is TRUE and the corresponding expressions have been executed. Again, It’s important that the else if keywords comes on the same line as the closing bracket of the previous part of the control construct! if (condition1) { expr1 } else if (condition2) { expr2 } else if (condition3) { expr3 } else { expr4 } “Your number of views is average” is printed if num_views is between 15 (inclusive) and 10 (exclusive). # Control structure for num_views if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) } else if (num_views &lt;= 15 &amp; num_views &gt; 10) { print(&quot;Your number of views is average&quot;) } else { print(&quot;Try to be more visible!&quot;) } ## [1] &quot;Your number of views is average&quot; You can even put in another set of conditional statements. Check the outputs of num variable if it was 6, 4, 100 or 2500. for (num in c(6, 4, 100, 2500)){ number &lt;- num if (number &lt; 10){ if (number &lt; 5){ result &lt;- &quot;extra small&quot; } else { result &lt;- &quot;small&quot; } } else if (number &lt; 100) { result &lt;- &quot;medium&quot; } else { result &lt;- &quot;large&quot; } print(paste(number, &quot;is&quot;, result)) } ## [1] &quot;6 is small&quot; ## [1] &quot;4 is extra small&quot; ## [1] &quot;100 is large&quot; ## [1] &quot;2500 is large&quot; Bring together. # Variables related to your last day of recordings li &lt;- 15 fb &lt;- 9 # Code the control-flow construct if (li &gt;= 15 &amp; fb &gt;= 15) { sms &lt;- 2 * (li + fb) } else if (li &lt; 10 &amp; fb &lt; 10) { sms &lt;- 0.5 * (li + fb) } else { sms &lt;- li + fb } # Print the resulting sms to the console sms ## [1] 24 2.2 Loops 2.2.1 While loop Remember that the condition part of this recipe should become FALSE at some point during the execution. Otherwise, the while loop will go on indefinitely. If your session expires when you run your code, check the body of your while loop carefully. Syntax: while (condition) { expr } # Initialize the speed variable speed &lt;- 64 # Code the while loop while (speed &gt; 30) { print(paste(&quot;Slow down!&quot;, speed)) speed &lt;- speed - 7 } ## [1] &quot;Slow down! 64&quot; ## [1] &quot;Slow down! 57&quot; ## [1] &quot;Slow down! 50&quot; ## [1] &quot;Slow down! 43&quot; ## [1] &quot;Slow down! 36&quot; # Print out the speed variable speed ## [1] 29 Throw in more conditionals. # Initialize the speed variable speed &lt;- 64 # Extend/adapt the while loop while (speed &gt; 30) { print(paste(&quot;Your speed is&quot;,speed)) if (speed &gt; 48 ) { print(&quot;Slow down big time! Minus 11&quot;) speed &lt;- speed - 11 } else { print(&quot;Slow down! Minus 6&quot;) speed &lt;- speed - 6 } } ## [1] &quot;Your speed is 64&quot; ## [1] &quot;Slow down big time! Minus 11&quot; ## [1] &quot;Your speed is 53&quot; ## [1] &quot;Slow down big time! Minus 11&quot; ## [1] &quot;Your speed is 42&quot; ## [1] &quot;Slow down! Minus 6&quot; ## [1] &quot;Your speed is 36&quot; ## [1] &quot;Slow down! Minus 6&quot; Break the while loop Remember that the break statement is a control statement. When R encounters it, the while loop is abandoned completely. Adapt the while loop such that it is abandoned when the speed of the vehicle is greater than 80. # Initialize the speed variable speed &lt;- 88 while (speed &gt; 30) { print(paste(&quot;Your speed is&quot;, speed)) # Break the while loop when speed exceeds 80 if (speed &gt; 80 ) { break } if (speed &gt; 48) { print(&quot;Slow down big time!&quot;) speed &lt;- speed - 11 } else { print(&quot;Slow down!&quot;) speed &lt;- speed - 6 } } ## [1] &quot;Your speed is 88&quot; Build a while loop from scratch. Prints out the triple of i, so 3 * i, at each run. Is abandoned with a break if the triple of i is divisible by 8, but still prints out this triple before breaking. # Initialize i as 1 i &lt;- 1 # Code the while loop while (i &lt;= 10) { print(3 * i) if (3 * i %% 8 == 0) { break } i &lt;- i + 1 } ## [1] 3 ## [1] 6 ## [1] 9 ## [1] 12 ## [1] 15 ## [1] 18 ## [1] 21 ## [1] 24 2.2.2 For loop Syntax: for (var in seq) { expr } Loop over a vector Loop version 1: Concise, easy to read, but no access to looping index. # The linkedin vector has already been defined for you linkedin ## [1] 16 9 13 5 2 17 14 # Loop version 1 for (val in linkedin) { print(val) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 Loop version 2: Harder to read and write, but more versatile. # Loop version 2 for (index2 in 1:length(linkedin)) { print(linkedin[index2]) } ## [1] 16 ## [1] 9 ## [1] 13 ## [1] 5 ## [1] 2 ## [1] 17 ## [1] 14 Loop over a list Looping over a list is just as easy and convenient as looping over a vector. Notice that you need double square brackets - [[ ]] - to select the list elements in loop version 2. primes_list &lt;- list(2, 3, 5, 7, 11, 13) # loop version 2 for (i in 1:length(primes_list)) { print(primes_list[[i]]) } # The nyc list is already specified nyc &lt;- list(pop = 8405837, boroughs = c(&quot;Manhattan&quot;, &quot;Bronx&quot;, &quot;Brooklyn&quot;, &quot;Queens&quot;, &quot;Staten Island&quot;), capital = FALSE) # Loop version 1 for (nyc_val in nyc) { print(nyc_val) } ## [1] 8405837 ## [1] &quot;Manhattan&quot; &quot;Bronx&quot; &quot;Brooklyn&quot; &quot;Queens&quot; ## [5] &quot;Staten Island&quot; ## [1] FALSE # Loop version 2 for (nyc_ind in 1:length(nyc)) { print(nyc[[nyc_ind]]) } ## [1] 8405837 ## [1] &quot;Manhattan&quot; &quot;Bronx&quot; &quot;Brooklyn&quot; &quot;Queens&quot; ## [5] &quot;Staten Island&quot; ## [1] FALSE Loop over a matrix: nested loop There’s a matrix ttt, that represents the status of a tic-tac-toe game. It contains the values “X”, “O” and “NA”. On row 1 and column 1, there’s “O”, while on row 3 and column 2 there’s “NA”. # The tic-tac-toe matrix ttt &lt;- matrix(c(&quot;O&quot;, NA, &quot;X&quot;, NA, &quot;O&quot;, &quot;O&quot;, &quot;X&quot;, NA, &quot;X&quot;), byrow = T, nrow = 3); ttt ## [,1] [,2] [,3] ## [1,] &quot;O&quot; NA &quot;X&quot; ## [2,] NA &quot;O&quot; &quot;O&quot; ## [3,] &quot;X&quot; NA &quot;X&quot; To solve this exercise, you’ll need a for loop inside a for loop, often called a nested loop. for (var1 in seq1) { for (var2 in seq2) { expr } } # define the double for loop for (r in 1:nrow(ttt)) { for (c in 1:ncol(ttt)) { print(paste(&quot;On row&quot;, r, &quot;and column&quot;, c, &quot;the board contains&quot;, ttt[r, c])) } } ## [1] &quot;On row 1 and column 1 the board contains O&quot; ## [1] &quot;On row 1 and column 2 the board contains NA&quot; ## [1] &quot;On row 1 and column 3 the board contains X&quot; ## [1] &quot;On row 2 and column 1 the board contains NA&quot; ## [1] &quot;On row 2 and column 2 the board contains O&quot; ## [1] &quot;On row 2 and column 3 the board contains O&quot; ## [1] &quot;On row 3 and column 1 the board contains X&quot; ## [1] &quot;On row 3 and column 2 the board contains NA&quot; ## [1] &quot;On row 3 and column 3 the board contains X&quot; Mix it up with control flow if else: linkedin views exceed 10 or not. # Code the for loop with conditionals for (li in linkedin) { if (li &gt; 10) { print(&quot;You&#39;re popular!&quot;) } else { print(&quot;Be more visible!&quot;) } print(li) } ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 ## [1] &quot;Be more visible!&quot; ## [1] 9 ## [1] &quot;You&#39;re popular!&quot; ## [1] 13 ## [1] &quot;Be more visible!&quot; ## [1] 5 ## [1] &quot;Be more visible!&quot; ## [1] 2 ## [1] &quot;You&#39;re popular!&quot; ## [1] 17 ## [1] &quot;You&#39;re popular!&quot; ## [1] 14 break: The break statement abandons the active loop: the remaining code in the loop is skipped and the loop is not iterated over anymore. next: The next statement skips the remainder of the code in the loop, but continues the iteration. If the vector element’s value exceeds 16, print out “This is ridiculous, I’m outta here!” and have R abandon the for loop (break). If the value is lower than 5, print out “This is too embarrassing!” and fast-forward to the next iteration (next). # Adapt/extend the for loop for (li in linkedin) { if (li &gt; 10) { print(&quot;You&#39;re popular!&quot;) } else { print(&quot;Be more visible!&quot;) } # Add if statement with break if (li &gt; 16) { print(&quot;This is ridiculous, I&#39;m outta here!&quot;) break } # Add if statement with next if (li &lt; 5) { print(&quot;This is too embarrassing!&quot;) next } print(li) } ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 ## [1] &quot;Be more visible!&quot; ## [1] 9 ## [1] &quot;You&#39;re popular!&quot; ## [1] 13 ## [1] &quot;Be more visible!&quot; ## [1] 5 ## [1] &quot;Be more visible!&quot; ## [1] &quot;This is too embarrassing!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;This is ridiculous, I&#39;m outta here!&quot; 2.2.3 Bring it all together The rquote variable has been split up into a vector that contains separate letters and has been stored in a vector chars with the strsplit() function. Write code that counts the number of r’s that come before the first u in rquote. # Pre-defined variables rquote &lt;- &quot;r&#39;s internals are irrefutably intriguing&quot; chars &lt;- strsplit(rquote, split = &quot;&quot;)[[1]]; chars ## [1] &quot;r&quot; &quot;&#39;&quot; &quot;s&quot; &quot; &quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;r&quot; &quot;n&quot; &quot;a&quot; &quot;l&quot; &quot;s&quot; &quot; &quot; &quot;a&quot; &quot;r&quot; &quot;e&quot; &quot; &quot; &quot;i&quot; ## [20] &quot;r&quot; &quot;r&quot; &quot;e&quot; &quot;f&quot; &quot;u&quot; &quot;t&quot; &quot;a&quot; &quot;b&quot; &quot;l&quot; &quot;y&quot; &quot; &quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;r&quot; &quot;i&quot; &quot;g&quot; &quot;u&quot; &quot;i&quot; ## [39] &quot;n&quot; &quot;g&quot; # Initialize rcount rcount &lt;- 0 # Finish the for loop for (char in chars) { if (char == &quot;r&quot;) { rcount &lt;- rcount + 1 } else if (char == &quot;u&quot;) { break } else { next } } # Print out rcount rcount ## [1] 5 2.3 Function 2.3.1 Introduction to function Function documentation All the relevant details such as a description, usage, and arguments can be found in the documentation. To consult the documentation on the function, for example, you can use one of following R commands: help(sample) ?sample A quick hack to see the arguments of the function is the args() function. args(sample) ## function (x, size, replace = FALSE, prob = NULL) ## NULL Use a function The ‘Default S3 method’ of mean() function is: mean(x, trim = 0, na.rm = FALSE, ...) x is required; if you do not specify it, R will throw an error. trim and na.rm are optional arguments: they have a default value which is used if the arguments are not explicitly specified. The ... is called the ellipsis. It is a way for R to pass arguments along without the function having to name them explicitly. # Calculate the mean of the sum avg_sum &lt;- mean(linkedin + facebook); avg_sum ## [1] 22.28571 # Calculate the trimmed mean of the sum avg_sum_trimmed &lt;- mean(linkedin + facebook, trim = 0.2); avg_sum_trimmed ## [1] 22.6 Let’s see what happens if your vectors linkedin and facebook contain missing values (NA). # The linkedin and facebook vectors have already been created for you linkedin &lt;- c(16, 9, 13, 5, NA, 17, 14) facebook &lt;- c(17, NA, 5, 16, 8, 13, 14) # Basic average of linkedin mean(linkedin) ## [1] NA # Advanced average of linkedin: remove missing values mean(linkedin, na.rm = TRUE) ## [1] 12.33333 Functions inside functions # Calculate the mean absolute deviation mean(abs(linkedin - facebook), na.rm = TRUE) ## [1] 4.8 2.3.2 Writing functions Syntax: my_fun &lt;- function(arg1, arg2) { body } # Create a function pow_two(): it takes one argument and returns that number squared (that number times itself). pow_two &lt;- function(a) { a ^ 2 } # Use the function pow_two(12) ## [1] 144 # Next, create a function sum_abs(), that takes two arguments and returns the sum of the absolute values of both arguments. sum_abs &lt;- function(b, c) { abs(b) + abs(c) } # Use the function sum_abs(-2, 3) ## [1] 5 Function without input There are situations in which your function does not require an input. For example: throw_die &lt;- function() { number &lt;- sample(1:6, size = 1) number } throw_die() ## [1] 1 # Define the function hello() hello &lt;- function() { print(&quot;Hi there!&quot;) return(TRUE) } hello() ## [1] &quot;Hi there!&quot; ## [1] TRUE Function with default argument You can define default argument values in your own R functions as well. Syntax: my_fun &lt;- function(arg1, arg2 = val2) { body } # Finish the pow_two() function pow_two &lt;- function(x, print_info = T) { y &lt;- x ^ 2 if (print_info == T) { print(paste(x, &quot;to the power two equals&quot;, y)) } return(y) } pow_two(2) ## [1] &quot;2 to the power two equals 4&quot; ## [1] 4 pow_two(3, print_info = F) ## [1] 9 R passes arguments by value If R were to pass a to triple() by reference, the override of the x inside the function would ripple through to the variable a, outside the function. However, R passes by value, so the R objects you pass to a function can never change unless you do an explicit assignment. a remains equal to 5, even after calling triple(a). triple &lt;- function(x) { x &lt;- 3*x x } a &lt;- 5 triple(a) ## [1] 15 a ## [1] 5 Function with control flow It’s perfectly possible to add control-flow constructs, loops and even other functions to your function body. linkedin &lt;- c(16, 9, 13, 5, 2, 17, 14) facebook &lt;- c(17, 7, 5, 16, 8, 13, 14) # Define the interpret function interpret &lt;- function(num_views) { if (num_views &gt; 15) { print(&quot;You&#39;re popular!&quot;) return(num_views) } else { print(&quot;Try to be more visible!&quot;) return(0) } } # Call the interpret function twice interpret(linkedin[1]) ## [1] &quot;You&#39;re popular!&quot; ## [1] 16 interpret(facebook[2]) ## [1] &quot;Try to be more visible!&quot; ## [1] 0 Function in function. # Define the interpret_all() function # views: vector with data to interpret # return_sum: return total number of views on popular days? interpret_all &lt;- function(views, return_sum = T) { count &lt;- 0 for (v in views) { count &lt;- count + interpret(v) } if (return_sum == T) { return(count) } else { return(NULL) } } # Call the interpret_all() function on both linkedin and facebook interpret_all(linkedin) ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] 33 interpret_all(facebook, return_sum = F) ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;You&#39;re popular!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## [1] &quot;Try to be more visible!&quot; ## NULL 2.3.3 Packages Install packages: install.packages() Load packages: library() , require() Load package = attach package to search list The library() and require() functions are not very picky when it comes down to argument types. Both syntax below work perfectly fine for loading a package. # Chunk 1 library(data.table) require(rjson) # Chunk 2 library(&quot;data.table&quot;) require(rjson) # Check out the currently attached packages search() ## [1] &quot;.GlobalEnv&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [4] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [7] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; # Load the ggplot2 package library(ggplot2) # Retry the qplot() function qplot(mtcars$wt, mtcars$hp) ## Warning: `qplot()` was deprecated in ggplot2 3.4.0. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` ## to see where this warning was generated. # Check out the currently attached packages again search() ## [1] &quot;.GlobalEnv&quot; &quot;package:ggplot2&quot; &quot;package:stats&quot; ## [4] &quot;package:graphics&quot; &quot;package:grDevices&quot; &quot;package:utils&quot; ## [7] &quot;package:datasets&quot; &quot;package:methods&quot; &quot;Autoloads&quot; ## [10] &quot;package:base&quot; 2.4 The apply family 2.4.1 lapply To put it generally, lapply takes a vector or list X, and applies the function FUN to each of its members. If FUN requires additional arguments, you pass them after you’ve specified X and FUN (...). The output of lapply() is a list, the same length as X, where each element is the result of applying FUN on the corresponding element of X. lapply(X, FUN, ...) Compare with for loop cities &lt;- c(&quot;New York&quot;, &quot;Paris&quot;, &quot;London&quot;, &quot;Tokyo&quot;, &quot;Rio de Janeiro&quot;, &quot;Cape Town&quot;) # for loop num_chars &lt;- c() for(i in 1:length(cities)) { num_chars[i] &lt;- nchar(cities[i]) } num_chars ## [1] 8 5 6 5 14 9 # lapply unlist(lapply(cities, nchar)) ## [1] 8 5 6 5 14 9 With a built-in R function # The vector pioneers has already been created for you pioneers &lt;- c(&quot;GAUSS:1777&quot;, &quot;BAYES:1702&quot;, &quot;PASCAL:1623&quot;, &quot;PEARSON:1857&quot;) # Split names from birth year split_math &lt;- strsplit(pioneers, split = &quot;:&quot;); split_math ## [[1]] ## [1] &quot;GAUSS&quot; &quot;1777&quot; ## ## [[2]] ## [1] &quot;BAYES&quot; &quot;1702&quot; ## ## [[3]] ## [1] &quot;PASCAL&quot; &quot;1623&quot; ## ## [[4]] ## [1] &quot;PEARSON&quot; &quot;1857&quot; # Convert to lowercase strings: split_low split_low &lt;- lapply(split_math, tolower) # Take a look at the structure of split_low str(split_low) ## List of 4 ## $ : chr [1:2] &quot;gauss&quot; &quot;1777&quot; ## $ : chr [1:2] &quot;bayes&quot; &quot;1702&quot; ## $ : chr [1:2] &quot;pascal&quot; &quot;1623&quot; ## $ : chr [1:2] &quot;pearson&quot; &quot;1857&quot; With your own function # Write function select_first() select_first &lt;- function(x) { x[1] } # Apply select_first() over split_low: names names &lt;- lapply(split_low, select_first) print(unlist(names)) ## [1] &quot;gauss&quot; &quot;bayes&quot; &quot;pascal&quot; &quot;pearson&quot; # Write function select_second() select_second &lt;- function(x) { x[2] } # Apply select_second() over split_low: years years &lt;- lapply(split_low, select_second) print(unlist(years)) ## [1] &quot;1777&quot; &quot;1702&quot; &quot;1623&quot; &quot;1857&quot; With anonymous functions To not give the function a name. This is called an anonymous function. # Named function triple &lt;- function(x) { 3 * x } # Anonymous function with same implementation function(x) { 3 * x } # Transform: use anonymous function inside lapply names_an &lt;- unlist(lapply(split_low, function(x) {x[1]})); names_an ## [1] &quot;gauss&quot; &quot;bayes&quot; &quot;pascal&quot; &quot;pearson&quot; # Transform: use anonymous function inside lapply years_an &lt;- unlist(lapply(split_low, function(x) {x[2]})); years_an ## [1] &quot;1777&quot; &quot;1702&quot; &quot;1623&quot; &quot;1857&quot; With additional arguments In select_el(), it takes a vector as its first argument, and an index as its second argument. It returns the vector’s element at the specified index. # Generic select function select_el &lt;- function(x, index) { x[index] } # Use lapply() twice on split_low: names and years names_ad &lt;- unlist(lapply(split_low, select_el, index = 1)); names_ad ## [1] &quot;gauss&quot; &quot;bayes&quot; &quot;pascal&quot; &quot;pearson&quot; years_ad &lt;- unlist(lapply(split_low, select_el, index = 2)); years_ad ## [1] &quot;1777&quot; &quot;1702&quot; &quot;1623&quot; &quot;1857&quot; With functions that return NULL lapply(split_low, function(x) { if (nchar(x[1]) &gt; 5) { return(NULL) } else { return(x[2]) } }) ## [[1]] ## [1] &quot;1777&quot; ## ## [[2]] ## [1] &quot;1702&quot; ## ## [[3]] ## NULL ## ## [[4]] ## NULL 2.4.2 sapply The first argument of sapply() is the list or vector X over which you want to apply a function, FUN. And it will return a vector (lapply return list). Potential additional arguments to this function are specified afterwards (...): sapply(X, FUN, ...) In the next couple of exercises, you’ll be working with the variable temp, that contains temperature measurements for 7 days. temp is a list of length 7, where each element is a vector of length 5, representing 5 measurements on a given day. temp &lt;- list(c(3, 7, 9, 6, -1), c(6, 9, 12, 13, 5), c(4, 8, 3, -1, -3), c(1, 4, 7, 2, -2), c(5, 7, 9, 4, 2), c(-3, 5, 8, 9, 4), c(3, 6, 9, 4, 1)) str(temp) ## List of 7 ## $ : num [1:5] 3 7 9 6 -1 ## $ : num [1:5] 6 9 12 13 5 ## $ : num [1:5] 4 8 3 -1 -3 ## $ : num [1:5] 1 4 7 2 -2 ## $ : num [1:5] 5 7 9 4 2 ## $ : num [1:5] -3 5 8 9 4 ## $ : num [1:5] 3 6 9 4 1 Compare with lapply The former returns a list, while the latter returns a vector that is a simplified version of this list. # Use lapply() to find each day&#39;s minimum temperature lapply(temp, min) ## [[1]] ## [1] -1 ## ## [[2]] ## [1] 5 ## ## [[3]] ## [1] -3 ## ## [[4]] ## [1] -2 ## ## [[5]] ## [1] 2 ## ## [[6]] ## [1] -3 ## ## [[7]] ## [1] 1 # Use sapply() to find each day&#39;s minimum temperature sapply(temp, min) ## [1] -1 5 -3 -2 2 -3 1 With your own function # Finish function definition of extremes_avg extremes_avg &lt;- function(x) { ( min(x) + max(x)) / 2 } # Apply extremes_avg() over temp using sapply() sapply(temp, extremes_avg) ## [1] 4.0 9.0 2.5 2.5 5.5 3.0 5.0 # Apply extremes_avg() over temp using lapply() lapply(temp, extremes_avg) ## [[1]] ## [1] 4 ## ## [[2]] ## [1] 9 ## ## [[3]] ## [1] 2.5 ## ## [[4]] ## [1] 2.5 ## ## [[5]] ## [1] 5.5 ## ## [[6]] ## [1] 3 ## ## [[7]] ## [1] 5 With function returning vector What if the function you’re applying over a list or a vector returns a vector of length greater than 1? # Create a function that returns min and max of a vector: extremes extremes &lt;- function(x) { c(min = min(x), max = max(x)) } # Apply extremes() over temp with sapply() sapply(temp, extremes) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1 5 -3 -2 2 -3 1 ## max 9 13 8 7 9 9 9 # Apply extremes() over temp with lapply() lapply(temp, extremes) ## [[1]] ## min max ## -1 9 ## ## [[2]] ## min max ## 5 13 ## ## [[3]] ## min max ## -3 8 ## ## [[4]] ## min max ## -2 7 ## ## [[5]] ## min max ## 2 9 ## ## [[6]] ## min max ## -3 9 ## ## [[7]] ## min max ## 1 9 Do not output vector names, argument: USE.NAMES cities ## [1] &quot;New York&quot; &quot;Paris&quot; &quot;London&quot; &quot;Tokyo&quot; ## [5] &quot;Rio de Janeiro&quot; &quot;Cape Town&quot; unlist(lapply(cities,nchar)) ## [1] 8 5 6 5 14 9 sapply(cities, nchar, USE.NAMES = FALSE) ## [1] 8 5 6 5 14 9 Can’t simplify # Definition of below_zero() below_zero &lt;- function(x) { return(x[x &lt; 0]) } # Apply below_zero over temp using sapply(): freezing_s freezing_s &lt;- sapply(temp, below_zero); freezing_s ## [[1]] ## [1] -1 ## ## [[2]] ## numeric(0) ## ## [[3]] ## [1] -1 -3 ## ## [[4]] ## [1] -2 ## ## [[5]] ## numeric(0) ## ## [[6]] ## [1] -3 ## ## [[7]] ## numeric(0) # Apply below_zero over temp using lapply(): freezing_l freezing_l &lt;- lapply(temp, below_zero) # Are freezing_s and freezing_l identical? base::identical(freezing_l, freezing_s) ## [1] TRUE With functions that return NULL sapply() does not simplify the list of NULL‘s. That’s because the ’vector-version’ of a list of NULL’s would simply be a NULL, which is no longer a vector with the same length as the input. print_info &lt;- function(x) { cat(&quot;The average temperature is&quot;, mean(x), &quot;\\n&quot;) } # Apply print_info() over temp using sapply() sapply(temp, print_info) ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL # Apply print_info() over temp using lapply() lapply(temp, print_info) ## The average temperature is 4.8 ## The average temperature is 9 ## The average temperature is 2.2 ## The average temperature is 2.4 ## The average temperature is 5.4 ## The average temperature is 4.6 ## The average temperature is 4.6 ## [[1]] ## NULL ## ## [[2]] ## NULL ## ## [[3]] ## NULL ## ## [[4]] ## NULL ## ## [[5]] ## NULL ## ## [[6]] ## NULL ## ## [[7]] ## NULL With anonymous functions sapply(list(runif (10), runif (10)), function(x) c(min = min(x), mean = mean(x), max = max(x))) ## [,1] [,2] ## min 0.003497243 0.01888264 ## mean 0.491852633 0.55130720 ## max 0.936032265 0.86305679 2.4.3 vapply Over the elements inside X, the function FUN is applied. The FUN.VALUE argument expects a template for the return argument of this function FUN. USE.NAMES is TRUE by default; in this case vapply() tries to generate a named array, if possible. vapply(X, FUN, FUN.VALUE, ..., USE.NAMES = TRUE) Notice how, just as with sapply(), vapply() neatly transfers the names that you specify in the basics() function to the row names of the matrix that it returns. # Definition of basics() basics &lt;- function(x) { c(min = min(x), mean = mean(x), max = max(x)) } # Apply basics() over temp using vapply() vapply(temp, basics, numeric(3)) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## min -1.0 5 -3.0 -2.0 2.0 -3.0 1.0 ## mean 4.8 9 2.2 2.4 5.4 4.6 4.6 ## max 9.0 13 8.0 7.0 9.0 9.0 9.0 There are cases where the structure of the output of the function you want to apply, FUN, does not correspond to the template you specify in FUN.VALUE. In that case, vapply() will throw an error that informs you about the misalignment between expected and actual output. # Definition of the basics() function basics &lt;- function(x) { c(min = min(x), mean = mean(x), median = median(x), max = max(x)) } # An error should pop up. That&#39;s because vapply() still expects basics() to return a vector of length 3. # Error in vapply(temp, basics, numeric(3)) : values must be length 3, but FUN(X[[1]]) result is length 4 vapply(temp, basics, numeric(3)) vapply() can be considered a more robust version of sapply(), because you explicitly restrict the output of the function you want to apply. # Convert to vapply() expression vapply(temp, max, numeric(1)) ## [1] 9 13 8 7 9 9 9 # Convert to vapply() expression vapply(temp, function(x, y) { mean(x) &gt; y }, y = 5, logical(1)) ## [1] FALSE TRUE FALSE FALSE TRUE FALSE FALSE 2.4.4 lapply vs sapply vs vapply lapply sapply vapply apply function over list or vector apply function over list or vector apply function over list or vector output = list try to simplify list to array explicitly specify output format 2.5 Utilities 2.5.1 Mathematical utilities abs(): Calculate the absolute value. (絕對值) sum(): Calculate the sum of all the values in a data structure. mean(): Calculate the arithmetic mean. round(): Round the values to 0 decimal places by default.Can change the number of digits to round to. (小數點後四捨五入) # The errors vector has already been defined for you errors &lt;- c(1.9, -2.6, 4.0, -9.5, -3.4, 7.3) # Sum of absolute rounded values of errors sum(abs(round(errors))) ## [1] 29 2.5.2 Data structures utilities seq(): Generate sequences, by specifying the from, to, and by arguments. rep(): Replicate elements of vectors and lists. sort(): Sort a vector in ascending order. Works on numerics, but also on character strings and logicals. rev(): Reverse the elements in a data structures for which reversal is defined. str(): Display the structure of any R object. append(): Merge vectors or lists. is.*(): Check for the class of an R object. as.*(): Convert an R object from one class to another. unlist(): Flatten (possibly embedded) lists to produce a vector. # The linkedin and facebook lists have already been created for you linkedin &lt;- list(16, 9, 13, 5, 2, 17, 14) facebook &lt;- list(17, 7, 5, 16, 8, 13, 14) # Convert linkedin and facebook to a vector: li_vec and fb_vec li_vec &lt;- unlist(linkedin); li_vec ## [1] 16 9 13 5 2 17 14 fb_vec &lt;- unlist(facebook); fb_vec ## [1] 17 7 5 16 8 13 14 is.vector(li_vec) ## [1] TRUE # Append fb_vec to li_vec: social_vec social_vec &lt;- append(li_vec, fb_vec) # Sort social_vec sort(social_vec, decreasing = T) ## [1] 17 17 16 16 14 14 13 13 9 8 7 5 5 2 # Fix me rep(seq(1, 7, by = 2), times = 3) ## [1] 1 3 5 7 1 3 5 7 1 3 5 7 rep(seq(1, 7, by = 2), each = 3) ## [1] 1 1 1 3 3 3 5 5 5 7 7 7 # Create first sequence: seq1 seq1 &lt;- seq(1, 500, by = 3) # Create second sequence: seq2 seq2 &lt;- seq(1200, 900, by = -7) # Calculate total sum of the sequences sum(seq1, seq2) ## [1] 87029 2.5.3 Regular Expressions Regular expressions can be used to see whether a pattern exists inside a character string or a vector of character strings. Sequence of (meta)characters Pattern existence Pattern replacement Pattern extraction grepl &amp; grep grepl(), which returns TRUE when a pattern is found in the corresponding character string. grepl(pattern = &lt;regex&gt;, x = &lt;string&gt;) grep(), which returns a vector of indices of the character strings that contains the pattern.(can use which()) grep(pattern = &lt;regex&gt;, x = &lt;string&gt;) Both functions need a pattern and an x argument, where pattern is the regular expression you want to match for, and the x argument is the character vector from which matches should be sought. # The emails vector has already been defined for you emails &lt;- c(&quot;john.doe@ivyleague.edu&quot;, &quot;education@world.gov&quot;, &quot;dalai.lama@peace.org&quot;,&quot;invalid.edu&quot;, &quot;quant@bigdatacollege.edu&quot;, &quot;cookie.monster@sesame.tv&quot;) # Use grepl() to match for &quot;edu&quot; grepl(pattern = &quot;edu&quot;, x = emails) ## [1] TRUE TRUE FALSE TRUE TRUE FALSE # Use grep() or which() to match for &quot;edu&quot;, save result to hits hits &lt;- grep(pattern = &quot;edu&quot;, x = emails); hits ## [1] 1 2 4 5 hits_w &lt;- which(grepl(pattern = &quot;edu&quot;, x = emails)); hits_w ## [1] 1 2 4 5 # Subset emails using hits emails[hits] ## [1] &quot;john.doe@ivyleague.edu&quot; &quot;education@world.gov&quot; ## [3] &quot;invalid.edu&quot; &quot;quant@bigdatacollege.edu&quot; regex Chinese document English document # Use grepl() to match for .edu addresses more robustly grepl(pattern = &quot;.+@{1}.*\\\\.edu$&quot;, x = emails) ## [1] TRUE FALSE FALSE FALSE TRUE FALSE # Use grep() to match for .edu addresses more robustly, save result to hits hits &lt;- grep(pattern = &quot;.+@{1}.*\\\\.edu$&quot;, x = emails) # Subset emails using hits emails[hits] ## [1] &quot;john.doe@ivyleague.edu&quot; &quot;quant@bigdatacollege.edu&quot; sub &amp; gsub sub() and gsub() take it one step further: you can specify a replacement argument.If inside the character vector x, the regular expression patternis found, the matching element(s) will be replaced with replacement. sub():only replaces the first match. sub(pattern = &lt;regex&gt;, replacement = &lt;str&gt;, x = &lt;str&gt;) gsub():replaces all matches gsub(pattern = &lt;regex&gt;, replacement = &lt;str&gt;, x = &lt;str&gt;) # Use sub() to convert the email domains to datacamp.edu sub(pattern = &quot;@.*\\\\.edu$&quot;, replacement = &quot;@datacamp.edu&quot;, x = emails) ## [1] &quot;john.doe@datacamp.edu&quot; &quot;education@world.gov&quot; ## [3] &quot;dalai.lama@peace.org&quot; &quot;invalid.edu&quot; ## [5] &quot;quant@datacamp.edu&quot; &quot;cookie.monster@sesame.tv&quot; .*: any character that is matched zero or more times. \\\\s: Match a space. Escaping it (\\\\) makes it a metacharacter. [0-9]+: Match the numbers 0 to 9, at least once (+). ([0-9]+): The parentheses are used to make parts of the matching string available to define the replacement. The \\\\1 in the replacement argument of sub() gets set to the string that is captured by the regular expression [0-9]+. awards &lt;- c(&quot;Won 1 Oscar.&quot;, &quot;Won 1 Oscar. Another 9 wins &amp; 24 nominations.&quot;, &quot;1 win and 2 nominations.&quot;, &quot;2 wins &amp; 3 nominations.&quot;, &quot;Nominated for 2 Golden Globes. 1 more win &amp; 2 nominations.&quot;, &quot;4 wins &amp; 1 nomination.&quot;) sub(&quot;.*\\\\s([0-9]+)\\\\snomination.*$&quot;, &quot;\\\\1&quot;, awards) ## [1] &quot;Won 1 Oscar.&quot; &quot;24&quot; &quot;2&quot; &quot;3&quot; &quot;2&quot; ## [6] &quot;1&quot; The ([0-9]+) selects the entire number that comes before the word “nomination” in the string, and the entire match gets replaced by this number because of the \\\\1 that reference to the content inside the parentheses. Another example &quot;A. J. Burnett&quot; ## [1] &quot;A. J. Burnett&quot; gsub(&quot;([A-Z])\\\\.\\\\s([A-Z])\\\\.&quot;, &quot;\\\\1.\\\\2.&quot;, &quot;A. J. Burnett&quot;) ## [1] &quot;A.J. Burnett&quot; 2.5.4 Dates &amp; Times Today Sys.Date() Sys.time() Date: dates, under hood, Date objects store the number of days since the 1970-01-01. POSIXct: times, under hood, POSIXct objects store the number of seconds since the 1970-01-01. # Get the current date: today today &lt;- Sys.Date(); today ## [1] &quot;2024-01-16&quot; # See what today looks like under the hood paste(unclass(today), &quot;days&quot;) ## [1] &quot;19738 days&quot; # Get the current time: now now &lt;- Sys.time(); now ## [1] &quot;2024-01-16 13:33:14 CST&quot; # See what now looks like under the hood paste(unclass(now), &quot;seconds&quot;) ## [1] &quot;1705383194.09113 seconds&quot; Format of dates To create a Date object from a simple character string in R, you can use the as.Date() function. Ex: 13 January, 1982. Symbol: %Y: 4-digit year (1982) %y: 2-digit year (82) %m: 2-digit month (01) %d: 2-digit day of the month (13) %A: weekday (Wednesday) %a: abbreviated weekday (Wed) %B: month (January) %b: abbreviated month (Jan) R default matches your character string to the formats \"%Y-%m-%d\" or \"%Y/%m/%d\". You can also use format() to convert dates to character strings. # Change to UTC timezone Sys.timezone() ## [1] &quot;Asia/Taipei&quot; Sys.setlocale(&quot;LC_TIME&quot;, &quot;C&quot;) ## [1] &quot;C&quot; # Definition of character strings representing dates str1 &lt;- &quot;May 23, &#39;96&quot; str2 &lt;- &quot;2012-03-15&quot; str3 &lt;- &quot;30/January/2006&quot; # Convert the strings to dates: date1, date2, date3 date1 &lt;- as.Date(str1, format = &quot;%b %d, &#39;%y&quot;) date2 &lt;- as.Date(str2) date3 &lt;- as.Date(str3, &quot;%d/%B/%Y&quot;) # Convert dates to formatted strings format(date1, &quot;This is %A.&quot;) ## [1] &quot;This is Thursday.&quot; format(date2, &quot;%d&quot;) ## [1] &quot;15&quot; format(date3, &quot;%b %Y&quot;) ## [1] &quot;Jan 2006&quot; Create and format times You can use as.POSIXct() to convert from a character string to a POSIXct object, and format() to convert from a POSIXct object to a character string. Again, you have a wide variety of symbols: %H: hours as a decimal number (00-23) %I: hours as a decimal number (01-12) %M: minutes as a decimal number %S: seconds as a decimal number %T: shorthand notation for the typical format %H:%M:%S %p: AM/PM indicator For a full list of conversion symbols, consult the strptime.Default format of as.POSIXct() is %Y-%m-%d %H:%M:%S. # Definition of character strings representing times str1 &lt;- &quot;May 23, &#39;96 hours:23 minutes:01 seconds:45&quot; str2 &lt;- &quot;2012-3-12 14:23:08&quot; # Convert the strings to POSIXct objects: time1, time2 time1 &lt;- as.POSIXct(str1, format = &quot;%B %d, &#39;%y hours:%H minutes:%M seconds:%S&quot;) time2 &lt;- as.POSIXct(str2, format = &quot;%Y-%m-%d %T&quot;) # Convert times to formatted strings format(time1, &quot;%M&quot;) ## [1] &quot;01&quot; format(time2, &quot;%I:%M %p&quot;) ## [1] &quot;02:23 PM&quot; Calculations with Dates Both Date and POSIXct R objects are represented by simple numerical values under the hood. This makes calculation with time and date objects very straightforward: R performs the calculations using the underlying numerical values, and then converts the result back to human-readable time information again. You can increment and decrement Date objects, or do actual calculations. # Create date day1 &lt;- as.Date(&quot;2023-09-15&quot;) day2 &lt;- as.Date(&quot;2023-09-17&quot;) day3 &lt;- as.Date(&quot;2023-09-22&quot;) day4 &lt;- as.Date(&quot;2023-09-28&quot;) day5 &lt;- as.Date(&quot;2023-10-03&quot;) # Difference between last and first pizza day day5 - day1 ## Time difference of 18 days # Create vector pizza pizza &lt;- c(day1, day2, day3, day4, day5) pizza ## [1] &quot;2023-09-15&quot; &quot;2023-09-17&quot; &quot;2023-09-22&quot; &quot;2023-09-28&quot; &quot;2023-10-03&quot; # Create differences between consecutive pizza days: day_diff day_diff &lt;- diff(pizza); day_diff ## Time differences in days ## [1] 2 5 6 5 # Average period between two consecutive pizza days mean(day_diff) ## Time difference of 4.5 days Calculations with Times Calculations using POSIXct objects are completely analogous to those using Date objects. # Create time login &lt;- c(as.POSIXct(&quot;2023-09-19 10:18:04&quot;), as.POSIXct(&quot;2023-09-24 09:14:18&quot;), as.POSIXct(&quot;2023-09-24 12:21:51&quot;), as.POSIXct(&quot;2023-09-24 12:37:24&quot;), as.POSIXct(&quot;2023-09-26 21:37:55&quot;)); login ## [1] &quot;2023-09-19 10:18:04 CST&quot; &quot;2023-09-24 09:14:18 CST&quot; ## [3] &quot;2023-09-24 12:21:51 CST&quot; &quot;2023-09-24 12:37:24 CST&quot; ## [5] &quot;2023-09-26 21:37:55 CST&quot; logout &lt;- c(as.POSIXct(&quot;2023-09-19 10:56:29&quot;), as.POSIXct(&quot;2023-09-24 09:14:52&quot;), as.POSIXct(&quot;2023-09-24 12:35:48&quot;), as.POSIXct(&quot;2023-09-24 13:17:22&quot;), as.POSIXct(&quot;2023-09-26 22:08:47&quot;)); logout ## [1] &quot;2023-09-19 10:56:29 CST&quot; &quot;2023-09-24 09:14:52 CST&quot; ## [3] &quot;2023-09-24 12:35:48 CST&quot; &quot;2023-09-24 13:17:22 CST&quot; ## [5] &quot;2023-09-26 22:08:47 CST&quot; # Calculate the difference between login and logout: time_online time_online &lt;- logout - login # Inspect the variable time_online time_online ## Time differences in secs ## [1] 2305 34 837 2398 1852 # Calculate the total time online sum(time_online) ## Time difference of 7426 secs # Calculate the average time online mean(time_online) ## Time difference of 1485.2 secs # Create vector astro &lt;- c(spring = &quot;20-Mar-2015&quot;, summer = &quot;25-Jun-2015&quot;, fall = &quot;23-Sep-2015&quot;, winter = &quot;22-Dec-2015&quot;); astro ## spring summer fall winter ## &quot;20-Mar-2015&quot; &quot;25-Jun-2015&quot; &quot;23-Sep-2015&quot; &quot;22-Dec-2015&quot; meteo &lt;- c(spring = &quot;March 1, 15&quot;, summer = &quot;June 1, 15&quot;, fall = &quot;September 1, 15&quot;, winter = &quot;December 1, 15&quot;); meteo ## spring summer fall winter ## &quot;March 1, 15&quot; &quot;June 1, 15&quot; &quot;September 1, 15&quot; &quot;December 1, 15&quot; # Convert astro to vector of Date objects: astro_dates astro_dates &lt;- as.Date(astro, format = &quot;%d-%b-%Y&quot;); astro_dates ## spring summer fall winter ## &quot;2015-03-20&quot; &quot;2015-06-25&quot; &quot;2015-09-23&quot; &quot;2015-12-22&quot; # Convert meteo to vector of Date objects: meteo_dates meteo_dates &lt;- as.Date(meteo, format = &quot;%B %d, %y&quot;); meteo_dates ## spring summer fall winter ## &quot;2015-03-01&quot; &quot;2015-06-01&quot; &quot;2015-09-01&quot; &quot;2015-12-01&quot; # Calculate the maximum absolute difference between astro_dates and meteo_dates max(abs(astro_dates - meteo_dates)) ## Time difference of 24 days "],["introduction-to-the-tidyverse.html", "Chapter 3 Introduction to the Tidyverse 3.1 Data wrangling 3.2 Data visualization 3.3 Grouping &amp; summarizing 3.4 Types of visualizations", " Chapter 3 Introduction to the Tidyverse 3.1 Data wrangling 3.1.1 Load dataset # Load the gapminder package library(gapminder) ## Warning: package &#39;gapminder&#39; was built under R version 4.3.1 # Load the dplyr package library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union # Look at the gapminder dataset gapminder ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. ## # ℹ 1,694 more rows 3.1.2 Filtering The filter verb extracts particular observations based on a condition. pipe (%&gt;%) # Filter the gapminder dataset for the year 1957 # version 1 gapminder %&gt;% filter(year == 1957) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1957 30.3 9240934 821. ## 2 Albania Europe 1957 59.3 1476505 1942. ## 3 Algeria Africa 1957 45.7 10270856 3014. ## 4 Angola Africa 1957 32.0 4561361 3828. ## 5 Argentina Americas 1957 64.4 19610538 6857. ## 6 Australia Oceania 1957 70.3 9712569 10950. ## 7 Austria Europe 1957 67.5 6965860 8843. ## 8 Bahrain Asia 1957 53.8 138655 11636. ## 9 Bangladesh Asia 1957 39.3 51365468 662. ## 10 Belgium Europe 1957 69.2 8989111 9715. ## # ℹ 132 more rows # version 2 gapminder[gapminder$year == 1957, ] ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1957 30.3 9240934 821. ## 2 Albania Europe 1957 59.3 1476505 1942. ## 3 Algeria Africa 1957 45.7 10270856 3014. ## 4 Angola Africa 1957 32.0 4561361 3828. ## 5 Argentina Americas 1957 64.4 19610538 6857. ## 6 Australia Oceania 1957 70.3 9712569 10950. ## 7 Austria Europe 1957 67.5 6965860 8843. ## 8 Bahrain Asia 1957 53.8 138655 11636. ## 9 Bangladesh Asia 1957 39.3 51365468 662. ## 10 Belgium Europe 1957 69.2 8989111 9715. ## # ℹ 132 more rows Use the filter() verb to set two conditions. # Filter for China in 2002 # version 1 gapminder %&gt;% filter(country == &quot;China&quot;, year == 2002) ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 China Asia 2002 72.0 1280400000 3119. # version 2 gapminder[gapminder$country == &quot;China&quot; &amp; gapminder$year == 2002, ] ## # A tibble: 1 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 China Asia 2002 72.0 1280400000 3119. 3.1.3 Arrange Use arrange() to sort observations in ascending or descending order of a particular variable. # Sort in ascending order of lifeExp gapminder %&gt;% arrange(lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Rwanda Africa 1992 23.6 7290203 737. ## 2 Afghanistan Asia 1952 28.8 8425333 779. ## 3 Gambia Africa 1952 30 284320 485. ## 4 Angola Africa 1952 30.0 4232095 3521. ## 5 Sierra Leone Africa 1952 30.3 2143249 880. ## 6 Afghanistan Asia 1957 30.3 9240934 821. ## 7 Cambodia Asia 1977 31.2 6978607 525. ## 8 Mozambique Africa 1952 31.3 6446316 469. ## 9 Sierra Leone Africa 1957 31.6 2295678 1004. ## 10 Burkina Faso Africa 1952 32.0 4469979 543. ## # ℹ 1,694 more rows # Sort in descending order of lifeExp gapminder %&gt;% arrange(desc(lifeExp)) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. ## 3 Japan Asia 2002 82 127065841 28605. ## 4 Iceland Europe 2007 81.8 301931 36181. ## 5 Switzerland Europe 2007 81.7 7554661 37506. ## 6 Hong Kong, China Asia 2002 81.5 6762476 30209. ## 7 Australia Oceania 2007 81.2 20434176 34435. ## 8 Spain Europe 2007 80.9 40448191 28821. ## 9 Sweden Europe 2007 80.9 9031088 33860. ## 10 Israel Asia 2007 80.7 6426679 25523. ## # ℹ 1,694 more rows Filtering and arranging # Filter for the year 1957, then arrange in descending order of population gapminder %&gt;% filter(year == 1957) %&gt;% arrange(desc(pop)) ## # A tibble: 142 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 China Asia 1957 50.5 637408000 576. ## 2 India Asia 1957 40.2 409000000 590. ## 3 United States Americas 1957 69.5 171984000 14847. ## 4 Japan Asia 1957 65.5 91563009 4318. ## 5 Indonesia Asia 1957 39.9 90124000 859. ## 6 Germany Europe 1957 69.1 71019069 10188. ## 7 Brazil Americas 1957 53.3 65551171 2487. ## 8 United Kingdom Europe 1957 70.4 51430000 11283. ## 9 Bangladesh Asia 1957 39.3 51365468 662. ## 10 Italy Europe 1957 67.8 49182000 6249. ## # ℹ 132 more rows 3.1.4 Mutate Can change exist columns, or create new columns. # Use mutate to change lifeExp to be in months gapminder %&gt;% mutate(lifeExp = 12 * lifeExp) ## # A tibble: 1,704 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 346. 8425333 779. ## 2 Afghanistan Asia 1957 364. 9240934 821. ## 3 Afghanistan Asia 1962 384. 10267083 853. ## 4 Afghanistan Asia 1967 408. 11537966 836. ## 5 Afghanistan Asia 1972 433. 13079460 740. ## 6 Afghanistan Asia 1977 461. 14880372 786. ## 7 Afghanistan Asia 1982 478. 12881816 978. ## 8 Afghanistan Asia 1987 490. 13867957 852. ## 9 Afghanistan Asia 1992 500. 16317921 649. ## 10 Afghanistan Asia 1997 501. 22227415 635. ## # ℹ 1,694 more rows # Use mutate to create a new column called lifeExpMonths gapminder %&gt;% mutate(lifeExpMonths = 12 * lifeExp) ## # A tibble: 1,704 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 346. ## 2 Afghanistan Asia 1957 30.3 9240934 821. 364. ## 3 Afghanistan Asia 1962 32.0 10267083 853. 384. ## 4 Afghanistan Asia 1967 34.0 11537966 836. 408. ## 5 Afghanistan Asia 1972 36.1 13079460 740. 433. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 461. ## 7 Afghanistan Asia 1982 39.9 12881816 978. 478. ## 8 Afghanistan Asia 1987 40.8 13867957 852. 490. ## 9 Afghanistan Asia 1992 41.7 16317921 649. 500. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 501. ## # ℹ 1,694 more rows Combining filter, mutate, and arrange. # Filter, mutate, and arrange the gapminder dataset gapminder %&gt;% filter(year == 2007) %&gt;% mutate(lifeExpMonths = 12*lifeExp) %&gt;% arrange(desc(lifeExpMonths)) ## # A tibble: 142 × 7 ## country continent year lifeExp pop gdpPercap lifeExpMonths ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Japan Asia 2007 82.6 127467972 31656. 991. ## 2 Hong Kong, China Asia 2007 82.2 6980412 39725. 986. ## 3 Iceland Europe 2007 81.8 301931 36181. 981. ## 4 Switzerland Europe 2007 81.7 7554661 37506. 980. ## 5 Australia Oceania 2007 81.2 20434176 34435. 975. ## 6 Spain Europe 2007 80.9 40448191 28821. 971. ## 7 Sweden Europe 2007 80.9 9031088 33860. 971. ## 8 Israel Asia 2007 80.7 6426679 25523. 969. ## 9 France Europe 2007 80.7 61083916 30470. 968. ## 10 Canada Americas 2007 80.7 33390141 36319. 968. ## # ℹ 132 more rows 3.2 Data visualization 3.2.1 Subset df variable # Load the ggplot2 package as well library(ggplot2) # Create gapminder_1952 gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) 3.2.2 ggplot aesthetics There are three parts to a ggplot graph. Data: The data that we’re visualizing. Aesthetic mapping: The mapping of variables in your dataset to aesthetics in your graph. (In a scatterplot, your two dimensions are the x axis and the y axis) Layer: Specifying the type of graph you’re creating. You do that by adding a layer to the graph - use a + after the ggplot, and then geom_point(). The “geom” means you’re adding a type of geometric object to the graph, the “point” indicates it’s a scatter plot, where each observation corresponds to one point. # Change to put pop on the x-axis and gdpPercap on the y-axis ggplot(gapminder_1952, aes(x = pop, y = gdpPercap)) + geom_point() # Create a scatter plot with pop on the x-axis and lifeExp on the y-axis ggplot(gapminder_1952, aes(x = pop, y = lifeExp)) + geom_point() 3.2.2.1 Log scales Putting the x-axes on a log scale Since population is spread over several orders of magnitude, with some countries having a much higher population than others, it’s a good idea to put the x-axis on a log scale. # Change this plot to put the x-axis on a log scale ggplot(gapminder_1952, aes(x = pop, y = lifeExp)) + geom_point() + scale_x_log10() Putting the x- and y- axes on a log scale # Scatter plot comparing pop and gdpPercap, with both axes on a log scale ggplot(gapminder_1952, aes(x = pop, y = gdpPercap)) + geom_point() + scale_x_log10() + scale_y_log10() 3.2.2.2 Colors By syntax aes(..., color = var) # Scatter plot comparing pop and lifeExp, with color representing continent ggplot(gapminder_1952, aes(x = pop, y = lifeExp, color = continent)) + geom_point() + scale_x_log10() 3.2.2.3 Size By syntax aes(..., size = var) # Add the size aesthetic to represent a country&#39;s gdpPercap ggplot(gapminder_1952, aes(x = pop, y = lifeExp, color = continent, size = gdpPercap)) + geom_point() + scale_x_log10() 3.2.3 Faceting ggplot2 lets you divide your plot into subplots to get one smaller graph for each factor of categorical variables. By syntax facet_wrap(~ var). # Scatter plot comparing pop and lifeExp, faceted by continent ggplot(gapminder_1952, aes(x = pop, y = lifeExp)) + geom_point() + scale_x_log10() + facet_wrap(~ continent) Faceting by year Now that you’re able to use faceting, however, you can create a graph showing all the country-level data from 1952 to 2007, to understand how global statistics have changed over time. # Scatter plot comparing gdpPercap and lifeExp, with color representing continent # and size representing population, faceted by year ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point() + scale_x_log10() + facet_wrap(~ year) 3.3 Grouping &amp; summarizing 3.3.1 Summarize Turn many rows into one.(變項的summary) Fuctions you can use in summarize(): mean, sum, median, min, max. Summarize multiple variables at once: # Filter for 1957 then summarize the median life expectancy and the maximum GDP per capita gapminder %&gt;% filter(year == 1957) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## # A tibble: 1 × 2 ## medianLifeExp maxGdpPercap ## &lt;dbl&gt; &lt;dbl&gt; ## 1 48.4 113523. 3.3.2 Group by Turn groups into one row each. Summarizing by one variable. # Find median life expectancy and maximum GDP per capita in each year gapminder %&gt;% group_by(year) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## # A tibble: 12 × 3 ## year medianLifeExp maxGdpPercap ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 45.1 108382. ## 2 1957 48.4 113523. ## 3 1962 50.9 95458. ## 4 1967 53.8 80895. ## 5 1972 56.5 109348. ## 6 1977 59.7 59265. ## 7 1982 62.4 33693. ## 8 1987 65.8 31541. ## 9 1992 67.7 34933. ## 10 1997 69.4 41283. ## 11 2002 70.8 44684. ## 12 2007 71.9 49357. # Find median life expectancy and maximum GDP per capita in each continent in 1957 gapminder %&gt;% filter(year == 1957) %&gt;% group_by(continent) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## # A tibble: 5 × 3 ## continent medianLifeExp maxGdpPercap ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 40.6 5487. ## 2 Americas 56.1 14847. ## 3 Asia 48.3 113523. ## 4 Europe 67.6 17909. ## 5 Oceania 70.3 12247. Summarizing by more variables. # Find median life expectancy and maximum GDP per capita in each continent/year combination gapminder %&gt;% group_by(year, continent) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. ## You can override using the `.groups` ## argument. ## # A tibble: 60 × 4 ## # Groups: year [12] ## year continent medianLifeExp maxGdpPercap ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1952 Africa 38.8 4725. ## 2 1952 Americas 54.7 13990. ## 3 1952 Asia 44.9 108382. ## 4 1952 Europe 65.9 14734. ## 5 1952 Oceania 69.3 10557. ## 6 1957 Africa 40.6 5487. ## 7 1957 Americas 56.1 14847. ## 8 1957 Asia 48.3 113523. ## 9 1957 Europe 67.6 17909. ## 10 1957 Oceania 70.3 12247. ## # ℹ 50 more rows 3.3.3 Visualizing summarized data Add expand_limits(y = 0) to make sure the plot’s y-axis includes zero. # Create a scatter plot showing the change in medianLifeExp over time by_year &lt;- gapminder %&gt;% group_by(year) %&gt;% summarize(medianLifeExp = median(lifeExp), maxGdpPercap = max(gdpPercap)) ggplot(by_year, aes(x = year, y = medianLifeExp)) + geom_point() + expand_limits(y = 0) # Summarize medianGdpPercap within each continent within each year: by_year_continent by_year_continent &lt;- gapminder %&gt;% group_by(continent, year) %&gt;% summarize(medianGdpPercap = median(gdpPercap)) ## `summarise()` has grouped output by ## &#39;continent&#39;. You can override using the ## `.groups` argument. # Plot the change in medianGdpPercap in each continent over time ggplot(by_year_continent, aes(x = year, y = medianGdpPercap, color = continent)) + geom_point() + expand_limits(y = 0) # Summarize the median GDP and median life expectancy per continent in 2007 by_continent_2007 &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarize(medianLifeExp = median(lifeExp), medianGdpPercap = median(gdpPercap)) # Use a scatter plot to compare the median GDP and median life expectancy ggplot(by_continent_2007, aes(x = medianGdpPercap, y = medianLifeExp, color = continent)) + geom_point() 3.4 Types of visualizations line plots bar plots histograms box plots usage useful for showing change over time comparing statistics for each of several categories describe the distribution of a one-dimensional numeric variable compare the distribution of a numeric variable among several categories geom geom_line() geom_col() geom_histogram(bins = int) geom_boxplot() 3.4.1 Line plots A line plot is useful for visualizing trends over time. # Summarize the median gdpPercap by year, then save it as by_year by_year &lt;- gapminder %&gt;% group_by(year) %&gt;% summarize(medianGdpPercap = median(gdpPercap)) # Create a line plot showing the change in medianGdpPercap over time ggplot(by_year, aes(x = year, y = medianGdpPercap)) + geom_line() + expand_limits(y = 0) # Summarize the median gdpPercap by year &amp; continent, save as by_year_continent by_year_continent &lt;- gapminder %&gt;% group_by(year, continent) %&gt;% summarize(medianGdpPercap = median(gdpPercap)) ## `summarise()` has grouped output by &#39;year&#39;. ## You can override using the `.groups` ## argument. # Create a line plot showing the change in medianGdpPercap by continent over time ggplot(by_year_continent, aes(x = year, y = medianGdpPercap, color = continent)) + geom_line() + expand_limits(y = 0) 3.4.2 Bar plots A bar plot is useful for visualizing summary statistics. # Summarize the median gdpPercap by continent in 1952 by_continent &lt;- gapminder %&gt;% filter(year == 1952) %&gt;% group_by(continent) %&gt;% summarise(medianGdpPercap = median(gdpPercap)) # Create a bar plot showing medianGdp by continent ggplot(by_continent, aes(x = continent, y = medianGdpPercap)) + geom_col() # Filter for observations in the Oceania continent in 1952 oceania_1952 &lt;- gapminder %&gt;% filter(year == 1952 &amp; continent == &quot;Oceania&quot;) # Create a bar plot of gdpPercap by country ggplot(oceania_1952, aes(x = country, y = gdpPercap)) + geom_col() 3.4.3 Histograms A histogram is useful for examining the distribution of a numeric variable. So there is only x-axis. gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) %&gt;% mutate(pop_by_mil = pop / 1000000) # Create a histogram of population (pop_by_mil) ggplot(gapminder_1952, aes(x = pop_by_mil)) + geom_histogram(bins = 50) To make the histogram more informative, you can try putting the x-axis on a log scale. gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) # Create a histogram of population (pop), with x on a log scale ggplot(gapminder_1952, aes(x = pop)) + geom_histogram(bins = 40) + scale_x_log10() 3.4.4 Box plots A boxplot is useful for comparing a distribution of values across several groups. gapminder_1952 &lt;- gapminder %&gt;% filter(year == 1952) # Create a boxplot comparing gdpPercap among continents ggplot(gapminder_1952, aes(x = continent, y = gdpPercap)) + geom_boxplot() + scale_y_log10() "],["data-manipulation.html", "Chapter 4 Data Manipulation 4.1 Transforming Data 4.2 Aggregating Data 4.3 Selecting and Transforming Data 4.4 Case Study: The babynames Dataset", " Chapter 4 Data Manipulation 4.1 Transforming Data 4.1.1 Exploring data Useful verbs: glimpse(): similar to str(). select(): select columns. select(var_name1, var_name2,...) filter(): select rows. For multiple conditions, can separate by: 1. comma filter(con1, con2,...) 2. logic operator &amp; or |, filter(con1 &amp; con2) 3.%in% : used to filter for multiple values. filter(var %in% c(\"var_value1\", \"var_value2\")) arrange(): sort by variables. mutate(): change exist or create new variables. library(tidyverse) ## ── Attaching core tidyverse packages ───────── ## ✔ forcats 1.0.0 ✔ stringr 1.5.0 ## ✔ lubridate 1.9.2 ✔ tibble 3.2.1 ## ✔ purrr 1.0.1 ✔ tidyr 1.3.0 ## ✔ readr 2.1.4 ## ── Conflicts ──────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors counties &lt;- read_csv(&quot;data/counties.csv&quot;) ## Rows: 3138 Columns: 40 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): state, county, region, metro ## dbl (36): census_id, population, men, women, hispanic, white, black, native,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. glimpse(counties) ## Rows: 3,138 ## Columns: 40 ## $ census_id &lt;dbl&gt; 1001, 1003, 1005, 1007, 1009, 1011, 1013, 1015, 101… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabam… ## $ county &lt;chr&gt; &quot;\\&quot;Autauga\\&quot;&quot;, &quot;\\&quot;Baldwin\\&quot;&quot;, &quot;\\&quot;Barbour\\&quot;&quot;, &quot;\\&quot;Bib… ## $ region &lt;chr&gt; &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South&quot;, &quot;South… ## $ metro &lt;chr&gt; &quot;Metro&quot;, &quot;Metro&quot;, &quot;Nonmetro&quot;, &quot;Metro&quot;, &quot;Metro&quot;, &quot;No… ## $ population &lt;dbl&gt; 55221, 195121, 26932, 22604, 57710, 10678, 20354, 1… ## $ men &lt;dbl&gt; 26700, 95300, 14500, 12100, 28500, 5660, 9500, 5630… ## $ women &lt;dbl&gt; 28500, 99800, 12400, 10500, 29200, 5020, 10900, 604… ## $ hispanic &lt;dbl&gt; 2.6, 4.5, 4.6, 2.2, 8.6, 4.4, 1.2, 3.5, 0.4, 1.5, 7… ## $ white &lt;dbl&gt; 75.8, 83.1, 46.2, 74.5, 87.9, 22.2, 53.3, 73.0, 57.… ## $ black &lt;dbl&gt; 18.5, 9.5, 46.7, 21.4, 1.5, 70.7, 43.8, 20.3, 40.3,… ## $ native &lt;dbl&gt; 0.4, 0.6, 0.2, 0.4, 0.3, 1.2, 0.1, 0.2, 0.2, 0.6, 0… ## $ asian &lt;dbl&gt; 1.0, 0.7, 0.4, 0.1, 0.1, 0.2, 0.4, 0.9, 0.8, 0.3, 0… ## $ pacific &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0… ## $ citizens &lt;dbl&gt; 40725, 147695, 20714, 17495, 42345, 8057, 15581, 88… ## $ income &lt;dbl&gt; 51281, 50254, 32964, 38678, 45813, 31938, 32229, 41… ## $ income_err &lt;dbl&gt; 2391, 1263, 2973, 3995, 3141, 5884, 1793, 925, 2949… ## $ income_per_cap &lt;dbl&gt; 24974, 27317, 16824, 18431, 20532, 17580, 18390, 21… ## $ income_per_cap_err &lt;dbl&gt; 1080, 711, 798, 1618, 708, 2055, 714, 489, 1366, 15… ## $ poverty &lt;dbl&gt; 12.9, 13.4, 26.7, 16.8, 16.7, 24.6, 25.4, 20.5, 21.… ## $ child_poverty &lt;dbl&gt; 18.6, 19.2, 45.3, 27.9, 27.2, 38.4, 39.2, 31.6, 37.… ## $ professional &lt;dbl&gt; 33.2, 33.1, 26.8, 21.5, 28.5, 18.8, 27.5, 27.3, 23.… ## $ service &lt;dbl&gt; 17.0, 17.7, 16.1, 17.9, 14.1, 15.0, 16.6, 17.7, 14.… ## $ office &lt;dbl&gt; 24.2, 27.1, 23.1, 17.8, 23.9, 19.7, 21.9, 24.2, 26.… ## $ construction &lt;dbl&gt; 8.6, 10.8, 10.8, 19.0, 13.5, 20.1, 10.3, 10.5, 11.5… ## $ production &lt;dbl&gt; 17.1, 11.2, 23.1, 23.7, 19.9, 26.4, 23.7, 20.4, 24.… ## $ drive &lt;dbl&gt; 87.5, 84.7, 83.8, 83.2, 84.9, 74.9, 84.5, 85.3, 85.… ## $ carpool &lt;dbl&gt; 8.8, 8.8, 10.9, 13.5, 11.2, 14.9, 12.4, 9.4, 11.9, … ## $ transit &lt;dbl&gt; 0.1, 0.1, 0.4, 0.5, 0.4, 0.7, 0.0, 0.2, 0.2, 0.2, 0… ## $ walk &lt;dbl&gt; 0.5, 1.0, 1.8, 0.6, 0.9, 5.0, 0.8, 1.2, 0.3, 0.6, 1… ## $ other_transp &lt;dbl&gt; 1.3, 1.4, 1.5, 1.5, 0.4, 1.7, 0.6, 1.2, 0.4, 0.7, 1… ## $ work_at_home &lt;dbl&gt; 1.8, 3.9, 1.6, 0.7, 2.3, 2.8, 1.7, 2.7, 2.1, 2.5, 1… ## $ mean_commute &lt;dbl&gt; 26.5, 26.4, 24.1, 28.8, 34.9, 27.5, 24.6, 24.1, 25.… ## $ employed &lt;dbl&gt; 23986, 85953, 8597, 8294, 22189, 3865, 7813, 47401,… ## $ private_work &lt;dbl&gt; 73.6, 81.5, 71.8, 76.8, 82.0, 79.5, 77.4, 74.1, 85.… ## $ public_work &lt;dbl&gt; 20.9, 12.3, 20.8, 16.1, 13.5, 15.1, 16.2, 20.8, 12.… ## $ self_employed &lt;dbl&gt; 5.5, 5.8, 7.3, 6.7, 4.2, 5.4, 6.2, 5.0, 2.8, 7.9, 4… ## $ family_work &lt;dbl&gt; 0.0, 0.4, 0.1, 0.4, 0.4, 0.0, 0.2, 0.1, 0.0, 0.5, 0… ## $ unemployment &lt;dbl&gt; 7.6, 7.5, 17.6, 8.3, 7.7, 18.0, 10.9, 12.3, 8.9, 7.… ## $ land_area &lt;dbl&gt; 594, 1590, 885, 623, 645, 623, 777, 606, 597, 554, … counties %&gt;% # Select the five columns select(state, county, population, men, women) %&gt;% # Add the proportion_men variable mutate(proportion_men = men / population) %&gt;% # Filter for population of at least 10,000 filter(population &gt;= 10000, state == &quot;California&quot;) %&gt;% # Arrange proportion of men in descending order arrange(desc(proportion_men)) ## # A tibble: 55 × 6 ## state county population men women proportion_men ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 California &quot;\\&quot;Lassen\\&quot;&quot; 32645 21800 10800 0.668 ## 2 California &quot;\\&quot;Kings\\&quot;&quot; 150998 84000 67000 0.556 ## 3 California &quot;\\&quot;Del Norte\\&quot;&quot; 27788 15400 12400 0.554 ## 4 California &quot;\\&quot;Amador\\&quot;&quot; 36995 20000 17000 0.541 ## 5 California &quot;\\&quot;Tuolumne\\&quot;&quot; 54079 28200 25900 0.521 ## 6 California &quot;\\&quot;Colusa\\&quot;&quot; 21396 11100 10300 0.519 ## 7 California &quot;\\&quot;Mono\\&quot;&quot; 14146 7310 6840 0.517 ## 8 California &quot;\\&quot;Trinity\\&quot;&quot; 13373 6880 6500 0.514 ## 9 California &quot;\\&quot;Kern\\&quot;&quot; 865736 445000 421000 0.514 ## 10 California &quot;\\&quot;Imperial\\&quot;&quot; 178206 91200 87000 0.512 ## # ℹ 45 more rows 4.2 Aggregating Data 4.2.1 Count One way we can aggregate data is to count(): to find out the number of observations. Count variable and sort: count(var, sort = TRUE) counties_selected &lt;- counties %&gt;% select(county, region, state, population, citizens) # Use count to find the number of counties in each region counties_selected %&gt;% count(region, sort = TRUE) ## # A tibble: 3 × 2 ## region n ## &lt;chr&gt; &lt;int&gt; ## 1 South 1420 ## 2 North~ 1271 ## 3 West 447 Add weight: count(var1, wt = var2, sort = TRUE) counties_selected &lt;- counties %&gt;% select(county, region, state, population, citizens) # Find number of counties per state, weighted by citizens, sorted in descending order counties_selected %&gt;% count(state, wt = citizens, sort = TRUE) ## # A tibble: 50 × 2 ## state n ## &lt;chr&gt; &lt;dbl&gt; ## 1 California 24280349 ## 2 Texas 16864864 ## 3 Florida 13933052 ## 4 New York 13531404 ## 5 Pennsylvan 9710416 ## 6 Illinois 8979999 ## 7 Ohio 8709050 ## 8 Michigan 7380136 ## 9 North Caro 7107998 ## 10 Georgia 6978660 ## # ℹ 40 more rows Mutating and counting: “What are the US states where the most people walk to work?” counties_selected &lt;- counties %&gt;% select(county, region, state, population, walk) counties_selected %&gt;% # Add population_walk containing the total number of people who walk to work mutate(population_walk = walk * population / 100) %&gt;% # Count weighted by the new column, sort in descending order count(state, wt = population_walk, sort = TRUE) ## # A tibble: 50 × 2 ## state n ## &lt;chr&gt; &lt;dbl&gt; ## 1 New York 1237938. ## 2 California 1017964. ## 3 Pennsylvan 505397. ## 4 Texas 430783. ## 5 Illinois 400346. ## 6 Massachuse 316765. ## 7 Florida 284723. ## 8 New Jersey 273047. ## 9 Ohio 266911. ## 10 Washington 239764. ## # ℹ 40 more rows 4.2.2 group_by, summarize, ungroup summarize(): takes many observations and turns them into one observation. Also can define multiple variables in a summarize call. group_by(): aggregate within groups. We can group by multiple columns by passing multiple column names to group_by. When you group by multiple columns and then summarize, it’s important to remember that the summarize “peels off” one of the groups, but leaves the rest on. For example, if you group_by(X, Y) then summarize, the result will still be grouped by X. ungroup(): If you don’t want to keep variable as a group, you can add another ungroup(). Summarize both population and land area by state, with the purpose of finding the density (in people per square miles). counties %&gt;% select(state, county, population, land_area) %&gt;% # Group by state group_by(state) %&gt;% # Find the total area and population summarize(total_area = sum(land_area), total_population = sum(population)) %&gt;% # Add a density column mutate(density = total_population / total_area) %&gt;% # Sort by density in descending order arrange(desc(density)) ## # A tibble: 50 × 4 ## state total_area total_population density ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 New Jersey 7356. 8904413 1210. ## 2 Rhode Isla 1034. 1053661 1019. ## 3 Massachuse 7800. 6705586 860. ## 4 Connecticu 4843 3593222 742. ## 5 Maryland 9707. 5930538 611. ## 6 Delaware 1948 926454 476. ## 7 New York 47127. 19673174 417. ## 8 Florida 53627 19645772 366. ## 9 Pennsylvan 44740 12779559 286. ## 10 Ohio 40855 11575977 283. ## # ℹ 40 more rows You can group by multiple columns instead of grouping by one.Summarizing by state and region: counties %&gt;% select(region, state, county, population) %&gt;% # Group and summarize to find the total population group_by(region, state) %&gt;% summarize(total_pop = sum(population)) %&gt;% # Calculate the average_pop and median_pop columns summarize(average_pop = mean(total_pop), median_pop = median(total_pop)) ## `summarise()` has grouped output by &#39;region&#39;. ## You can override using the `.groups` ## argument. ## # A tibble: 3 × 3 ## region average_pop median_pop ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 North~ 5881989. 5419171 ## 2 South 7370486 4804098 ## 3 West 5722755. 2798636 4.2.3 slice_min, slice_max slice_max() operates on a grouped table, and returns the largest observations in each group. The function takes two arguments: the column we want to base the ordering on, and the number of observations to extract from each group, specified with the n argument. # The largest observations slice_max(var, n = 1) # The 3 largest observations slice_max(var, n = 3) Similarly, slice_min() returns the smallest observations in each group. # The smallest observations slice_min(var, n = 1) # The 3 smallest observations slice_min(var, n = 3) The slicing are often used when creating visualizations, where we may want to highlight the extreme observations on the plot. Q: In how many states do more people live in metro areas than non-metro areas? counties %&gt;% select(state, metro, population) %&gt;% # Find the total population for each combination of state and metro group_by(state, metro) %&gt;% summarize(total_pop = sum(population)) %&gt;% # Extract the most populated row for each state slice_max(total_pop, n = 1) %&gt;% # Count the states with more people in Metro or Nonmetro areas ungroup() %&gt;% count(metro, sort = TRUE) ## `summarise()` has grouped output by &#39;state&#39;. ## You can override using the `.groups` ## argument. ## # A tibble: 2 × 2 ## metro n ## &lt;chr&gt; &lt;int&gt; ## 1 Metro 44 ## 2 Nonmetro 6 4.3 Selecting and Transforming Data 4.3.1 Select 4.3.1.1 Range Select a range of columns. select(var1, var3:var6) # For example select(state, county, drive:work_at_home) counties %&gt;% # Select state, county, population, and industry-related columns select(state, county, population, professional:production) %&gt;% # Arrange service in descending order arrange(desc(service)) ## # A tibble: 3,138 × 8 ## state county population professional service office construction production ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Missis… &quot;i … 10477 23.9 36.6 21.5 3.5 14.5 ## 2 Texas &quot;\\&quot;Ki… 3577 30 36.5 11.6 20.5 1.3 ## 3 Texas &quot;\\&quot;Ke… 565 24.9 34.1 20.5 20.5 0 ## 4 New Yo… &quot;\\&quot;Br… 1428357 24.3 33.3 24.2 7.1 11 ## 5 Texas &quot;\\&quot;Br… 7221 19.6 32.4 25.3 11.1 11.5 ## 6 Colora… &quot;\\&quot;Fr… 46809 26.6 32.2 22.8 10.7 7.6 ## 7 Texas &quot;\\&quot;Cu… 2296 20.1 32.2 24.2 15.7 7.8 ## 8 Califo… &quot;\\&quot;De… 27788 33.9 31.5 18.8 8.9 6.8 ## 9 Minnes… &quot;\\&quot;Ma… 5496 26.8 31.5 18.7 13.1 9.9 ## 10 Virgin… &quot;\\&quot;La… 11129 30.3 31.2 22.8 8.1 7.6 ## # ℹ 3,128 more rows 4.3.1.2 Contains Specify criteria for choosing columns. select(var1, var3, contains(&quot;char&quot;)) # For example select(state, county, contains(&quot;work&quot;)) 4.3.1.3 Starts with To select only the columns that start with a particular prefix. select(var1, var3, starts_with(&quot;char&quot;)) # For example select(state, county, starts_with(&quot;income&quot;)) 4.3.1.4 Ends with Finds columns ending in a string. select(var1, var3, ends_with(&quot;char&quot;)) # For example select(state, county, ends_with(&quot;tion&quot;)) counties %&gt;% # Select the state, county, population, and those ending with &quot;work&quot; select(state, county, population, ends_with(&quot;work&quot;)) %&gt;% # Filter for counties that have at least 50% of people engaged in public work filter(public_work &gt; 50) ## # A tibble: 7 × 6 ## state county population private_work public_work family_work ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Alaska &quot;\\&quot;Lake and Penins… 1474 42.2 51.6 0.2 ## 2 Alaska &quot;\\&quot;Yukon-Koyukuk C… 5644 33.3 61.7 0 ## 3 California &quot;\\&quot;Lassen\\&quot;&quot; 32645 42.6 50.5 0.1 ## 4 Hawaii &quot;\\&quot;Kalawao\\&quot;&quot; 85 25 64.1 0 ## 5 North Dako &quot;ta \\&quot;Sioux\\&quot;&quot; 4380 32.9 56.8 0.1 ## 6 South Dako &quot;ta \\&quot;Todd\\&quot;&quot; 9942 34.4 55 0.8 ## 7 Wisconsin &quot;\\&quot;Menominee\\&quot;&quot; 4451 36.8 59.1 0.4 4.3.1.5 Last column last_col() grabs the last column. 4.3.1.6 Matches Selects columns that have a specified pattern. select(matches(&quot;regex&quot;)) # For example select(matches(&quot;.\\_.&quot;)) 4.3.1.7 Removing a variable We can use select to remove variables from a table by adding a - in front of the column name to remove. select(-(var1)) # For example select(-census_id) 4.3.2 Rename Often, rather than only selecting columns, we’ll sometimes want to rename the ones we already have. Compare these two ways: Select When select variables, change variables’ name at the same time. select(var1, var2, var_newname = var_oldname) # For example counties %&gt;% select(state, county, population, unemployment_rate = unemployment) counties %&gt;% # Select state, county, and poverty as poverty_rate select(state, county, poverty_rate = poverty) ## # A tibble: 3,138 × 3 ## state county poverty_rate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Alabama &quot;\\&quot;Autauga\\&quot;&quot; 12.9 ## 2 Alabama &quot;\\&quot;Baldwin\\&quot;&quot; 13.4 ## 3 Alabama &quot;\\&quot;Barbour\\&quot;&quot; 26.7 ## 4 Alabama &quot;\\&quot;Bibb\\&quot;&quot; 16.8 ## 5 Alabama &quot;\\&quot;Blount\\&quot;&quot; 16.7 ## 6 Alabama &quot;\\&quot;Bullock\\&quot;&quot; 24.6 ## 7 Alabama &quot;\\&quot;Butler\\&quot;&quot; 25.4 ## 8 Alabama &quot;\\&quot;Calhoun\\&quot;&quot; 20.5 ## 9 Alabama &quot;\\&quot;Chambers\\&quot;&quot; 21.6 ## 10 Alabama &quot;\\&quot;Cherokee\\&quot;&quot; 19.2 ## # ℹ 3,128 more rows Rename rename() is often useful for changing the name of a column that comes out of another verb. rename(var_newname = var_oldname) # For example counties %&gt;% select(state, county, population, unemployment) %&gt;% rename(unemployment_rate = unemployment) counties %&gt;% # Count the number of counties in each state count(state) ## # A tibble: 50 × 2 ## state n ## &lt;chr&gt; &lt;int&gt; ## 1 Alabama 67 ## 2 Alaska 28 ## 3 Arizona 15 ## 4 Arkansas 75 ## 5 California 58 ## 6 Colorado 64 ## 7 Connecticu 8 ## 8 Delaware 3 ## 9 Florida 67 ## 10 Georgia 159 ## # ℹ 40 more rows counties %&gt;% # Count the number of counties in each state count(state) %&gt;% # Rename the n column to num_counties rename(num_counties = n) ## # A tibble: 50 × 2 ## state num_counties ## &lt;chr&gt; &lt;int&gt; ## 1 Alabama 67 ## 2 Alaska 28 ## 3 Arizona 15 ## 4 Arkansas 75 ## 5 California 58 ## 6 Colorado 64 ## 7 Connecticu 8 ## 8 Delaware 3 ## 9 Florida 67 ## 10 Georgia 159 ## # ℹ 40 more rows 4.3.3 Transmute Transmute is like a combination of select and mutate: it returns a subset of the columns like select, but it can also transform and change the columns, like mutate, at the same time. It control which variables you keep, which variables you calculate, and which variables you drop. transmute(var1, var2, var_new = var_old*do caculate) #For example counties %&gt;% transmute(state, county, fraction_men = men / population) counties %&gt;% # Keep the state, county, and populations columns, and add a density column transmute(state, county, population, density = population/land_area) %&gt;% # Filter for counties with a population greater than one million filter(population &gt; 1000000) %&gt;% # Sort density in ascending order arrange(density) ## # A tibble: 41 × 4 ## state county population density ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 California &quot;\\&quot;San Bernardino\\&quot;&quot; 2094769 104. ## 2 Nevada &quot;\\&quot;Clark\\&quot;&quot; 2035572 258. ## 3 California &quot;\\&quot;Riverside\\&quot;&quot; 2298032 319. ## 4 Arizona &quot;\\&quot;Maricopa\\&quot;&quot; 4018143 437. ## 5 Florida &quot;\\&quot;Palm Beach\\&quot;&quot; 1378806 700. ## 6 California &quot;\\&quot;San Diego\\&quot;&quot; 3223096 766. ## 7 Washington &quot;\\&quot;King\\&quot;&quot; 2045756 967. ## 8 Texas &quot;\\&quot;Travis\\&quot;&quot; 1121645 1133. ## 9 Florida &quot;\\&quot;Hillsborough\\&quot;&quot; 1302884 1277. ## 10 Florida &quot;\\&quot;Orange\\&quot;&quot; 1229039 1361. ## # ℹ 31 more rows Summary # Change the name of the unemployment column counties %&gt;% rename(unemployment_rate = unemployment) # Keep the state and county columns, and the columns containing poverty counties %&gt;% select(state, county, contains(&quot;poverty&quot;)) # Calculate the fraction_women column without dropping the other columns counties %&gt;% mutate(fraction_women = women / population) # Keep only the state, county, and employment_rate columns counties %&gt;% transmute(state, county, employment_rate = employed / population) 4.4 Case Study: The babynames Dataset 4.4.1 Load dataset library(babynames) ## Warning: package &#39;babynames&#39; was built under R version 4.3.1 babynames ## # A tibble: 1,924,665 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 ## 2 1880 F Anna 2604 0.0267 ## 3 1880 F Emma 2003 0.0205 ## 4 1880 F Elizabeth 1939 0.0199 ## 5 1880 F Minnie 1746 0.0179 ## 6 1880 F Margaret 1578 0.0162 ## 7 1880 F Ida 1472 0.0151 ## 8 1880 F Alice 1414 0.0145 ## 9 1880 F Bertha 1320 0.0135 ## 10 1880 F Sarah 1288 0.0132 ## # ℹ 1,924,655 more rows 4.4.2 Exploring data Filtering and arranging for one year babynames %&gt;% # Filter for the year 1990 filter(year == 1990) %&gt;% # Sort the number column in descending order arrange(desc(n)) ## # A tibble: 24,719 × 5 ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1990 M Michael 65282 0.0303 ## 2 1990 M Christopher 52332 0.0243 ## 3 1990 F Jessica 46475 0.0226 ## 4 1990 F Ashley 45558 0.0222 ## 5 1990 M Matthew 44800 0.0208 ## 6 1990 M Joshua 43216 0.0201 ## 7 1990 F Brittany 36538 0.0178 ## 8 1990 F Amanda 34408 0.0168 ## 9 1990 M Daniel 33815 0.0157 ## 10 1990 M David 33742 0.0157 ## # ℹ 24,709 more rows Finding the most popular names each year babynames %&gt;% # Find the most common name in each year group_by(year) %&gt;% slice_max(n, n = 1) ## # A tibble: 138 × 5 ## # Groups: year [138] ## year sex name n prop ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 M John 9655 0.0815 ## 2 1881 M John 8769 0.0810 ## 3 1882 M John 9557 0.0783 ## 4 1883 M John 8894 0.0791 ## 5 1884 M John 9388 0.0765 ## 6 1885 F Mary 9128 0.0643 ## 7 1886 F Mary 9889 0.0643 ## 8 1887 F Mary 9888 0.0636 ## 9 1888 F Mary 11754 0.0620 ## 10 1889 F Mary 11648 0.0616 ## # ℹ 128 more rows Visualizing names with ggplot2 selected_names &lt;- babynames %&gt;% # Filter for the names Steven, Thomas, and Matthew. And female only filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;) &amp; sex == &quot;F&quot;) # Plot the names using a different color for each name ggplot(selected_names, aes(x = year, y = n, color = name)) + geom_line() Finding the year each name is most common # Calculate the fraction of people born each year with the same name babynames %&gt;% group_by(year) %&gt;% mutate(year_total = sum(n)) %&gt;% ungroup() %&gt;% mutate(fraction = n / year_total) %&gt;% # Find the year each name is most common group_by(name) %&gt;% slice_max(fraction, n = 1) ## # A tibble: 97,351 × 7 ## # Groups: name [97,310] ## year sex name n prop year_total fraction ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2014 M Aaban 16 0.00000783 3696311 0.00000433 ## 2 2014 F Aabha 9 0.00000461 3696311 0.00000243 ## 3 2016 M Aabid 5 0.00000248 3652968 0.00000137 ## 4 2016 M Aabir 5 0.00000248 3652968 0.00000137 ## 5 2016 F Aabriella 11 0.0000057 3652968 0.00000301 ## 6 2015 F Aada 5 0.00000257 3688687 0.00000136 ## 7 2015 M Aadam 22 0.0000108 3688687 0.00000596 ## 8 2009 M Aadan 23 0.0000108 3815638 0.00000603 ## 9 2014 M Aadarsh 18 0.0000088 3696311 0.00000487 ## 10 2009 M Aaden 1267 0.000598 3815638 0.000332 ## # ℹ 97,341 more rows Adding the total and maximum for each name You’ll divide each name by the maximum for that name. This means that every name will peak at 1. babynames %&gt;% # Add columns name_total and name_max for each name group_by(name) %&gt;% mutate(name_total = sum(n), name_max = max(n)) %&gt;% # Ungroup the table ungroup() %&gt;% # Add the fraction_max column containing the number by the name maximum mutate(fraction_max = n / name_max) ## # A tibble: 1,924,665 × 8 ## year sex name n prop name_total name_max fraction_max ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1880 F Mary 7065 0.0724 4138360 73982 0.0955 ## 2 1880 F Anna 2604 0.0267 891245 15666 0.166 ## 3 1880 F Emma 2003 0.0205 655629 22704 0.0882 ## 4 1880 F Elizabeth 1939 0.0199 1634860 20744 0.0935 ## 5 1880 F Minnie 1746 0.0179 159562 3274 0.533 ## 6 1880 F Margaret 1578 0.0162 1250392 28466 0.0554 ## 7 1880 F Ida 1472 0.0151 186955 4450 0.331 ## 8 1880 F Alice 1414 0.0145 562006 11956 0.118 ## 9 1880 F Bertha 1320 0.0135 208550 5051 0.261 ## 10 1880 F Sarah 1288 0.0132 1077215 28484 0.0452 ## # ℹ 1,924,655 more rows Visualizing the normalized change in popularity You picked a few names and calculated each of them as a fraction of their peak. This is a type of “normalizing” a name, where you’re focused on the relative change within each name rather than the overall popularity of the name. names_normalized &lt;- babynames %&gt;% group_by(name) %&gt;% mutate(name_total = sum(n), name_max = max(n)) %&gt;% ungroup() %&gt;% mutate(fraction_max = n / name_max) names_filtered &lt;- names_normalized %&gt;% # Filter for the names Steven, Thomas, and Matthew. And male only filter(name %in% c(&quot;Steven&quot;, &quot;Thomas&quot;, &quot;Matthew&quot;) &amp; sex == &quot;M&quot;) # Visualize these names over time ggplot(names_filtered, aes(x = year, y = fraction_max, color = name)) + geom_line() As you can see, the line for each name hits a peak at 1, although the peak year differs for each name. Using ratios to describe the frequency of a name Window function: lag() v &lt;- c(1, 3, 6, 14); v ## 1 3 6 14 lag(v) ## NA 1 3 6 v - lag(v) ## NA 2 3 8 Notice that the first observation for each name is missing a ratio, since there is no previous year. babynames_fraction &lt;- babynames %&gt;% group_by(year) %&gt;% mutate(year_total = sum(n)) %&gt;% ungroup() %&gt;% mutate(fraction = n / year_total) babynames_fraction %&gt;% # Arrange the data in order of name, then year arrange(name, year) %&gt;% # Group the data by name group_by(name) %&gt;% # Add a ratio column that contains the ratio of fraction between each year mutate(ratio = fraction / lag(fraction)) ## # A tibble: 1,924,665 × 8 ## # Groups: name [97,310] ## year sex name n prop year_total fraction ratio ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2007 M Aaban 5 0.00000226 3994007 0.00000125 NA ## 2 2009 M Aaban 6 0.00000283 3815638 0.00000157 1.26 ## 3 2010 M Aaban 9 0.00000439 3690700 0.00000244 1.55 ## 4 2011 M Aaban 11 0.00000542 3651914 0.00000301 1.24 ## 5 2012 M Aaban 11 0.00000543 3650462 0.00000301 1.00 ## 6 2013 M Aaban 14 0.00000694 3637310 0.00000385 1.28 ## 7 2014 M Aaban 16 0.00000783 3696311 0.00000433 1.12 ## 8 2015 M Aaban 15 0.00000736 3688687 0.00000407 0.939 ## 9 2016 M Aaban 9 0.00000446 3652968 0.00000246 0.606 ## 10 2017 M Aaban 11 0.0000056 3546301 0.00000310 1.26 ## # ℹ 1,924,655 more rows Biggest jumps in a name To look further into the names that experienced the biggest jumps in popularity in consecutive years. babynames_ratios_filtered &lt;- babynames_fraction %&gt;% arrange(name, year) %&gt;% group_by(name) %&gt;% mutate(ratio = fraction / lag(fraction)) %&gt;% filter(fraction &gt;= 0.00001) babynames_ratios_filtered %&gt;% # Extract the largest ratio from each name slice_max(ratio, n = 1) %&gt;% # Sort the ratio column in descending order arrange(desc(ratio)) %&gt;% # Filter for fractions greater than or equal to 0.001 filter(fraction &gt;= 0.001) ## # A tibble: 529 × 8 ## # Groups: name [529] ## year sex name n prop year_total fraction ratio ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2010 F Sophia 20639 0.0105 3690700 0.00559 2371. ## 2 2001 F Olivia 13978 0.00706 3741451 0.00374 2352. ## 3 2002 F Isabella 12166 0.00616 3736042 0.00326 2031. ## 4 2014 M Joseph 12106 0.00592 3696311 0.00328 2018. ## 5 2016 F Emma 19471 0.0101 3652968 0.00533 1966. ## 6 2016 F Elizabeth 9542 0.00495 3652968 0.00261 1927. ## 7 2013 M Luke 9568 0.00474 3637310 0.00263 1914. ## 8 2011 M Samuel 11340 0.00559 3651914 0.00311 1890 ## 9 2015 F Evelyn 9358 0.00481 3688687 0.00254 1875. ## 10 2010 M Isaac 9354 0.00456 3690700 0.00253 1871. ## # ℹ 519 more rows "],["joining-data.html", "Chapter 5 Joining Data 5.1 Inner join 5.2 Left &amp; Right Joins 5.3 Full, Semi &amp; Anti Joins 5.4 Case study: Joins on Stack Overflow Data", " Chapter 5 Joining Data Summary Mutating joins Filtering joins descriptions combine the variables from two tables. keeps or removes observations from the first table, but it doesn’t add new variables. verbs inner_join Keeps only observations which match exactly between two tables. left_join Keep all observations from the first table in your joins. right_join Keep all observations from the second table in your joins. full_join Keep all observations from both tables. semi_join() Filter the first table for observations which also exist in the second table. anti_join() Filter the first table for observations that do not exist in the second table. Load datasets from Rebrickable. And library package. library(tidyverse) sets &lt;- read_csv(&quot;data/lego/sets.csv&quot;) themes &lt;- read_csv(&quot;data/lego/themes.csv&quot;) parts &lt;- read_csv(&quot;data/lego/parts.csv&quot;) part_categories &lt;- read_csv(&quot;data/lego/part_categories.csv&quot;) inventories &lt;- read_csv(&quot;data/lego/inventories.csv&quot;) inventory_parts &lt;- read_csv(&quot;data/lego/inventory_parts.csv&quot;) colors &lt;- read_csv(&quot;data/lego/colors.csv&quot;) 5.1 Inner join An inner join keeps an observation only if it has an exact match between the first and the second tables. by argument tells inner join how to match the tables. If there are same variables name in each table, suffix argument can change variable name by adding characters you assign. Syntax: table1 %&gt;% inner_join(table2, by = c(table2_key = table1_key), suffix = c(&quot;_chr1&quot;, &quot;_chr2&quot;)) # For example sets %&gt;% inner_join(themes, by = c(&quot;theme_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_theme&quot;)) 5.1.1 Join by keys # Add the correct verb, table, and joining column parts %&gt;% inner_join(part_categories, by = c(&quot;part_cat_id&quot; = &quot;id&quot;)) ## # A tibble: 52,600 × 5 ## part_num name.x part_cat_id part_material name.y ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 003381 Sticker Sheet for Set 663-1 58 Plastic Stick… ## 2 003383 Sticker Sheet for Sets 618-1, 628-2 58 Plastic Stick… ## 3 003402 Sticker Sheet for Sets 310-3, 311-… 58 Plastic Stick… ## 4 003429 Sticker Sheet for Set 1550-1 58 Plastic Stick… ## 5 003432 Sticker Sheet for Sets 357-1, 355-… 58 Plastic Stick… ## 6 003434 Sticker Sheet for Set 575-2, 653-1… 58 Plastic Stick… ## 7 003435 Sticker Sheet for Set 687-1 58 Plastic Stick… ## 8 003436 Sticker Sheet for Set 180-1 58 Plastic Stick… ## 9 003437 Sticker Sheet for Set 181-1 58 Plastic Stick… ## 10 003438 Sticker Sheet for Set 131-1 58 Plastic Stick… ## # ℹ 52,590 more rows # Use the suffix argument to replace .x and .y suffixes parts %&gt;% inner_join(part_categories, by = c(&quot;part_cat_id&quot; = &quot;id&quot;), suffix = c(&quot;_part&quot;, &quot;_category&quot;)) ## # A tibble: 52,600 × 5 ## part_num name_part part_cat_id part_material name_category ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 003381 Sticker Sheet for Set 663-1 58 Plastic Stickers ## 2 003383 Sticker Sheet for Sets 618-… 58 Plastic Stickers ## 3 003402 Sticker Sheet for Sets 310-… 58 Plastic Stickers ## 4 003429 Sticker Sheet for Set 1550-1 58 Plastic Stickers ## 5 003432 Sticker Sheet for Sets 357-… 58 Plastic Stickers ## 6 003434 Sticker Sheet for Set 575-2… 58 Plastic Stickers ## 7 003435 Sticker Sheet for Set 687-1 58 Plastic Stickers ## 8 003436 Sticker Sheet for Set 180-1 58 Plastic Stickers ## 9 003437 Sticker Sheet for Set 181-1 58 Plastic Stickers ## 10 003438 Sticker Sheet for Set 131-1 58 Plastic Stickers ## # ℹ 52,590 more rows 5.1.2 Join with a one-to-many relationship This is an example of a one-to-many relationship. And by the same variable. Notice that the table increased in the number of rows after the join. # Combine the parts and inventory_parts tables parts %&gt;% inner_join(inventory_parts, by = &quot;part_num&quot;) ## # A tibble: 1,179,869 × 9 ## part_num name part_cat_id part_material inventory_id color_id quantity ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 003381 Sticker Sh… 58 Plastic 15865 9999 1 ## 2 003383 Sticker Sh… 58 Plastic 8376 9999 1 ## 3 003383 Sticker Sh… 58 Plastic 11738 9999 1 ## 4 003402 Sticker Sh… 58 Plastic 470 9999 1 ## 5 003402 Sticker Sh… 58 Plastic 885 9999 1 ## 6 003402 Sticker Sh… 58 Plastic 12659 9999 1 ## 7 003429 Sticker Sh… 58 Plastic 3993 9999 1 ## 8 003432 Sticker Sh… 58 Plastic 8191 9999 1 ## 9 003434 Sticker Sh… 58 Plastic 8756 9999 1 ## 10 003434 Sticker Sh… 58 Plastic 12826 9999 1 ## # ℹ 1,179,859 more rows ## # ℹ 2 more variables: is_spare &lt;lgl&gt;, img_url &lt;chr&gt; 5.1.3 Join in either direction An inner_join works the same way with either table in either position. The table that is specified first is arbitrary, since you will end up with the same information in the resulting table either way.(table先後順序不同所得結果一樣) # Combine the parts and inventory_parts tables inventory_parts %&gt;% inner_join(parts, by = &quot;part_num&quot;) ## # A tibble: 1,179,869 × 9 ## inventory_id part_num color_id quantity is_spare img_url name part_cat_id ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 48379c01 72 1 FALSE https:… Larg… 41 ## 2 1 48395 7 1 FALSE https:… Spor… 27 ## 3 1 stickerupn… 9999 1 FALSE &lt;NA&gt; Stic… 58 ## 4 1 upn0342 0 1 FALSE &lt;NA&gt; Spor… 27 ## 5 1 upn0350 25 1 FALSE &lt;NA&gt; Spor… 13 ## 6 3 2343 47 1 FALSE https:… Equi… 27 ## 7 3 3003 29 1 FALSE https:… Bric… 11 ## 8 3 30176 2 1 FALSE https:… Plan… 28 ## 9 3 3020 15 1 FALSE https:… Plat… 14 ## 10 3 3022 15 2 FALSE https:… Plat… 14 ## # ℹ 1,179,859 more rows ## # ℹ 1 more variable: part_material &lt;chr&gt; This is the same join as the last exercise, but the order of the tables is reversed. For an inner_join, either direction will yield a table that contains the same information! Note that the columns will appear in a different order depending on which table comes first. 5.1.4 Join three or more tables sets %&gt;% # Add inventories using an inner join inner_join(inventories, by = &quot;set_num&quot;) %&gt;% # Add inventory_parts using an inner join inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) ## # A tibble: 1,113,729 × 13 ## set_num name year theme_id num_parts img_url.x id version part_num ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 132a ## 2 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 3020 ## 3 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 3062c ## 4 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 3404bc01 ## 5 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 36 ## 6 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 7039 ## 7 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 7049bc01 ## 8 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 715 ## 9 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 741 ## 10 001-1 Gears 1965 1 43 https://cdn.re… 24696 1 742 ## # ℹ 1,113,719 more rows ## # ℹ 4 more variables: color_id &lt;dbl&gt;, quantity &lt;dbl&gt;, is_spare &lt;lgl&gt;, ## # img_url.y &lt;chr&gt; # Count the number of colors and sort sets %&gt;% inner_join(inventories, by = &quot;set_num&quot;) %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_color&quot;)) %&gt;% count(name_color) %&gt;% arrange(desc(n)) ## # A tibble: 250 × 2 ## name_color n ## &lt;chr&gt; &lt;int&gt; ## 1 Black 198156 ## 2 White 126555 ## 3 Light Bluish Gray 121453 ## 4 Dark Bluish Gray 89669 ## 5 Red 84521 ## 6 Yellow 57755 ## 7 Blue 45305 ## 8 Reddish Brown 37606 ## 9 Tan 34831 ## 10 Light Gray 28104 ## # ℹ 240 more rows 5.2 Left &amp; Right Joins 5.2.1 Left join Left joining two sets by part and color Each of these observations isn’t just a part, but a combination of a part and a color. Notice, you can specify this with by = c(\"var1\", \"var2\"). That specifies we want to join on both columns. # Prepare tables inventory_parts_joined &lt;- inventories %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% select(-id, -version) %&gt;% arrange(desc(quantity)) millennium_falcon &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;7965-1&quot;) star_destroyer &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;75190-1&quot;) # Combine the star_destroyer and millennium_falcon tables millennium_falcon %&gt;% left_join(star_destroyer, by = c(&quot;part_num&quot;, &quot;color_id&quot;), suffix = c(&quot;_falcon&quot;, &quot;_star_destroyer&quot;)) ## Warning in left_join(., star_destroyer, by = c(&quot;part_num&quot;, &quot;color_id&quot;), : Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 4 of `x` matches multiple rows in `y`. ## ℹ Row 3 of `y` matches multiple rows in `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. ## # A tibble: 247 × 10 ## set_num_falcon part_num color_id quantity_falcon is_spare_falcon ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; ## 1 7965-1 63868 71 62 FALSE ## 2 7965-1 3023 0 60 FALSE ## 3 7965-1 3021 72 46 FALSE ## 4 7965-1 2780 0 37 FALSE ## 5 7965-1 2780 0 37 FALSE ## 6 7965-1 60478 72 36 FALSE ## 7 7965-1 6636 71 34 FALSE ## 8 7965-1 3009 71 28 FALSE ## 9 7965-1 3665 71 22 FALSE ## 10 7965-1 2412b 72 20 FALSE ## # ℹ 237 more rows ## # ℹ 5 more variables: img_url_falcon &lt;chr&gt;, set_num_star_destroyer &lt;chr&gt;, ## # quantity_star_destroyer &lt;dbl&gt;, is_spare_star_destroyer &lt;lgl&gt;, ## # img_url_star_destroyer &lt;chr&gt; Left joining two sets by color # Aggregate Millennium Falcon for the total quantity in each part millennium_falcon_colors &lt;- millennium_falcon %&gt;% group_by(color_id) %&gt;% summarize(total_quantity = sum(quantity)) # Aggregate Star Destroyer for the total quantity in each part star_destroyer_colors &lt;- star_destroyer %&gt;% group_by(color_id) %&gt;% summarize(total_quantity = sum(quantity)) # Left join the Millennium Falcon colors to the Star Destroyer colors millennium_falcon_colors %&gt;% left_join(star_destroyer_colors, by = &quot;color_id&quot;, suffix = c(&quot;_falcon&quot;, &quot;_star_destroyer&quot;)) ## # A tibble: 19 × 3 ## color_id total_quantity_falcon total_quantity_star_destroyer ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 196 327 ## 2 1 15 24 ## 3 4 17 56 ## 4 14 3 5 ## 5 15 12 13 ## 6 19 91 13 ## 7 28 3 16 ## 8 33 5 NA ## 9 36 1 14 ## 10 41 6 16 ## 11 47 5 NA ## 12 70 6 1 ## 13 71 586 533 ## 14 72 282 373 ## 15 80 3 NA ## 16 179 2 NA ## 17 320 12 2 ## 18 1103 1 NA ## 19 9999 1 1 Finding an observation that doesn’t have a match For example, the inventories table has a version column, for when a LEGO kit gets some kind of change or upgrade. It would be fair to assume that all sets (which joins well with inventories) would have at least a version 1. And use the replace_na, which takes a list of column names and the values with which NAs should be replaced, to clean up our table. replace_na(list(colname = replace_value)) inventory_version_1 &lt;- inventories %&gt;% filter(version == 1) colnames(inventory_version_1) ## [1] &quot;id&quot; &quot;version&quot; &quot;set_num&quot; colnames(sets) ## [1] &quot;set_num&quot; &quot;name&quot; &quot;year&quot; &quot;theme_id&quot; &quot;num_parts&quot; &quot;img_url&quot; # Join versions to sets sets %&gt;% left_join(inventory_version_1, by = &quot;set_num&quot;) %&gt;% # Filter for where version is na filter(is.na(version)) %&gt;% # Use replace_na to replace missing values in the version column replace_na(list(version = 0)) ## # A tibble: 3 × 8 ## set_num name year theme_id num_parts img_url id version ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10875-1 Cargo Train 2018 634 105 https:… NA 0 ## 2 76081-1 The Milano vs. The Abi… 2017 704 462 https:… NA 0 ## 3 S1-1 Baseplate with steerin… 1970 453 1 https:… NA 0 5.2.2 Right join In this exercise, we’ll count the part_cat_id from parts, before using a right_join to join with part_categories. The reason we do this is because we don’t only want to know the count of part_cat_id in parts, but we also want to know if there are any part_cat_ids not present in parts. parts %&gt;% # Count the part_cat_id count(part_cat_id) %&gt;% # Right join part_categories right_join(part_categories, by = c(&quot;part_cat_id&quot; = &quot;id&quot;)) %&gt;% # Filter for NA filter(is.na(n)) ## # A tibble: 0 × 3 ## # ℹ 3 variables: part_cat_id &lt;dbl&gt;, n &lt;int&gt;, name &lt;chr&gt; Joining tables to themselves In the themes table, you’ll notice there is both an id column and a parent_id column. Keeping that in mind, you can join the themes table to itself to determine the parent-child relationships that exist for different themes. Joining themes to their children In this exercise, you’ll try a similar approach of joining themes to their own children, which is similar but reversed. themes %&gt;% # Inner join the themes table inner_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% # Filter for the &quot;Harry Potter&quot; parent name filter(name_parent == &quot;Harry Potter&quot;) ## # A tibble: 1 × 5 ## id name_parent parent_id id_child name_child ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 246 Harry Potter NA 667 Fantastic Beasts Joining themes to their grandchildren Some themes actually have grandchildren: their children’s children. Here, we can inner join themes to a filtered version of itself again to establish a connection between our last join’s children and their children. # Join themes to itself again to find the grandchild relationships themes %&gt;% inner_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% inner_join(themes, by = c(&quot;id_child&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_grandchild&quot;)) ## # A tibble: 23 × 7 ## id_parent name_parent parent_id id_child name_child id_grandchild name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 206 Seasonal NA 207 Advent 208 City ## 2 206 Seasonal NA 207 Advent 209 Star Wars ## 3 206 Seasonal NA 207 Advent 210 Belville ## 4 206 Seasonal NA 207 Advent 211 Castle ## 5 206 Seasonal NA 207 Advent 212 Classic Ba… ## 6 206 Seasonal NA 207 Advent 213 Clikits ## 7 206 Seasonal NA 207 Advent 214 Creator ## 8 206 Seasonal NA 207 Advent 215 Pirates ## 9 206 Seasonal NA 207 Advent 216 Friends ## 10 206 Seasonal NA 207 Advent 710 Harry Pott… ## # ℹ 13 more rows Left joining a table to itself themes %&gt;% # Left join the themes table to its own children left_join(themes, by = c(&quot;id&quot; = &quot;parent_id&quot;), suffix = c(&quot;_parent&quot;, &quot;_child&quot;)) %&gt;% # Filter for themes that have no child themes filter(is.na(name_child)) ## # A tibble: 412 × 5 ## id name_parent parent_id id_child name_child ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 3 Competition 1 NA &lt;NA&gt; ## 2 4 Expert Builder 1 NA &lt;NA&gt; ## 3 16 RoboRiders 1 NA &lt;NA&gt; ## 4 17 Speed Slammers 1 NA &lt;NA&gt; ## 5 18 Star Wars 1 NA &lt;NA&gt; ## 6 19 Supplemental 1 NA &lt;NA&gt; ## 7 20 Throwbot Slizer 1 NA &lt;NA&gt; ## 8 21 Universal Building Set 1 NA &lt;NA&gt; ## 9 23 Basic Model 22 NA &lt;NA&gt; ## 10 35 Bricks &amp; More 34 NA &lt;NA&gt; ## # ℹ 402 more rows 5.3 Full, Semi &amp; Anti Joins 5.3.1 Full join # keep all both batmobile %&gt;% full_join(batwing, by = c(&quot;part_num&quot;, &quot;color_id&quot;), suffix = c(&quot;_batmobile&quot;, &quot;_batwing&quot;)) Differences between Batman and Star Wars Now, you’ll compare two themes, each of which is made up of many sets. Since each theme is made up of many sets, combining these tables is the first step towards being able to compare different themes. inventory_parts_joined &lt;- inventories %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% arrange(desc(quantity)) %&gt;% select(-id, -version) # Start with inventory_parts_joined table inventory_sets_themes &lt;- inventory_parts_joined %&gt;% # Combine with the sets table inner_join(sets, by = &quot;set_num&quot;) %&gt;% # Combine with the themes table inner_join(themes, by = c(&quot;theme_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_theme&quot;)) inventory_sets_themes ## # A tibble: 1,113,729 × 13 ## set_num part_num color_id quantity is_spare img_url.x name_set year theme_id ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 31203-1 6141 15 3064 FALSE https://… World M… 2021 709 ## 2 31203-1 98138 3 1879 FALSE https://… World M… 2021 709 ## 3 31203-1 98138 322 1607 FALSE https://… World M… 2021 709 ## 4 k34432… 3024 15 1440 FALSE https://… Lego Mo… 2003 277 ## 5 k34433… 3024 15 1170 FALSE https://… Lego Mo… 2003 277 ## 6 31203-1 98138 27 1060 FALSE https://… World M… 2021 709 ## 7 40179-1 3024 14 900 FALSE https://… Persona… 2016 277 ## 8 40179-1 3024 71 900 FALSE https://… Persona… 2016 277 ## 9 40179-1 3024 72 900 FALSE https://… Persona… 2016 277 ## 10 40179-1 3024 15 900 FALSE https://… Persona… 2016 277 ## # ℹ 1,113,719 more rows ## # ℹ 4 more variables: num_parts &lt;dbl&gt;, img_url.y &lt;chr&gt;, name_theme &lt;chr&gt;, ## # parent_id &lt;dbl&gt; Aggregating each theme Before doing this comparison, you’ll want to aggregate the data to learn more about the pieces that are a part of each theme, as well as the colors of those pieces. # filtered for each theme batman &lt;- inventory_sets_themes %&gt;% filter(name_theme == &quot;Batman&quot;) star_wars &lt;- inventory_sets_themes %&gt;% filter(name_theme == &quot;Star Wars&quot;) # Count the part number and color id, weight by quantity batman_parts &lt;- batman %&gt;% count(part_num, color_id, wt = quantity); batman_parts ## # A tibble: 3,455 × 3 ## part_num color_id n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10100596 9999 1 ## 2 10104735 9999 1 ## 3 10104995 9999 1 ## 4 10172 179 1 ## 5 10172 297 1 ## 6 10187 179 3 ## 7 10187 297 3 ## 8 10190 0 8 ## 9 10201 4 3 ## 10 10201 14 5 ## # ℹ 3,445 more rows star_wars_parts &lt;- star_wars %&gt;% count(part_num, color_id, wt = quantity); star_wars_parts ## # A tibble: 9,081 × 3 ## part_num color_id n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 01571 9999 1 ## 2 01709 9999 1 ## 3 01832 9999 1 ## 4 10001722 9999 1 ## 5 10001831 9999 1 ## 6 10050 1063 4 ## 7 10050 1103 1 ## 8 10100101 9999 1 ## 9 10100763 9999 1 ## 10 10100764 9999 1 ## # ℹ 9,071 more rows Full joining Batman and Star Wars LEGO parts Now that you’ve got separate tables for the pieces in the batman and star_wars themes, you’ll want to be able to combine them to see any similarities or differences between the two themes. parts_joined &lt;- batman_parts %&gt;% # Combine the star_wars_parts table full_join(star_wars_parts, by = c(&quot;part_num&quot;, &quot;color_id&quot;), suffix = c(&quot;_batman&quot;, &quot;_star_wars&quot;)) %&gt;% # Replace NAs with 0s in the n_batman and n_star_wars columns replace_na(list(n_batman = 0, n_star_wars = 0)) parts_joined ## # A tibble: 9,964 × 4 ## part_num color_id n_batman n_star_wars ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 10100596 9999 1 0 ## 2 10104735 9999 1 0 ## 3 10104995 9999 1 0 ## 4 10172 179 1 0 ## 5 10172 297 1 0 ## 6 10187 179 3 0 ## 7 10187 297 3 3 ## 8 10190 0 8 3 ## 9 10201 4 3 7 ## 10 10201 14 5 2 ## # ℹ 9,954 more rows Comparing Batman and Star Wars LEGO parts However, we have more information about each of these parts that we can gain by combining this table with some of the information we have in other tables. parts_joined %&gt;% # Sort the number of star wars pieces in descending order arrange(desc(n_star_wars)) %&gt;% # Join the colors table to the parts_joined table inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;)) %&gt;% # Join the parts table to the previous join inner_join(parts, by = &quot;part_num&quot;, suffix = c(&quot;_color&quot;, &quot;_part&quot;)) ## # A tibble: 9,964 × 10 ## part_num color_id n_batman n_star_wars name_color rgb is_trans name_part ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 2780 0 377 5200 Black 0513… FALSE Technic … ## 2 4274 1 166 2052 Blue 0055… FALSE Technic … ## 3 3023 71 101 1923 Light Bluish… A0A5… FALSE Plate 1 … ## 4 3023 72 170 1724 Dark Bluish … 6C6E… FALSE Plate 1 … ## 5 3023 0 240 1457 Black 0513… FALSE Plate 1 … ## 6 43093 1 95 1394 Blue 0055… FALSE Technic … ## 7 6141 36 86 1246 Trans-Red C91A… TRUE Plate Ro… ## 8 2412b 72 83 1245 Dark Bluish … 6C6E… FALSE Tile Spe… ## 9 6141 72 94 1178 Dark Bluish … 6C6E… FALSE Plate Ro… ## 10 6558 1 33 1133 Blue 0055… FALSE Technic … ## # ℹ 9,954 more rows ## # ℹ 2 more variables: part_cat_id &lt;dbl&gt;, part_material &lt;chr&gt; 5.3.2 Semi &amp; Anti joins Something within one set but not another Determine which parts are in both the batwing and batmobile sets, and which sets are in one, but not the other. # Two sets batmobile &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;7784-1&quot;) %&gt;% select(-set_num) batwing &lt;- inventory_parts_joined %&gt;% filter(set_num == &quot;70916-1&quot;) %&gt;% select(-set_num) # Filter the batwing set for parts that are also in the batmobile set batwing %&gt;% semi_join(batmobile, by = &quot;part_num&quot;) ## # A tibble: 142 × 5 ## part_num color_id quantity is_spare img_url ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 3023 0 22 FALSE https://cdn.rebrickable.com/media/parts/… ## 2 3024 0 22 FALSE https://cdn.rebrickable.com/media/parts/… ## 3 3623 0 20 FALSE https://cdn.rebrickable.com/media/parts/… ## 4 2780 0 17 FALSE https://cdn.rebrickable.com/media/parts/… ## 5 3666 0 16 FALSE https://cdn.rebrickable.com/media/parts/… ## 6 3710 0 14 FALSE https://cdn.rebrickable.com/media/parts/… ## 7 6141 4 12 FALSE https://cdn.rebrickable.com/media/parts/… ## 8 2412b 71 10 FALSE https://cdn.rebrickable.com/media/parts/… ## 9 6141 72 10 FALSE https://cdn.rebrickable.com/media/parts/… ## 10 6558 1 9 FALSE https://cdn.rebrickable.com/media/parts/… ## # ℹ 132 more rows # Filter the batwing set for parts that aren&#39;t in the batmobile set batwing %&gt;% anti_join(batmobile, by = &quot;part_num&quot;) ## # A tibble: 181 × 5 ## part_num color_id quantity is_spare img_url ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;chr&gt; ## 1 11477 0 18 FALSE https://cdn.rebrickable.com/media/parts/… ## 2 99207 71 18 FALSE https://cdn.rebrickable.com/media/parts/… ## 3 22385 0 14 FALSE https://cdn.rebrickable.com/media/parts/… ## 4 99563 0 13 FALSE https://cdn.rebrickable.com/media/parts/… ## 5 10247 72 12 FALSE https://cdn.rebrickable.com/media/parts/… ## 6 2877 72 12 FALSE https://cdn.rebrickable.com/media/parts/… ## 7 61409 72 12 FALSE https://cdn.rebrickable.com/media/parts/… ## 8 11153 0 10 FALSE https://cdn.rebrickable.com/media/parts/… ## 9 98138 46 10 FALSE https://cdn.rebrickable.com/media/parts/… ## 10 2419 72 9 FALSE https://cdn.rebrickable.com/media/parts/… ## # ℹ 171 more rows What colors are included in at least one set? you could also use a filtering join like semi_join to find out which colors ever appear in any inventory part. # Use inventory_parts to find colors included in at least one set colors %&gt;% semi_join(inventory_parts, by = c(&quot;id&quot; = &quot;color_id&quot;)) ## # A tibble: 250 × 4 ## id name rgb is_trans ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 -1 [Unknown] 0033B2 FALSE ## 2 0 Black 05131D FALSE ## 3 1 Blue 0055BF FALSE ## 4 2 Green 237841 FALSE ## 5 3 Dark Turquoise 008F9B FALSE ## 6 4 Red C91A09 FALSE ## 7 5 Dark Pink C870A0 FALSE ## 8 6 Brown 583927 FALSE ## 9 7 Light Gray 9BA19D FALSE ## 10 8 Dark Gray 6D6E5C FALSE ## # ℹ 240 more rows Which set is missing version 1? Let’s start by looking at the first version of each set to see if there are any sets that don’t include a first version. # Use filter() to extract version 1 version_1_inventories &lt;- inventories %&gt;% filter(version == 1); version_1_inventories ## # A tibble: 35,612 × 3 ## id version set_num ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 1 7922-1 ## 2 3 1 3931-1 ## 3 4 1 6942-1 ## 4 15 1 5158-1 ## 5 16 1 903-1 ## 6 17 1 850950-1 ## 7 19 1 4444-1 ## 8 21 1 3474-1 ## 9 22 1 30277-1 ## 10 25 1 71012-11 ## # ℹ 35,602 more rows # Use anti_join() to find which set is missing a version 1 sets %&gt;% anti_join(version_1_inventories, by = &quot;set_num&quot;) ## # A tibble: 3 × 6 ## set_num name year theme_id num_parts img_url ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 10875-1 Cargo Train 2018 634 105 https:… ## 2 76081-1 The Milano vs. The Abilisk 2017 704 462 https:… ## 3 S1-1 Baseplate with steering control tong… 1970 453 1 https:… This is likely a data quality issue, and anti_join is a great tool for finding problems like that. 5.3.3 Visualizing set differences Aggregating sets to look at their differences To compare two individual sets, and the kinds of LEGO pieces that comprise them, we’ll need to aggregate the data into separate themes. In addition to being able to view the sets for Batman and Star Wars separately, adding the column also allowed us to be able to look at the fraction differences between the sets, rather than only being able to compare the numbers of pieces. inventory_parts_themes &lt;- inventories %&gt;% inner_join(inventory_parts, by = c(&quot;id&quot; = &quot;inventory_id&quot;)) %&gt;% arrange(desc(quantity)) %&gt;% select(-id, -version) %&gt;% inner_join(sets, by = &quot;set_num&quot;) %&gt;% inner_join(themes, by = c(&quot;theme_id&quot; = &quot;id&quot;), suffix = c(&quot;_set&quot;, &quot;_theme&quot;)) batman_colors &lt;- inventory_parts_themes %&gt;% # Filter the inventory_parts_themes table for the Batman theme filter(name_theme == &quot;Batman&quot;) %&gt;% group_by(color_id) %&gt;% summarize(total = sum(quantity)) %&gt;% # Add a fraction column of the total divided by the sum of the total mutate(fraction = total / sum(total)) # Filter and aggregate the Star Wars set data; add a fraction column star_wars_colors &lt;- inventory_parts_themes %&gt;% filter(name_theme == &quot;Star Wars&quot;) %&gt;% group_by(color_id) %&gt;% summarize(total = sum(quantity)) %&gt;% mutate(fraction = total / sum(total)) Combining sets Prior to visualizing the data, you’ll want to combine these tables to be able to directly compare the themes’ colors. batman_colors %&gt;% # Join the Batman and Star Wars colors full_join(star_wars_colors, by = &quot;color_id&quot;, suffix = c(&quot;_batman&quot;, &quot;_star_wars&quot;)) %&gt;% # Replace NAs in the total_batman and total_star_wars columns replace_na(list(total_batman = 0, total_star_wars = 0)) %&gt;% inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;)) %&gt;% # Create the difference and total columns mutate(difference = fraction_batman - fraction_star_wars, total = total_batman + total_star_wars) %&gt;% # Filter for totals greater than 200 filter(total &gt;= 200) ## # A tibble: 42 × 10 ## color_id total_batman fraction_batman total_star_wars fraction_star_wars ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 9630 0.343 49016 0.184 ## 2 1 495 0.0176 8643 0.0325 ## 3 2 273 0.00972 1300 0.00489 ## 4 4 889 0.0317 7203 0.0271 ## 5 14 962 0.0343 4302 0.0162 ## 6 15 755 0.0269 23618 0.0889 ## 7 19 863 0.0307 11915 0.0448 ## 8 25 166 0.00591 2151 0.00809 ## 9 27 125 0.00445 615 0.00231 ## 10 28 818 0.0291 5194 0.0195 ## # ℹ 32 more rows ## # ℹ 5 more variables: name &lt;chr&gt;, rgb &lt;chr&gt;, is_trans &lt;lgl&gt;, difference &lt;dbl&gt;, ## # total &lt;dbl&gt; Visualizing the difference: Batman and Star Wars Now you’ll create a bar plot with one bar for each color (name), showing the difference in fractions. library(forcats) colors_joined &lt;- batman_colors %&gt;% full_join(star_wars_colors, by = &quot;color_id&quot;, suffix = c(&quot;_batman&quot;, &quot;_star_wars&quot;)) %&gt;% replace_na(list(total_batman = 0, total_star_wars = 0)) %&gt;% inner_join(colors, by = c(&quot;color_id&quot; = &quot;id&quot;)) %&gt;% mutate(difference = fraction_batman - fraction_star_wars, total = total_batman + total_star_wars) %&gt;% filter(total &gt;= 200) %&gt;% replace_na(list(difference = 0)) %&gt;% mutate(name = fct_reorder(name, difference)) # we need to add # to create our palette colors_joined$rgb &lt;- paste(&quot;#&quot;, colors_joined$rgb, sep=&quot;&quot;) color_palette &lt;- setNames(colors_joined$rgb, colors_joined$name) # Create a bar plot using colors_joined and the name and difference columns ggplot(colors_joined, aes(x = name, y = difference, fill = name)) + geom_col() + coord_flip() + scale_fill_manual(values = color_palette, guide = &quot;none&quot;) + labs(y = &quot;Difference: Batman - Star Wars&quot;) 5.4 Case study: Joins on Stack Overflow Data 5.4.1 Load dataset Three of the Stack Overflow survey datasets are questions, question_tags, and tags: questions: an ID and the score, or how many times the question has been upvoted; the data only includes R-based questions. question_tags: a tag ID for each question and the question’s id. tags: a tag id and the tag’s name, which can be used to identify the subject of each question, such as ggplot2 or dplyr. questions &lt;- read_csv(&quot;data/stackoverflow/questions.csv&quot;) question_tags &lt;- read_csv(&quot;data/stackoverflow/question_tags.csv&quot;) tags &lt;- read_csv(&quot;data/stackoverflow/tags.csv&quot;) answers &lt;- read_csv(&quot;data/stackoverflow/answers.csv&quot;) colnames(questions) ## [1] &quot;id&quot; &quot;creation_date&quot; &quot;score&quot; colnames(question_tags) ## [1] &quot;question_id&quot; &quot;tag_id&quot; colnames(tags) ## [1] &quot;id&quot; &quot;tag_name&quot; colnames(answers) ## [1] &quot;id&quot; &quot;creation_date&quot; &quot;question_id&quot; &quot;score&quot; 5.4.2 Stack Overflow questions 5.4.2.1 Left joining questions and tags Note that we’ll be using left_joins in this exercise to ensure we keep all questions, even those without a corresponding tag. However, since we know the questions data is all R data, we’ll want to manually tag these as R questions with replace_na. questions_with_tags &lt;- questions %&gt;% left_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% left_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)) %&gt;% replace_na(list(tag_name = &quot;only-r&quot;)) ## Warning in left_join(., question_tags, by = c(id = &quot;question_id&quot;)): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 1 of `x` matches multiple rows in `y`. ## ℹ Row 328658 of `y` matches multiple rows in ## `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. questions_with_tags ## # A tibble: 809,962 × 5 ## id creation_date score tag_id tag_name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22557677 3/21/2014 1 18 regex ## 2 22557677 3/21/2014 1 139 string ## 3 22557677 3/21/2014 1 16088 time-complexity ## 4 22557677 3/21/2014 1 1672 backreference ## 5 22557677 3/21/2014 1 18 regex ## 6 22557677 3/21/2014 1 139 string ## 7 22557677 3/21/2014 1 6088 hosts ## 8 22557677 3/21/2014 1 1672 backreference ## 9 22557707 3/21/2014 2 NA only-r ## 10 22558084 3/21/2014 2 6419 time-series ## # ℹ 809,952 more rows 5.4.2.2 Comparing scores across tags questions_with_tags %&gt;% # Group by tag_name group_by(tag_name) %&gt;% # Get mean score and num_questions (total number of questions: n()) summarize(score = mean(score), num_questions = n()) %&gt;% # Sort num_questions in descending order arrange(desc(num_questions)) ## # A tibble: 7,842 × 3 ## tag_name score num_questions ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 only-r 1.17 64166 ## 2 ggplot2 2.33 42627 ## 3 dataframe 2.22 28183 ## 4 dplyr 1.66 23227 ## 5 shiny 1.28 22685 ## 6 plot 2.15 15844 ## 7 data.table 2.55 12961 ## 8 matrix 1.59 8810 ## 9 loops 0.711 7728 ## 10 regex 1.79 7269 ## # ℹ 7,832 more rows 5.4.2.3 What tags never appear on R questions The tags table includes all Stack Overflow tags, but some have nothing to do with R. How could you filter for just the tags that never appear on an R question? The tags and question_tags tables have been preloaded for you. # Using a join, filter for tags that are never on an R question tags %&gt;% anti_join(question_tags, by = c(&quot;id&quot; = &quot;tag_id&quot;)) ## # A tibble: 40,458 × 2 ## id tag_name ## &lt;dbl&gt; &lt;chr&gt; ## 1 124399 laravel-dusk ## 2 124402 spring-cloud-vault-config ## 3 124404 spring-vault ## 4 124405 apache-bahir ## 5 124407 astc ## 6 124408 simulacrum ## 7 124410 angulartics2 ## 8 124411 django-rest-viewsets ## 9 124414 react-native-lightbox ## 10 124417 java-module ## # ℹ 40,448 more rows 5.4.3 Joining questions and answers 5.4.3.1 Finding gaps between questions and answers Now we’ll join together questions with answers so we can measure the time between questions and answers. questions %&gt;% # Inner join questions and answers with proper suffixes inner_join(answers, by = c(&quot;id&quot; = &quot;question_id&quot;), suffix = c(&quot;_question&quot;, &quot;_answer&quot;)) %&gt;% # Subtract creation_date_question from creation_date_answer to create gap mutate(gap = as.integer(as.Date(creation_date_answer, format = &quot;%m/%d/%Y&quot;) - as.Date(creation_date_question, format = &quot;%m/%d/%Y&quot;))) ## Warning in inner_join(., answers, by = c(id = &quot;question_id&quot;), suffix = c(&quot;_question&quot;, : Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 2 of `x` matches multiple rows in `y`. ## ℹ Row 118226 of `y` matches multiple rows in ## `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. ## # A tibble: 592,547 × 7 ## id creation_date_question score_question id_answer creation_date_answer ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22557677 3/21/2014 1 22560670 3/21/2014 ## 2 22557707 3/21/2014 2 22558516 3/21/2014 ## 3 22557707 3/21/2014 2 22558726 3/21/2014 ## 4 22558084 3/21/2014 2 22558085 3/21/2014 ## 5 22558084 3/21/2014 2 22606545 3/24/2014 ## 6 22558084 3/21/2014 2 22610396 3/24/2014 ## 7 22558084 3/21/2014 2 34374729 12/19/2015 ## 8 22558395 3/21/2014 2 22559327 3/21/2014 ## 9 22558395 3/21/2014 2 22560102 3/21/2014 ## 10 22558395 3/21/2014 2 22560288 3/21/2014 ## # ℹ 592,537 more rows ## # ℹ 2 more variables: score_answer &lt;dbl&gt;, gap &lt;int&gt; 5.4.3.2 Joining question and answer counts We can also determine how many questions actually yield answers. If we count the number of answers for each question, we can then join the answers counts with the questions table. # Count and sort the question id column in the answers table answer_counts &lt;- answers %&gt;% count(question_id, sort = TRUE) # Combine the answer_counts and questions tables question_answer_counts &lt;- questions %&gt;% left_join(answer_counts, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% # Replace the NAs in the n column replace_na(list(n = 0)); question_answer_counts ## # A tibble: 394,732 × 4 ## id creation_date score n ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 22557677 3/21/2014 1 1 ## 2 22557707 3/21/2014 2 2 ## 3 22558084 3/21/2014 2 4 ## 4 22558395 3/21/2014 2 3 ## 5 22558613 3/21/2014 0 1 ## 6 22558677 3/21/2014 2 2 ## 7 22558887 3/21/2014 8 1 ## 8 22559180 3/21/2014 1 1 ## 9 22559312 3/21/2014 0 1 ## 10 22559322 3/21/2014 2 5 ## # ℹ 394,722 more rows 5.4.3.3 Joining questions, answers, and tags Let’s build on the last exercise by adding the tags table to our previous joins. This will allow us to do a better job of identifying which R topics get the most traction on Stack Overflow. tagged_answers &lt;- question_answer_counts %&gt;% # Join the question_tags tables inner_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% # Join the tags table inner_join(tags, by =c(&quot;tag_id&quot; = &quot;id&quot;)); tagged_answers ## Warning in inner_join(., question_tags, by = c(id = &quot;question_id&quot;)): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 1 of `x` matches multiple rows in `y`. ## ℹ Row 328658 of `y` matches multiple rows in ## `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. ## # A tibble: 745,796 × 6 ## id creation_date score n tag_id tag_name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22557677 3/21/2014 1 1 18 regex ## 2 22557677 3/21/2014 1 1 139 string ## 3 22557677 3/21/2014 1 1 16088 time-complexity ## 4 22557677 3/21/2014 1 1 1672 backreference ## 5 22557677 3/21/2014 1 1 18 regex ## 6 22557677 3/21/2014 1 1 139 string ## 7 22557677 3/21/2014 1 1 6088 hosts ## 8 22557677 3/21/2014 1 1 1672 backreference ## 9 22558084 3/21/2014 2 4 6419 time-series ## 10 22558084 3/21/2014 2 4 92764 panel-data ## # ℹ 745,786 more rows 5.4.3.4 Average answers by question You can use tagged_answers to determine, on average, how many answers each questions gets. Some of the important variables from this table include: n, the number of answers for each question, and tag_name, the name of each tag associated with each question. tagged_answers %&gt;% # Aggregate by tag_name group_by(tag_name) %&gt;% # Summarize questions and average_answers summarize(questions = n(), average_answers = mean(n)) %&gt;% # Sort the questions in descending order arrange(desc(questions)) ## # A tibble: 7,841 × 3 ## tag_name questions average_answers ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 ggplot2 42627 1.29 ## 2 dataframe 28183 1.88 ## 3 dplyr 23227 1.87 ## 4 shiny 22685 1.07 ## 5 plot 15844 1.32 ## 6 data.table 12961 1.65 ## 7 matrix 8810 1.55 ## 8 loops 7728 1.55 ## 9 regex 7269 2.15 ## 10 function 7218 1.50 ## # ℹ 7,831 more rows 5.4.4 The bind_rows verb # 增加row = rbind() bind_rows() #增加column = cbind() bind_cols() bind_rows() / rbind() bind_cols() / cbind() 5.4.4.1 Joining questions and answers with tags To learn more about the questions and answers tables, you’ll want to use the question_tags table to understand the tags associated with each question that was asked, and each answer that was provided. You’ll be able to combine these tables using two inner joins on both the questions table and the answers table. # Inner join the question_tags and tags tables with the questions table questions_with_tags &lt;- questions %&gt;% inner_join(question_tags, by = c(&quot;id&quot; = &quot;question_id&quot;)) %&gt;% inner_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)); questions_with_tags ## Warning in inner_join(., question_tags, by = c(id = &quot;question_id&quot;)): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 1 of `x` matches multiple rows in `y`. ## ℹ Row 328658 of `y` matches multiple rows in ## `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. ## # A tibble: 745,796 × 5 ## id creation_date score tag_id tag_name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 22557677 3/21/2014 1 18 regex ## 2 22557677 3/21/2014 1 139 string ## 3 22557677 3/21/2014 1 16088 time-complexity ## 4 22557677 3/21/2014 1 1672 backreference ## 5 22557677 3/21/2014 1 18 regex ## 6 22557677 3/21/2014 1 139 string ## 7 22557677 3/21/2014 1 6088 hosts ## 8 22557677 3/21/2014 1 1672 backreference ## 9 22558084 3/21/2014 2 6419 time-series ## 10 22558084 3/21/2014 2 92764 panel-data ## # ℹ 745,786 more rows # Inner join the question_tags and tags tables with the answers table answers_with_tags &lt;- answers %&gt;% inner_join(question_tags, by = &quot;question_id&quot;) %&gt;% inner_join(tags, by = c(&quot;tag_id&quot; = &quot;id&quot;)); answers_with_tags ## Warning in inner_join(., question_tags, by = &quot;question_id&quot;): Detected an unexpected many-to-many ## relationship between `x` and `y`. ## ℹ Row 3 of `x` matches multiple rows in `y`. ## ℹ Row 231352 of `y` matches multiple rows in ## `x`. ## ℹ If a many-to-many relationship is expected, ## set `relationship = &quot;many-to-many&quot;` to ## silence this warning. ## # A tibble: 800,378 × 6 ## id creation_date question_id score tag_id tag_name ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 39143935 8/25/2016 39142481 0 4240 average ## 2 39143935 8/25/2016 39142481 0 5571 summary ## 3 39144014 8/25/2016 39024390 0 85748 shiny ## 4 39144014 8/25/2016 39024390 0 83308 r-markdown ## 5 39144014 8/25/2016 39024390 0 116736 htmlwidgets ## 6 39144252 8/25/2016 39096741 6 67746 rstudio ## 7 39144375 8/25/2016 39143885 5 105113 data.table ## 8 39144430 8/25/2016 39144077 0 276 variables ## 9 39144625 8/25/2016 39142728 1 46457 dataframe ## 10 39144625 8/25/2016 39142728 1 9047 subset ## # ℹ 800,368 more rows 5.4.4.2 Binding and counting posts with tags First, you’ll want to combine these tables into a single table called posts_with_tags. Once the information is consolidated into a single table, you can add more information by creating a date variable using the lubridate package. # Combine the two tables into posts_with_tags posts_with_tags &lt;- bind_rows(questions_with_tags %&gt;% mutate(type = &quot;question&quot;), answers_with_tags %&gt;% mutate(type = &quot;answer&quot;)) # Add a year column, then count by type, year, and tag_name by_type_year_tag &lt;- posts_with_tags %&gt;% mutate(year = year(as.Date(creation_date, format = &quot;%m/%d/%Y&quot;))) %&gt;% count(type, year, tag_name); by_type_year_tag ## # A tibble: 57,331 × 4 ## type year tag_name n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 answer 2008 bayesian 1 ## 2 answer 2008 dataframe 3 ## 3 answer 2008 dirichlet 1 ## 4 answer 2008 eof 1 ## 5 answer 2008 file 1 ## 6 answer 2008 file-io 1 ## 7 answer 2008 function 6 ## 8 answer 2008 global-variables 6 ## 9 answer 2008 math 2 ## 10 answer 2008 mathematical-optimization 1 ## # ℹ 57,321 more rows 5.4.4.3 Visualizing questions and answers in tags Let’s create a plot to examine the information that the table contains about questions and answers for the dplyr and ggplot2 tags. # Filter for the dplyr and ggplot2 tag names by_type_year_tag_filtered &lt;- by_type_year_tag %&gt;% filter(tag_name %in% c(&quot;dplyr&quot;, &quot;ggplot2&quot;)); by_type_year_tag_filtered ## # A tibble: 38 × 4 ## type year tag_name n ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; ## 1 answer 2009 ggplot2 57 ## 2 answer 2010 ggplot2 381 ## 3 answer 2011 ggplot2 864 ## 4 answer 2012 dplyr 8 ## 5 answer 2012 ggplot2 1734 ## 6 answer 2013 dplyr 2 ## 7 answer 2013 ggplot2 2678 ## 8 answer 2014 dplyr 826 ## 9 answer 2014 ggplot2 3006 ## 10 answer 2015 dplyr 3121 ## # ℹ 28 more rows # Create a line plot faceted by the tag name ggplot(by_type_year_tag_filtered, aes(x = year, y = n, color = type)) + geom_line() + facet_wrap(~ tag_name) Notice answers on dplyr questions are growing faster than dplyr questions themselves; meaning the average dplyr question has more answers than the average ggplot2 question. "],["introduction-to-statistics.html", "Chapter 6 Introduction to Statistics 6.1 Summary Statistics 6.2 Random Numbers and Probability 6.3 More Distributions &amp; Central Limit Theorem 6.4 Correlation and Experimental Design", " Chapter 6 Introduction to Statistics 6.1 Summary Statistics 6.1.1 Measures of center Mean: sum of all the data points divided by the total number of data points. Median: middle value of the dataset where 50% of the data is less than the median, and 50% of the data is greater than the median. Mode: most frequent value. You’ll be working with the 2018 Food Carbon Footprint Index from nu3. The food_consumption dataset contains information about the kilograms of food consumed per person per year in each country in each food category (consumption) as well as information about the carbon footprint of that food category (co2_emissions) measured in kilograms of carbon dioxide, or CO2, per person per year in each country. #library package library(tidyverse) # Load dataset food_consumption &lt;- read_rds(&quot;data/food_consumption.rds&quot;) When you want to compare summary statistics between groups, it’s much easier to do a group_by() and one summarize() instead of filtering and calling the same functions multiple times. food_consumption %&gt;% # Filter for Belgium and USA filter(country %in% c(&quot;Belgium&quot;, &quot;USA&quot;)) %&gt;% # Group by country group_by(country) %&gt;% # Get mean_consumption and median_consumption summarize(mean_consumption = mean(consumption), median_consumption = median(consumption)) ## # A tibble: 2 × 3 ## country mean_consumption median_consumption ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Belgium 42.1 12.6 ## 2 USA 44.6 14.6 6.1.1.1 Mean vs. median Visualizing Right-skewed food_consumption %&gt;% # Filter for rice food category filter(food_category == &quot;rice&quot;) %&gt;% # Create histogram of co2_emission ggplot(aes(x = co2_emission)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. Central tendency Given the skew of this data, the measure median of central tendency best summarizes the kilograms of CO2 emissions per person per year for rice. food_consumption %&gt;% # Filter for rice food category filter(food_category == &quot;rice&quot;) %&gt;% # Get mean_co2 and median_co2 summarize(mean_co2 = mean(co2_emission), median_co2 = median(co2_emission)) ## # A tibble: 1 × 2 ## mean_co2 median_co2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 37.6 15.2 6.1.2 Measures of spread Useful functions: Variance = var() Standard Deviation = sd() Quartiles = quantile() / quantile(data, probs = ()) Mean absolute deviation = mean(abs(dist)) Boxplots use quartiles 6.1.2.1 Quartiles, quantiles, and quintiles Quantiles are a great way of summarizing numerical data since they can be used to measure center and spread, as well as to get a sense of where a data point stands in relation to the rest of the dataset. # Calculate the quartiles of co2_emission quantile(food_consumption$co2_emission) ## 0% 25% 50% 75% 100% ## 0.0000 5.2100 16.5300 62.5975 1712.0000 # Calculate the six quantiles that split up the data into 5 pieces (quintiles) of the co2_emission column quantile(food_consumption$co2_emission, probs = seq(0, 1, 0.2)) ## 0% 20% 40% 60% 80% 100% ## 0.000 3.540 11.026 25.590 99.978 1712.000 # Calculate the eleven quantiles of co2_emission that split up the data into ten pieces (deciles) quantile(food_consumption$co2_emission, probs = seq(0, 1, 0.1)) ## 0% 10% 20% 30% 40% 50% 60% 70% ## 0.000 0.668 3.540 7.040 11.026 16.530 25.590 44.271 ## 80% 90% 100% ## 99.978 203.629 1712.000 6.1.2.2 Variance &amp; standard deviation Variance and standard deviation are two of the most common ways to measure the spread of a variable, especially when making predictions. # Calculate variance and sd of co2_emission for each food_category food_consumption %&gt;% group_by(food_category) %&gt;% summarize(var_co2 = var(co2_emission), sd_co2 = sd(co2_emission)) ## # A tibble: 11 × 3 ## food_category var_co2 sd_co2 ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 beef 88748. 298. ## 2 eggs 21.4 4.62 ## 3 fish 922. 30.4 ## 4 lamb_goat 16476. 128. ## 5 dairy 17672. 133. ## 6 nuts 35.6 5.97 ## 7 pork 3095. 55.6 ## 8 poultry 245. 15.7 ## 9 rice 2281. 47.8 ## 10 soybeans 0.880 0.938 ## 11 wheat 71.0 8.43 # Plot food_consumption with co2_emission on x-axis ggplot(food_consumption, aes(x = co2_emission)) + # Create a histogram geom_histogram() + # Create a separate sub-graph for each food_category facet_wrap(~ food_category) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. Beef has the biggest amount of variation in its CO2 emissions, while eggs, nuts, and soybeans have relatively small amounts of variation. 6.1.2.3 Finding outliers using IQR Outliers can have big effects on statistics like mean, as well as statistics that rely on the mean, such as variance and standard deviation. Interquartile range, or IQR, is another way of measuring spread that’s less influenced by outliers. IQR is also often used to find outliers. If a value is less than Q1−1.5×IQR or greater than Q3+1.5×IQR, it’s considered an outlier. In fact, this is how the lengths of the whiskers in a ggplot2 box plot are calculated. IQR = Q3 - Q1 Outlier = data &lt; Q1 − 1.5 × IQR or data &gt; Q3 + 1.5 × IQR # Calculate total co2_emission per country: emissions_by_country emissions_by_country &lt;- food_consumption %&gt;% group_by(country) %&gt;% summarize(total_emission = sum(co2_emission)) # Compute the first and third quartiles and IQR of total_emission q1 &lt;- quantile(emissions_by_country$total_emission, 0.25); q1 ## 25% ## 446.66 q3 &lt;- quantile(emissions_by_country$total_emission, 0.75); q3 ## 75% ## 1111.152 iqr &lt;- q3 - q1; iqr ## 75% ## 664.4925 # Calculate the lower and upper cutoffs for outliers lower &lt;- q1 - 1.5 * iqr upper &lt;- q3 + 1.5 * iqr # Filter emissions_by_country to find outliers emissions_by_country %&gt;% filter(total_emission &lt; lower | total_emission &gt; upper) ## # A tibble: 1 × 2 ## country total_emission ## &lt;chr&gt; &lt;dbl&gt; ## 1 Argentina 2172. 6.2 Random Numbers and Probability 6.2.1 Chances 6.2.1.1 Calculating probabilities You want to randomly select a few of the deals that he’s worked on over the past year so that you can look at them more deeply. Before you start selecting deals, you’ll first figure out what the chances are of selecting certain deals. # Load dataset amir_deals &lt;- read_rds(&quot;data/seller_1.rds&quot;) amir_deals %&gt;% # Count the deals for each product count(product) %&gt;% # Calculate probability of picking a deal with each product mutate(prob = n / sum(n)) ## product n prob ## 1 Product A 23 0.12921348 ## 2 Product B 62 0.34831461 ## 3 Product C 15 0.08426966 ## 4 Product D 40 0.22471910 ## 5 Product E 5 0.02808989 ## 6 Product F 11 0.06179775 ## 7 Product G 2 0.01123596 ## 8 Product H 8 0.04494382 ## 9 Product I 7 0.03932584 ## 10 Product J 2 0.01123596 ## 11 Product N 3 0.01685393 6.2.1.2 Sampling deals With or without replacement? With replacement: 抽出後放回, events are independent, sample_n(n, replace = T) Flipping a coin 3 times Rolling a die twice Without replacement: 抽出後不放回, events are dependent, sample_n() From a deck of cards, dealing 3 players 7 cards each Randomly picking 3 people to work on the weekend from a group of 20 people Randomly selecting 5 products from the assembly line to test for quality assurance Now it’s time to randomly pick five deals so that you can reach out to each customer and ask if they were satisfied with the service they received. You’ll try doing this both with and without replacement. Additionally, you want to make sure this is done randomly and that it can be reproduced in case you get asked how you chose the deals, so you’ll need to set the random seed set.seed() before sampling. In this case, without replacement sampling would be suitable. # Set random seed to 31 set.seed(31) # Sample 5 deals without replacement amir_deals %&gt;% sample_n(5) ## product client status amount num_users ## 1 Product D Current Lost 3086.88 55 ## 2 Product C Current Lost 3727.66 19 ## 3 Product D Current Lost 4274.80 9 ## 4 Product B Current Won 4965.08 9 ## 5 Product A Current Won 5827.35 50 # Sample 5 deals with replacement amir_deals %&gt;% sample_n(5, replace = TRUE) ## product client status amount num_users ## 1 Product A Current Won 6010.04 24 ## 2 Product B Current Lost 5701.70 53 ## 3 Product D Current Won 6733.62 27 ## 4 Product F Current Won 6780.85 80 ## 5 Product C Current Won -539.23 11 6.2.2 Discrete distributions 6.2.2.1 Creating a probability distribution A new restaurant opened a few months ago, and the restaurant’s management wants to optimize its seating space based on the size of the groups that come most often. On one night, there are 10 groups of people waiting to be seated at the restaurant, but instead of being called in the order they arrived, they will be called randomly. In this exercise, you’ll investigate the probability of groups of different sizes getting picked first. # create restaurant_groups dataset group_id &lt;- LETTERS[seq(1, 10)] group_size &lt;- c(2, 4, 6, 2, 2, 2, 3, 2, 4, 2) restaurant_groups &lt;- data.frame(group_id, group_size); restaurant_groups ## group_id group_size ## 1 A 2 ## 2 B 4 ## 3 C 6 ## 4 D 2 ## 5 E 2 ## 6 F 2 ## 7 G 3 ## 8 H 2 ## 9 I 4 ## 10 J 2 # Create a histogram of group_size ggplot(restaurant_groups, aes(x = group_size)) + geom_histogram(bins = 5) # Create probability distribution size_distribution &lt;- restaurant_groups %&gt;% # Count number of each group size count(group_size) %&gt;% # Calculate probability mutate(probability = n / sum(n)) size_distribution ## group_size n probability ## 1 2 6 0.6 ## 2 3 1 0.1 ## 3 4 2 0.2 ## 4 6 1 0.1 # Calculate expected group size expected_val &lt;- sum(size_distribution$group_size * size_distribution$probability) expected_val ## [1] 2.9 # Calculate probability of picking group of 4 or more size_distribution %&gt;% # Filter for groups of 4 or larger filter(group_size &gt;= 4) %&gt;% # Calculate prob_4_or_more by taking sum of probabilities summarize(prob_4_or_more = sum(probability)) ## prob_4_or_more ## 1 0.3 6.2.2.2 Law of large numbers As the size of your sample increases, the sample mean will approach the expected value. 6.2.3 Continuous distributions The sales software used at your company is set to automatically back itself up, but no one knows exactly what time the back-ups happen. It is known, however, that back-ups happen exactly every 30 minutes. Amir comes back from sales meetings at random times to update the data on the client he just met with. He wants to know how long he’ll have to wait for his newly-entered data to get backed up. # Min and max wait times for back-up that happens every 30 min min &lt;- 0 max &lt;- 30 # Calculate probability of waiting less than 5 mins prob_less_than_5 &lt;- punif(5, min = 0, max = 30) prob_less_than_5 ## [1] 0.1666667 # Calculate probability of waiting more than 5 mins prob_greater_than_5 &lt;- punif(5, min = 0, max = 30, lower.tail = FALSE) prob_greater_than_5 ## [1] 0.8333333 # Calculate probability of waiting 10-20 mins prob_between_10_and_20 &lt;- punif(20, min = 0, max = 30) - punif(10, min = 0, max = 30) prob_between_10_and_20 ## [1] 0.3333333 To give Amir a better idea of how long he’ll have to wait, you’ll simulate Amir waiting 1000 times and create a histogram to show him what he should expect. Notice: simulation_nb, runif # Create dataset, normal distribution wait_times &lt;- data.frame(simulation_nb = 1:1000) # Set random seed to 334 set.seed(334) # Generate 1000 wait times between 0 and 30 mins, save in time column wait_times %&gt;% mutate(time = runif(1000, min = 0, max = 30)) %&gt;% # Create a histogram of simulated times ggplot(aes(x = time)) + geom_histogram(bins = 30) 6.2.4 Binomial distribution Probability distribution of the number of successes in a sequence of independent trials. n: total number of trials p: probability of success Binary outcomes: rbinom(n of trials, n of coins, probability of heads/success) # 1 = head, 0 = tail # A single flip rbinom(1, 1, 0.5) # 8 flips of 1 coin with 50% chance of success rbinom(8, 1, 0.5) # 10 flips of 3 coin with 25% chance of success rbinom(10, 3, 0.25) # p(head = n) dbinom(num heads, num trials, prob of heads) # p(head &gt;= or &lt;= n) pbinom(num heads, num trials, prob of heads, lower.tail = T or F) 6.2.4.1 Simulating Assume that Amir usually works on 3 deals per week, and overall, he wins 30% of deals he works on. Each deal has a binary outcome: it’s either lost, or won, so you can model his sales deals with a binomial distribution. In this exercise, you’ll help Amir simulate a year’s worth of his deals so he can better understand his performance. # Set random seed to 10 set.seed(10) # Simulate a single deal rbinom(1, 1, 0.3) ## [1] 0 # Simulate 1 week of 3 deals rbinom(1, 3, 0.3) ## [1] 0 # Simulate 52 weeks of 3 deals deals &lt;- rbinom(52, 3, 0.3) # Calculate mean deals won per week mean(deals) ## [1] 0.8076923 6.2.4.2 Calculating binomial probabilities Just as in the last exercise, assume that Amir wins 30% of deals. He wants to get an idea of how likely he is to close a certain number of deals each week. In this exercise, you’ll calculate what the chances are of him closing different numbers of deals using the binomial distribution. # What&#39;s the probability that Amir closes all 3 deals in a week? # Probability of closing 3 out of 3 deals dbinom(3, 3, 0.3) ## [1] 0.027 # What&#39;s the probability that Amir closes 1 or fewer deals in a week? # Probability of closing &lt;= 1 deal out of 3 deals pbinom(1, 3, 0.3) ## [1] 0.784 # What&#39;s the probability that Amir closes more than 1 deal? # Probability of closing &gt; 1 deal out of 3 deals pbinom(1, 3, 0.3, lower.tail = F) ## [1] 0.216 1 - pbinom(1, 3, 0.3) ## [1] 0.216 6.2.4.3 Expected value Now Amir wants to know how many deals he can expect to close each week if his win rate changes. Luckily, you can use your binomial distribution knowledge to help him calculate the expected value in different situations. # Expected number won with 30% win rate won_30pct &lt;- 3 * 0.3 won_30pct ## [1] 0.9 # Expected number won with 25% win rate won_25pct &lt;- 3 * 0.25 won_25pct ## [1] 0.75 # Expected number won with 35% win rate won_35pct &lt;- 3 * 0.35 won_35pct ## [1] 1.05 6.3 More Distributions &amp; Central Limit Theorem 6.3.1 Normal distribution Areas under the normal distribution 68% falls within 1 standard deviation 95% falls within 2 standard deviations 99.7% falls within 3 standard deviations Get percentage or value # Percent pnorm(value, mean = , sd = , lower.tail = ) # Value qnorm(probability, mean = , sd = , lower.tail = ) Generating random numbers # Generate 10 random number rnorm(10, mean = , sd = ) 6.3.1.1 Visualing distribution Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals As part of Amir’s performance review, you want to be able to estimate the probability of him selling different amounts, but before you can do this, you’ll need to determine what kind of distribution the amount variable follows. # Histogram of amount with 10 bins ggplot(amir_deals, aes(x = amount)) + geom_histogram(bins = 10) 6.3.1.2 Probabilities from the normal distribution Since each deal Amir worked on (both won and lost) was different, each was worth a different amount of money. These values are stored in the amount column of amir_deals and follow a normal distribution with a mean of 5000 dollars and a standard deviation of 2000 dollars. As part of his performance metrics, you want to calculate the probability of Amir closing a deal worth various amounts. # Probability of deal &lt; 7500 pnorm(7500, mean = 5000, sd = 2000) ## [1] 0.8943502 # Probability of deal &gt; 1000 pnorm(1000, mean = 5000, sd = 2000, lower.tail = FALSE) ## [1] 0.9772499 # Probability of deal between 3000 and 7000 pnorm(7000, mean = 5000, sd = 2000) - pnorm(3000, mean = 5000, sd = 2000) ## [1] 0.6826895 # Calculate amount that 75% of deals will be more than qnorm(0.75, mean = 5000, sd = 2000, lower.tail = F) ## [1] 3651.02 6.3.1.3 Simulating under new conditions The company’s financial analyst is predicting that next quarter, the worth of each sale will increase by 20% and the volatility, or standard deviation, of each sale’s worth will increase by 30%. To see what Amir’s sales might look like next quarter under these new market conditions, you’ll simulate new sales amounts using the normal distribution and store these in the new_sales data frame. # Calculate new average amount new_mean &lt;- 5000 * 1.2 # Calculate new standard deviation new_sd &lt;- 2000 * 1.3 # Simulate 36 sales new_sales &lt;- data.frame(sale_num = 1:36) new_sales &lt;- new_sales %&gt;% mutate(amount = rnorm(36, mean = new_mean, sd = new_sd)); new_sales ## sale_num amount ## 1 1 3732.387 ## 2 2 5735.421 ## 3 3 5340.171 ## 4 4 1180.275 ## 5 5 5797.340 ## 6 6 8518.272 ## 7 7 6480.807 ## 8 8 2412.147 ## 9 9 2267.663 ## 10 10 6941.427 ## 11 11 1426.374 ## 12 12 5156.186 ## 13 13 4305.936 ## 14 14 8825.034 ## 15 15 4017.383 ## 16 16 3845.477 ## 17 17 8169.632 ## 18 18 3484.105 ## 19 19 5925.080 ## 20 20 6604.565 ## 21 21 5216.857 ## 22 22 4238.202 ## 23 23 7703.592 ## 24 24 4958.342 ## 25 25 5130.153 ## 26 26 9556.680 ## 27 27 11558.194 ## 28 28 7315.130 ## 29 29 8044.490 ## 30 30 3654.249 ## 31 31 7385.532 ## 32 32 4320.675 ## 33 33 6756.567 ## 34 34 2782.254 ## 35 35 4813.942 ## 36 36 3841.161 # Create histogram with 10 bins ggplot(new_sales, aes(x = amount)) + geom_histogram(bins = 10) 6.3.2 Central limit theorem The sampling distribution of a statistic becomes closer to the normal distribution as the number of trials increases. It’s important to note that the central limit theorem only applies when samples are taken randomly and are independent. Regardless of the shape of the distribution you’re taking sample means from, the central limit theorem will apply if the sampling distribution contains enough sample means. sample() works the same way as sample_n(), except it samples from a vector instead of a data frame. sample(vector to sample from, size of the sample, replace = T/F) replicate() use to repeat doing the same thing. replicate(num of replication, run code) 6.3.2.1 The CLT in action The central limit theorem states that a sampling distribution of a sample statistic approaches the normal distribution as you take more samples, no matter the original distribution being sampled from. In this exercise, you’ll focus on the sample mean and see the central limit theorem in action while examining the num_users column of amir_deals more closely, which contains the number of people who intend to use the product Amir is selling. # Set seed to 104 set.seed(104) # Sample 20 num_users from amir_deals and take mean sample(amir_deals$num_users, size = 20, replace = TRUE) %&gt;% mean() ## [1] 30.35 # Repeat the above 100 times sample_means &lt;- replicate(100, sample(amir_deals$num_users, size = 20, replace = TRUE) %&gt;% mean()) # Create data frame for plotting samples &lt;- data.frame(mean = sample_means) # Histogram of sample means ggplot(samples, aes(x = mean)) + geom_histogram(bins = 10) 6.3.2.2 The mean of means You want to know what the average number of users (num_users) is per deal, but you want to know this number for the entire company so that you can see if Amir’s deals have more or fewer users than the company’s average deal. The problem is that over the t year, the company has worked on more than ten thousand deals, so it’s not realistic to compile all the data. Instead, you’ll estimate the mean by taking several random samples of deals, since this is much easier than collecting data from everyone in the company. # Load dataset all_deals &lt;- read.table(file = &quot;data/all_deals.txt&quot;, header = TRUE) # Set seed to 321 set.seed(321) # Take 30 samples of 20 values of num_users, take mean of each sample sample_means &lt;- replicate(30, sample(all_deals$num_users, 20) %&gt;% mean()) # Calculate mean of sample_means mean(sample_means) ## [1] 37.02667 # Calculate mean of num_users in amir_deals mean(amir_deals$num_users) ## [1] 37.65169 Amir’s average number of users is very close to the overall average, so it looks like he’s meeting expectations. 6.3.3 Poisson distribution A Poisson process is a process where events appear to happen at a certain rate, but completely at random. Example: Number of people arriving at a restaurant per hour. Number of earthquakes in California per year. The Poisson distribution describes the probability of some number of events happening over a fixed period of time.(discrete) Lambda (λ): average number of events per time interval.(event) Notice: Lambda changes the shape of the distribution. But no matter what, the distribution’s peak is always at its lambda value. Syntax: Note that if you provide dpois() or ppois() with a non-integer, it returns 0 and throws a warning since the Poisson distribution only applies to integers. # Probability of a single value dpois(value, lambda = ) # Probability of less than or equal to ppois(value, lambda = ) # Probability of greater than ppois(value, lambda = , lower.tail = FALSE) # Sampling from a Poisson distribution rpois(num of sample, lambda = ) 6.3.3.1 Tracking lead responses Your company uses sales software to keep track of new sales leads. It organizes them into a queue so that anyone can follow up on one when they have a bit of free time. Since the number of lead responses is a countable outcome over a period of time, this scenario corresponds to a Poisson distribution. On average, Amir responds to 4 leads each day. In this exercise, you’ll calculate probabilities of Amir responding to different numbers of leads. # What&#39;s the probability that Amir responds to 5 leads in a day, given that he responds to an average of 4? # Probability of 5 responses dpois(5, lambda = 4) ## [1] 0.1562935 # Amir&#39;s coworker responds to an average of 5.5 leads per day. What is the probability that she answers 5 leads in a day? # Probability of 5 responses from coworker dpois(5, lambda = 5.5) ## [1] 0.1714007 # Probability of 2 or fewer responses ppois(2, lambda = 4) ## [1] 0.2381033 # Probability of &gt; 10 responses ppois(10, lambda = 4, lower.tail = F) ## [1] 0.002839766 6.3.4 Exponential distribution Probability of time between Poisson events. (continuous) Example: Probability of &lt; 10 minutes between restaurant arrivals. Probability of 6-8 months between earthquakes. Lambda (λ): rate Notice: The rate(λ) affects the shape of the distribution and how steeply it declines. Expected value of exponential distribution In terms of rate (Poisson): λ In terms of time (exponential): 1/λ Syntax: pexp(value, rate = ) # For example # P (wait &lt; 1 min) pexp(1, rate = 0.5) # P (wait &gt; 4 min) pexp(4, rate = 0.5, lower.tail = FALSE) # P (1 min &lt; wait &lt; 4 min) pexp(4, rate = 0.5) - pexp(1, rate = 0.5) 6.3.4.1 Modeling time between leads To further evaluate Amir’s performance, you want to know how much time it takes him to respond to a lead after he opens it. On average, it takes 2.5 hours for him to respond. In this exercise, you’ll calculate probabilities of different amounts of time passing between Amir receiving a lead and sending a response. # 2.5 is the time expected value, which equal to 1/λ. So in order to get rate(λ), we have to 1 / 2.5 first. (1/λ = 2.5 &gt; λ = 1/2.5) # Probability response takes &lt; 1 hour pexp(1, rate = 1 / 2.5) ## [1] 0.32968 # Probability response takes &gt; 4 hours pexp(4, rate = 1 / 2.5, lower.tail = F) ## [1] 0.2018965 # Probability response takes 3-4 hours pexp(4, rate = 1/2.5) - pexp(3, rate = 1/2.5) ## [1] 0.09929769 6.3.5 (Student’s) t-distribution Similar shape as the normal distribution, but it does have thicker tails and higher variance than the normal distribution. Has parameter degrees of freedom (df) which affects the thickness of the tails. Higher df will closer to normal distribution. 6.3.6 Log-normal distribution Variable whose logarithm is normally distributed. Example: Adult blood pressure Number of hospitalizations in the 2003 SARS outbreak 6.4 Correlation and Experimental Design 6.4.1 Correlation 6.4.1.1 Visualizing relationships To visualize relationships between two variables, we can use a scatterplot created using geom_point. We can add a linear trendline to the scatterplot using geom_smooth. We’ll set the method argument to “lm” to indicate that we want a linear trendline, and se to FALSE so that there aren’t error margins around the line. ggplot(df, aes(x, y)) + geom_point() + # Adding a trendline geom_smooth(method = &quot;lm&quot;, se = FALSE) 6.4.1.2 Computing correlation The correlation coefficient can’t account for any relationships that aren’t linear, regardless of strength. # between -1~1 cor(df$x, df$y) # To ignore data points where one or both values are missing cor(df$x, df$y, use = &quot;pairwise.complete.obs&quot;) 6.4.1.3 Relationships between variables You’ll be working with a dataset world_happiness containing results from the 2019 World Happiness Report. The report scores various countries based on how happy people in that country are. It also ranks each country on various societal aspects such as social support, freedom, corruption, and others. The dataset also includes the GDP per capita and life expectancy for each country. # Create dataset world_happiness &lt;- readRDS(&quot;data/world_happiness_sugar.rds&quot;) # Create a scatterplot of happiness_score vs. life_exp ggplot(world_happiness, aes(x = life_exp, y = happiness_score)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Correlation between life_exp and happiness_score cor(world_happiness$life_exp, world_happiness$happiness_score) ## [1] 0.7737615 6.4.2 Correlation caveats Non-linear relationships: always visualize your data. Skewed data: transformations Log transformation (log(x)) Square root transformation (sqrt(x)) Reciprocal transformation (1 / x) Combinations of these, e.g.: log(x) and log(y) sqrt(x) and 1 / y Correlation does not imply causation. Confounding variables. # Scatterplot of gdp_per_cap and life_exp ggplot(world_happiness, aes(x = gdp_per_cap, y = life_exp)) + geom_point() # Correlation between gdp_per_cap and life_exp cor(world_happiness$gdp_per_cap, world_happiness$life_exp) ## [1] 0.7235027 Log transformations are great to use on variables with a skewed distribution, such as GDP. # Create log_gdp_per_cap column world_happiness &lt;- world_happiness %&gt;% mutate(log_gdp_per_cap = log(gdp_per_cap)) # Scatterplot of happiness_score vs. log_gdp_per_cap ggplot(world_happiness, aes(x = log_gdp_per_cap, y = happiness_score)) + geom_point() # Calculate correlation cor(world_happiness$log_gdp_per_cap, world_happiness$happiness_score) ## [1] 0.7965484 Does sugar improve happiness? # Scatterplot of grams_sugar_per_day and happiness_score ggplot(world_happiness, aes(x = grams_sugar_per_day, y = happiness_score)) + geom_point() # Correlation between grams_sugar_per_day and happiness_score cor(world_happiness$happiness_score, world_happiness$grams_sugar_per_day) ## [1] 0.69391 Increased sugar consumption is associated with a higher happiness score. If correlation always implied that one thing causes another, people may do some nonsensical things, like eat more sugar to be happier. 6.4.3 Design of experiments Controlled experiments vs Observational studies Gold standard of experiments: randomized controlled trial, placebo, double-blind trial Observational: participants are not assigned randomly to groups, establish association not causation Longitudinal vs. Cross-sectional studies "],["introduction-to-data-visualization-with-ggplot2.html", "Chapter 7 Introduction to Data Visualization with ggplot2 7.1 Introduction 7.2 Aesthetics 7.3 Geometries 7.4 Themes", " Chapter 7 Introduction to Data Visualization with ggplot2 7.1 Introduction 7.1.1 Data columns types affect plot types The mtcars dataset contains information on 32 cars from a 1973 issue of Motor Trend magazine. library(ggplot2) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... After changing variable data type, the x-axis does not contain variables like 5 or 7, only the values that are present in the dataset. # Original command ggplot(mtcars, aes(cyl, mpg)) + geom_point() # Change the command below so that cyl is treated as factor ggplot(mtcars, aes(factor(cyl), mpg)) + geom_point() 7.1.2 The grammar of graphics The seven grammatical elements Element Description Data The data-set being plotted. essential grammatical elements Aesthetics The scales onto which we map our data. essential grammatical elements Geometries The visual elements used for our data. essential grammatical elements Themes All non-data ink. Statistics Representations of our data to aid understanding. Coordinates The space on which the data will be plotted. Facets Plotting small multiples. Jargon for each element 7.1.2.1 Mapping data columns to aesthetics # Edit to add a color aesthetic mapped to disp ggplot(mtcars, aes(wt, mpg, color = disp)) + geom_point() # Change the color aesthetic to a size aesthetic ggplot(mtcars, aes(wt, mpg, size = disp)) + geom_point() 7.1.3 ggplot2 layers 7.1.3.1 Adding geometries geom_smooth() adds a smooth trend curve. str(diamonds) ## tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) ## $ carat : num [1:53940] 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num [1:53940] 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num [1:53940] 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int [1:53940] 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num [1:53940] 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num [1:53940] 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num [1:53940] 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... ggplot(diamonds, aes(carat, price)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 7.1.3.2 Changing one geom or every geom geom_point() has an alpha argument that controls the opacity of the points. A value of 1 (the default) means that the points are totally opaque; a value of 0 means the points are totally transparent (and therefore invisible). # Make the points 40% opaque ggplot(diamonds, aes(carat, price, color = clarity)) + geom_point(alpha = 0.4) + geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; 7.1.3.3 Saving plots as variables Plots can be saved as variables, which can be added to later on using the + operator. This is really useful if you want to make multiple related plots from a common base. # Draw a ggplot plt_price_vs_carat &lt;- ggplot( # Use the diamonds dataset diamonds, # For the aesthetics, map x to carat and y to price aes(carat, price) ) # Add a point layer to plt_price_vs_carat plt_price_vs_carat + geom_point() # Edit this to make points 20% opaque: plt_price_vs_carat_transparent plt_price_vs_carat_transparent &lt;- plt_price_vs_carat + geom_point(alpha = 0.2) # See the plot plt_price_vs_carat_transparent # Edit this to map color to clarity, # Assign the updated plot to a new object plt_price_vs_carat_by_clarity &lt;- plt_price_vs_carat + geom_point(aes(color = clarity)) # See the plot plt_price_vs_carat_by_clarity 7.2 Aesthetics 7.2.1 Visible aesthetics Typical visible aesthetics Aesthetic Description x X axis position y Y axis position fill Fill color (點的填滿顏色) color Color of points, outlines of othergeoms (點的外框顏色) size Area or radius of points, thickness of lines alpha Transparency (點的透明度) linetype line dash pattern labels Text on a plot or axes shape Shape of points label and shape are only applicable to categorical data. 7.2.1.1 Aesthetics: color, shape and size # transformed cyl, am into a factor fcyl, fam mtcars$fcyl &lt;- factor(mtcars$cyl) mtcars$fam &lt;- factor(mtcars$am) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 13 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ fcyl: Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... ## $ fam : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 1 1 1 1 1 ... # Map x to wt, y to mpg and color to fcyl ggplot(mtcars, aes(wt, mpg, color = fcyl)) + # Set the shape and size of the points geom_point(shape = 1, size = 4) 7.2.1.2 Aesthetics: color vs. fill Typically, the color aesthetic changes the outline of a geom and the fill aesthetic changes the inside. geom_point() is an exception: you use color (not fill) for the point color. However, some shapes have special behavior. The default geom_point() uses shape = 19: a solid circle. An alternative is shape = 21: a circle that allow you to use both fill for the inside and color for the outline. This is lets you to map two aesthetics to each point. All shape values are described on the ?points help page. # Map fcyl to fill ggplot(mtcars, aes(wt, mpg, fill = fcyl)) + geom_point(shape = 1, size = 4) ggplot(mtcars, aes(wt, mpg, fill = fcyl)) + # Change point shape; set alpha geom_point(shape = 21, size = 4, alpha = 0.6) # Map color to fam ggplot(mtcars, aes(wt, mpg, fill = fcyl, color = fam)) + geom_point(shape = 21, size = 4, alpha = 0.6) Notice that mapping a categorical variable onto fill doesn’t change the colors, although a legend is generated! This is because the default shape for points only has a color attribute and not a fill attribute! Use fill when you have another shape (such as a bar), or when using a point that does have a fill and a color attribute, such as shape = 21, which is a circle with an outline. Any time you use a solid color, make sure to use alpha blending to account for over plotting. 7.2.1.3 Comparing aesthetics # Establish the base layer plt_mpg_vs_wt &lt;- ggplot(mtcars, aes(x = wt, y = mpg)) # Map fcyl to size plt_mpg_vs_wt + geom_point(aes(size = fcyl)) ## Warning: Using size for a discrete variable is not ## advised. # Map fcyl to alpha, not size plt_mpg_vs_wt + geom_point(aes(alpha = fcyl)) ## Warning: Using alpha for a discrete variable is not ## advised. # Map fcyl to shape, not alpha plt_mpg_vs_wt + geom_point(aes(shape = fcyl)) # Use text layer and map fcyl to label plt_mpg_vs_wt + geom_text(aes(label = fcyl)) 7.2.2 Using attributes Set attributes in geom_*(). Attributes are always called in the geom layer! 7.2.2.1 Attributes: color, shape, size and alpha This time you’ll use these arguments to set attributes of the plot, not map variables onto aesthetics. You can specify colors in R using hex codes: a hash followed by two hexadecimal numbers each for red, green, and blue (\"#RRGGBB\"). Hexadecimal is base-16 counting. You have 0 to 9, and A representing 10 up to F representing 15. Pairs of hexadecimal numbers give you a range from 0 to 255. \"#000000\" is “black” (no color), \"#FFFFFF\" means “white”, and `“#00FFFF” is cyan (mixed green and blue). # A hexadecimal color my_blue &lt;- &quot;#4ABEFF&quot; ggplot(mtcars, aes(wt, mpg)) + # Set the point color and alpha geom_point(color = my_blue, alpha = 0.6) # Change the color mapping to a fill mapping ggplot(mtcars, aes(wt, mpg, fill = fcyl)) + # Set point size and shape geom_point(color = my_blue, size = 10, shape = 1) 7.2.2.2 Attributes: conflicts with aesthetics ggplot(mtcars, aes(wt, mpg, color = fcyl)) + # Add point layer with alpha 0.5 geom_point(alpha = 0.5) ggplot(mtcars, aes(wt, mpg, color = fcyl)) + # Add text layer with label rownames of the dataset mtcars and color red geom_text(label = rownames(mtcars), color = &quot;red&quot;) ggplot(mtcars, aes(wt, mpg, color = fcyl)) + # Add points layer with shape 24 and color yellow geom_point(shape = 24, color = &quot;yellow&quot;) Notice that adding more aesthetic mappings to your plot is not always a good idea! You may just increase complexity and decrease readability. # 3 aesthetics: qsec vs. mpg, colored by fcyl ggplot(mtcars, aes(x = mpg, y = qsec, color = fcyl)) + geom_point() # 4 aesthetics: add a mapping of shape to fam ggplot(mtcars, aes(mpg, qsec, color = fcyl, shape = fam)) + geom_point() # 5 aesthetics: add a mapping of size to hp / wt ggplot(mtcars, aes(mpg, qsec, color = fcyl, shape = fam, size = hp/wt)) + geom_point() Between the x and y dimensions, the color, shape, and size of the points, your plot displays five dimensions of the dataset. 7.2.3 Modifying aesthetics 7.2.3.1 Adjustment for overlapping Positions position = \"*\" or position_*() identity: default. Don’t adjust data positions. dodge: preserves the vertical position of a geom while adjusting the horizontal position. position_dodge(width = NULL, preserve = c(\"total\", \"single\")). stack: stacks bars on to of each other. This is the default of geom_bar and geom_area. fill: stacks bars and standardizes each stack to have constant height. jitter: add some random noise on both the x and y axes. position_jitter(width = NULL, height = NULL, seed = num) jitterdodage nudge 7.2.3.2 Scale functions scale_*_*() Appropriately enough, we can access all the scales with the scale underscore functions. The second part of the function defines which scale we want to modify. The third part must match the type of data we are using. scale_x_*() / scale_x_continuous() scale_y_*() scale_color_() / scale_color_discrete() Also scale_colour_*() / scale_colour_*() scale_fill_*() scale_shape_*() scale_linetype_*() scale_size_*() There are many arguments for the scale functions. Most common are limits, breaks, expand and labels. limits: describe the scale’s range. breaks: control the tick mark positions. expand: a numeric vector of length two, giving a multiplicative and additive constant used to expand the range of the scales so that there is a small gap between the data and the axes. labels: adjust the category names. labs: change the axis labels. ggplot(mtcars, aes(fcyl, fill = fam)) + geom_bar() + # Set the axis labels labs(x = &quot;Number of Cylinders&quot;, y = &quot;Count&quot;) scale_fill_manual() defines properties of the color scale (i.e. axis). The first argument sets the legend title. values is a named vector of colors to use. levels(mtcars$fam) &lt;- c(&quot;automatic&quot;, &quot;manual&quot;) str(mtcars$fam) ## Factor w/ 2 levels &quot;automatic&quot;,&quot;manual&quot;: 2 2 2 1 1 1 1 1 1 1 ... palette &lt;- c(automatic = &quot;#377EB8&quot;, manual = &quot;#E41A1C&quot;) ggplot(mtcars, aes(fcyl, fill = fam)) + geom_bar() + labs(x = &quot;Number of Cylinders&quot;, y = &quot;Count&quot;) + # Set the fill color scale scale_fill_manual(&quot;Transmission&quot;, values = palette) # Set the position ggplot(mtcars, aes(fcyl, fill = fam)) + geom_bar(position = &quot;dodge&quot;) + labs(x = &quot;Number of Cylinders&quot;, y = &quot;Count&quot;) scale_fill_manual(&quot;Transmission&quot;, values = palette) ## &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; ## aesthetics: fill ## axis_order: function ## break_info: function ## break_positions: function ## breaks: waiver ## call: call ## clone: function ## dimension: function ## drop: TRUE ## expand: waiver ## get_breaks: function ## get_breaks_minor: function ## get_labels: function ## get_limits: function ## guide: legend ## is_discrete: function ## is_empty: function ## labels: waiver ## limits: function ## make_sec_title: function ## make_title: function ## map: function ## map_df: function ## n.breaks.cache: NULL ## na.translate: TRUE ## na.value: grey50 ## name: Transmission ## palette: function ## palette.cache: NULL ## position: left ## range: environment ## rescale: function ## reset: function ## scale_name: manual ## train: function ## train_df: function ## transform: function ## transform_df: function ## super: &lt;ggproto object: Class ScaleDiscrete, Scale, gg&gt; 7.2.3.3 Setting a dummy aesthetic You can make univariate plots in ggplot2, but you will need to add a fake y axis by mapping y to zero. When using setting y-axis limits, you can specify the limits as separate arguments, or as a single numeric vector. That is, ylim(low, high) or ylim(c(low, high)). # Plot 0 vs. mpg ggplot(mtcars, aes(x = mpg, y = 0)) + # Add jitter geom_point(position = &quot;jitter&quot;) + # Set the y-axis limits ylim(-2, 2) 7.2.4 Aesthetics best practices Mapping continuous variables Mapping categorical variables 7.3 Geometries geom_* Each geom is associated with specific aesthetic mappings, some of which are essential, some of which are optional(e.g, alpha, color, fill, shape, size, stroke). Common plot types 7.3.1 Scatter plots Possible geoms: points, jitter, abline, smooth, count Essential aes: x, y 7.3.1.1 Overplotting 1: large datasets Typically, alpha blending (i.e. adding transparency) is recommended when using solid shapes. Alternatively, you can use opaque, hollow shapes. Small points are suitable for large datasets with regions of high density (lots of overlapping). # Plot price vs. carat, colored by clarity plt_price_vs_carat_by_clarity &lt;- ggplot(diamonds, aes(carat, price, color = clarity)) # Add a point layer with tiny points plt_price_vs_carat_by_clarity + geom_point(alpha = 0.5, shape = &quot;.&quot;) # Set transparency to 0.5 plt_price_vs_carat_by_clarity + geom_point(alpha = 0.5, shape = 16) 7.3.1.2 Overplotting 2: Aligned values This occurs when one axis is continuous and the other is categorical, which can be overcome with some form of jittering. # Plot base plt_mpg_vs_fcyl_by_fam &lt;- ggplot(mtcars, aes(fcyl, mpg, color = fam)) # Default points are shown for comparison plt_mpg_vs_fcyl_by_fam + geom_point() # Alter the point positions by jittering, width 0.3 plt_mpg_vs_fcyl_by_fam + geom_point(position = position_jitter(width = 0.3)) # Now jitter and dodge the point positions plt_mpg_vs_fcyl_by_fam + geom_point(position = position_jitterdodge(jitter.width = 0.3, dodge.width = 0.3)) 7.3.1.3 Overplotting 3: Low-precision data This results from low-resolution measurements like in the iris dataset, which is measured to 1mm precision. It’s similar to case 2, but in this case we can jitter on both the x and y axis. Notice that jitter can be a geom itself (i.e. geom_jitter()), an argument in geom_point() (i.e. position = \"jitter\"), or a position function, (i.e. position_jitter()). ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + # Swap for jitter layer with width 0.1 geom_jitter(width = 0.1, alpha = 0.5) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + # Set the position to jitter geom_point(alpha = 0.5, position = &quot;jitter&quot;) ggplot(iris, aes(Sepal.Length, Sepal.Width, color = Species)) + # Use a jitter position function with width 0.1 geom_point(alpha = 0.5, position = position_jitter(width = 0.1)) 7.3.1.4 Overplotting 4: Integer data This can be type integer (i.e. 1 ,2, 3…) or categorical (i.e. class factor) variables. factor is just a special class of type integer. You’ll typically have a small, defined number of intersections between two variables, which is similar to case 3, but you may miss it if you don’t realize that integer and factor data are the same as low precision data. library(tidyverse) Vocab &lt;- read_csv(&quot;data/Vocab.csv&quot;) # Examine the structure of Vocab str(Vocab) ## spc_tbl_ [21,638 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ year : num [1:21638] 2004 2004 2004 2004 2004 ... ## $ sex : chr [1:21638] &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ... ## $ education : num [1:21638] 9 14 14 17 14 14 12 10 11 9 ... ## $ vocabulary: num [1:21638] 3 6 9 8 1 7 6 6 5 1 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. year = col_double(), ## .. sex = col_character(), ## .. education = col_double(), ## .. vocabulary = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Convert data structure Vocab$sex &lt;- factor(Vocab$sex) str(Vocab) ## spc_tbl_ [21,638 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ year : num [1:21638] 2004 2004 2004 2004 2004 ... ## $ sex : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 2 1 2 2 1 2 2 1 ... ## $ education : num [1:21638] 9 14 14 17 14 14 12 10 11 9 ... ## $ vocabulary: num [1:21638] 3 6 9 8 1 7 6 6 5 1 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. year = col_double(), ## .. sex = col_character(), ## .. education = col_double(), ## .. vocabulary = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Plot vocabulary vs. education ggplot(Vocab, aes(education, vocabulary)) + # Add a point layer geom_point() ggplot(Vocab, aes(education, vocabulary)) + # Change to a jitter layer geom_jitter() ggplot(Vocab, aes(education, vocabulary)) + # Set the transparency to 0.2 geom_jitter(alpha = 0.2) ggplot(Vocab, aes(education, vocabulary)) + # Set the shape to 1 geom_jitter(alpha = 0.2, shape = 1) Notice how jittering and alpha blending serves as a great solution to the overplotting problem here. Setting the shape to 1 didn’t really help, but it was useful in the previous exercises when you had less data. You need to consider each plot individually. 7.3.2 Histograms A histogram is a special type of bar plot that shows the binned distribution of a continuous variable. Essential aes: x (continuous variable) A plot of binned values. Always set a meaningful bin widths for your data. No spaces between bars. X axis labels are between bars (represent intervals and not actual values). 7.3.2.1 Drawing histograms Recall that histograms cut up a continuous variable into discrete bins and, by default, maps the internally calculated count variable (the number of observations in each bin) onto the y aesthetic. An internal variable called density can be accessed by using the .. notation, i.e. ..density... Plotting this variable will show the relative frequency, which is the height times the width of each bin. # Plot mpg ggplot(mtcars, aes(mpg)) + # Add a histogram layer geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. ggplot(mtcars, aes(mpg)) + # Set the binwidth to 1 geom_histogram(binwidth = 1) If you want to use density on the y-axis be sure to set your binwidth to an intuitive value. # Map y to ..density.. ggplot(mtcars, aes(mpg, ..density..)) + geom_histogram(binwidth = 1) ## Warning: The dot-dot notation (`..density..`) was ## deprecated in ggplot2 3.4.0. ## ℹ Please use `after_stat(density)` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` ## to see where this warning was generated. datacamp_light_blue &lt;- &quot;#51A8C9&quot; ggplot(mtcars, aes(mpg, ..density..)) + # Set the fill color to datacamp_light_blue geom_histogram(binwidth = 1, fill = datacamp_light_blue) 7.3.2.2 Positions in histograms geom_histogram(), a special case of geom_bar(), has a position argument. stack (default): Bars for different groups are stacked on top of each other. # Update the aesthetics so the fill color is by fam ggplot(mtcars, aes(mpg, fill = fam)) + geom_histogram(binwidth = 1) dodge: Bars for different groups are placed side by side. ggplot(mtcars, aes(mpg, fill = fam)) + # Change the position to dodge geom_histogram(binwidth = 1, position = &quot;dodge&quot;) fill: Bars for different groups are shown as proportions. ggplot(mtcars, aes(mpg, fill = fam)) + # Change the position to fill geom_histogram(binwidth = 1, position = &quot;fill&quot;) ## Warning: Removed 16 rows containing missing values ## (`geom_bar()`). identity: Plot the values as they appear in the dataset. ggplot(mtcars, aes(mpg, fill = fam)) + # Change the position to identity, with transparency 0.4 geom_histogram(binwidth = 1, position = &quot;identity&quot;, alpha = 0.4) 7.3.3 Bar plots A categorical X-axis Two types: Absolute counts Distributions: dynamite plots (avg &amp; sd) Note that the function geom_col() is just geom_bar() where both the position and stat arguments are set to \"identity\". It is used when we want the heights of the bars to represent the exact values in the data. 7.3.3.1 Position in bar and col plots All positions are available. # Plot fcyl, filled by fam ggplot(mtcars, aes(fcyl, fill = fam)) + # Add a bar layer geom_bar() ggplot(mtcars, aes(fcyl, fill = fam)) + # Set the position to &quot;fill&quot; geom_bar(position = &quot;fill&quot;) ggplot(mtcars, aes(fcyl, fill = fam)) + # Change the position to &quot;dodge&quot; geom_bar(position = &quot;dodge&quot;) 7.3.3.2 Overlapping bar plots You can use position_dodge() (and position_jitter()) is to specify how much dodging (or jittering) you want. ggplot(mtcars, aes(cyl, fill = fam)) + # Change position to use the functional form, with width 0.2 geom_bar(position = position_dodge(width = 0.2)) ggplot(mtcars, aes(cyl, fill = fam)) + # Set the transparency to 0.6 geom_bar(position = position_dodge(width = 0.2), alpha = 0.6) 7.3.3.3 Sequential color palette In this bar plot, we’ll fill each segment according to an ordinal variable. The best way to do that is with a sequential color palette. # Convert data structure Vocab$vocabulary &lt;- factor(Vocab$vocabulary, ordered = T) # Plot education, filled by vocabulary ggplot(Vocab, aes(education, fill = vocabulary)) + geom_bar() # Plot education, filled by vocabulary ggplot(Vocab, aes(education, fill = vocabulary)) + # Add a bar layer with position &quot;fill&quot; geom_bar(position = &quot;fill&quot;) Notice warning massage. # Plot education, filled by vocabulary ggplot(Vocab, aes(education, fill = vocabulary)) + # Add a bar layer with position &quot;fill&quot; geom_bar(position = &quot;fill&quot;) + # Add a brewer fill scale with default palette scale_fill_brewer() ## Warning in RColorBrewer::brewer.pal(n, pal): n too large, allowed maximum for palette Blues is 9 ## Returning the palette you asked for with that many colors library(RColorBrewer) # Definition of a set of blue colors blues &lt;- brewer.pal(9, &quot;Blues&quot;) # from the RColorBrewer package # 1 - Make a color range using colorRampPalette() and the set of blues blue_range &lt;- colorRampPalette(blues) # Plot education, filled by vocabulary ggplot(Vocab, aes(education, fill = vocabulary)) + # Add a bar layer with position &quot;fill&quot; geom_bar(position = &quot;fill&quot;) + # Add a brewer fill scale with default palette scale_fill_manual(values = blue_range(11)) 7.3.4 Line plots Very well-suited in time series. Possible geoms: line, path Essential aes: x, y When we have multiple lines, we have to consider which aesthetic is more appropriate in allowing us to distinguish individual trends. Aesthetic: linetype size color: The most salient choice, when available, since it allows the easiest way of distinguishing between each series. geom_area(): which defaults to position \"stack\", so instead of overlapping time series, they are added together at each point. geom_area(position = \"fill\"): we’ll get a proportion the total capture for each fish at each time-point. Note that we’d need to change the y axis label since these are no longer counts! This kind of plot is pretty popular in looking at proportional trends over time. A difficulty with this kind of plot is that only the bottom and top groups are actually drawn on a common scale, all the other ones are irregular shapes so they can be a bit difficult to decipher. geom_ribbon(): want to have overlapping areas plots. In this case we’d have to force the y-min to be 0. We need to set the alpha level so that we can see the overlap. There is still a challenge in deciphering all the time series, in particularly at the bottom, where there are many overlapping series. 7.3.4.1 Basic line plots The economics dataset contains a time series for unemployment and population statistics from the Federal Reserve Bank of St. Louis in the United States. # Print the head of economics head(economics) ## # A tibble: 6 × 6 ## date pce pop psavert uempmed unemploy ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967-07-01 507. 198712 12.6 4.5 2944 ## 2 1967-08-01 510. 198911 12.6 4.7 2945 ## 3 1967-09-01 516. 199113 11.9 4.6 2958 ## 4 1967-10-01 512. 199311 12.9 4.9 3143 ## 5 1967-11-01 517. 199498 12.8 4.7 3066 ## 6 1967-12-01 525. 199657 11.8 4.8 3018 # Using economics, plot unemploy vs. date ggplot(economics, aes(date, unemploy)) + # Make it a line plot geom_line() # Change the y-axis to the proportion of the population that is unemployed ggplot(economics, aes(x = date, y = unemploy / pop)) + geom_line() 7.3.4.2 Multiple time series fish.species contains the global capture rates of seven salmon species from 1950–2010. # Read dataset fish &lt;- load(&quot;data/fish.RData&quot;) str(fish.species) ## &#39;data.frame&#39;: 61 obs. of 8 variables: ## $ Year : int 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 ... ## $ Pink : int 100600 259000 132600 235900 123400 244400 203400 270119 200798 200085 ... ## $ Chum : int 139300 155900 113800 99800 148700 143700 158480 125377 132407 113114 ... ## $ Sockeye : int 64100 51200 58200 66100 83800 72000 84800 69676 100520 62472 ... ## $ Coho : int 30500 40900 33600 32400 38300 45100 40000 39900 39200 32865 ... ## $ Rainbow : int 0 100 100 100 100 100 100 100 100 100 ... ## $ Chinook : int 23200 25500 24900 25300 24500 27700 25300 21200 20900 20335 ... ## $ Atlantic: int 10800 9701 9800 8800 9600 7800 8100 9000 8801 8700 ... str(fish.tidy) ## &#39;data.frame&#39;: 427 obs. of 3 variables: ## $ Species: Factor w/ 7 levels &quot;Pink&quot;,&quot;Chum&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Year : int 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 ... ## $ Capture: int 100600 259000 132600 235900 123400 244400 203400 270119 200798 200085 ... # Plot the Rainbow Salmon time series ggplot(fish.species, aes(x = Year, y = Rainbow)) + geom_line() # Plot the Pink Salmon time series ggplot(fish.species, aes(x = Year, y = Pink)) + geom_line() # Plot multiple time-series by grouping by species ggplot(fish.tidy, aes(Year, Capture)) + geom_line(aes(group = Species)) # Plot multiple time-series by coloring by species ggplot(fish.tidy, aes(x = Year, y = Capture, color = Species)) + geom_line() 7.4 Themes All non-data ink. Visual elements not part of the data. 7.4.1 Themes from scratch 7.4.1.1 Moving the legend To change stylistic elements of a plot, call theme() and set plot properties to a new value. For example, the following changes the legend position. p + theme(legend.position = new_value) Here, the new value can be \"top\", \"bottom\", \"left\", or \"right'\": place it at that side of the plot. \"none\": don’t draw it. c(x, y): c(0, 0) means the bottom-left and c(1, 1) means the top-right. plt_prop_unemployed_over_time &lt;- ggplot(economics, aes(date, unemploy/pop )) + geom_line(aes(color = pce)) + theme(legend.position = &quot;right&quot;) + ggtitle(&quot;unemployed proportion over time&quot;) # View the default plot plt_prop_unemployed_over_time # Remove legend entirely plt_prop_unemployed_over_time + theme(legend.position = &quot;none&quot;) # Position the legend at the bottom of the plot plt_prop_unemployed_over_time + theme(legend.position = &quot;bottom&quot;) # Position the legend inside the plot at (0.6, 0.1) plt_prop_unemployed_over_time + theme(legend.position = c(0.6, 0.1)) 7.4.1.2 Modifying theme elements Many plot elements have multiple properties that can be set. For example, line elements in the plot such as axes and gridlines have a color, a thickness (size), and a line type (solid line, dashed, or dotted). To set the style of a line, you use element_line(). Similarly, element_rect() changes rectangles and element_text() changes text. You can remove a plot element using element_blank(). plt_prop_unemployed_over_time + theme( # For all rectangles, set the fill color to grey92 rect = element_rect(fill = &quot;grey92&quot;), # For the legend key, turn off the outline legend.key = element_rect(color = NA) ) plt_prop_unemployed_over_time + theme( rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), # Turn off axis ticks axis.ticks = element_blank(), # Turn off the panel grid panel.grid = element_blank() ) plt_prop_unemployed_over_time + theme( rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), axis.ticks = element_blank(), panel.grid = element_blank(), # Add major y-axis panel grid lines back panel.grid.major.y = element_line( # Set the color to red color = &quot;red&quot;, # Set the size to 0.5 size = 0.5, # Set the line type to dotted linetype = &quot;dotted&quot; ) ) ## Warning: The `size` argument of `element_line()` is ## deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument ## instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` ## to see where this warning was generated. plt_prop_unemployed_over_time + theme( rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), axis.ticks = element_blank(), panel.grid = element_blank(), panel.grid.major.y = element_line( color = &quot;red&quot;, size = 0.5, linetype = &quot;dotted&quot; ), # Set the axis text color to grey25 axis.text = element_text(color = &quot;grey25&quot;), # Set the plot title font face to italic and font size to 16 plot.title = element_text(size = 16, face = &quot;italic&quot;) ) 7.4.1.3 Modifying whitespace Whitespace means all the non-visible margins and spacing in the plot. To set a single whitespace value, use unit(x, unit), where x is the amount and unit is the unit of measure. Borders require you to set 4 positions, so use margin(top, right, bottom, left, unit). The default unit is \"pt\" (points), which scales well with text. Other options include “cm”, “in” (inches) and “lines” (of text). plt_mpg_vs_wt_by_cyl &lt;- ggplot(mtcars, aes(mpg, wt, color = fcyl)) + geom_point() + theme(panel.border = element_rect(color = &quot;blue&quot;, fill = NA, size = 0.6, linetype = &quot;dotted&quot;), legend.box.background = element_rect(color = &quot;blue&quot;, fill = NA, size = 0.6, linetype = &quot;dotted&quot;), legend.position = &quot;right&quot;) ## Warning: The `size` argument of `element_rect()` is ## deprecated as of ggplot2 3.4.0. ## ℹ Please use the `linewidth` argument ## instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` ## to see where this warning was generated. plt_mpg_vs_wt_by_cyl plt_mpg_vs_wt_by_cyl + theme( # Set the axis tick length to 2 lines axis.ticks.length = unit(2, &quot;lines&quot;) ) plt_mpg_vs_wt_by_cyl + theme( # Set the legend key size to 3 centimeters legend.key.size = unit(3, &quot;cm&quot;) ) plt_mpg_vs_wt_by_cyl + theme( # Set the legend margin to (20, 30, 40, 50) points legend.margin = ggplot2::margin(20, 30, 40, 50, &quot;pt&quot;) ) plt_mpg_vs_wt_by_cyl + theme( # Set the plot margin to (10, 30, 50, 70) millimeters plot.margin = ggplot2::margin(10, 30, 50, 70, &quot;mm&quot;) ) 7.4.2 Theme flexibility 7.4.2.1 Built-in themes In addition to making your own themes, there are several out-of-the-box solutions that may save you lots of time. theme_gray() is the default. theme_bw() is useful when you use transparency. theme_classic() is more traditional. theme_void() removes everything but the data. # Add a black and white theme plt_prop_unemployed_over_time + theme_bw() # Add a classic theme plt_prop_unemployed_over_time + theme_classic() # Add a void theme plt_prop_unemployed_over_time + theme_void() 7.4.2.2 Exploring ggthemes Outside of ggplot2, another source of built-in themes is the ggthemes package. library(ggthemes) ## Warning: package &#39;ggthemes&#39; was built under R version 4.3.1 # Use the fivethirtyeight theme plt_prop_unemployed_over_time + theme_fivethirtyeight() # Use Tufte&#39;s theme plt_prop_unemployed_over_time + theme_tufte() # Use the Wall Street Journal theme plt_prop_unemployed_over_time + theme_wsj() 7.4.2.3 Setting themes Reusing a theme across many plots helps to provide a consistent style. You have several options for this. Assign the theme to a variable, and add it to each plot. Set your theme as the default using theme_set(). A good strategy that you’ll use here is to begin with a built-in theme then modify it. # Save the theme as theme_recession theme_recession &lt;- theme( rect = element_rect(fill = &quot;grey92&quot;), legend.key = element_rect(color = NA), axis.ticks = element_blank(), panel.grid = element_blank(), panel.grid.major.y = element_line(color = &quot;white&quot;, size = 0.5, linetype = &quot;dotted&quot;), axis.text = element_text(color = &quot;grey25&quot;), plot.title = element_text(face = &quot;italic&quot;, size = 16), legend.position = c(0.6, 0.1) ) # Combine the Tufte theme with theme_recession theme_tufte_recession &lt;- theme_tufte() + theme_recession # Add the Tufte recession theme to the plot plt_prop_unemployed_over_time + theme_tufte_recession Using theme_set() will also get the same output. # Set theme_tufte_recession as the default theme theme_set(theme_tufte_recession) # Draw the plot (without explicitly adding a theme) plt_prop_unemployed_over_time 7.4.2.4 Publication-quality plots plt_prop_unemployed_over_time + # Add Tufte&#39;s theme theme_tufte() plt_prop_unemployed_over_time + theme_tufte() + # Add individual theme elements theme( # Turn off the legend legend.position = &quot;none&quot;, # Turn off the axis ticks axis.ticks = element_blank() ) plt_prop_unemployed_over_time + theme_tufte() + # Add individual theme elements theme( legend.position = &quot;none&quot;, axis.ticks = element_blank(), # Set the axis title&#39;s text color to grey60 axis.title = element_text(color = &quot;grey60&quot;), # Set the axis text&#39;s text color to grey60 axis.text = element_text(color = &quot;grey60&quot;) ) plt_prop_unemployed_over_time + theme_tufte() + # Add individual theme elements theme( legend.position = &quot;none&quot;, axis.ticks = element_blank(), axis.title = element_text(color = &quot;grey60&quot;), axis.text = element_text(color = &quot;grey60&quot;), # Set the panel gridlines major y values panel.grid.major.y = element_line( # Set the color to grey60 color = &quot;grey60&quot;, # Set the size to 0.25 size = 0.25, # Set the linetype to dotted linetype = &quot;dotted&quot; ) ) 7.4.3 Effective explanatory plots 7.4.3.1 Using geoms for explanatory plots gm2007 &lt;- read_csv(&quot;data/gm2007.csv&quot;) # Add a geom_segment() layer ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) + geom_point(size = 4) + geom_segment(aes(xend = 30, yend = country), size = 2) # Add a geom_text() layer ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) + geom_point(size = 4) + geom_segment(aes(xend = 30, yend = country), size = 2) + geom_text(aes(label = lifeExp), color = &quot;white&quot;, size = 1.5) # Set the color scale palette &lt;- brewer.pal(5, &quot;RdYlBu&quot;)[-(2:4)] # Modify the scales ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) + geom_point(size = 4) + geom_segment(aes(xend = 30, yend = country), size = 2) + geom_text(aes(label = round(lifeExp,1)), color = &quot;white&quot;, size = 1.5) + scale_x_continuous(&quot;&quot;, expand = c(0, 0), limits = c(30, 90), position = &quot;top&quot;) + scale_color_gradientn(colors = palette) # Set the color scale palette &lt;- brewer.pal(5, &quot;RdYlBu&quot;)[-(2:4)] # Add a title and caption plt_country_vs_lifeExp &lt;- ggplot(gm2007, aes(x = lifeExp, y = country, color = lifeExp)) + geom_point(size = 4) + geom_segment(aes(xend = 30, yend = country), size = 2) + geom_text(aes(label = round(lifeExp,1)), color = &quot;white&quot;, size = 1.5) + scale_x_continuous(&quot;&quot;, expand = c(0,0), limits = c(30,90), position = &quot;top&quot;) + scale_color_gradientn(colors = palette) + labs(title = &quot;Highest and lowest life expectancies, 2007&quot;, caption = &quot;Source: gapminder&quot;) plt_country_vs_lifeExp 7.4.3.2 Using annotate() for embellishments In this exercise, you’ll use annotate() to add text and a curve to the plot. # Define the theme plt_country_vs_lifeExp + theme_classic() + theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color = &quot;black&quot;), axis.title = element_blank(), legend.position = &quot;none&quot;) gm2007_full &lt;- read_csv(&quot;data/gm2007_full.csv&quot;) global_mean &lt;- mean(gm2007_full$lifeExp) x_start &lt;- global_mean + 4 y_start &lt;- 5.5 x_end &lt;- global_mean y_end &lt;- 7.5 plt_country_vs_lifeExp + theme_classic() + theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color = &quot;black&quot;), axis.title = element_blank(), legend.position = &quot;none&quot;) + # Add a vertical line geom_vline(xintercept = global_mean, color = &quot;grey40&quot;, linetype = 3) plt_country_vs_lifeExp + theme_classic() + theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color = &quot;black&quot;), axis.title = element_blank(), legend.position = &quot;none&quot;) + geom_vline(xintercept = global_mean, color = &quot;grey40&quot;, linetype = 3) + # Add text annotate( &quot;text&quot;, x = x_start, y = y_start, label = &quot;The\\nglobal\\naverage&quot;, vjust = 1, size = 3, color = &quot;grey40&quot; ) plt_country_vs_lifeExp + theme_classic() + theme(axis.line.y = element_blank(), axis.ticks.y = element_blank(), axis.text = element_text(color = &quot;black&quot;), axis.title = element_blank(), legend.position = &quot;none&quot;) + geom_vline(xintercept = global_mean, color = &quot;grey40&quot;, linetype = 3) + annotate( &quot;text&quot;, x = x_start, y = y_start, label = &quot;The\\nglobal\\naverage&quot;, vjust = 1, size = 3, color = &quot;grey40&quot; ) + # Add a curve annotate( &quot;curve&quot;, x = x_start, y = y_start, xend = x_end, yend = y_end, arrow = arrow(length = unit(0.2, &quot;cm&quot;), type = &quot;closed&quot;), color = &quot;grey40&quot; ) "],["intermediate-data-visualization-with-ggplot2.html", "Chapter 8 Intermediate Data Visualization with ggplot2 8.1 Statistics 8.2 Coordinates 8.3 Facets 8.4 Best Practices", " Chapter 8 Intermediate Data Visualization with ggplot2 8.1 Statistics 8.1.1 Stats with geoms 用來計算新的值的演算法稱之為 stat，為 statistical transformation 的簡稱。因為每個 geom 都有一個預設的 stat，反之亦然，所以我們可以把 geom 與 stat 交換使用。也因此，geom_bar()與stat_count()會到相同的結果。 For example: stat_ geom_ stat_bin() geom_histogram() , geom_freqpoly() stat_count() geom_bar() stat_smooth() geom_smooth() stat_boxplot() geom_boxplot() stat_bindot() geom_dotplot() stat_bin2d() geom_bin2d() stat_binhex() geom_hex() stat_contour() geom_contour() stat_quantile() geom_quantile() stat_sum() geom_count() library(ggplot2) library(tidyverse) mtcars$fcyl &lt;- factor(mtcars$cyl) mtcars$fam &lt;- factor(mtcars$am) str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 13 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... ## $ fcyl: Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... ## $ fam : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 1 1 1 1 1 1 1 ... 8.1.1.1 Smoothing # Amend the plot to add a smooth layer ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; Used se = FALSE in stat_smooth() to remove the 95% Confidence Interval. # Amend the plot. Use lin. reg. smoothing; turn off std err ribbon ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Amend the plot. Swap geom_smooth() for stat_smooth(). ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; You can use either stat_smooth() or geom_smooth() to apply a linear model. 8.1.1.2 Grouping variables Considering the situation of looking at sub-groups in our dataset. For this we’ll encounter the invisible group aesthetic. # Amend the plot to add another smooth layer with dummy grouping ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl)) + geom_point() + stat_smooth(method = &quot;lm&quot;, se = FALSE) + stat_smooth(aes(group = 1), method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: The following aesthetics were dropped during ## statistical transformation: colour ## ℹ This can happen when ggplot fails to infer ## the correct grouping structure in the data. ## ℹ Did you forget to specify a `group` ## aesthetic or to convert a numerical ## variable into a factor? #### Modifying stat_smooth The default span for LOESS is 0.9. A lower span will result in a better fit with more detail; but don’t overdo it or you’ll end up over-fitting! ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Add 3 smooth LOESS stats, varying span &amp; color stat_smooth(color = &quot;red&quot;, span = 0.9, se = F) + stat_smooth(color = &quot;green&quot;, span = 0.6, se = F) + stat_smooth(color = &quot;blue&quot;, span = 0.3, se = F) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; # Amend the plot to color by fcyl ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Add a smooth LOESS stat, no ribbon stat_smooth(se = F, color = &quot;red&quot;) + # Add a smooth lin. reg. stat, no ribbon stat_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Amend the plot ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl)) + geom_point() + # Map color to dummy variable &quot;All&quot; stat_smooth(se = FALSE, aes(color = &quot;All&quot;)) + stat_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; In this exercise we’ll take a look at the standard error ribbons, which show the 95% confidence interval of smoothing models. Vocab &lt;- read_csv(&quot;data/Vocab.csv&quot;) %&gt;% mutate(year_group = ifelse(year &gt; 1995, &quot;(1995,2016]&quot;, &quot;[1974,1995]&quot;)) ## Rows: 21638 Columns: 4 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): sex ## dbl (3): year, education, vocabulary ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Vocab$year_group &lt;- factor(Vocab$year_group) Vocab$vocabulary &lt;- as.numeric(Vocab$vocabulary) str(Vocab) ## tibble [21,638 × 5] (S3: tbl_df/tbl/data.frame) ## $ year : num [1:21638] 2004 2004 2004 2004 2004 ... ## $ sex : chr [1:21638] &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Female&quot; ... ## $ education : num [1:21638] 9 14 14 17 14 14 12 10 11 9 ... ## $ vocabulary: num [1:21638] 3 6 9 8 1 7 6 6 5 1 ... ## $ year_group: Factor w/ 2 levels &quot;(1995,2016]&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... # Using Vocab, plot vocabulary vs. education, colored by year group ggplot(Vocab, aes(x = education, y = vocabulary, color = year_group)) + # Add jittered points with transparency 0.25 geom_jitter(alpha = 0.25) + # Add a smooth lin. reg. line (with ribbon) stat_smooth(method = &quot;lm&quot;, se = T) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Amend the plot ggplot(Vocab, aes(x = education, y = vocabulary, color = year_group)) + geom_jitter(alpha = 0.25) + # Map the fill color to year_group, set the line size to 2 stat_smooth(method = &quot;lm&quot;, aes(fill = year_group), linewidth = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 8.1.2 Stats: sum &amp; quantile geom_count() and stat_sum() get the same result. geom_quantile(): dealing with heteroscedasticity. Same as stat_quantile(). 8.1.2.1 Quantiles Linear regression predicts the mean response from the explanatory variables, quantile regression predicts a quantile response (e.g. the median) from the explanatory variables. Specific quantiles can be specified with the quantiles argument. ggplot(Vocab, aes(x = education, y = vocabulary)) + geom_jitter(alpha = 0.25) + # Add a quantile stat, at 0.05, 0.5, and 0.95 stat_quantile(quantiles = c(0.05, 0.5, 0.95)) ## Smoothing formula not specified. Using: y ~ x # Amend the plot to color by year_group ggplot(Vocab, aes(x = education, y = vocabulary, color = year_group)) + geom_jitter(alpha = 0.25) + stat_quantile(quantiles = c(0.05, 0.5, 0.95)) ## Smoothing formula not specified. Using: y ~ x ## Smoothing formula not specified. Using: y ~ x Quantile regression is a great tool for getting a more detailed overview of a large dataset. 8.1.2.2 Using stat_sum In the Vocab dataset, education and vocabulary are integer variables. In the first course, you saw that this is one of the four causes of overplotting. You’d get a single point at each intersection between the two variables. One solution, shown in the step 1, is jittering with transparency. Another solution is to use stat_sum(), which calculates the total number of overlapping observations and maps that onto the size aesthetic. stat_sum() allows a special variable, ..prop.., to show the proportion of values within the dataset. # Run this, look at the plot, then update it ggplot(Vocab, aes(x = education, y = vocabulary)) + # Replace this with a sum stat stat_sum(alpha = 0.25) # Modify the size aesthetic with the appropriate scale function ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_sum() + # Add a size scale, from 1 to 10 scale_size(range = c(1,10)) # Amend the stat to use proportion sizes, so circle size represents the proportion of the whole dataset ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_sum(aes(size = ..prop..)) # Amend the plot to group by education, so that circle size represents the proportion of the group ggplot(Vocab, aes(x = education, y = vocabulary, group = education)) + stat_sum(aes(size = ..prop..)) If a few data points overlap, jittering is great. When you have lots of overlaps (particularly where continuous data has been rounded), using stat_sum() to count the overlaps is more useful. 8.1.3 Stats outside geoms stat_ Description stat_summary() summarize y values at distinct x values. stat_function() compute y values from a function of x values. stat_qq() perform calculations for a quantile-quantile plot. # ggplot2, 1 SD mean_sdl(xx, mult = 1) # ggplot2, stat_summary(function, function argument) stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1)) stat_summary(fun.data = mean_sdl, fun.args = list(mult=1), geom = &quot;errorbar&quot;, width = 0.1) # ggplot2, 95% confidence interval mean_cl_normal(xx) # Normal distribution ggplot(mam.new, aes(x = body)) + geom_histogram(aes( y = ..density..)) + geom_rug() + stat_function(fun = dnorm, color = &quot;red&quot;, args = list(mean = mean(mam.new$body), sd = sd(mam.new$body))) # QQ plot ggplot(mam.new, aes(sample = body)) + stat_qq() + geom_qq_line(col = &quot;red&quot;) 8.1.3.1 Preparations Here, we’ll establish our positions and base layer of the plot. Establishing these items as independent objects will allow us to recycle them easily in many layers, or plots. position_jitter() adds jittering (e.g. for points). position_dodge() dodges geoms, (e.g. bar, col, boxplot, violin, errorbar, pointrange). position_jitterdodge() jitters and dodges geoms, (e.g. points). # Define position objects, promotes consistency between layers # 1. Jitter with width 0.2 posn_j &lt;- position_jitter(width = 0.2) # 2. Dodge with width 0.1 posn_d &lt;- position_dodge(width = 0.1) # 3. Jitter-dodge with jitter.width 0.2 and dodge.width 0.1 posn_jd &lt;- position_jitterdodge(jitter.width = 0.2, dodge.width = 0.1) # Create the plot base: wt vs. fcyl, colored by fam p_wt_vs_fcyl_by_fam &lt;- ggplot(mtcars, aes(x = fcyl, y = wt, color = fam)) # Add a point layer p_wt_vs_fcyl_by_fam + geom_point() 8.1.3.2 Using position objects # Add jittering only p_wt_vs_fcyl_by_fam + geom_point(position = posn_j) # Add dodging only p_wt_vs_fcyl_by_fam + geom_point(position = posn_d) # Add jittering and dodging p_wt_vs_fcyl_by_fam + geom_point(position = posn_jd) 8.1.3.3 Plotting variations Ｎow let’s explore stat_summary(). Summary statistics refers to a combination of location (mean or median) and spread (standard deviation or confidence interval). These metrics are calculated in stat_summary() by passing a function to the fun.data argument. mean_sdl(), calculates multiples of the standard deviation and mean_cl_normal() calculates the t-corrected 95% CI. You can always assign your own function to the fun.data argument as long as the result is a data frame and the variable names match the aesthetics that you will need for the geom layer. Arguments to the data function are passed to stat_summary()’s fun.args argument as a list. p_wt_vs_fcyl_by_fam_jit &lt;- p_wt_vs_fcyl_by_fam + geom_point(position = posn_j) # Add error bars representing the standard deviation # The default geom for stat_summary() is &quot;pointrange&quot; p_wt_vs_fcyl_by_fam_jit + # Add a summary stat of std deviation limits stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn_d) p_wt_vs_fcyl_by_fam_jit + # Change the geom to be an errorbar stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = posn_d, geom = &quot;errorbar&quot;) # add a summary stat of 95% confidence limits p_wt_vs_fcyl_by_fam_jit + # Add a summary stat of normal confidence limits stat_summary(fun.data = mean_cl_normal, position = posn_d) 8.2 Coordinates 8.2.1 Coordinates layer Controls plot dimensions coord_: e.g.coord_cartesian() 8.2.1.1 Zooming in coord_cartesian(xlim = ...) Really zooming in using the coord_cartesian function. You can see the zoom because the LOESS curve continues past the data presented, and the models look the same as in the original plot. scale_x_continuous(limits = ...) Since the LOESS model is only defined for the points shown, although there is data beyond this region. (because the limits we set in scale_x_continuous were a smaller range than the data and thus values were filtered out.) That’s also why the models look different. xlim(...) A quick and dirty alternative is to call xlim as a function itself. It has the same effect. original plot coord_cartesian() scale_x_continuous() xlim() outcome 放大原圖的特定區域 改變原圖 改變原圖 dataset no changes to the dataset change the dataset change the dataset # Run the code, view the plot, then update it ggplot(mtcars, aes(x = wt, y = hp, color = fam)) + geom_point() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; Using the scale function to zoom in meant that there wasn’t enough data to calculate the trend line, and geom_smooth() failed. ggplot(mtcars, aes(x = wt, y = hp, color = fam)) + geom_point() + geom_smooth() + # Add a continuous x scale from 3 to 6 scale_x_continuous(limits = c(3, 6)) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; ## Warning: Removed 12 rows containing non-finite values ## (`stat_smooth()`). ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : span too small. fewer data values than degrees of freedom. ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : at 3.168 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : radius 4e-06 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : all data on boundary of neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : pseudoinverse used at 3.168 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : neighborhood radius 0.002 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : reciprocal condition number 1 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : at 3.572 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : radius 4e-06 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : all data on boundary of neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : There are other near singularities as well. 4e-06 ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : zero-width neighborhood. make span bigger ## Warning in simpleLoess(y, x, w, span, degree = degree, parametric = parametric, ## : zero-width neighborhood. make span bigger ## Warning: Computation failed in `stat_smooth()` ## Caused by error in `predLoess()`: ## ! NA/NaN/Inf in foreign function call (arg 5) ## Warning: Removed 12 rows containing missing values ## (`geom_point()`). When coord_cartesian() was applied, the full dataset was used for the trend calculation. ggplot(mtcars, aes(x = wt, y = hp, color = fam)) + geom_point() + geom_smooth() + # Add Cartesian coordinates with x limits from 3 to 6 coord_cartesian(xlim = c(3, 6)) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; 8.2.1.2 Aspect ratio Function: coord_fixed(ratio = y/x) Height-to-width ratio Watch out for deception! No universal standard so far Typically use 1:1 if data is on the same scale Aspect ratio I: 1:1 ratios We can set the aspect ratio of a plot with coord_fixed(), which uses ratio = 1 as a default. A 1:1 aspect ratio is most appropriate when two continuous variables are on the same scale, so it only makes sense that one unit on the plot should be the same physical distance on each axis. This gives a more truthful depiction of the relationship between the two variables since the aspect ratio can change the angle of our smoothing line. This would give an erroneous impression of the data. Of course the underlying linear models don’t change, but our perception can be influenced by the angle drawn. ggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Fix the coordinate ratio coord_fixed() ## `geom_smooth()` using formula = &#39;y ~ x&#39; Aspect ratio II: setting ratios library(zoo) sunspots.m &lt;- data.frame( year = index(sunspot.month), value = reshape2::melt(sunspot.month)$value) sun_plot &lt;- ggplot(sunspots.m, aes(x = year, y = value)) + geom_line() sun_plot # Fix the aspect ratio to 1:1 sun_plot + coord_fixed() ## Don&#39;t know how to automatically pick scale ## for object of type &lt;ts&gt;. Defaulting to ## continuous. Making a wide plot by calling coord_fixed() with a low ratio is often useful for long time series. # Change the aspect ratio to 1:20 sun_plot + coord_fixed(1/20) ## Don&#39;t know how to automatically pick scale ## for object of type &lt;ts&gt;. Defaulting to ## continuous. 8.2.1.3 Expand and clip The coord_*() layer functions offer two useful arguments that work well together: expand and clip. expand sets a buffer margin around the plot, so data and axes don’t overlap. Setting expand to 0 draws the axes to the limits of the data. clip decides whether plot elements that would lie outside the plot panel are displayed or ignored (“clipped”). ggplot(mtcars, aes(wt, mpg)) + geom_point(size = 2) + theme_classic() ggplot(mtcars, aes(wt, mpg)) + geom_point(size = 2) + # Add Cartesian coordinates with zero expansion coord_cartesian(expand = 0) + theme_classic() Setting expand to 0 caused points at the edge of the plot panel to be cut off. Set the clip argument to \"off\" to prevent this. ggplot(mtcars, aes(wt, mpg)) + geom_point(size = 2) + # Turn clipping off coord_cartesian(expand = 0, clip = &quot;off&quot;) + theme_classic() + # Remove axis lines theme(axis.line = element_blank()) These arguments make clean and accurate plots by not cutting off data. 8.2.2 Coordinates vs. scales 8.2.2.1 Log-transforming scales Using scale_y_log10() and scale_x_log10() is equivalent to transforming our actual dataset before getting to ggplot2. Using coord_trans(), setting x = \"log10\" and/or y = \"log10\" arguments, transforms the data after statistics have been calculated. The plot will look the same as with using scale_*_log10(), but we’ll see the original values on our log10 transformed axes. Let’s see this in action with positively skewed data - the brain and body weight of mammals from the msleep dataset. str(msleep) ## tibble [83 × 11] (S3: tbl_df/tbl/data.frame) ## $ name : chr [1:83] &quot;Cheetah&quot; &quot;Owl monkey&quot; &quot;Mountain beaver&quot; &quot;Greater short-tailed shrew&quot; ... ## $ genus : chr [1:83] &quot;Acinonyx&quot; &quot;Aotus&quot; &quot;Aplodontia&quot; &quot;Blarina&quot; ... ## $ vore : chr [1:83] &quot;carni&quot; &quot;omni&quot; &quot;herbi&quot; &quot;omni&quot; ... ## $ order : chr [1:83] &quot;Carnivora&quot; &quot;Primates&quot; &quot;Rodentia&quot; &quot;Soricomorpha&quot; ... ## $ conservation: chr [1:83] &quot;lc&quot; NA &quot;nt&quot; &quot;lc&quot; ... ## $ sleep_total : num [1:83] 12.1 17 14.4 14.9 4 14.4 8.7 7 10.1 3 ... ## $ sleep_rem : num [1:83] NA 1.8 2.4 2.3 0.7 2.2 1.4 NA 2.9 NA ... ## $ sleep_cycle : num [1:83] NA NA NA 0.133 0.667 ... ## $ awake : num [1:83] 11.9 7 9.6 9.1 20 9.6 15.3 17 13.9 21 ... ## $ brainwt : num [1:83] NA 0.0155 NA 0.00029 0.423 NA NA NA 0.07 0.0982 ... ## $ bodywt : num [1:83] 50 0.48 1.35 0.019 600 ... # Produce a scatter plot of brainwt vs. bodywt ggplot(msleep, aes(bodywt, brainwt)) + geom_point() + ggtitle(&quot;Raw Values&quot;) ## Warning: Removed 27 rows containing missing values ## (`geom_point()`). # Add scale_*_*() functions ggplot(msleep, aes(bodywt, brainwt)) + geom_point() + scale_x_log10() + scale_y_log10() + ggtitle(&quot;Scale_ functions&quot;) ## Warning: Removed 27 rows containing missing values ## (`geom_point()`). # Perform a log10 coordinate system transformation ggplot(msleep, aes(bodywt, brainwt)) + geom_point() + coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) ## Warning: Removed 27 rows containing missing values ## (`geom_point()`). 8.2.2.2 Adding stats to transformed scales Remember that statistics are calculated on the untransformed data. A linear model may end up looking not-so-linear after an coord_trans axis transformation. # Plot with a scale_*_*() function: ggplot(msleep, aes(bodywt, brainwt)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add a log10 x scale scale_x_log10() + # Add a log10 y scale scale_y_log10() + ggtitle(&quot;Scale functions&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 27 rows containing non-finite values ## (`stat_smooth()`). ## Warning: Removed 27 rows containing missing values ## (`geom_point()`). # Plot with transformed coordinates ggplot(msleep, aes(bodywt, brainwt)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add a log10 coordinate transformation for x and y axes coord_trans(x = &quot;log10&quot;, y = &quot;log10&quot;) + ggtitle(&quot;Coord_trans function&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 27 rows containing non-finite values ## (`stat_smooth()`). ## Removed 27 rows containing missing values ## (`geom_point()`). The smooth trend line is calculated after scale transformations but not coordinate transformations, so the second plot doesn’t make sense. Be careful when using the coord_trans() function! 8.2.3 Double &amp; flipped axes 8.2.3.1 Useful double axes Double x or y axes Add raw and transformed values. A scale that is unintuitive for many people can be made easier by adding a transformation as a double axis. airquality &lt;- read_csv(&quot;data/airquality.csv&quot;) ## Rows: 153 Columns: 7 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): Date ## dbl (6): Ozone, Solar.R, Wind, Temp, Month, Day ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. str(airquality) ## spc_tbl_ [153 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Ozone : num [1:153] 41 36 12 18 NA 28 23 19 8 NA ... ## $ Solar.R: num [1:153] 190 118 149 313 NA NA 299 99 19 194 ... ## $ Wind : num [1:153] 7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ... ## $ Temp : num [1:153] 67 72 74 62 56 66 65 59 61 69 ... ## $ Month : num [1:153] 5 5 5 5 5 5 5 5 5 5 ... ## $ Day : num [1:153] 1 2 3 4 5 6 7 8 9 10 ... ## $ Date : chr [1:153] &quot;5/1/1973&quot; &quot;5/2/1973&quot; &quot;5/3/1973&quot; &quot;5/4/1973&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Ozone = col_double(), ## .. Solar.R = col_double(), ## .. Wind = col_double(), ## .. Temp = col_double(), ## .. Month = col_double(), ## .. Day = col_double(), ## .. Date = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; airquality$Date &lt;- as.Date(airquality$Date, format = &quot;%m/%d/%Y&quot;) # Using airquality, plot Temp vs. Date ggplot(airquality, aes(x = Date, y = Temp)) + # Add a line layer geom_line() + labs(x = &quot;Date (1973)&quot;, y = &quot;Fahrenheit&quot;) # Define breaks (Fahrenheit) y_breaks &lt;- c(59, 68, 77, 86, 95, 104) # Convert y_breaks from Fahrenheit to Celsius y_labels &lt;- (y_breaks - 32)*5/9 # Create a secondary x-axis secondary_y_axis &lt;- sec_axis( # Use identity transformation trans = identity, name = &quot;Celsius&quot;, # Define breaks and labels as above breaks = y_breaks, labels = y_labels ) # Examine the object secondary_y_axis ## &lt;ggproto object: Class AxisSecondary, gg&gt; ## axis: NULL ## break_info: function ## breaks: 59 68 77 86 95 104 ## create_scale: function ## detail: 1000 ## empty: function ## guide: waiver ## init: function ## labels: 15 20 25 30 35 40 ## make_title: function ## mono_test: function ## name: Celsius ## trans: function ## transform_range: function ## super: &lt;ggproto object: Class AxisSecondary, gg&gt; # Update the plot ggplot(airquality, aes(Date, Temp)) + geom_line() + # Add the secondary y-axis scale_y_continuous(sec.axis = secondary_y_axis) + labs(x = &quot;Date (1973)&quot;, y = &quot;Fahrenheit&quot;) Double axes are most useful when you want to display the same value in two differnt units. 8.2.3.2 Flipping axes Flipping axes means to reverse the variables mapped onto the x and y aesthetics. We can just change the mappings in aes(), but we can also use the coord_flip() layer function. There are two reasons to use this function: We want a vertical geom to be horizontal, or We’ve completed a long series of plotting functions and want to flip it without having to rewrite all our commands. # Plot fcyl bars, filled by fam ggplot(mtcars, aes(fcyl, fill = fam)) + # Place bars side by side geom_bar(position = &quot;dodge&quot;) To get horizontal bars, add a coord_flip() function. Horizontal bars are especially useful when the axis labels are long. # Plot fcyl bars, filled by fam ggplot(mtcars, aes(fcyl, fill = fam)) + # Place bars side by side geom_bar(position = &quot;dodge&quot;) + coord_flip() ggplot(mtcars, aes(fcyl, fill = fam)) + # Set a dodge width of 0.5 for partially overlapping bars geom_bar(position = position_dodge(width = 0.5)) + coord_flip() In this exercise, we’ll continue to use the coord_flip() layer function to reverse the variables mapped onto the x and y aesthetics. mtcars &lt;- mtcars %&gt;% mutate(car = row.names(mtcars)) # Plot of wt vs. car ggplot(mtcars, aes(x = car, y = wt)) + # Add a point layer geom_point() + labs(x = &quot;car&quot;, y = &quot;weight&quot;) It would be easier to read if car was mapped to the y axis. Flip the coordinates. Notice that the labels also get flipped! # Flip the axes to set car to the y axis ggplot(mtcars, aes(car, wt)) + geom_point() + labs(x = &quot;car&quot;, y = &quot;weight&quot;) + coord_flip() 8.2.4 Polar coordinates Cartesian (2d) Orthogonal x and y-axes Maps Many projections Polar Transformed Cartesian space 8.2.4.1 Pie charts The coord_polar() function converts a planar x-y Cartesian plot to polar coordinates. This can be useful if you are producing pie charts. We can imagine two forms for pie charts - the typical filled circle, or a colored ring. # mapping the angle to the y variable by setting theta to &quot;y&quot; ggplot(mtcars, aes(x = 1, fill = fcyl)) + geom_bar() + # Add a polar coordinate system coord_polar(theta = &quot;y&quot;) ggplot(mtcars, aes(x = 1, fill = fcyl)) + # Reduce the bar width to 0.1 geom_bar(width = 0.1) + coord_polar(theta = &quot;y&quot;) + # Add a continuous x scale from 0.5 to 1.5 scale_x_continuous(limits = c(0.5, 1.5)) Polar coordinates are particularly useful if you are dealing with a cycle, like yearly data, compass direction or time of day. 8.2.4.2 Wind rose plots The wind dataset contains hourly measurements for windspeed (ws) and direction (wd) from London in 2003. wind &lt;- read.table(&quot;data/wind.txt&quot;, sep = &quot;\\t&quot;, header = T) head(wind) ## date ws wd ## 1 1/1/2003 0:00 4 - 6 SSE ## 2 1/1/2003 1:00 4 - 6 SE ## 3 1/1/2003 2:00 2 - 4 SE ## 4 1/1/2003 3:00 4 - 6 SE ## 5 1/1/2003 4:00 4 - 6 SE ## 6 1/1/2003 5:00 4 - 6 SE wind &lt;- wind %&gt;% mutate(date = as.POSIXct(date, format = &quot;%m/%d/%Y %H:%M&quot;), ws = factor(ws), wd = factor(wd)) str(wind) ## &#39;data.frame&#39;: 8753 obs. of 3 variables: ## $ date: POSIXct, format: &quot;2003-01-01 00:00:00&quot; &quot;2003-01-01 01:00:00&quot; ... ## $ ws : Factor w/ 7 levels &quot;0 - 2&quot;,&quot;10 - 12&quot;,..: 5 5 4 5 5 5 5 5 4 4 ... ## $ wd : Factor w/ 16 levels &quot;E&quot;,&quot;ENE&quot;,&quot;ESE&quot;,..: 11 10 10 10 10 10 11 11 11 11 ... Use a geom_bar() layer, since we want to aggregate over all date values, and set the width argument to 1, to eliminate any spaces between the bars. # Using wind, plot wd filled by ws ggplot(wind, aes(x = wd, fill = ws)) + # Add a bar layer with width 1 geom_bar(width = 1) # Convert to polar coordinates: ggplot(wind, aes(wd, fill = ws)) + geom_bar(width = 1) + coord_polar() Set the start argument to position North at the top of the plot. # Convert to polar coordinates: ggplot(wind, aes(wd, fill = ws)) + geom_bar(width = 1) + coord_polar(start = -pi/2.3) 8.3 Facets 8.3.1 The facets layer Faceting splits the data up into groups, according to a categorical variable, then plots each group in its own panel. ggplot2 will coerce variables to factors when used in facets. 8.3.1.1 Basics facet_grid() For splitting the data by one or two categorical variables, facet_grid() is best. # Example, muti cols p + facet_grid(cols = vars(variable_name)) # Example, muti rows p + facet_grid(rows = vars(variable_name) # Both cols and rows by different variables p + facet_grid(rows = vars(A), cols = vars(B)) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet rows by am facet_grid(rows = vars(am)) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet columns by cyl facet_grid(cols = vars(cyl)) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet rows by am and columns by cyl facet_grid(rows = vars(am), cols = vars(cyl)) 8.3.1.2 Many variables Two variables are mapped onto the color aesthetic, using hue and lightness. To achieve this we combined fcyl and fam into a single interaction variable, fcyl_fam. This will allow us to take advantage of Color Brewer’s Paired color palette. mtcars &lt;- mtcars %&gt;% mutate(fcyl_fam = interaction(fcyl, fam, sep = &quot;:&quot;)) mtcars$fcyl_fam ## [1] 6:1 6:1 4:1 6:0 8:0 6:0 8:0 4:0 4:0 6:0 6:0 8:0 8:0 8:0 8:0 8:0 8:0 4:1 4:1 ## [20] 4:1 4:0 8:0 8:0 8:0 8:0 4:1 4:1 4:1 8:1 6:1 8:1 4:1 ## Levels: 4:0 6:0 8:0 4:1 6:1 8:1 # Color the points by fcyl_fam ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam)) + geom_point() + # Use a paired color palette scale_color_brewer(palette = &quot;Paired&quot;) # Update the plot to map disp to size ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam, size = disp)) + geom_point() + scale_color_brewer(palette = &quot;Paired&quot;) # Update the plot ggplot(mtcars, aes(x = wt, y = mpg, color = fcyl_fam, size = disp)) + geom_point() + scale_color_brewer(palette = &quot;Paired&quot;) + # Grid facet on gear and vs facet_grid(rows = vars(gear), cols = vars(vs)) 8.3.1.3 Formula notation lm function Everything on the left of the tilde (~) will split according to rows, and everything on the right will split according to columns. Modern notation Formula notation facet_grid(rows = vars(A)) facet_grid(A ~ .) facet_grid(cols = vars(B)) facet_grid(. ~ B) facet_grid(rows = vars(A), cols = vars(B)) facet_grid(A ~ B) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet rows by am using formula notation facet_grid(am ~ .) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet columns by cyl using formula notation facet_grid(. ~ cyl) ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet rows by am and columns by cyl using formula notation facet_grid(am ~ cyl) 8.3.2 Facet labels &amp; order Two typical problems with facets: Poorly labeled (e.g. non descriptive) Wrong or inappropriate order Solutions: Easy: Add labels in ggplot Argument in the facet layer: labeller = label_value: default, displays only the value labeller = label_both: displays both the value and the variable name labeller = label_context: displays only the values or both the values and variables depending on whether multiple factors are faceted Better: Relabel and rearrange factor variables in your dataframe (assign proper labels in your original data before plotting). Argument: fct_recode(): relabeling and reordering factors fct_relevel(): change order of levels 8.3.2.1 Labelling facets # Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + # The default is label_value facet_grid(cols = vars(cyl)) # Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Displaying both the values and the variables facet_grid(cols = vars(cyl), labeller = label_both) # Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Label context facet_grid(cols = vars(cyl), labeller = label_context) # Plot wt by mpg ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Two variables facet_grid(cols = vars(vs, cyl), labeller = label_context) 8.3.2.2 Setting order If you want to change the order of your facets, it’s best to properly define your factor variables before plotting. Let’s see this in action with the mtcars transmission variable am. In this case, 0 = \"automatic\" and 1 = \"manual\". Here, we’ll make am a factor variable and relabel the numbers to proper names. The default order is alphabetical. # Make factor, set proper labels explictly mtcars$fam &lt;- factor(mtcars$am, labels = c(`0` = &quot;automatic&quot;, `1` = &quot;manual&quot;)) # Check default level str(mtcars$fam) ## Factor w/ 2 levels &quot;automatic&quot;,&quot;manual&quot;: 2 2 2 1 1 1 1 1 1 1 ... # Default order is alphabetical ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(cols = vars(fam)) # Make factor, set proper labels explictly, and # manually set the label order mtcars$fam &lt;- factor(mtcars$am, levels = c(1, 0), labels = c(&quot;manual&quot;, &quot;automatic&quot;)) # Check level changed str(mtcars$fam) ## Factor w/ 2 levels &quot;manual&quot;,&quot;automatic&quot;: 1 1 1 2 2 2 2 2 2 2 ... # View again ggplot(mtcars, aes(wt, mpg)) + geom_point() + facet_grid(cols = vars(fam)) 8.3.3 Facet plotting spaces Shared scales make it easy to compare between facets, but can be confusing if the data ranges are very different. In that case, used free scales. 8.3.3.1 Continuous variables plotting spaces By default every facet of a plot has the same axes. If the data ranges vary wildly between facets, it can be clearer if each facet has its own scale. This is achieved with the scales argument to facet_grid(). \"fixed\" (default): axes are shared between facets. free: each facet has its own axes. free_x: each facet has its own x-axis, but the y-axis is shared. free_y: each facet has its own y-axis, but the x-axis is shared. When faceting by columns, \"free_y\" has no effect, but we can adjust the x-axis. In contrast, when faceting by rows, \"free_x\" has no effect, but we can adjust the y-axis. # Shared scales ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Facet columns by cyl facet_grid(cols = vars(cyl)) # X axis values are different between variables - free_x ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Update the faceting to free the x-axis scales facet_grid(cols = vars(cyl), scales = &quot;free_x&quot;) # y axis values are different between variables - free_y ggplot(mtcars, aes(wt, mpg)) + geom_point() + # Swap cols for rows; free the y-axis scales facet_grid(rows = vars(cyl), scales = &quot;free_y&quot;) 8.3.3.2 Categorical variables plotting spaces When you have a categorical variable with many levels which are not all present in each sub-group of another variable, it’s usually desirable to drop the unused levels. By default, each facet of a plot is the same size. This behavior can be changed with the spaces argument, which works in the same way as scales: \"free_x\" allows different sized facets on the x-axis, \"free_y\", allows different sized facets on the y-axis, \"free\" allows different sizes in both directions. ggplot(mtcars, aes(x = mpg, y = car, color = fam)) + geom_point() + # Facet rows by gear facet_grid(rows = vars(gear)) Notice that every car is listed in every facet, resulting in many lines without data. To remove blank lines, set the scales and space arguments in facet_grid() to free_y. Freeing the y-scale to remove blank lines helps focus attention on the actual data present. ggplot(mtcars, aes(x = mpg, y = car, color = fam)) + geom_point() + # Free the y scales and space facet_grid(rows = vars(gear), scales =&quot;free_y&quot;, space = &quot;free_y&quot;) 8.3.4 Facet wrap &amp; margins Using facet_wrap() When you want both x and y axes to be free on every individual plot i.e. Not just per row or column as per facet_grid() When your categorical (factor) variable has many groups (levels) i.e. too many sub plots for column or row-wise faceting A more typical scenario 8.3.4.1 Wrapping for many levels facet_grid() is fantastic for categorical variables with a small number of levels. Although it is possible to facet variables with many levels, the resulting plot will be very wide or very tall, which can make it difficult to view. The solution is to use facet_wrap() which separates levels along one axis but wraps all the subsets across a given number of rows or columns. ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + # Create facets, wrapping by year, using vars() facet_wrap(vars(year)) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + # Create facets, wrapping by year, using a formula facet_wrap(~ year) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ggplot(Vocab, aes(x = education, y = vocabulary)) + stat_smooth(method = &quot;lm&quot;, se = FALSE) + # Update the facet layout, using 8 columns facet_wrap(~ year, ncol = 8) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 8.3.4.2 Margin plots Facets are great for seeing subsets in a variable, but sometimes you want to see both those subsets and all values in a variable. Here, the margins argument to facet_grid() is your friend. FALSE (default): no margins. TRUE: add margins to every variable being faceted by. c(\"variable1\", \"variable2\"): only add margins to the variables listed. # Convert vs to categorical variable fvs mtcars$fvs &lt;- factor(mtcars$vs, labels = c(`0` = &quot;V-shaped&quot;, `1` = &quot;straight&quot;)) ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Facet rows by fvs and cols by fam facet_grid(rows = vars(fvs, fam), cols = vars(gear)) Add all possible margins to the plot. ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Update the facets to add margins facet_grid(rows = vars(fvs, fam), cols = vars(gear), margin = TRUE) Update the facets to only show margins on \"fam\". ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Update the facets to only show margins on fam facet_grid(rows = vars(fvs, fam), cols = vars(gear), margins = &quot;fam&quot;) Update the facets to only show margins on \"gear\" and \"fvs\". ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + # Update the facets to only show margins on gear and fvs facet_grid(rows = vars(fvs, fam), cols = vars(gear), margins = c(&quot;gear&quot;, &quot;fvs&quot;)) 8.4 Best Practices Common pitfalls in Data Viz Best way to represent data For effective explanatory (communication), and For effective exploratory (investigation) plots 8.4.1 Bar plots 8.4.1.1 Dynamite plots In the video we saw many reasons why “dynamite plots” (bar plots with error bars) are not well suited for their intended purpose of depicting distributions. (A further perceptual problem is that our bars give the impression of having data where there is no data. Plus, the region above the mean contains values but no ink!) If you really want error bars on bar plots, you can of course get them, but you’ll need to set the positions manually. A point geom will typically serve you much better. # Plot wt vs. fcyl ggplot(mtcars, aes(x = fcyl, y = wt)) + # Add a bar summary stat of means, colored skyblue stat_summary(fun = mean, geom = &quot;bar&quot;, fill = &quot;skyblue&quot;) + # Add an errorbar summary stat std deviation limits stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1) Remember, we can specify any function in fun.data or fun.y and we can also specify any geom, as long as it’s appropriate to the data type. 8.4.1.2 Position dodging In this exercise we will add a distinction between transmission type, fam, for the dynamite plots and explore position dodging (where bars are side-by-side). # Update the aesthetics to color and fill by fam ggplot(mtcars, aes(x = fcyl, y = wt, color = fam, fill = fam)) + stat_summary(fun = mean, geom = &quot;bar&quot;) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, width = 0.1) The stacked bars are tricky to interpret. Make them transparent and side-by-side. # Set alpha for the first and set position for each stat summary function ggplot(mtcars, aes(x = fcyl, y = wt, color = fam, fill = fam)) + stat_summary(fun = mean, geom = &quot;bar&quot;, alpha = 0.5, position = &quot;dodge&quot;) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;errorbar&quot;, position = &quot;dodge&quot;, width = 0.1) The error bars are incorrectly positioned. Use a position object. # Define a dodge position object with width 0.9 posn_d &lt;- position_dodge(width = 0.9) # For each summary stat, update the position to posn_d ggplot(mtcars, aes(x = fcyl, y = wt, color = fam, fill = fam)) + stat_summary(fun = mean, geom = &quot;bar&quot;, position = posn_d, alpha = 0.5) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), width = 0.1, position = posn_d, geom = &quot;errorbar&quot;) 8.4.1.3 Using aggregated data Here, we’ve created a summary data frame called mtcars_by_cyl which contains the average (mean_wt), standard deviations (sd_wt) and count (n_wt) of car weights, for each cylinder group, cyl. It also contains the proportion (prop) of each cylinder represented in the entire dataset. Use the console to familiarize yourself with the mtcars_by_cyl data frame. # mean and sd mtcars_by_cyl1 &lt;- mtcars %&gt;% group_by(cyl) %&gt;% summarise(mean_wt = mean(wt), sd_wt = sd(wt)) mtcars_by_cyl1 ## # A tibble: 3 × 3 ## cyl mean_wt sd_wt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 2.29 0.570 ## 2 6 3.12 0.356 ## 3 8 4.00 0.759 # n and prop mtcars_by_cyl2 &lt;- mtcars %&gt;% count(cyl) %&gt;% mutate(prop = n / nrow(mtcars)) %&gt;% rename(n_wt = n) mtcars_by_cyl2 ## cyl n_wt prop ## 1 4 11 0.34375 ## 2 6 7 0.21875 ## 3 8 14 0.43750 # inner join by cyl column mtcars_by_cyl &lt;- mtcars_by_cyl1 %&gt;% inner_join(mtcars_by_cyl2, by = &quot;cyl&quot;); mtcars_by_cyl ## # A tibble: 3 × 5 ## cyl mean_wt sd_wt n_wt prop ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 4 2.29 0.570 11 0.344 ## 2 6 3.12 0.356 7 0.219 ## 3 8 4.00 0.759 14 0.438 If it is appropriate to use bar plots, then it nice to give an impression of the number of values in each group. stat_summary() doesn’t keep track of the count. stat_sum() does (that’s the whole point), but it’s difficult to access. It’s more straightforward to calculate exactly what we want to plot ourselves. # Using mtcars_cyl, plot mean_wt vs. cyl ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) + # Add a bar layer with identity stat, filled skyblue geom_bar(stat = &quot;identity&quot;, fill = &quot;skyblue&quot;) ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) + # Swap geom_bar() for geom_col() and remove stat argument geom_col(fill = &quot;skyblue&quot;) ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) + # Set the width aesthetic to prop geom_col(fill = &quot;skyblue&quot;, aes(width = prop)) ## Warning in geom_col(fill = &quot;skyblue&quot;, aes(width = prop)): Ignoring unknown ## aesthetics: width ggplot(mtcars_by_cyl, aes(x = cyl, y = mean_wt)) + geom_col(aes(width = prop), fill = &quot;skyblue&quot;) + # Add an errorbar layer geom_errorbar( # ... at mean weight plus or minus 1 std dev aes(ymin = mean_wt - sd_wt, ymax = mean_wt + sd_wt), # with width 0.1 width = 0.1 ) ## Warning in geom_col(aes(width = prop), fill = &quot;skyblue&quot;): Ignoring unknown ## aesthetics: width 8.4.2 Heatmaps use case scenario 8.4.2.1 Heat maps Since heat maps encode color on a continuous scale, they are difficult to accurately decode. Hence, heat maps are most useful if you have a small number of boxes and/or a clear pattern that allows you to overcome decoding difficulties. To produce them, map two categorical variables onto the x and y aesthetics, along with a continuous variable onto fill. The geom_tile() layer adds the boxes. # To get barley dataset library(lattice) # Using barley, plot variety vs. year, filled by yield ggplot(lattice::barley, aes(year, variety, fill = yield)) + # Add a tile geom geom_tile() Add a facet_wrap() function with facets as vars(site) and ncol = 1. Strip names will be above the panels, not to the side (as with facet_grid()). # Previously defined ggplot(lattice::barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + # Facet, wrapping by site, with 3 column facet_wrap(facets = vars(site), ncol = 3) + # Add a fill scale using an 2-color gradient scale_fill_gradient(low = &quot;white&quot;, high = &quot;red&quot;) Update the fill scale to use an n-color gradient with scale_fill_gradientn() (note the n). Set the scale colors to the red brewer palette. library(RColorBrewer) # A palette of 9 reds red_brewer_palette &lt;- brewer.pal(9, &quot;Reds&quot;) # Update the plot ggplot(lattice::barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap(facets = vars(site), ncol = 3) + # Update scale to use n-colors from red_brewer_palette scale_fill_gradientn(colors = red_brewer_palette) 8.4.2.2 Heat map alternatives There are several alternatives to heat maps. The best choice really depends on the data and the story you want to tell with this data. If there is a time component, the most obvious choice is a line plot. # The heat map we want to replace # Don&#39;t remove, it&#39;s here to help you! ggplot(lattice::barley, aes(x = year, y = variety, fill = yield)) + geom_tile() + facet_wrap( ~ site, ncol = 1) + scale_fill_gradientn(colors = brewer.pal(9, &quot;Reds&quot;)) # Using barley, plot yield vs. year, colored and grouped by variety ggplot(lattice::barley, aes(x = year, y = yield, color = variety, group = variety)) + # Add a line layer geom_line() + # Facet, wrapping by site, with 1 row facet_wrap( ~ site, nrow = 1) Display only means and ribbons for spread. # Using barely, plot yield vs. year, colored, grouped, and filled by site ggplot(lattice::barley, aes(x = year, y = yield, color = site, group = site, fill = site)) + # Add a line summary stat aggregated by mean stat_summary(fun = mean, geom = &quot;line&quot;) + # Add a ribbon summary stat with 10% opacity, no color stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), geom = &quot;ribbon&quot;, alpha = 0.1, color = NA) Whenever you see a heat map, ask yourself it it’s really necessary. Many people use them because they look fancy and complicated - signs of poor communication skills. 8.4.3 When good data makes bad plots 8.4.3.1 Bad plots: style Color Not color-blind-friendly (e.g. primarily red and green) Wrong palette for data type (remember sequential, qualitative and divergent) Indistinguishable groups (i.e. colors are too similar) Ugly (high saturation primary colors) Text Illegible (e.g. too small, poor resolution) Non-descriptive (e.g. “length” -- of what? which units?) Missing Inappropriate (e.g. comic sans) 8.4.3.2 Bad plots: structure and content Information content Too much information (TMI) Too little information (TLI) No clear message or purpose Axes Poor aspect ratio Suppression of the origin Broken x or y axes Common, but unaligned scales Wrong or no transformation Statistics Visualization doesn’t match actual statistics Geometries Wrong plot type Wrong orientation Non-data Ink Inappropriate use 3D plots Perceptual problems Useless 3rd axis Use your common sense: Is there anything on my plot that obscure a clear reading of the data or the take-home message? 8.4.3.3 Typical problems Let’s take a look at the steps we could take to produce and improve the plot in the view. The data comes from an experiment where the effect of two different types of vitamin C sources, orange juice or ascorbic acid, were tested on the growth of the odontoblasts (cells responsible for tooth growth) in 60 guinea pigs. The data is stored in the TG data frame, which contains three variables: dose, len, and supp. TG &lt;- read_csv(&quot;data/TG.csv&quot;) str(TG) ## spc_tbl_ [60 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ len : num [1:60] 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: chr [1:60] &quot;VC&quot; &quot;VC&quot; &quot;VC&quot; &quot;VC&quot; ... ## $ dose: num [1:60] 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. len = col_double(), ## .. supp = col_character(), ## .. dose = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; # Initial plot growth_by_dose &lt;- ggplot(TG, aes(dose, len, color = supp)) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = position_dodge(0.1)) + theme_gray(3) # View plot growth_by_dose The first plot contains purposely illegible labels. It’s a common problem that can occur when resizing plots. There is also too much non-data ink. # Initial plot growth_by_dose &lt;- ggplot(TG, aes(dose, len, color = supp)) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = position_dodge(0.1)) + theme_classic() # View plot growth_by_dose Use the appropriate geometry for the data: In the new stat_summary() function, set fun to to calculate the mean and the geom to a \"line\" to connect the points at their mean values. # Plot growth_by_dose &lt;- ggplot(TG, aes(dose, len, color = supp)) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = position_dodge(0.2)) + # Use the right geometry stat_summary(fun = mean, geom = &quot;line&quot;, position = position_dodge(0.1)) + theme_classic() # View plot growth_by_dose Make sure the labels are informative: Add the units \"(mg/day)\" and \"(mean, standard deviation)\" to the x and y labels, respectively. Use the \"Set1\" palette. Set the legend labels to \"Orange juice\" and \"Ascorbic acid\". # Plot growth_by_dose &lt;- ggplot(TG, aes(dose, len, color = supp)) + stat_summary(fun.data = mean_sdl, fun.args = list(mult = 1), position = position_dodge(0.2)) + stat_summary(fun = mean, geom = &quot;line&quot;, position = position_dodge(0.1)) + theme_classic() + # Adjust labels and colors: labs(x = &quot;Dose (mg/day)&quot;, y = &quot;Odontoblasts length (mean, standard deviation)&quot;, color = &quot;Supplement&quot;) + scale_color_brewer(palette = &quot;Set1&quot;, labels = c(&quot;Orange juice&quot;, &quot;Ascorbic acid&quot;)) + scale_y_continuous(limits = c(0,35), breaks = seq(0, 35, 5), expand = c(0,0)) # View plot growth_by_dose "],["introduction-to-importing-data-in-r.html", "Chapter 9 Introduction to Importing Data in R 9.1 Flat files with utils 9.2 readr &amp; data.table 9.3 Excel data 9.4 Reproducible Excel work - XLConnect", " Chapter 9 Introduction to Importing Data in R 9.1 Flat files with utils By default function: read.csv(): csv file read.delim(): tab-delimited file (txt file) read.table(): any file 9.1.1 read.csv The utils package, which is automatically loaded in your R session on startup, can import CSV files with the read.csv() function. Defaults header = TRUE sep = \",\" # Import swimming_pools.csv: pools pools &lt;- read.csv(&quot;data/swimming_pools.csv&quot;) # Print the structure of pools str(pools) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... ## $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... With stringsAsFactors, you can tell R whether it should convert strings in the flat file to factors. For all importing functions in the utils package, this argument is TRUE, which means that you import strings as factors. This only makes sense if the strings you import represent categorical variables in R. If you set stringsAsFactors to FALSE, the data frame columns corresponding to strings in your text file will be character. # Import swimming_pools.csv correctly: pools pools &lt;- read.csv(&quot;data/swimming_pools.csv&quot;, stringsAsFactors = FALSE) # Check the structure of pools str(pools) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... ## $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... 9.1.2 read.delim There are also the .txt files which are basically text files. You can import these functions with read.delim(). Defaults header = TRUE sep = \"\\t\" # Import hotdogs.txt: hotdogs hotdogs &lt;- read.delim(&quot;data/hotdogs.txt&quot;, header = FALSE) # Summarize hotdogs summary(hotdogs) ## V1 V2 V3 ## Length:54 Min. : 86.0 Min. :144.0 ## Class :character 1st Qu.:132.0 1st Qu.:362.5 ## Mode :character Median :145.0 Median :405.0 ## Mean :145.4 Mean :424.8 ## 3rd Qu.:172.8 3rd Qu.:503.5 ## Max. :195.0 Max. :645.0 Add column names by col.names(). # Finish the read.delim() call hotdogs &lt;- read.delim(&quot;data/hotdogs.txt&quot;, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;)) # Select the hot dog with the least calories: lily lily &lt;- hotdogs[which.min(hotdogs$calories), ] # Select the observation with the most sodium: tom tom &lt;- hotdogs[which.max(hotdogs$sodium), ] # Print lily and tom rbind(lily, tom) ## type calories sodium ## 50 Poultry 86 358 ## 15 Beef 190 645 By setting the colClasses argument to a vector of strings representing classes. If a column is set to \"NULL\" in the colClasses vector, this column will be skipped and will not be loaded into the data frame. # Display structure of hotdogs str(hotdogs) ## &#39;data.frame&#39;: 54 obs. of 3 variables: ## $ type : chr &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; ... ## $ calories: int 186 181 176 149 184 190 158 139 175 148 ... ## $ sodium : int 495 477 425 322 482 587 370 322 479 375 ... # Edit the colClasses argument to import the data correctly: hotdogs2 hotdogs2 &lt;- read.delim(&quot;data/hotdogs.txt&quot;, header = FALSE, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), colClasses = c(&quot;factor&quot;, &quot;NULL&quot;, &quot;numeric&quot;)) # Display structure of hotdogs2 str(hotdogs2) ## &#39;data.frame&#39;: 54 obs. of 2 variables: ## $ type : Factor w/ 3 levels &quot;Beef&quot;,&quot;Meat&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ sodium: num 495 477 425 322 482 587 370 322 479 375 ... 9.1.3 read.table If you’re dealing with more exotic flat file formats, you’ll want to use read.table(). It’s the most basic importing function; you can specify tons of different arguments in this function. Defaults header = FALSE sep = \"\" # Path to the hotdogs.txt file: path path &lt;- file.path(&quot;data&quot;, &quot;hotdogs.txt&quot;) # Import the hotdogs.txt file: hotdogs hotdogs &lt;- read.table(path, sep = &quot;\\t&quot;, col.names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;)) # Call head() on hotdogs head(hotdogs) ## type calories sodium ## 1 Beef 186 495 ## 2 Beef 181 477 ## 3 Beef 176 425 ## 4 Beef 149 322 ## 5 Beef 184 482 ## 6 Beef 190 587 9.2 readr &amp; data.table 9.2.1 readr 9.2.1.1 read_csv # Load the readr package library(readr) # Import potatoes.csv with read_csv(): potatoes potatoes &lt;- read_csv(&quot;data/potatoes.csv&quot;) ## Rows: 160 Columns: 8 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## dbl (8): area, temp, size, storage, method, texture, flavor, moistness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 9.2.1.2 read_tsv TSV is short for tab-separated values. # Column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) # Import potatoes.txt: potatoes potatoes &lt;- read_tsv(&quot;data/potatoes.txt&quot;, col_names = properties) ## Rows: 160 Columns: 8 ## ── Column specification ────────────────────── ## Delimiter: &quot;\\t&quot; ## dbl (8): area, temp, size, storage, method, texture, flavor, moistness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Call head() on potatoes head(potatoes) ## # A tibble: 6 × 8 ## area temp size storage method texture flavor moistness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3 1.7 9.2.1.3 read_delim Just as read.table() was the main utils function, read_delim() is the main readr function. read_delim() takes two mandatory arguments: file: the file that contains the data delim: the character that separates the values in the data file others arguments: col_names: use if there no column names col_types: use if wanna manually set the types # Column names properties &lt;- c(&quot;area&quot;, &quot;temp&quot;, &quot;size&quot;, &quot;storage&quot;, &quot;method&quot;, &quot;texture&quot;, &quot;flavor&quot;, &quot;moistness&quot;) # Import potatoes.txt using read_delim(): potatoes potatoes &lt;- read_delim(&quot;data/potatoes.txt&quot;, delim = &quot;\\t&quot;, col_names = properties) ## Rows: 160 Columns: 8 ## ── Column specification ────────────────────── ## Delimiter: &quot;\\t&quot; ## dbl (8): area, temp, size, storage, method, texture, flavor, moistness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print out potatoes potatoes ## # A tibble: 160 × 8 ## area temp size storage method texture flavor moistness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3 3 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2 2.8 1.9 ## # ℹ 150 more rows Through skip and n_max you can control which part of your flat file you’re actually importing into R. skip specifies the number of rows you’re ignoring in the flat file before actually starting to import data. n_max specifies the number of rows you’re actually importing. Say for example you have a CSV file with 20 rows, and set skip = 2 and n_max = 3, you’re only reading in rows 3, 4 and 5 of the file. Watch out: Once you skip some rows, you also skip the first row that can contain column names! # Import observations 7, 8, 9, 10 and 11 # Import 5 observations from potatoes.txt: potatoes_fragment potatoes_fragment &lt;- read_tsv(&quot;data/potatoes.txt&quot;, skip = 6, n_max = 5, col_names = properties) ## Rows: 5 Columns: 8 ## ── Column specification ────────────────────── ## Delimiter: &quot;\\t&quot; ## dbl (8): area, temp, size, storage, method, texture, flavor, moistness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. potatoes_fragment ## # A tibble: 5 × 8 ## area temp size storage method texture flavor moistness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 2 2 2.6 3.1 2.4 ## 2 1 1 1 2 3 3 3 2.9 ## 3 1 1 1 2 4 2.2 3.2 2.5 ## 4 1 1 1 2 5 2 2.8 1.9 ## 5 1 1 1 3 1 1.8 2.6 1.5 You specify which types the columns with col_types. You can manually set the types with a string, where each character denotes the class of the column: character, double, integer and logical. _ skips the column as a whole. # Import all data, but force all columns to be character: potatoes_char potatoes_char &lt;- read_tsv(&quot;data/potatoes.txt&quot;, col_types = &quot;cccccccc&quot;, col_names = properties) # Print out structure of potatoes_char str(potatoes_char) ## spc_tbl_ [160 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ area : chr [1:160] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ temp : chr [1:160] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ size : chr [1:160] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ storage : chr [1:160] &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; ... ## $ method : chr [1:160] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ texture : chr [1:160] &quot;2.9&quot; &quot;2.3&quot; &quot;2.5&quot; &quot;2.1&quot; ... ## $ flavor : chr [1:160] &quot;3.2&quot; &quot;2.5&quot; &quot;2.8&quot; &quot;2.9&quot; ... ## $ moistness: chr [1:160] &quot;3.0&quot; &quot;2.6&quot; &quot;2.8&quot; &quot;2.4&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. area = col_character(), ## .. temp = col_character(), ## .. size = col_character(), ## .. storage = col_character(), ## .. method = col_character(), ## .. texture = col_character(), ## .. flavor = col_character(), ## .. moistness = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; Another way of setting the types of the imported columns is using collectors. Collector functions can be passed in a list() to the col_types argument of read_ functions to tell them how to interpret values in a column. For this exercise you will need two collector functions: col_integer(): the column should be interpreted as an integer. col_factor(levels, ordered = FALSE): the column should be interpreted as a factor with levels. # Display the summary of hotdogs summary(hotdogs) ## type calories sodium ## Length:54 Min. : 86.0 Min. :144.0 ## Class :character 1st Qu.:132.0 1st Qu.:362.5 ## Mode :character Median :145.0 Median :405.0 ## Mean :145.4 Mean :424.8 ## 3rd Qu.:172.8 3rd Qu.:503.5 ## Max. :195.0 Max. :645.0 # The collectors you will need to import the data fac &lt;- col_factor(levels = c(&quot;Beef&quot;, &quot;Meat&quot;, &quot;Poultry&quot;)) int &lt;- col_integer() # Edit the col_types argument to import the data correctly: hotdogs_factor hotdogs_factor &lt;- read_tsv(&quot;data/hotdogs.txt&quot;, col_names = c(&quot;type&quot;, &quot;calories&quot;, &quot;sodium&quot;), col_types = list(fac, int, int)) # Display the summary of hotdogs_factor summary(hotdogs_factor) ## type calories sodium ## Beef :20 Min. : 86.0 Min. :144.0 ## Meat :17 1st Qu.:132.0 1st Qu.:362.5 ## Poultry:17 Median :145.0 Median :405.0 ## Mean :145.4 Mean :424.8 ## 3rd Qu.:172.8 3rd Qu.:503.5 ## Max. :195.0 Max. :645.0 9.2.2 data.table 9.2.2.1 fread Infer column types and separators It simply works Extremely fast Possible to specify numerous parameters Improved read.table() Fast, convenient, customizable # load the data.table package using library() library(data.table) ## ## Attaching package: &#39;data.table&#39; ## The following objects are masked from &#39;package:lubridate&#39;: ## ## hour, isoweek, mday, minute, month, quarter, second, wday, week, ## yday, year ## The following object is masked from &#39;package:purrr&#39;: ## ## transpose ## The following objects are masked from &#39;package:dplyr&#39;: ## ## between, first, last # Import potatoes.csv with fread(): potatoes potatoes &lt;- fread(&quot;data/potatoes.csv&quot;) # Print out potatoes potatoes ## area temp size storage method texture flavor moistness ## 1: 1 1 1 1 1 2.9 3.2 3.0 ## 2: 1 1 1 1 2 2.3 2.5 2.6 ## 3: 1 1 1 1 3 2.5 2.8 2.8 ## 4: 1 1 1 1 4 2.1 2.9 2.4 ## 5: 1 1 1 1 5 1.9 2.8 2.2 ## --- ## 156: 2 2 2 4 1 2.7 3.3 2.6 ## 157: 2 2 2 4 2 2.6 2.8 2.3 ## 158: 2 2 2 4 3 2.5 3.1 2.6 ## 159: 2 2 2 4 4 3.4 3.3 3.0 ## 160: 2 2 2 4 5 2.5 2.8 2.3 There are two arguments of the fread() function: drop and select, to drop or select variables of interest. # Suppose you have a dataset that contains 5 variables and you want to keep the first and fifth variable, named &quot;a&quot; and &quot;e&quot;. fread(&quot;path/to/file.txt&quot;, drop = 2:4) fread(&quot;path/to/file.txt&quot;, select = c(1, 5)) fread(&quot;path/to/file.txt&quot;, drop = c(&quot;b&quot;, &quot;c&quot;, &quot;d&quot;)) fread(&quot;path/to/file.txt&quot;, select = c(&quot;a&quot;, &quot;e&quot;)) # Import columns 6 and 8 of potatoes.csv: potatoes potatoes_fread &lt;- fread(&quot;data/potatoes.csv&quot;, select = c(6, 8)) # Plot texture (x) and moistness (y) of potatoes library(ggplot2) ggplot(potatoes_fread, aes(x = texture, y = moistness)) + geom_point() The class of the result: fread(): data.table and data.frame read_csv(): tbl_df, tbl, data.frame 9.3 Excel data 9.3.1 List the sheets of xls file Before you can start importing from Excel, you should find out which sheets are available in the workbook. You can use the excel_sheets() function for this. # Load the readxl package library(readxl) # Print the names of all worksheets excel_sheets(&quot;data/urbanpop.xlsx&quot;) ## [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; 9.3.2 Import an Excel sheet You can do this with the read_excel() function. Have a look at this recipe: data &lt;- read_excel(&quot;data.xlsx&quot;, sheet = &quot;my_sheet&quot;) This call simply imports the sheet with the name \"my_sheet\" from the \"data.xlsx\" file. You can also pass a number to the sheet argument; this will cause read_excel() to import the sheet with the given sheet number. sheet = 1 will import the first sheet, sheet = 2 will import the second sheet, and so on. # Read the sheets, one by one # pop_1 same as read_excel(&quot;data/urbanpop.xlsx&quot;, sheet = &quot;1960-1966&quot;) pop_1 &lt;- read_excel(&quot;data/urbanpop.xlsx&quot;, sheet = 1) pop_2 &lt;- read_excel(&quot;data/urbanpop.xlsx&quot;, sheet = 2) pop_3 &lt;- read_excel(&quot;data/urbanpop.xlsx&quot;, sheet = 3) # Put pop_1, pop_2 and pop_3 in a list: pop_list pop_list &lt;- list(pop_1, pop_2, pop_3) # Display the structure of pop_list str(pop_list) ## List of 3 ## $ : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1960 : num [1:209] 769308 494443 3293999 NA NA ... ## ..$ 1961 : num [1:209] 814923 511803 3515148 13660 8724 ... ## ..$ 1962 : num [1:209] 858522 529439 3739963 14166 9700 ... ## ..$ 1963 : num [1:209] 903914 547377 3973289 14759 10748 ... ## ..$ 1964 : num [1:209] 951226 565572 4220987 15396 11866 ... ## ..$ 1965 : num [1:209] 1000582 583983 4488176 16045 13053 ... ## ..$ 1966 : num [1:209] 1058743 602512 4649105 16693 14217 ... ## $ : tibble [209 × 9] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1967 : num [1:209] 1119067 621180 4826104 17349 15440 ... ## ..$ 1968 : num [1:209] 1182159 639964 5017299 17996 16727 ... ## ..$ 1969 : num [1:209] 1248901 658853 5219332 18619 18088 ... ## ..$ 1970 : num [1:209] 1319849 677839 5429743 19206 19529 ... ## ..$ 1971 : num [1:209] 1409001 698932 5619042 19752 20929 ... ## ..$ 1972 : num [1:209] 1502402 720207 5815734 20263 22406 ... ## ..$ 1973 : num [1:209] 1598835 741681 6020647 20742 23937 ... ## ..$ 1974 : num [1:209] 1696445 763385 6235114 21194 25482 ... ## $ : tibble [209 × 38] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1975 : num [1:209] 1793266 785350 6460138 21632 27019 ... ## ..$ 1976 : num [1:209] 1905033 807990 6774099 22047 28366 ... ## ..$ 1977 : num [1:209] 2021308 830959 7102902 22452 29677 ... ## ..$ 1978 : num [1:209] 2142248 854262 7447728 22899 31037 ... ## ..$ 1979 : num [1:209] 2268015 877898 7810073 23457 32572 ... ## ..$ 1980 : num [1:209] 2398775 901884 8190772 24177 34366 ... ## ..$ 1981 : num [1:209] 2493265 927224 8637724 25173 36356 ... ## ..$ 1982 : num [1:209] 2590846 952447 9105820 26342 38618 ... ## ..$ 1983 : num [1:209] 2691612 978476 9591900 27655 40983 ... ## ..$ 1984 : num [1:209] 2795656 1006613 10091289 29062 43207 ... ## ..$ 1985 : num [1:209] 2903078 1037541 10600112 30524 45119 ... ## ..$ 1986 : num [1:209] 3006983 1072365 11101757 32014 46254 ... ## ..$ 1987 : num [1:209] 3113957 1109954 11609104 33548 47019 ... ## ..$ 1988 : num [1:209] 3224082 1146633 12122941 35095 47669 ... ## ..$ 1989 : num [1:209] 3337444 1177286 12645263 36618 48577 ... ## ..$ 1990 : num [1:209] 3454129 1198293 13177079 38088 49982 ... ## ..$ 1991 : num [1:209] 3617842 1215445 13708813 39600 51972 ... ## ..$ 1992 : num [1:209] 3788685 1222544 14248297 41049 54469 ... ## ..$ 1993 : num [1:209] 3966956 1222812 14789176 42443 57079 ... ## ..$ 1994 : num [1:209] 4152960 1221364 15322651 43798 59243 ... ## ..$ 1995 : num [1:209] 4347018 1222234 15842442 45129 60598 ... ## ..$ 1996 : num [1:209] 4531285 1228760 16395553 46343 60927 ... ## ..$ 1997 : num [1:209] 4722603 1238090 16935451 47527 60462 ... ## ..$ 1998 : num [1:209] 4921227 1250366 17469200 48705 59685 ... ## ..$ 1999 : num [1:209] 5127421 1265195 18007937 49906 59281 ... ## ..$ 2000 : num [1:209] 5341456 1282223 18560597 51151 59719 ... ## ..$ 2001 : num [1:209] 5564492 1315690 19198872 52341 61062 ... ## ..$ 2002 : num [1:209] 5795940 1352278 19854835 53583 63212 ... ## ..$ 2003 : num [1:209] 6036100 1391143 20529356 54864 65802 ... ## ..$ 2004 : num [1:209] 6285281 1430918 21222198 56166 68301 ... ## ..$ 2005 : num [1:209] 6543804 1470488 21932978 57474 70329 ... ## ..$ 2006 : num [1:209] 6812538 1512255 22625052 58679 71726 ... ## ..$ 2007 : num [1:209] 7091245 1553491 23335543 59894 72684 ... ## ..$ 2008 : num [1:209] 7380272 1594351 24061749 61118 73335 ... ## ..$ 2009 : num [1:209] 7679982 1635262 24799591 62357 73897 ... ## ..$ 2010 : num [1:209] 7990746 1676545 25545622 63616 74525 ... ## ..$ 2011 : num [1:209] 8316976 1716842 26216968 64817 75207 ... Import with lapply Loading in every sheet manually and then merging them in a list can be quite tedious. Luckily, you can automate this with lapply(). my_workbook &lt;- lapply(excel_sheets(&quot;data.xlsx&quot;), read_excel, path = &quot;data.xlsx&quot;) # Read all Excel sheets with lapply(): pop_list pop_list &lt;- lapply(excel_sheets(&quot;data/urbanpop.xlsx&quot;), read_excel, path = &quot;data/urbanpop.xlsx&quot;) # Display the structure of pop_list str(pop_list) ## List of 3 ## $ : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1960 : num [1:209] 769308 494443 3293999 NA NA ... ## ..$ 1961 : num [1:209] 814923 511803 3515148 13660 8724 ... ## ..$ 1962 : num [1:209] 858522 529439 3739963 14166 9700 ... ## ..$ 1963 : num [1:209] 903914 547377 3973289 14759 10748 ... ## ..$ 1964 : num [1:209] 951226 565572 4220987 15396 11866 ... ## ..$ 1965 : num [1:209] 1000582 583983 4488176 16045 13053 ... ## ..$ 1966 : num [1:209] 1058743 602512 4649105 16693 14217 ... ## $ : tibble [209 × 9] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1967 : num [1:209] 1119067 621180 4826104 17349 15440 ... ## ..$ 1968 : num [1:209] 1182159 639964 5017299 17996 16727 ... ## ..$ 1969 : num [1:209] 1248901 658853 5219332 18619 18088 ... ## ..$ 1970 : num [1:209] 1319849 677839 5429743 19206 19529 ... ## ..$ 1971 : num [1:209] 1409001 698932 5619042 19752 20929 ... ## ..$ 1972 : num [1:209] 1502402 720207 5815734 20263 22406 ... ## ..$ 1973 : num [1:209] 1598835 741681 6020647 20742 23937 ... ## ..$ 1974 : num [1:209] 1696445 763385 6235114 21194 25482 ... ## $ : tibble [209 × 38] (S3: tbl_df/tbl/data.frame) ## ..$ country: chr [1:209] &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## ..$ 1975 : num [1:209] 1793266 785350 6460138 21632 27019 ... ## ..$ 1976 : num [1:209] 1905033 807990 6774099 22047 28366 ... ## ..$ 1977 : num [1:209] 2021308 830959 7102902 22452 29677 ... ## ..$ 1978 : num [1:209] 2142248 854262 7447728 22899 31037 ... ## ..$ 1979 : num [1:209] 2268015 877898 7810073 23457 32572 ... ## ..$ 1980 : num [1:209] 2398775 901884 8190772 24177 34366 ... ## ..$ 1981 : num [1:209] 2493265 927224 8637724 25173 36356 ... ## ..$ 1982 : num [1:209] 2590846 952447 9105820 26342 38618 ... ## ..$ 1983 : num [1:209] 2691612 978476 9591900 27655 40983 ... ## ..$ 1984 : num [1:209] 2795656 1006613 10091289 29062 43207 ... ## ..$ 1985 : num [1:209] 2903078 1037541 10600112 30524 45119 ... ## ..$ 1986 : num [1:209] 3006983 1072365 11101757 32014 46254 ... ## ..$ 1987 : num [1:209] 3113957 1109954 11609104 33548 47019 ... ## ..$ 1988 : num [1:209] 3224082 1146633 12122941 35095 47669 ... ## ..$ 1989 : num [1:209] 3337444 1177286 12645263 36618 48577 ... ## ..$ 1990 : num [1:209] 3454129 1198293 13177079 38088 49982 ... ## ..$ 1991 : num [1:209] 3617842 1215445 13708813 39600 51972 ... ## ..$ 1992 : num [1:209] 3788685 1222544 14248297 41049 54469 ... ## ..$ 1993 : num [1:209] 3966956 1222812 14789176 42443 57079 ... ## ..$ 1994 : num [1:209] 4152960 1221364 15322651 43798 59243 ... ## ..$ 1995 : num [1:209] 4347018 1222234 15842442 45129 60598 ... ## ..$ 1996 : num [1:209] 4531285 1228760 16395553 46343 60927 ... ## ..$ 1997 : num [1:209] 4722603 1238090 16935451 47527 60462 ... ## ..$ 1998 : num [1:209] 4921227 1250366 17469200 48705 59685 ... ## ..$ 1999 : num [1:209] 5127421 1265195 18007937 49906 59281 ... ## ..$ 2000 : num [1:209] 5341456 1282223 18560597 51151 59719 ... ## ..$ 2001 : num [1:209] 5564492 1315690 19198872 52341 61062 ... ## ..$ 2002 : num [1:209] 5795940 1352278 19854835 53583 63212 ... ## ..$ 2003 : num [1:209] 6036100 1391143 20529356 54864 65802 ... ## ..$ 2004 : num [1:209] 6285281 1430918 21222198 56166 68301 ... ## ..$ 2005 : num [1:209] 6543804 1470488 21932978 57474 70329 ... ## ..$ 2006 : num [1:209] 6812538 1512255 22625052 58679 71726 ... ## ..$ 2007 : num [1:209] 7091245 1553491 23335543 59894 72684 ... ## ..$ 2008 : num [1:209] 7380272 1594351 24061749 61118 73335 ... ## ..$ 2009 : num [1:209] 7679982 1635262 24799591 62357 73897 ... ## ..$ 2010 : num [1:209] 7990746 1676545 25545622 63616 74525 ... ## ..$ 2011 : num [1:209] 8316976 1716842 26216968 64817 75207 ... Now that you can read in Excel data, let’s try to clean and merge it. # Extend the cbind() call to include urban_sheet3: urban urban &lt;- cbind(pop_list[[1]], pop_list[[2]][-1], pop_list[[3]][-1]) # Remove all rows with NAs from urban: urban_clean urban_clean &lt;- na.omit(urban) # Print out a summary of urban_clean summary(urban_clean) ## country 1960 1961 1962 ## Length:197 Min. : 3378 Min. : 3433 Min. : 3481 ## Class :character 1st Qu.: 87735 1st Qu.: 92905 1st Qu.: 98331 ## Mode :character Median : 599714 Median : 630788 Median : 659464 ## Mean : 5012388 Mean : 5282488 Mean : 5440972 ## 3rd Qu.: 3130085 3rd Qu.: 3155370 3rd Qu.: 3250211 ## Max. :126469700 Max. :129268133 Max. :131974143 ## 1963 1964 1965 ## Min. : 3532 Min. : 3586 Min. : 3644 ## 1st Qu.: 104988 1st Qu.: 112084 1st Qu.: 119322 ## Median : 704989 Median : 740609 Median : 774957 ## Mean : 5612312 Mean : 5786961 Mean : 5964970 ## 3rd Qu.: 3416490 3rd Qu.: 3585464 3rd Qu.: 3666724 ## Max. :134599886 Max. :137205240 Max. :139663053 ## 1966 1967 1968 ## Min. : 3706 Min. : 3771 Min. : 3835 ## 1st Qu.: 128565 1st Qu.: 138024 1st Qu.: 147846 ## Median : 809768 Median : 838449 Median : 890270 ## Mean : 6126413 Mean : 6288771 Mean : 6451367 ## 3rd Qu.: 3871757 3rd Qu.: 4019906 3rd Qu.: 4158186 ## Max. :141962708 Max. :144201722 Max. :146340364 ## 1969 1970 1971 ## Min. : 3893 Min. : 3941 Min. : 4017 ## 1st Qu.: 158252 1st Qu.: 171063 1st Qu.: 181483 ## Median : 929450 Median : 976471 Median : 1008630 ## Mean : 6624909 Mean : 6799110 Mean : 6980895 ## 3rd Qu.: 4300669 3rd Qu.: 4440047 3rd Qu.: 4595966 ## Max. :148475901 Max. :150922373 Max. :152863831 ## 1972 1973 1974 ## Min. : 4084 Min. : 4146 Min. : 4206 ## 1st Qu.: 189492 1st Qu.: 197792 1st Qu.: 205410 ## Median : 1048738 Median : 1097293 Median : 1159402 ## Mean : 7165338 Mean : 7349454 Mean : 7540446 ## 3rd Qu.: 4766545 3rd Qu.: 4838297 3rd Qu.: 4906384 ## Max. :154530473 Max. :156034106 Max. :157488074 ## 1975 1976 1977 ## Min. : 4267 Min. : 4334 Min. : 4402 ## 1st Qu.: 211746 1st Qu.: 216991 1st Qu.: 222209 ## Median : 1223146 Median : 1249829 Median : 1311276 ## Mean : 7731973 Mean : 7936401 Mean : 8145945 ## 3rd Qu.: 5003370 3rd Qu.: 5121118 3rd Qu.: 5227677 ## Max. :159452730 Max. :165583752 Max. :171550310 ## 1978 1979 1980 ## Min. : 4470 Min. : 4539 Min. : 4607 ## 1st Qu.: 227605 1st Qu.: 233461 1st Qu.: 242583 ## Median : 1340811 Median : 1448185 Median : 1592397 ## Mean : 8361360 Mean : 8583138 Mean : 8808772 ## 3rd Qu.: 5352746 3rd Qu.: 5558850 3rd Qu.: 5815772 ## Max. :177605736 Max. :183785364 Max. :189947471 ## 1981 1982 1983 ## Min. : 4645 Min. : 4681 Min. : 4716 ## 1st Qu.: 248948 1st Qu.: 257944 1st Qu.: 274139 ## Median : 1673079 Median : 1713060 Median : 1730626 ## Mean : 9049163 Mean : 9295226 Mean : 9545035 ## 3rd Qu.: 6070457 3rd Qu.: 6337995 3rd Qu.: 6619987 ## Max. :199385258 Max. :209435968 Max. :219680098 ## 1984 1985 1986 ## Min. : 4750 Min. : 4782 Min. : 4809 ## 1st Qu.: 284939 1st Qu.: 300928 1st Qu.: 307699 ## Median : 1749033 Median : 1786125 Median : 1850910 ## Mean : 9798559 Mean : 10058661 Mean : 10323839 ## 3rd Qu.: 6918261 3rd Qu.: 6931780 3rd Qu.: 6935763 ## Max. :229872397 Max. :240414890 Max. :251630158 ## 1987 1988 1989 ## Min. : 4835 Min. : 4859 Min. : 4883 ## 1st Qu.: 321125 1st Qu.: 334616 1st Qu.: 347348 ## Median : 1953694 Median : 1997011 Median : 1993544 ## Mean : 10595817 Mean : 10873041 Mean : 11154458 ## 3rd Qu.: 6939905 3rd Qu.: 6945022 3rd Qu.: 6885378 ## Max. :263433513 Max. :275570541 Max. :287810747 ## 1990 1991 1992 ## Min. : 4907 Min. : 4946 Min. : 4985 ## 1st Qu.: 370152 1st Qu.: 394611 1st Qu.: 418788 ## Median : 2066505 Median : 2150230 Median : 2237405 ## Mean : 11438543 Mean : 11725076 Mean : 12010922 ## 3rd Qu.: 6830026 3rd Qu.: 6816589 3rd Qu.: 6820099 ## Max. :300165618 Max. :314689997 Max. :329099365 ## 1993 1994 1995 ## Min. : 5024 Min. : 5062 Min. : 5100 ## 1st Qu.: 427457 1st Qu.: 435959 1st Qu.: 461993 ## Median : 2322158 Median : 2410297 Median : 2482393 ## Mean : 12296949 Mean : 12582930 Mean : 12871480 ## 3rd Qu.: 7139656 3rd Qu.: 7499901 3rd Qu.: 7708571 ## Max. :343555327 Max. :358232230 Max. :373035157 ## 1996 1997 1998 ## Min. : 5079 Min. : 5055 Min. : 5029 ## 1st Qu.: 488136 1st Qu.: 494203 1st Qu.: 498002 ## Median : 2522460 Median : 2606125 Median : 2664983 ## Mean : 13165924 Mean : 13463675 Mean : 13762861 ## 3rd Qu.: 7686092 3rd Qu.: 7664316 3rd Qu.: 7784056 ## Max. :388936607 Max. :405031716 Max. :421147610 ## 1999 2000 2001 ## Min. : 5001 Min. : 4971 Min. : 5003 ## 1st Qu.: 505144 1st Qu.: 525629 1st Qu.: 550638 ## Median : 2737809 Median : 2826647 Median : 2925851 ## Mean : 14063387 Mean : 14369278 Mean : 14705743 ## 3rd Qu.: 8083488 3rd Qu.: 8305564 3rd Qu.: 8421967 ## Max. :437126845 Max. :452999147 Max. :473204511 ## 2002 2003 2004 ## Min. : 5034 Min. : 5064 Min. : 5090 ## 1st Qu.: 567531 1st Qu.: 572094 1st Qu.: 593900 ## Median : 2928252 Median : 2944934 Median : 2994356 ## Mean : 15043381 Mean : 15384513 Mean : 15730299 ## 3rd Qu.: 8448628 3rd Qu.: 8622732 3rd Qu.: 8999112 ## Max. :493402140 Max. :513607776 Max. :533892175 ## 2005 2006 2007 ## Min. : 5111 Min. : 5135 Min. : 5155 ## 1st Qu.: 620511 1st Qu.: 632659 1st Qu.: 645172 ## Median : 3057923 Median : 3269963 Median : 3432024 ## Mean : 16080262 Mean : 16435872 Mean : 16797484 ## 3rd Qu.: 9394001 3rd Qu.: 9689807 3rd Qu.: 9803381 ## Max. :554367818 Max. :575050081 Max. :595731464 ## 2008 2009 2010 ## Min. : 5172 Min. : 5189 Min. : 5206 ## 1st Qu.: 658017 1st Qu.: 671085 1st Qu.: 684302 ## Median : 3589395 Median : 3652338 Median : 3676309 ## Mean : 17164898 Mean : 17533997 Mean : 17904811 ## 3rd Qu.: 10210317 3rd Qu.: 10518289 3rd Qu.: 10618596 ## Max. :616552722 Max. :637533976 Max. :658557734 ## 2011 ## Min. : 5233 ## 1st Qu.: 698009 ## Median : 3664664 ## Mean : 18276297 ## 3rd Qu.: 10731193 ## Max. :678796403 9.3.3 col_names &amp; skip argument Default: read_excel(path, sheet = num, col_names = TRUE, col_types = NULL, skip = 0) You can set col_names to FALSE. In this case, R will choose column names for you. You can also choose to set col_names to a character vector with names for each column. # Import the first Excel sheet of urbanpop_nonames.xlsx (R gives names): pop_a pop_a &lt;- read_excel(&quot;data/urbanpop_nonames.xlsx&quot;, sheet = 1, col_names = FALSE) ## New names: ## • `` -&gt; `...1` ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...6` ## • `` -&gt; `...7` ## • `` -&gt; `...8` # Import the first Excel sheet of urbanpop_nonames.xlsx (specify col_names): pop_b cols &lt;- c(&quot;country&quot;, paste0(&quot;year_&quot;, 1960:1966)) pop_b &lt;- read_excel(&quot;data/urbanpop_nonames.xlsx&quot;, sheet = 1, col_names = cols) # Print the summary of pop_a summary(pop_a) ## ...1 ...2 ...3 ...4 ## Length:209 Min. : 3378 Min. : 1028 Min. : 1090 ## Class :character 1st Qu.: 88978 1st Qu.: 70644 1st Qu.: 74974 ## Mode :character Median : 580675 Median : 570159 Median : 593968 ## Mean : 4988124 Mean : 4991613 Mean : 5141592 ## 3rd Qu.: 3077228 3rd Qu.: 2807280 3rd Qu.: 2948396 ## Max. :126469700 Max. :129268133 Max. :131974143 ## NA&#39;s :11 ## ...5 ...6 ...7 ## Min. : 1154 Min. : 1218 Min. : 1281 ## 1st Qu.: 81870 1st Qu.: 84953 1st Qu.: 88633 ## Median : 619331 Median : 645262 Median : 679109 ## Mean : 5303711 Mean : 5468966 Mean : 5637394 ## 3rd Qu.: 3148941 3rd Qu.: 3296444 3rd Qu.: 3317422 ## Max. :134599886 Max. :137205240 Max. :139663053 ## ## ...8 ## Min. : 1349 ## 1st Qu.: 93638 ## Median : 735139 ## Mean : 5790281 ## 3rd Qu.: 3418036 ## Max. :141962708 ## # Print the summary of pop_b summary(pop_b) ## country year_1960 year_1961 year_1962 ## Length:209 Min. : 3378 Min. : 1028 Min. : 1090 ## Class :character 1st Qu.: 88978 1st Qu.: 70644 1st Qu.: 74974 ## Mode :character Median : 580675 Median : 570159 Median : 593968 ## Mean : 4988124 Mean : 4991613 Mean : 5141592 ## 3rd Qu.: 3077228 3rd Qu.: 2807280 3rd Qu.: 2948396 ## Max. :126469700 Max. :129268133 Max. :131974143 ## NA&#39;s :11 ## year_1963 year_1964 year_1965 ## Min. : 1154 Min. : 1218 Min. : 1281 ## 1st Qu.: 81870 1st Qu.: 84953 1st Qu.: 88633 ## Median : 619331 Median : 645262 Median : 679109 ## Mean : 5303711 Mean : 5468966 Mean : 5637394 ## 3rd Qu.: 3148941 3rd Qu.: 3296444 3rd Qu.: 3317422 ## Max. :134599886 Max. :137205240 Max. :139663053 ## ## year_1966 ## Min. : 1349 ## 1st Qu.: 93638 ## Median : 735139 ## Mean : 5790281 ## 3rd Qu.: 3418036 ## Max. :141962708 ## With skip, you can tell R to ignore a specified number of rows inside the Excel sheets you’re trying to pull data from. If the first row of this sheet contained the column names, this information will also be ignored by readxl. Make sure to set col_names to FALSE or manually specify column names in this case! # Import the second sheet of urbanpop.xlsx, skipping the first 21 rows: urbanpop_sel urbanpop_sel &lt;- read_excel(&quot;data/urbanpop.xlsx&quot;, sheet = 2, col_names = FALSE, skip = 21) ## New names: ## • `` -&gt; `...1` ## • `` -&gt; `...2` ## • `` -&gt; `...3` ## • `` -&gt; `...4` ## • `` -&gt; `...5` ## • `` -&gt; `...6` ## • `` -&gt; `...7` ## • `` -&gt; `...8` ## • `` -&gt; `...9` # Print out the first observation from urbanpop_sel head(urbanpop_sel, 1) ## # A tibble: 1 × 9 ## ...1 ...2 ...3 ...4 ...5 ...6 ...7 ...8 ...9 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Benin 382022. 411859. 443013. 475611. 515820. 557938. 602093. 648410. 9.4 Reproducible Excel work - XLConnect 9.4.1 Adapting sheets Bridge between Excel and R XLS and XLSX 9.4.1.1 Connect to a workbook When working with XLConnect, the first step will be to load a workbook in your R session with loadWorkbook(); this function will build a “bridge” between your Excel file and your R session. # Load the XLConnect package library(XLConnect) ## Warning: package &#39;XLConnect&#39; was built under R version 4.3.1 ## XLConnect 1.0.7 by Mirai Solutions GmbH [aut], ## Martin Studer [cre], ## The Apache Software Foundation [ctb, cph] (Apache POI), ## Graph Builder [ctb, cph] (Curvesapi Java library), ## Brett Woolridge [ctb, cph] (SparseBitSet Java library) ## https://mirai-solutions.ch ## https://github.com/miraisolutions/xlconnect # Build connection to urbanpop.xlsx: my_book my_book &lt;- loadWorkbook(&quot;data/urbanpop.xlsx&quot;) # Print out the class of my_book class(my_book) ## [1] &quot;workbook&quot; ## attr(,&quot;package&quot;) ## [1] &quot;XLConnect&quot; 9.4.1.2 List &amp; read Excel sheets To list the sheets in an Excel file, use getSheets(). To actually import data from a sheet, you can use readWorksheet(). # List the sheets in my_book getSheets(my_book) ## [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; # Import the second sheet in my_book readWorksheet(my_book, sheet = 2) ## country X1967 X1968 X1969 ## 1 Afghanistan 1.119067e+06 1.182159e+06 1.248901e+06 ## 2 Albania 6.211798e+05 6.399645e+05 6.588531e+05 ## 3 Algeria 4.826104e+06 5.017299e+06 5.219332e+06 ## 4 American Samoa 1.734866e+04 1.799551e+04 1.861868e+04 ## 5 Andorra 1.543962e+04 1.672699e+04 1.808832e+04 ## 6 Angola 7.574963e+05 7.984593e+05 8.412620e+05 ## 7 Antigua and Barbuda 2.208625e+04 2.214939e+04 2.218292e+04 ## 8 Argentina 1.775328e+07 1.812410e+07 1.851046e+07 ## 9 Armenia 1.337032e+06 1.392892e+06 1.449641e+06 ## 10 Aruba 2.941472e+04 2.957609e+04 2.973787e+04 ## 11 Australia 9.934404e+06 1.015397e+07 1.041239e+07 ## 12 Austria 4.803149e+06 4.831817e+06 4.852208e+06 ## 13 Azerbaijan 2.446990e+06 2.495725e+06 2.542062e+06 ## 14 Bahamas 9.868390e+04 1.036697e+05 1.084730e+05 ## 15 Bahrain 1.619616e+05 1.663785e+05 1.714590e+05 ## 16 Bangladesh 4.173453e+06 4.484842e+06 4.790505e+06 ## 17 Barbados 8.819371e+04 8.858041e+04 8.902489e+04 ## 18 Belarus 3.556448e+06 3.696854e+06 3.838003e+06 ## 19 Belgium 8.950504e+06 8.999366e+06 9.038506e+06 ## 20 Belize 5.879024e+04 5.971173e+04 6.049220e+04 ## 21 Benin 3.820221e+05 4.118595e+05 4.430131e+05 ## 22 Bermuda 5.200000e+04 5.300000e+04 5.400000e+04 ## 23 Bhutan 1.437897e+04 1.561689e+04 1.694642e+04 ## 24 Bolivia 1.527065e+06 1.575177e+06 1.625173e+06 ## 25 Bosnia and Herzegovina 8.516924e+05 8.902697e+05 9.294496e+05 ## 26 Botswana 3.431976e+04 4.057616e+04 4.722223e+04 ## 27 Brazil 4.719352e+07 4.931688e+07 5.148910e+07 ## 28 Brunei 6.128905e+04 6.622218e+04 7.150276e+04 ## 29 Bulgaria 4.019906e+06 4.158186e+06 4.300669e+06 ## 30 Burkina Faso 2.968238e+05 3.086611e+05 3.209607e+05 ## 31 Burundi 7.616560e+04 7.881625e+04 8.135573e+04 ## 32 Cambodia 8.357562e+05 9.263155e+05 1.017799e+06 ## 33 Cameroon 1.157892e+06 1.231243e+06 1.308158e+06 ## 34 Canada 1.510423e+07 1.546449e+07 1.579236e+07 ## 35 Cape Verde 4.724476e+04 4.923400e+04 5.135658e+04 ## 36 Cayman Islands 8.875000e+03 9.002000e+03 9.216000e+03 ## 37 Central African Republic 4.303721e+05 4.529338e+05 4.761054e+05 ## 38 Chad 3.315042e+05 3.605791e+05 3.909776e+05 ## 39 Channel Islands 4.329456e+04 4.344349e+04 4.358417e+04 ## 40 Chile 6.606825e+06 6.805959e+06 7.005123e+06 ## 41 China 1.343974e+08 1.368900e+08 1.396005e+08 ## 42 Colombia 1.033119e+07 1.078053e+07 1.123560e+07 ## 43 Comoros 3.978906e+04 4.183902e+04 4.396565e+04 ## 44 Congo, Dem. Rep. 5.161472e+06 5.475208e+06 5.802069e+06 ## 45 Congo, Rep. 4.506698e+05 4.733352e+05 4.972107e+05 ## 46 Costa Rica 6.217858e+05 6.499164e+05 6.782539e+05 ## 47 Cote d&#39;Ivoire 1.243350e+06 1.330719e+06 1.424438e+06 ## 48 Croatia 1.608233e+06 1.663051e+06 1.717607e+06 ## 49 Cuba 4.927341e+06 5.032014e+06 5.137260e+06 ## 50 Cyprus 2.319297e+05 2.378314e+05 2.439833e+05 ## 51 Czech Republic 6.204410e+06 6.266305e+06 6.326369e+06 ## 52 Denmark 3.777553e+06 3.826785e+06 3.874314e+06 ## 53 Djibouti 7.778804e+04 8.469435e+04 9.204577e+04 ## 54 Dominica 2.755036e+04 2.952732e+04 3.147562e+04 ## 55 Dominican Republic 1.535485e+06 1.625456e+06 1.718315e+06 ## 56 Ecuador 2.059355e+06 2.151395e+06 2.246891e+06 ## 57 Egypt 1.379817e+07 1.424834e+07 1.470386e+07 ## 58 El Salvador 1.345529e+06 1.387218e+06 1.429379e+06 ## 59 Equatorial Guinea 7.536450e+04 7.729503e+04 7.844574e+04 ## 60 Eritrea 2.025150e+05 2.121646e+05 2.221863e+05 ## 61 Estonia 8.283882e+05 8.472205e+05 8.662579e+05 ## 62 Ethiopia 2.139904e+06 2.249670e+06 2.365149e+06 ## 63 Faeroe Islands 9.878976e+03 1.017780e+04 1.047732e+04 ## 64 Fiji 1.632216e+05 1.690663e+05 1.749364e+05 ## 65 Finland 2.822234e+06 2.872371e+06 2.908120e+06 ## 66 France 3.486791e+07 3.554830e+07 3.622608e+07 ## 67 French Polynesia 5.087720e+04 5.421077e+04 5.768190e+04 ## 68 Gabon 1.380242e+05 1.478459e+05 1.582525e+05 ## 69 Gambia 7.036836e+04 7.628527e+04 8.261546e+04 ## 70 Georgia 1.863610e+06 1.900576e+06 1.938616e+06 ## 71 Germany 5.546852e+07 5.576506e+07 5.625874e+07 ## 72 Ghana 2.219604e+06 2.311442e+06 2.408851e+06 ## 73 Greece 4.300274e+06 4.415310e+06 4.518763e+06 ## 74 Greenland 2.879686e+04 3.040882e+04 3.206093e+04 ## 75 Grenada 3.004680e+04 3.019593e+04 3.031077e+04 ## 76 Guam 4.629560e+04 4.844571e+04 5.065242e+04 ## 77 Guatemala 1.739459e+06 1.802725e+06 1.868309e+06 ## 78 Guinea 5.618868e+05 5.962425e+05 6.304226e+05 ## 79 Guinea-Bissau 8.719596e+04 8.804516e+04 8.932212e+04 ## 80 Guyana 1.979563e+05 2.033071e+05 2.081042e+05 ## 81 Haiti 8.205857e+05 8.567168e+05 8.934834e+05 ## 82 Honduras 6.700552e+05 7.041621e+05 7.396318e+05 ## 83 Hong Kong, China 3.236781e+06 3.316190e+06 3.379661e+06 ## 84 Hungary 6.013289e+06 6.079237e+06 6.147720e+06 ## 85 Iceland 1.661399e+05 1.693063e+05 1.717736e+05 ## 86 India 9.936339e+07 1.025948e+08 1.059532e+08 ## 87 Indonesia 1.786885e+07 1.862152e+07 1.940053e+07 ## 88 Iran 1.024223e+07 1.074839e+07 1.127204e+07 ## 89 Iraq 4.785700e+06 5.053788e+06 5.335012e+06 ## 90 Ireland 1.448735e+06 1.472843e+06 1.499153e+06 ## 91 Isle of Man 2.974060e+04 3.041582e+04 3.107182e+04 ## 92 Israel 2.257543e+06 2.323491e+06 2.403561e+06 ## 93 Italy 3.322924e+07 3.369844e+07 3.414982e+07 ## 94 Jamaica 7.040407e+05 7.257254e+05 7.482876e+05 ## 95 Japan 6.997406e+07 7.101819e+07 7.332929e+07 ## 96 Jordan 7.024333e+05 7.513107e+05 7.991228e+05 ## 97 Kazakhstan 6.018757e+06 6.209379e+06 6.396692e+06 ## 98 Kenya 9.424282e+05 1.010199e+06 1.082085e+06 ## 99 Kiribati 9.944575e+03 1.054187e+04 1.115324e+04 ## 100 North Korea 6.359134e+06 6.797010e+06 7.252939e+06 ## 101 South Korea 1.067144e+07 1.142358e+07 1.219746e+07 ## 102 Kuwait 4.812897e+05 5.332849e+05 5.878232e+05 ## 103 Kyrgyz Republic 9.987404e+05 1.037698e+06 1.075216e+06 ## 104 Lao 2.214381e+05 2.333150e+05 2.458144e+05 ## 105 Latvia 1.343553e+06 1.374667e+06 1.404423e+06 ## 106 Lebanon 1.253621e+06 1.320402e+06 1.390579e+06 ## 107 Lesotho 7.042371e+04 7.636722e+04 8.253367e+04 ## 108 Liberia 3.145211e+05 3.336211e+05 3.536543e+05 ## 109 Libya 7.048490e+05 7.933851e+05 8.884915e+05 ## 110 Liechtenstein 3.771201e+03 3.835222e+03 3.893073e+03 ## 111 Lithuania 1.415402e+06 1.462854e+06 1.508107e+06 ## 112 Luxembourg 2.442931e+05 2.465394e+05 2.493815e+05 ## 113 Macao, China 2.193452e+05 2.292781e+05 2.376078e+05 ## 114 Macedonia, FYR 6.524718e+05 6.802103e+05 7.086757e+05 ## 115 Madagascar 7.919615e+05 8.337642e+05 8.775250e+05 ## 116 Malawi 2.242118e+05 2.398927e+05 2.565303e+05 ## 117 Malaysia 3.168042e+06 3.324289e+06 3.484442e+06 ## 118 Maldives 1.252289e+04 1.289746e+04 1.330701e+04 ## 119 Mali 7.656009e+05 7.972307e+05 8.302079e+05 ## 120 Malta 2.796928e+05 2.763384e+05 2.730307e+05 ## 121 Marshall Islands 8.640897e+03 9.323270e+03 1.007123e+04 ## 122 Mauritania 1.236419e+05 1.367608e+05 1.505604e+05 ## 123 Mauritius 3.058232e+05 3.195152e+05 3.332923e+05 ## 124 Mexico 2.691017e+07 2.808642e+07 2.931700e+07 ## 125 Micronesia, Fed. Sts. 1.354285e+04 1.419170e+04 1.477304e+04 ## 126 Moldova 8.569232e+05 8.959091e+05 9.356514e+05 ## 127 Monaco 2.304600e+04 2.323400e+04 2.344800e+04 ## 128 Mongolia 5.089148e+05 5.307544e+05 5.535133e+05 ## 129 Montenegro 1.244879e+05 1.292181e+05 1.340713e+05 ## 130 Morocco 4.639516e+06 4.848380e+06 5.061952e+06 ## 131 Mozambique 4.491451e+05 4.803006e+05 5.127060e+05 ## 132 Myanmar 5.297725e+06 5.512884e+06 5.737830e+06 ## 133 Namibia 1.504638e+05 1.578102e+05 1.656184e+05 ## 134 Nepal 4.268625e+05 4.411255e+05 4.559937e+05 ## 135 Netherlands 7.699643e+06 7.803192e+06 7.917513e+06 ## 136 New Caledonia 4.587712e+04 4.868702e+04 5.183153e+04 ## 137 New Zealand 2.173205e+06 2.204526e+06 2.236624e+06 ## 138 Nicaragua 9.730101e+05 1.022348e+06 1.073928e+06 ## 139 Niger 3.039535e+05 3.295439e+05 3.563980e+05 ## 140 Nigeria 1.131884e+07 1.186224e+07 1.242960e+07 ## 141 Northern Mariana Islands 7.518953e+03 8.073316e+03 8.655527e+03 ## 142 Norway 2.297185e+06 2.376327e+06 2.456007e+06 ## 143 Oman 1.682955e+05 1.833677e+05 1.995581e+05 ## 144 Pakistan 1.316562e+07 1.366756e+07 1.419101e+07 ## 145 Palau 6.521346e+03 6.627161e+03 6.736073e+03 ## 146 Panama 6.330562e+05 6.609825e+05 6.897512e+05 ## 147 Papua New Guinea 1.626460e+05 1.865556e+05 2.117910e+05 ## 148 Paraguay 8.397317e+05 8.662660e+05 8.931292e+05 ## 149 Peru 6.560955e+06 6.884271e+06 7.220337e+06 ## 150 Philippines 1.045064e+07 1.085199e+07 1.126489e+07 ## 151 Poland 1.628965e+07 1.657536e+07 1.683567e+07 ## 152 Portugal 3.340476e+06 3.360472e+06 3.364395e+06 ## 153 Puerto Rico 1.435077e+06 1.480203e+06 1.529021e+06 ## 154 Qatar 7.500451e+04 8.116982e+04 8.804065e+04 ## 155 Romania 7.568698e+06 7.775433e+06 7.962558e+06 ## 156 Russia 7.677947e+07 7.832602e+07 7.988771e+07 ## 157 Rwanda 1.005126e+05 1.065866e+05 1.129610e+05 ## 158 St. Kitts and Nevis 1.516557e+04 1.522598e+04 1.528050e+04 ## 159 St. Lucia 2.232508e+04 2.291663e+04 2.351565e+04 ## 160 St. Vincent and the Grenadines 2.564178e+04 2.633043e+04 2.703429e+04 ## 161 Samoa 2.636036e+04 2.727841e+04 2.815593e+04 ## 162 San Marino 1.030941e+04 1.071427e+04 1.109522e+04 ## 163 Sao Tome and Principe 1.684635e+04 1.841719e+04 2.006490e+04 ## 164 Saudi Arabia 2.195007e+06 2.382635e+06 2.586258e+06 ## 165 Senegal 1.035987e+06 1.096955e+06 1.161241e+06 ## 166 Serbia 2.505613e+06 2.595006e+06 2.683242e+06 ## 167 Seychelles 1.771880e+04 1.876104e+04 1.983538e+04 ## 168 Sierra Leone 5.281695e+05 5.535685e+05 5.797787e+05 ## 169 Singapore 1.978000e+06 2.012000e+06 2.043000e+06 ## 170 Slovak Republic 1.719618e+06 1.768967e+06 1.818929e+06 ## 171 Slovenia 5.795047e+05 6.000206e+05 6.187531e+05 ## 172 Solomon Islands 1.151482e+04 1.237527e+04 1.329659e+04 ## 173 Somalia 7.047038e+05 7.433007e+05 7.810217e+05 ## 174 South Africa 9.830232e+06 1.006591e+07 1.030848e+07 ## 175 Spain 2.064974e+07 2.123678e+07 2.176544e+07 ## 176 Sri Lanka 2.151152e+06 2.249555e+06 2.344592e+06 ## 177 Sudan 1.466502e+06 1.571927e+06 1.683562e+06 ## 178 Suriname 1.638993e+05 1.673102e+05 1.698198e+05 ## 179 Swaziland 3.199762e+04 3.554773e+04 3.929612e+04 ## 180 Sweden 6.187907e+06 6.285731e+06 6.393453e+06 ## 181 Switzerland 3.324087e+06 3.404449e+06 3.481651e+06 ## 182 Syria 2.377889e+06 2.499429e+06 2.626816e+06 ## 183 Tajikistan 9.611929e+05 1.000669e+06 1.041608e+06 ## 184 Tanzania 8.384494e+05 9.108258e+05 9.872961e+05 ## 185 Thailand 6.919690e+06 7.176231e+06 7.440174e+06 ## 186 Timor-Leste 6.802067e+04 7.108209e+04 7.435281e+04 ## 187 Togo 3.221940e+05 3.621139e+05 4.040164e+05 ## 188 Tonga 1.563131e+04 1.614767e+04 1.661674e+04 ## 189 Trinidad and Tobago 1.232921e+05 1.208498e+05 1.181071e+05 ## 190 Tunisia 1.992479e+06 2.070869e+06 2.149857e+06 ## 191 Turkey 1.191986e+07 1.244807e+07 1.299329e+07 ## 192 Turkmenistan 9.517698e+05 9.822601e+05 1.013434e+06 ## 193 Turks and Caicos Islands 2.798837e+03 2.804887e+03 2.829033e+03 ## 194 Tuvalu 1.415014e+03 1.480186e+03 1.545270e+03 ## 195 Uganda 5.120829e+05 5.499091e+05 5.891064e+05 ## 196 Ukraine 2.416635e+07 2.475757e+07 2.534887e+07 ## 197 United Arab Emirates 1.280378e+05 1.390527e+05 1.555970e+05 ## 198 United Kingdom 4.260294e+07 4.273308e+07 4.283308e+07 ## 199 United States 1.442017e+08 1.463404e+08 1.484759e+08 ## 200 Uruguay 2.247503e+06 2.273438e+06 2.295858e+06 ## 201 Uzbekistan 3.913188e+06 4.067599e+06 4.227790e+06 ## 202 Vanuatu 9.208354e+03 9.621427e+03 1.005774e+04 ## 203 Venezuela 6.678933e+06 6.994264e+06 7.324840e+06 ## 204 Vietnam 6.865532e+06 7.169607e+06 7.487421e+06 ## 205 Virgin Islands (U.S.) 3.342853e+04 3.661847e+04 4.004103e+04 ## 206 Yemen 6.973814e+05 7.369436e+05 7.769681e+05 ## 207 Zambia 9.841980e+05 1.069557e+06 1.160044e+06 ## 208 Zimbabwe 7.416051e+05 7.927728e+05 8.467739e+05 ## 209 South Sudan 3.157901e+05 3.210970e+05 3.268101e+05 ## X1970 X1971 X1972 X1973 X1974 ## 1 1.319849e+06 1.409001e+06 1.502402e+06 1.598835e+06 1.696445e+06 ## 2 6.778391e+05 6.989322e+05 7.202066e+05 7.416810e+05 7.633855e+05 ## 3 5.429743e+06 5.619042e+06 5.815734e+06 6.020647e+06 6.235114e+06 ## 4 1.920639e+04 1.975202e+04 2.026267e+04 2.074197e+04 2.119438e+04 ## 5 1.952896e+04 2.092873e+04 2.240584e+04 2.393705e+04 2.548198e+04 ## 6 8.864016e+05 9.550101e+05 1.027397e+06 1.103830e+06 1.184486e+06 ## 7 2.218087e+04 2.256087e+04 2.290776e+04 2.322129e+04 2.350292e+04 ## 8 1.891807e+07 1.932972e+07 1.976308e+07 2.021142e+07 2.066473e+07 ## 9 1.507620e+06 1.564368e+06 1.622104e+06 1.680498e+06 1.739063e+06 ## 10 2.990157e+04 3.008136e+04 3.027976e+04 3.046742e+04 3.060287e+04 ## 11 1.066409e+07 1.104771e+07 1.126995e+07 1.146112e+07 1.177293e+07 ## 12 4.872871e+06 4.895910e+06 4.925699e+06 4.954325e+06 4.964026e+06 ## 13 2.586413e+06 2.660993e+06 2.734825e+06 2.807955e+06 2.880447e+06 ## 14 1.130101e+05 1.171566e+05 1.209989e+05 1.246644e+05 1.283499e+05 ## 15 1.775008e+05 1.844398e+05 1.923163e+05 2.014935e+05 2.124162e+05 ## 16 5.078286e+06 5.456170e+06 5.812548e+06 6.161815e+06 6.530579e+06 ## 17 8.956543e+04 9.055245e+04 9.164208e+04 9.277639e+04 9.387156e+04 ## 18 3.978504e+06 4.132164e+06 4.286801e+06 4.440936e+06 4.592935e+06 ## 19 9.061057e+06 9.089909e+06 9.137946e+06 9.179155e+06 9.220531e+06 ## 20 6.114133e+04 6.183991e+04 6.240329e+04 6.294338e+04 6.362671e+04 ## 21 4.756114e+05 5.158195e+05 5.579376e+05 6.020932e+05 6.484097e+05 ## 22 5.500000e+04 5.460000e+04 5.420000e+04 5.380000e+04 5.340000e+04 ## 23 1.838141e+04 2.017266e+04 2.209976e+04 2.415974e+04 2.634254e+04 ## 24 1.677184e+06 1.731437e+06 1.787719e+06 1.845894e+06 1.905749e+06 ## 25 9.695495e+05 1.008630e+06 1.048738e+06 1.089648e+06 1.130966e+06 ## 26 5.428641e+04 6.186900e+04 6.992963e+04 7.852997e+04 8.775392e+04 ## 27 5.371642e+07 5.600051e+07 5.834048e+07 6.074473e+07 6.322438e+07 ## 28 7.714802e+04 8.088400e+04 8.478142e+04 8.880798e+04 9.291945e+04 ## 29 4.440047e+06 4.554372e+06 4.665864e+06 4.780947e+06 4.904324e+06 ## 30 3.336985e+05 3.475107e+05 3.618362e+05 3.767243e+05 3.922410e+05 ## 31 8.369155e+04 9.049313e+04 9.717071e+04 1.038732e+05 1.108747e+05 ## 32 1.107998e+06 9.614523e+05 8.076237e+05 6.470452e+05 4.811320e+05 ## 33 1.388878e+06 1.523689e+06 1.665342e+06 1.814545e+06 1.972201e+06 ## 34 1.613246e+07 1.637385e+07 1.663528e+07 1.691758e+07 1.722167e+07 ## 35 5.364682e+04 5.638241e+04 5.931521e+04 6.221562e+04 6.475257e+04 ## 36 9.545000e+03 1.000400e+04 1.058100e+04 1.125300e+04 1.199000e+04 ## 37 4.997496e+05 5.268630e+05 5.546158e+05 5.832534e+05 6.131560e+05 ## 38 4.229151e+05 4.628673e+05 5.049060e+05 5.488032e+05 5.940966e+05 ## 39 4.371195e+04 4.368323e+04 4.363962e+04 4.355859e+04 4.341204e+04 ## 40 7.204920e+06 7.398470e+06 7.592419e+06 7.785880e+06 7.977602e+06 ## 41 1.423868e+08 1.463523e+08 1.499932e+08 1.534576e+08 1.566609e+08 ## 42 1.169300e+07 1.214719e+07 1.260270e+07 1.306371e+07 1.353659e+07 ## 43 4.615440e+04 4.811136e+04 5.012270e+04 5.227286e+04 5.468356e+04 ## 44 6.140904e+06 6.282834e+06 6.425372e+06 6.570538e+06 6.721175e+06 ## 45 5.224066e+05 5.497894e+05 5.786398e+05 6.088504e+05 6.402364e+05 ## 46 7.067986e+05 7.335459e+05 7.604308e+05 7.879183e+05 8.166588e+05 ## 47 1.525425e+06 1.638738e+06 1.760508e+06 1.891241e+06 2.031395e+06 ## 48 1.773046e+06 1.826422e+06 1.879428e+06 1.932436e+06 1.984976e+06 ## 49 5.244279e+06 5.407254e+06 5.572975e+06 5.738231e+06 5.898512e+06 ## 50 2.501645e+05 2.612132e+05 2.724080e+05 2.837749e+05 2.953798e+05 ## 51 6.348795e+06 6.437055e+06 6.572632e+06 6.718466e+06 6.873458e+06 ## 52 3.930043e+06 3.981360e+06 4.028248e+06 4.076867e+06 4.120201e+06 ## 53 9.984522e+04 1.077997e+05 1.160982e+05 1.253916e+05 1.366062e+05 ## 54 3.332825e+04 3.476152e+04 3.604999e+04 3.726005e+04 3.850147e+04 ## 55 1.814060e+06 1.915590e+06 2.020157e+06 2.127714e+06 2.238204e+06 ## 56 2.345864e+06 2.453818e+06 2.565645e+06 2.681525e+06 2.801693e+06 ## 57 1.516286e+07 1.560366e+07 1.604781e+07 1.649863e+07 1.696083e+07 ## 58 1.472181e+06 1.527985e+06 1.584758e+06 1.642099e+06 1.699471e+06 ## 59 7.841107e+04 7.705529e+04 7.459606e+04 7.143896e+04 6.817926e+04 ## 60 2.325927e+05 2.420318e+05 2.517894e+05 2.620127e+05 2.729047e+05 ## 61 8.847697e+05 9.015668e+05 9.191148e+05 9.354101e+05 9.510326e+05 ## 62 2.487032e+06 2.609266e+06 2.738496e+06 2.870320e+06 2.998291e+06 ## 63 1.077427e+04 1.106567e+04 1.135462e+04 1.164494e+04 1.194279e+04 ## 64 1.809345e+05 1.868715e+05 1.929448e+05 1.991372e+05 2.054102e+05 ## 65 2.934402e+06 2.976176e+06 3.032239e+06 3.088022e+06 3.142947e+06 ## 66 3.691751e+07 3.740758e+07 3.790747e+07 3.840573e+07 3.888504e+07 ## 67 6.125900e+04 6.368624e+04 6.613374e+04 6.861999e+04 7.117748e+04 ## 68 1.694483e+05 1.845557e+05 2.007952e+05 2.181618e+05 2.365466e+05 ## 69 8.942094e+04 9.676352e+04 1.047188e+05 1.132281e+05 1.221660e+05 ## 70 1.904782e+06 1.943501e+06 2.058124e+06 2.096168e+06 2.134461e+06 ## 71 5.649607e+07 5.664462e+07 5.696131e+07 5.718614e+07 5.725360e+07 ## 72 2.515296e+06 2.601135e+06 2.695926e+06 2.795186e+06 2.892229e+06 ## 73 4.616575e+06 4.686154e+06 4.766545e+06 4.838297e+06 4.906384e+06 ## 74 3.375322e+04 3.449046e+04 3.545317e+04 3.612819e+04 3.665970e+04 ## 75 3.040587e+04 3.039084e+04 3.037836e+04 3.034479e+04 3.025489e+04 ## 76 5.291621e+04 5.791466e+04 6.308539e+04 6.843879e+04 7.399464e+04 ## 77 1.936380e+06 2.002850e+06 2.071676e+06 2.142378e+06 2.214270e+06 ## 78 6.636291e+05 7.000651e+05 7.353800e+05 7.696670e+05 8.032624e+05 ## 79 9.123325e+04 9.389158e+04 9.722136e+04 1.011893e+05 1.057146e+05 ## 80 2.120772e+05 2.155336e+05 2.181112e+05 2.201426e+05 2.221226e+05 ## 81 9.307198e+05 9.535772e+05 9.764460e+05 9.996672e+05 1.023722e+06 ## 82 7.769459e+05 8.163257e+05 8.577454e+05 9.014120e+05 9.475283e+05 ## 83 3.473191e+06 3.564807e+06 3.650021e+06 3.771147e+06 3.870519e+06 ## 84 6.214324e+06 6.276071e+06 6.338877e+06 6.403550e+06 6.476603e+06 ## 85 1.735679e+05 1.757064e+05 1.790372e+05 1.825107e+05 1.857581e+05 ## 86 1.094455e+08 1.137519e+08 1.182288e+08 1.228790e+08 1.277043e+08 ## 87 2.020553e+07 2.127053e+07 2.237329e+07 2.351361e+07 2.469105e+07 ## 88 1.181219e+07 1.239191e+07 1.299286e+07 1.362195e+07 1.428880e+07 ## 89 5.627633e+06 5.924798e+06 6.232252e+06 6.551369e+06 6.884387e+06 ## 90 1.529549e+06 1.558990e+06 1.593945e+06 1.631517e+06 1.670769e+06 ## 91 3.166567e+04 3.182827e+04 3.189547e+04 3.190477e+04 3.190731e+04 ## 92 2.503959e+06 2.598970e+06 2.681284e+06 2.808059e+06 2.909400e+06 ## 93 3.459238e+07 3.490238e+07 3.525021e+07 3.564021e+07 3.602531e+07 ## 94 7.723456e+05 7.935444e+05 8.162612e+05 8.398898e+05 8.633533e+05 ## 95 7.500006e+07 7.678337e+07 7.868950e+07 8.017343e+07 8.256444e+07 ## 96 8.440427e+05 8.861825e+05 9.252900e+05 9.628976e+05 1.001686e+06 ## 97 6.585936e+06 6.756162e+06 6.928193e+06 7.100036e+06 7.268241e+06 ## 98 1.158426e+06 1.261182e+06 1.370525e+06 1.486815e+06 1.610388e+06 ## 99 1.177903e+04 1.253191e+04 1.329569e+04 1.407663e+04 1.488213e+04 ## 100 7.721750e+06 8.009574e+06 8.299056e+06 8.584095e+06 8.857069e+06 ## 101 1.299394e+07 1.374559e+07 1.451567e+07 1.530510e+07 1.611498e+07 ## 102 6.451490e+05 7.009110e+05 7.585954e+05 8.180756e+05 8.792009e+05 ## 103 1.108956e+06 1.136687e+06 1.165919e+06 1.195227e+06 1.226436e+06 ## 104 2.590287e+05 2.739823e+05 2.898053e+05 3.060341e+05 3.219629e+05 ## 105 1.432319e+06 1.459146e+06 1.487488e+06 1.516637e+06 1.546838e+06 ## 106 1.465634e+06 1.541721e+06 1.622874e+06 1.705275e+06 1.783166e+06 ## 107 8.892443e+04 9.542557e+04 1.021606e+05 1.091860e+05 1.165855e+05 ## 108 3.746759e+05 3.980213e+05 4.225051e+05 4.482161e+05 4.752605e+05 ## 109 9.904397e+05 1.087657e+06 1.191671e+06 1.302852e+06 1.421573e+06 ## 110 3.941192e+03 4.016945e+03 4.084375e+03 4.146087e+03 4.206141e+03 ## 111 1.555873e+06 1.614349e+06 1.671308e+06 1.727112e+06 1.782930e+06 ## 112 2.522550e+05 2.566740e+05 2.618327e+05 2.667899e+05 2.723674e+05 ## 113 2.435455e+05 2.467800e+05 2.476067e+05 2.466418e+05 2.448335e+05 ## 114 7.381837e+05 7.584522e+05 7.793806e+05 8.010906e+05 8.237298e+05 ## 115 9.233980e+05 9.783692e+05 1.035964e+06 1.096280e+06 1.159402e+06 ## 116 2.742784e+05 2.974752e+05 3.221866e+05 3.484584e+05 3.762949e+05 ## 117 3.649615e+06 3.835042e+06 4.026657e+06 4.224277e+06 4.427442e+06 ## 118 1.376876e+04 1.548045e+04 1.732799e+04 1.930163e+04 2.137255e+04 ## 119 8.646754e+05 9.031346e+05 9.433393e+05 9.851630e+05 1.028372e+06 ## 120 2.714740e+05 2.715449e+05 2.713466e+05 2.711483e+05 2.709913e+05 ## 121 1.091076e+04 1.170290e+04 1.258814e+04 1.354212e+04 1.452511e+04 ## 122 1.650886e+05 1.839591e+05 2.038400e+05 2.247698e+05 2.467774e+05 ## 123 3.471843e+05 3.551136e+05 3.629438e+05 3.708224e+05 3.789698e+05 ## 124 3.061321e+07 3.194150e+07 3.333305e+07 3.478046e+07 3.627178e+07 ## 125 1.523980e+04 1.553743e+04 1.571629e+04 1.584482e+04 1.602333e+04 ## 126 9.764706e+05 1.015915e+06 1.056411e+06 1.097293e+06 1.137827e+06 ## 127 2.368900e+04 2.396800e+04 2.428200e+04 2.460500e+04 2.490200e+04 ## 128 5.773571e+05 6.041172e+05 6.320703e+05 6.610724e+05 6.908953e+05 ## 129 1.392938e+05 1.454891e+05 1.521163e+05 1.591069e+05 1.663149e+05 ## 130 5.278427e+06 5.516718e+06 5.759042e+06 6.006727e+06 6.261899e+06 ## 131 5.464057e+05 6.150199e+05 6.864334e+05 7.611387e+05 8.399119e+05 ## 132 5.973271e+06 6.178716e+06 6.392781e+06 6.613581e+06 6.838424e+06 ## 133 1.739636e+05 1.814829e+05 1.894921e+05 1.977924e+05 2.060961e+05 ## 134 4.714710e+05 5.035432e+05 5.369944e+05 5.718580e+05 6.081574e+05 ## 135 8.039946e+06 8.176234e+06 8.299848e+06 8.409656e+06 8.516996e+06 ## 136 5.533056e+04 5.909833e+04 6.291106e+04 6.663068e+04 7.014487e+04 ## 137 2.279646e+06 2.323472e+06 2.374612e+06 2.431429e+06 2.492750e+06 ## 138 1.127855e+06 1.171246e+06 1.216288e+06 1.263026e+06 1.311513e+06 ## 139 3.845578e+05 4.198226e+05 4.568167e+05 4.956246e+05 5.363483e+05 ## 140 1.302354e+07 1.367088e+07 1.434773e+07 1.506111e+07 1.582041e+07 ## 141 9.250286e+03 9.855667e+03 1.050168e+04 1.115197e+04 1.175108e+04 ## 142 2.534594e+06 2.574218e+06 2.615935e+06 2.656406e+06 2.695182e+06 ## 143 2.170597e+05 2.378383e+05 2.603733e+05 2.850917e+05 3.125531e+05 ## 144 1.473699e+07 1.533278e+07 1.595552e+07 1.661011e+07 1.730286e+07 ## 145 6.855879e+03 6.993553e+03 7.145486e+03 7.295512e+03 7.421072e+03 ## 146 7.192792e+05 7.438996e+05 7.689286e+05 7.943853e+05 8.203103e+05 ## 147 2.385030e+05 2.558776e+05 2.743358e+05 2.938021e+05 3.141259e+05 ## 148 9.201416e+05 9.528178e+05 9.860213e+05 1.020057e+06 1.055359e+06 ## 149 7.570234e+06 7.894058e+06 8.229659e+06 8.577138e+06 8.936488e+06 ## 150 1.169151e+07 1.222076e+07 1.276980e+07 1.333929e+07 1.392968e+07 ## 151 1.702627e+07 1.729526e+07 1.764742e+07 1.801889e+07 1.840518e+07 ## 152 3.368354e+06 3.388266e+06 3.417132e+06 3.452290e+06 3.535363e+06 ## 153 1.585301e+06 1.635614e+06 1.693250e+06 1.755806e+06 1.818827e+06 ## 154 9.580697e+04 1.046010e+05 1.144858e+05 1.249279e+05 1.351680e+05 ## 155 8.164758e+06 8.352698e+06 8.536653e+06 8.714774e+06 8.901463e+06 ## 156 8.146468e+07 8.297123e+07 8.449242e+07 8.602837e+07 8.757920e+07 ## 157 1.196576e+05 1.296515e+05 1.401857e+05 1.513041e+05 1.630587e+05 ## 158 1.532931e+04 1.530592e+04 1.531596e+04 1.529062e+04 1.526421e+04 ## 159 2.424170e+04 2.484224e+04 2.542559e+04 2.606504e+04 2.668730e+04 ## 160 2.775738e+04 2.852298e+04 2.931059e+04 3.011692e+04 3.093551e+04 ## 161 2.897331e+04 2.960049e+04 3.015656e+04 3.065566e+04 3.112000e+04 ## 162 1.144333e+04 1.199178e+04 1.250465e+04 1.300464e+04 1.352865e+04 ## 163 2.173410e+04 2.255666e+04 2.335055e+04 2.415061e+04 2.501460e+04 ## 164 2.809100e+06 3.050817e+06 3.315971e+06 3.607779e+06 3.929807e+06 ## 165 1.228874e+06 1.300559e+06 1.375866e+06 1.453826e+06 1.533013e+06 ## 166 2.770952e+06 2.834711e+06 2.898614e+06 2.962223e+06 3.025922e+06 ## 167 2.094045e+04 2.221236e+04 2.351875e+04 2.485369e+04 2.620824e+04 ## 168 6.067908e+05 6.355432e+05 6.652061e+05 6.959255e+05 7.279029e+05 ## 169 2.075000e+06 2.113000e+06 2.152000e+06 2.193000e+06 2.230000e+06 ## 170 1.863258e+06 1.918549e+06 1.982845e+06 2.050451e+06 2.120507e+06 ## 171 6.382787e+05 6.619232e+05 6.860343e+05 7.106715e+05 7.335425e+05 ## 172 1.429003e+04 1.487728e+04 1.550905e+04 1.617813e+04 1.687314e+04 ## 173 8.166815e+05 8.475888e+05 8.745210e+05 9.078108e+05 9.626845e+05 ## 174 1.055957e+07 1.081953e+07 1.108419e+07 1.135223e+07 1.162297e+07 ## 175 2.233044e+07 2.282103e+07 2.327235e+07 2.373034e+07 2.420854e+07 ## 176 2.441982e+06 2.475540e+06 2.508101e+06 2.552143e+06 2.588945e+06 ## 177 1.802344e+06 1.912728e+06 2.030472e+06 2.155450e+06 2.287267e+06 ## 178 1.710630e+05 1.743836e+05 1.764727e+05 1.777444e+05 1.788532e+05 ## 179 4.325858e+04 4.845133e+04 5.395107e+04 5.977098e+04 6.591935e+04 ## 180 6.517403e+06 6.589874e+06 6.636926e+06 6.675974e+06 6.723052e+06 ## 181 3.545846e+06 3.564515e+06 3.591810e+06 3.618437e+06 3.637988e+06 ## 182 2.760217e+06 2.878588e+06 3.002034e+06 3.130344e+06 3.263171e+06 ## 183 1.084708e+06 1.111673e+06 1.139645e+06 1.168044e+06 1.196054e+06 ## 184 1.068227e+06 1.195298e+06 1.330036e+06 1.472583e+06 1.622882e+06 ## 185 7.711257e+06 8.156822e+06 8.618420e+06 9.093762e+06 9.579568e+06 ## 186 7.788066e+04 8.202655e+04 8.651331e+04 9.088243e+04 9.445747e+04 ## 187 4.462997e+05 4.679159e+05 4.881497e+05 5.073627e+05 5.262916e+05 ## 188 1.703157e+04 1.728917e+04 1.748268e+04 1.763734e+04 1.779015e+04 ## 189 1.149191e+05 1.151237e+05 1.150568e+05 1.148504e+05 1.146878e+05 ## 190 2.229322e+06 2.307379e+06 2.389032e+06 2.475875e+06 2.569238e+06 ## 191 1.355938e+07 1.410119e+07 1.466411e+07 1.524684e+07 1.584676e+07 ## 192 1.045665e+06 1.075185e+06 1.105506e+06 1.136380e+06 1.167443e+06 ## 193 2.878290e+03 2.961101e+03 3.073893e+03 3.205822e+03 3.342540e+03 ## 194 1.611030e+03 1.683666e+03 1.756818e+03 1.830905e+03 1.905153e+03 ## 195 6.294769e+05 6.557359e+05 6.822662e+05 7.093838e+05 7.375558e+05 ## 196 2.594411e+07 2.648578e+07 2.703029e+07 2.757233e+07 2.810411e+07 ## 197 1.800752e+05 2.128010e+05 2.533435e+05 3.021131e+05 3.593418e+05 ## 198 4.292583e+07 4.316876e+07 4.337887e+07 4.352637e+07 4.361748e+07 ## 199 1.509224e+08 1.528638e+08 1.545305e+08 1.560341e+08 1.574881e+08 ## 200 2.313813e+06 2.326524e+06 2.334879e+06 2.341153e+06 2.348533e+06 ## 201 4.395765e+06 4.595966e+06 4.805551e+06 5.022305e+06 5.242853e+06 ## 202 1.052469e+04 1.103796e+04 1.158368e+04 1.215890e+04 1.275908e+04 ## 203 7.674281e+06 8.023652e+06 8.391094e+06 8.777606e+06 9.184011e+06 ## 204 7.819407e+06 8.043735e+06 8.277023e+06 8.518466e+06 8.766839e+06 ## 205 4.384296e+04 5.021305e+04 5.460843e+04 6.130639e+04 6.670296e+04 ## 206 8.172839e+05 8.485446e+05 8.800627e+05 9.133326e+05 9.504883e+05 ## 207 1.256178e+06 1.337898e+06 1.424498e+06 1.515871e+06 1.611725e+06 ## 208 9.039055e+05 9.620288e+05 1.023588e+06 1.088377e+06 1.155992e+06 ## 209 3.330133e+05 3.396491e+05 3.466912e+05 3.542318e+05 3.623528e+05 9.4.1.3 Customize readWorksheet To get a clear overview without having to open up the Excel file, you can execute the following code: my_book &lt;- loadWorkbook(&quot;data/urbanpop.xlsx&quot;) sheets &lt;- getSheets(my_book) all &lt;- lapply(sheets, readWorksheet, object = my_book) str(all) Suppose we’re only interested in urban population data of the years 1968, 1969 and 1970. The data for these years is in the columns 3, 4, and 5 of the second sheet. # Import columns 3, 4, and 5 from second sheet in my_book: urbanpop_sel urbanpop_sel &lt;- readWorksheet(my_book, sheet = 2, startCol = 3, endCol = 5) # Import first column from second sheet in my_book: countries countries &lt;- readWorksheet(my_book, sheet = 2, startCol = 1, endCol = 1) # cbind() urbanpop_sel and countries together: selection selection &lt;- cbind(countries, urbanpop_sel); selection ## country X1968 X1969 X1970 ## 1 Afghanistan 1.182159e+06 1.248901e+06 1.319849e+06 ## 2 Albania 6.399645e+05 6.588531e+05 6.778391e+05 ## 3 Algeria 5.017299e+06 5.219332e+06 5.429743e+06 ## 4 American Samoa 1.799551e+04 1.861868e+04 1.920639e+04 ## 5 Andorra 1.672699e+04 1.808832e+04 1.952896e+04 ## 6 Angola 7.984593e+05 8.412620e+05 8.864016e+05 ## 7 Antigua and Barbuda 2.214939e+04 2.218292e+04 2.218087e+04 ## 8 Argentina 1.812410e+07 1.851046e+07 1.891807e+07 ## 9 Armenia 1.392892e+06 1.449641e+06 1.507620e+06 ## 10 Aruba 2.957609e+04 2.973787e+04 2.990157e+04 ## 11 Australia 1.015397e+07 1.041239e+07 1.066409e+07 ## 12 Austria 4.831817e+06 4.852208e+06 4.872871e+06 ## 13 Azerbaijan 2.495725e+06 2.542062e+06 2.586413e+06 ## 14 Bahamas 1.036697e+05 1.084730e+05 1.130101e+05 ## 15 Bahrain 1.663785e+05 1.714590e+05 1.775008e+05 ## 16 Bangladesh 4.484842e+06 4.790505e+06 5.078286e+06 ## 17 Barbados 8.858041e+04 8.902489e+04 8.956543e+04 ## 18 Belarus 3.696854e+06 3.838003e+06 3.978504e+06 ## 19 Belgium 8.999366e+06 9.038506e+06 9.061057e+06 ## 20 Belize 5.971173e+04 6.049220e+04 6.114133e+04 ## 21 Benin 4.118595e+05 4.430131e+05 4.756114e+05 ## 22 Bermuda 5.300000e+04 5.400000e+04 5.500000e+04 ## 23 Bhutan 1.561689e+04 1.694642e+04 1.838141e+04 ## 24 Bolivia 1.575177e+06 1.625173e+06 1.677184e+06 ## 25 Bosnia and Herzegovina 8.902697e+05 9.294496e+05 9.695495e+05 ## 26 Botswana 4.057616e+04 4.722223e+04 5.428641e+04 ## 27 Brazil 4.931688e+07 5.148910e+07 5.371642e+07 ## 28 Brunei 6.622218e+04 7.150276e+04 7.714802e+04 ## 29 Bulgaria 4.158186e+06 4.300669e+06 4.440047e+06 ## 30 Burkina Faso 3.086611e+05 3.209607e+05 3.336985e+05 ## 31 Burundi 7.881625e+04 8.135573e+04 8.369155e+04 ## 32 Cambodia 9.263155e+05 1.017799e+06 1.107998e+06 ## 33 Cameroon 1.231243e+06 1.308158e+06 1.388878e+06 ## 34 Canada 1.546449e+07 1.579236e+07 1.613246e+07 ## 35 Cape Verde 4.923400e+04 5.135658e+04 5.364682e+04 ## 36 Cayman Islands 9.002000e+03 9.216000e+03 9.545000e+03 ## 37 Central African Republic 4.529338e+05 4.761054e+05 4.997496e+05 ## 38 Chad 3.605791e+05 3.909776e+05 4.229151e+05 ## 39 Channel Islands 4.344349e+04 4.358417e+04 4.371195e+04 ## 40 Chile 6.805959e+06 7.005123e+06 7.204920e+06 ## 41 China 1.368900e+08 1.396005e+08 1.423868e+08 ## 42 Colombia 1.078053e+07 1.123560e+07 1.169300e+07 ## 43 Comoros 4.183902e+04 4.396565e+04 4.615440e+04 ## 44 Congo, Dem. Rep. 5.475208e+06 5.802069e+06 6.140904e+06 ## 45 Congo, Rep. 4.733352e+05 4.972107e+05 5.224066e+05 ## 46 Costa Rica 6.499164e+05 6.782539e+05 7.067986e+05 ## 47 Cote d&#39;Ivoire 1.330719e+06 1.424438e+06 1.525425e+06 ## 48 Croatia 1.663051e+06 1.717607e+06 1.773046e+06 ## 49 Cuba 5.032014e+06 5.137260e+06 5.244279e+06 ## 50 Cyprus 2.378314e+05 2.439833e+05 2.501645e+05 ## 51 Czech Republic 6.266305e+06 6.326369e+06 6.348795e+06 ## 52 Denmark 3.826785e+06 3.874314e+06 3.930043e+06 ## 53 Djibouti 8.469435e+04 9.204577e+04 9.984522e+04 ## 54 Dominica 2.952732e+04 3.147562e+04 3.332825e+04 ## 55 Dominican Republic 1.625456e+06 1.718315e+06 1.814060e+06 ## 56 Ecuador 2.151395e+06 2.246891e+06 2.345864e+06 ## 57 Egypt 1.424834e+07 1.470386e+07 1.516286e+07 ## 58 El Salvador 1.387218e+06 1.429379e+06 1.472181e+06 ## 59 Equatorial Guinea 7.729503e+04 7.844574e+04 7.841107e+04 ## 60 Eritrea 2.121646e+05 2.221863e+05 2.325927e+05 ## 61 Estonia 8.472205e+05 8.662579e+05 8.847697e+05 ## 62 Ethiopia 2.249670e+06 2.365149e+06 2.487032e+06 ## 63 Faeroe Islands 1.017780e+04 1.047732e+04 1.077427e+04 ## 64 Fiji 1.690663e+05 1.749364e+05 1.809345e+05 ## 65 Finland 2.872371e+06 2.908120e+06 2.934402e+06 ## 66 France 3.554830e+07 3.622608e+07 3.691751e+07 ## 67 French Polynesia 5.421077e+04 5.768190e+04 6.125900e+04 ## 68 Gabon 1.478459e+05 1.582525e+05 1.694483e+05 ## 69 Gambia 7.628527e+04 8.261546e+04 8.942094e+04 ## 70 Georgia 1.900576e+06 1.938616e+06 1.904782e+06 ## 71 Germany 5.576506e+07 5.625874e+07 5.649607e+07 ## 72 Ghana 2.311442e+06 2.408851e+06 2.515296e+06 ## 73 Greece 4.415310e+06 4.518763e+06 4.616575e+06 ## 74 Greenland 3.040882e+04 3.206093e+04 3.375322e+04 ## 75 Grenada 3.019593e+04 3.031077e+04 3.040587e+04 ## 76 Guam 4.844571e+04 5.065242e+04 5.291621e+04 ## 77 Guatemala 1.802725e+06 1.868309e+06 1.936380e+06 ## 78 Guinea 5.962425e+05 6.304226e+05 6.636291e+05 ## 79 Guinea-Bissau 8.804516e+04 8.932212e+04 9.123325e+04 ## 80 Guyana 2.033071e+05 2.081042e+05 2.120772e+05 ## 81 Haiti 8.567168e+05 8.934834e+05 9.307198e+05 ## 82 Honduras 7.041621e+05 7.396318e+05 7.769459e+05 ## 83 Hong Kong, China 3.316190e+06 3.379661e+06 3.473191e+06 ## 84 Hungary 6.079237e+06 6.147720e+06 6.214324e+06 ## 85 Iceland 1.693063e+05 1.717736e+05 1.735679e+05 ## 86 India 1.025948e+08 1.059532e+08 1.094455e+08 ## 87 Indonesia 1.862152e+07 1.940053e+07 2.020553e+07 ## 88 Iran 1.074839e+07 1.127204e+07 1.181219e+07 ## 89 Iraq 5.053788e+06 5.335012e+06 5.627633e+06 ## 90 Ireland 1.472843e+06 1.499153e+06 1.529549e+06 ## 91 Isle of Man 3.041582e+04 3.107182e+04 3.166567e+04 ## 92 Israel 2.323491e+06 2.403561e+06 2.503959e+06 ## 93 Italy 3.369844e+07 3.414982e+07 3.459238e+07 ## 94 Jamaica 7.257254e+05 7.482876e+05 7.723456e+05 ## 95 Japan 7.101819e+07 7.332929e+07 7.500006e+07 ## 96 Jordan 7.513107e+05 7.991228e+05 8.440427e+05 ## 97 Kazakhstan 6.209379e+06 6.396692e+06 6.585936e+06 ## 98 Kenya 1.010199e+06 1.082085e+06 1.158426e+06 ## 99 Kiribati 1.054187e+04 1.115324e+04 1.177903e+04 ## 100 North Korea 6.797010e+06 7.252939e+06 7.721750e+06 ## 101 South Korea 1.142358e+07 1.219746e+07 1.299394e+07 ## 102 Kuwait 5.332849e+05 5.878232e+05 6.451490e+05 ## 103 Kyrgyz Republic 1.037698e+06 1.075216e+06 1.108956e+06 ## 104 Lao 2.333150e+05 2.458144e+05 2.590287e+05 ## 105 Latvia 1.374667e+06 1.404423e+06 1.432319e+06 ## 106 Lebanon 1.320402e+06 1.390579e+06 1.465634e+06 ## 107 Lesotho 7.636722e+04 8.253367e+04 8.892443e+04 ## 108 Liberia 3.336211e+05 3.536543e+05 3.746759e+05 ## 109 Libya 7.933851e+05 8.884915e+05 9.904397e+05 ## 110 Liechtenstein 3.835222e+03 3.893073e+03 3.941192e+03 ## 111 Lithuania 1.462854e+06 1.508107e+06 1.555873e+06 ## 112 Luxembourg 2.465394e+05 2.493815e+05 2.522550e+05 ## 113 Macao, China 2.292781e+05 2.376078e+05 2.435455e+05 ## 114 Macedonia, FYR 6.802103e+05 7.086757e+05 7.381837e+05 ## 115 Madagascar 8.337642e+05 8.775250e+05 9.233980e+05 ## 116 Malawi 2.398927e+05 2.565303e+05 2.742784e+05 ## 117 Malaysia 3.324289e+06 3.484442e+06 3.649615e+06 ## 118 Maldives 1.289746e+04 1.330701e+04 1.376876e+04 ## 119 Mali 7.972307e+05 8.302079e+05 8.646754e+05 ## 120 Malta 2.763384e+05 2.730307e+05 2.714740e+05 ## 121 Marshall Islands 9.323270e+03 1.007123e+04 1.091076e+04 ## 122 Mauritania 1.367608e+05 1.505604e+05 1.650886e+05 ## 123 Mauritius 3.195152e+05 3.332923e+05 3.471843e+05 ## 124 Mexico 2.808642e+07 2.931700e+07 3.061321e+07 ## 125 Micronesia, Fed. Sts. 1.419170e+04 1.477304e+04 1.523980e+04 ## 126 Moldova 8.959091e+05 9.356514e+05 9.764706e+05 ## 127 Monaco 2.323400e+04 2.344800e+04 2.368900e+04 ## 128 Mongolia 5.307544e+05 5.535133e+05 5.773571e+05 ## 129 Montenegro 1.292181e+05 1.340713e+05 1.392938e+05 ## 130 Morocco 4.848380e+06 5.061952e+06 5.278427e+06 ## 131 Mozambique 4.803006e+05 5.127060e+05 5.464057e+05 ## 132 Myanmar 5.512884e+06 5.737830e+06 5.973271e+06 ## 133 Namibia 1.578102e+05 1.656184e+05 1.739636e+05 ## 134 Nepal 4.411255e+05 4.559937e+05 4.714710e+05 ## 135 Netherlands 7.803192e+06 7.917513e+06 8.039946e+06 ## 136 New Caledonia 4.868702e+04 5.183153e+04 5.533056e+04 ## 137 New Zealand 2.204526e+06 2.236624e+06 2.279646e+06 ## 138 Nicaragua 1.022348e+06 1.073928e+06 1.127855e+06 ## 139 Niger 3.295439e+05 3.563980e+05 3.845578e+05 ## 140 Nigeria 1.186224e+07 1.242960e+07 1.302354e+07 ## 141 Northern Mariana Islands 8.073316e+03 8.655527e+03 9.250286e+03 ## 142 Norway 2.376327e+06 2.456007e+06 2.534594e+06 ## 143 Oman 1.833677e+05 1.995581e+05 2.170597e+05 ## 144 Pakistan 1.366756e+07 1.419101e+07 1.473699e+07 ## 145 Palau 6.627161e+03 6.736073e+03 6.855879e+03 ## 146 Panama 6.609825e+05 6.897512e+05 7.192792e+05 ## 147 Papua New Guinea 1.865556e+05 2.117910e+05 2.385030e+05 ## 148 Paraguay 8.662660e+05 8.931292e+05 9.201416e+05 ## 149 Peru 6.884271e+06 7.220337e+06 7.570234e+06 ## 150 Philippines 1.085199e+07 1.126489e+07 1.169151e+07 ## 151 Poland 1.657536e+07 1.683567e+07 1.702627e+07 ## 152 Portugal 3.360472e+06 3.364395e+06 3.368354e+06 ## 153 Puerto Rico 1.480203e+06 1.529021e+06 1.585301e+06 ## 154 Qatar 8.116982e+04 8.804065e+04 9.580697e+04 ## 155 Romania 7.775433e+06 7.962558e+06 8.164758e+06 ## 156 Russia 7.832602e+07 7.988771e+07 8.146468e+07 ## 157 Rwanda 1.065866e+05 1.129610e+05 1.196576e+05 ## 158 St. Kitts and Nevis 1.522598e+04 1.528050e+04 1.532931e+04 ## 159 St. Lucia 2.291663e+04 2.351565e+04 2.424170e+04 ## 160 St. Vincent and the Grenadines 2.633043e+04 2.703429e+04 2.775738e+04 ## 161 Samoa 2.727841e+04 2.815593e+04 2.897331e+04 ## 162 San Marino 1.071427e+04 1.109522e+04 1.144333e+04 ## 163 Sao Tome and Principe 1.841719e+04 2.006490e+04 2.173410e+04 ## 164 Saudi Arabia 2.382635e+06 2.586258e+06 2.809100e+06 ## 165 Senegal 1.096955e+06 1.161241e+06 1.228874e+06 ## 166 Serbia 2.595006e+06 2.683242e+06 2.770952e+06 ## 167 Seychelles 1.876104e+04 1.983538e+04 2.094045e+04 ## 168 Sierra Leone 5.535685e+05 5.797787e+05 6.067908e+05 ## 169 Singapore 2.012000e+06 2.043000e+06 2.075000e+06 ## 170 Slovak Republic 1.768967e+06 1.818929e+06 1.863258e+06 ## 171 Slovenia 6.000206e+05 6.187531e+05 6.382787e+05 ## 172 Solomon Islands 1.237527e+04 1.329659e+04 1.429003e+04 ## 173 Somalia 7.433007e+05 7.810217e+05 8.166815e+05 ## 174 South Africa 1.006591e+07 1.030848e+07 1.055957e+07 ## 175 Spain 2.123678e+07 2.176544e+07 2.233044e+07 ## 176 Sri Lanka 2.249555e+06 2.344592e+06 2.441982e+06 ## 177 Sudan 1.571927e+06 1.683562e+06 1.802344e+06 ## 178 Suriname 1.673102e+05 1.698198e+05 1.710630e+05 ## 179 Swaziland 3.554773e+04 3.929612e+04 4.325858e+04 ## 180 Sweden 6.285731e+06 6.393453e+06 6.517403e+06 ## 181 Switzerland 3.404449e+06 3.481651e+06 3.545846e+06 ## 182 Syria 2.499429e+06 2.626816e+06 2.760217e+06 ## 183 Tajikistan 1.000669e+06 1.041608e+06 1.084708e+06 ## 184 Tanzania 9.108258e+05 9.872961e+05 1.068227e+06 ## 185 Thailand 7.176231e+06 7.440174e+06 7.711257e+06 ## 186 Timor-Leste 7.108209e+04 7.435281e+04 7.788066e+04 ## 187 Togo 3.621139e+05 4.040164e+05 4.462997e+05 ## 188 Tonga 1.614767e+04 1.661674e+04 1.703157e+04 ## 189 Trinidad and Tobago 1.208498e+05 1.181071e+05 1.149191e+05 ## 190 Tunisia 2.070869e+06 2.149857e+06 2.229322e+06 ## 191 Turkey 1.244807e+07 1.299329e+07 1.355938e+07 ## 192 Turkmenistan 9.822601e+05 1.013434e+06 1.045665e+06 ## 193 Turks and Caicos Islands 2.804887e+03 2.829033e+03 2.878290e+03 ## 194 Tuvalu 1.480186e+03 1.545270e+03 1.611030e+03 ## 195 Uganda 5.499091e+05 5.891064e+05 6.294769e+05 ## 196 Ukraine 2.475757e+07 2.534887e+07 2.594411e+07 ## 197 United Arab Emirates 1.390527e+05 1.555970e+05 1.800752e+05 ## 198 United Kingdom 4.273308e+07 4.283308e+07 4.292583e+07 ## 199 United States 1.463404e+08 1.484759e+08 1.509224e+08 ## 200 Uruguay 2.273438e+06 2.295858e+06 2.313813e+06 ## 201 Uzbekistan 4.067599e+06 4.227790e+06 4.395765e+06 ## 202 Vanuatu 9.621427e+03 1.005774e+04 1.052469e+04 ## 203 Venezuela 6.994264e+06 7.324840e+06 7.674281e+06 ## 204 Vietnam 7.169607e+06 7.487421e+06 7.819407e+06 ## 205 Virgin Islands (U.S.) 3.661847e+04 4.004103e+04 4.384296e+04 ## 206 Yemen 7.369436e+05 7.769681e+05 8.172839e+05 ## 207 Zambia 1.069557e+06 1.160044e+06 1.256178e+06 ## 208 Zimbabwe 7.927728e+05 8.467739e+05 9.039055e+05 ## 209 South Sudan 3.210970e+05 3.268101e+05 3.330133e+05 9.4.2 Adapting sheets XLConnect’s approach of providing an actual interface to an Excel file makes it able to edit your Excel files from inside R. Create new empty sheet: createSheet(workbook, name = sheet_name) Add new data: writeWorksheet(workbook, dataframe, sheet = sheet_name/index) Save as new workbook: saveWorkbook(workbook, file = \"file_name\") Rename sheet: renameSheet(workbook, \"old_name\", \"new_name\") Remove sheet: removeSheet(workbook, sheet = sheet_name/index) 9.4.2.1 Add worksheet # Add a worksheet to my_book, named &quot;data_summary&quot; createSheet(my_book, name = &quot;data_summary&quot;) # Use getSheets() on my_book getSheets(my_book) ## [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; &quot;data_summary&quot; 9.4.2.2 Populate worksheet # Create data frame: summ sheets &lt;- getSheets(my_book)[1:3]; sheets ## [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; dims &lt;- sapply(sheets, function(x) dim(readWorksheet(my_book, sheet = x)), USE.NAMES = FALSE); dims ## [,1] [,2] [,3] ## [1,] 209 209 209 ## [2,] 8 9 38 summ &lt;- data.frame(sheets = sheets, nrows = dims[1, ], ncols = dims[2, ]); summ ## sheets nrows ncols ## 1 1960-1966 209 8 ## 2 1967-1974 209 9 ## 3 1975-2011 209 38 # Add data in summ to &quot;data_summary&quot; sheet writeWorksheet(my_book, summ, sheet = &quot;data_summary&quot;) # Save workbook as summary.xlsx saveWorkbook(my_book, file = &quot;data/summary.xlsx&quot;) 9.4.2.3 Renaming sheets # Rename &quot;data_summary&quot; sheet to &quot;summary&quot; renameSheet(my_book, &quot;data_summary&quot;, &quot;summary&quot;) # Print out sheets of my_book getSheets(my_book) ## [1] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; &quot;summary&quot; # Save workbook to &quot;renamed.xlsx&quot; saveWorkbook(my_book, file = &quot;data/renamed.xlsx&quot;) 9.4.2.4 Removing sheets # Build connection to renamed.xlsx: my_book my_book &lt;- loadWorkbook(&quot;data/renamed.xlsx&quot;) # Remove the fourth sheet removeSheet(my_book, sheet = &quot;summary&quot;) # Save workbook to &quot;clean.xlsx&quot; saveWorkbook(my_book, file = &quot;data/clean.xlsx&quot;) "],["cleaning-data-in-r.html", "Chapter 10 Cleaning Data in R 10.1 Common Data Problems 10.2 Categorical and Text Data 10.3 Advanced Data Problems 10.4 Record Linkage", " Chapter 10 Cleaning Data in R 10.1 Common Data Problems 10.1.1 Data type constraints Glimpsing at data types Checking data types: is.*(), return TRUE/FALSE class(), return data type Converting data types as.*() notice for factor to numeric: as.numeric(as.character()) 10.1.1.1 Converting data types Throughout this chapter, you’ll be working with San Francisco bike share ride data called bike_share_rides. It contains information on start and end stations of each trip, the trip duration, and some user information. library(tidyverse) bike_share_rides &lt;- read_rds(&quot;data/bike_share_rides_ch1_1.rds&quot;) # Glimpse at bike_share_rides glimpse(bike_share_rides) ## Rows: 35,229 ## Columns: 10 ## $ ride_id &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 83331… ## $ date &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04-03&quot;… ## $ duration &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;, &quot;6… ## $ station_A_id &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 67, 2… ## $ station_A_name &lt;chr&gt; &quot;San Francisco Caltrain Station 2 (Townsend St at 4th… ## $ station_B_id &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10, 80… ## $ station_B_name &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan St&quot;, … ## $ bike_id &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 3289, … ## $ user_gender &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;… ## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 1993, … # Summary of user_birth_year summary(bike_share_rides$user_birth_year) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1900 1979 1986 1984 1991 2001 # Convert user_birth_year to factor: user_birth_year_fct bike_share_rides &lt;- bike_share_rides %&gt;% mutate(user_birth_year_fct = as.factor(user_birth_year)) # check user_birth_year_fct is a factor is.factor(bike_share_rides$user_birth_year_fct) ## [1] TRUE # Summary of user_birth_year_fct summary(bike_share_rides$user_birth_year_fct) ## 1900 1902 1923 1931 1938 1939 1941 1942 1943 1945 1946 1947 1948 1949 1950 1951 ## 1 7 2 23 2 1 3 10 4 16 5 24 9 30 37 25 ## 1952 1953 1954 1955 1956 1957 1958 1959 1960 1961 1962 1963 1964 1965 1966 1967 ## 70 49 65 66 112 62 156 99 196 161 256 237 245 349 225 363 ## 1968 1969 1970 1971 1972 1973 1974 1975 1976 1977 1978 1979 1980 1981 1982 1983 ## 365 331 370 548 529 527 563 601 481 541 775 876 825 1016 1056 1262 ## 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 ## 1157 1318 1606 1672 2135 1872 2062 1582 1703 1498 1476 1185 813 358 365 348 ## 2000 2001 ## 473 30 10.1.1.2 Trimming strings Another common dirty data problem is having extra bits like percent signs or periods in numbers, causing them to be read in as characters. In order to be able to crunch these numbers, the extra bits need to be removed and the numbers need to be converted from character to numeric. bike_share_rides &lt;- bike_share_rides %&gt;% # Remove &#39;minutes&#39; from duration: duration_trimmed mutate(duration_trimmed = str_remove(duration, &quot;minutes&quot;), # Convert duration_trimmed to numeric: duration_mins duration_mins = as.numeric(duration_trimmed)) # Glimpse at bike_share_rides glimpse(bike_share_rides) ## Rows: 35,229 ## Columns: 13 ## $ ride_id &lt;int&gt; 52797, 54540, 87695, 45619, 70832, 96135, 29928, 8… ## $ date &lt;chr&gt; &quot;2017-04-15&quot;, &quot;2017-04-19&quot;, &quot;2017-04-14&quot;, &quot;2017-04… ## $ duration &lt;chr&gt; &quot;1316.15 minutes&quot;, &quot;8.13 minutes&quot;, &quot;24.85 minutes&quot;… ## $ station_A_id &lt;dbl&gt; 67, 21, 16, 58, 16, 6, 5, 16, 5, 81, 30, 16, 16, 6… ## $ station_A_name &lt;chr&gt; &quot;San Francisco Caltrain Station 2 (Townsend St at… ## $ station_B_id &lt;dbl&gt; 89, 64, 355, 368, 81, 66, 350, 91, 62, 81, 109, 10… ## $ station_B_name &lt;chr&gt; &quot;Division St at Potrero Ave&quot;, &quot;5th St at Brannan S… ## $ bike_id &lt;dbl&gt; 1974, 860, 2263, 1417, 507, 75, 388, 239, 1449, 32… ## $ user_gender &lt;chr&gt; &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;M… ## $ user_birth_year &lt;dbl&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19… ## $ user_birth_year_fct &lt;fct&gt; 1972, 1986, 1993, 1981, 1981, 1988, 1993, 1996, 19… ## $ duration_trimmed &lt;chr&gt; &quot;1316.15 &quot;, &quot;8.13 &quot;, &quot;24.85 &quot;, &quot;6.35 &quot;, &quot;9.8 &quot;, &quot;1… ## $ duration_mins &lt;dbl&gt; 1316.15, 8.13, 24.85, 6.35, 9.80, 17.47, 16.52, 14… # Assert duration_mins is numeric is.numeric(bike_share_rides$duration_mins) ## [1] TRUE # Calculate mean duration mean(bike_share_rides$duration_mins) ## [1] 13.06214 10.1.2 Range constraints Finding out of range values (不可能出現的值) For example: SAT score: 400-1600 -&gt; 352 or 1700 are out of range Adult heart rate: 60-100 beats per minute Handling out of range values Remove rows Treat as missing ( NA ) Replace with range limit Replace with other value based on domain knowledge and/or knowledge of dataset 10.1.2.1 Numerical data constraints Bikes are not allowed to be kept out for more than 24 hours, or 1440 minutes at a time, but issues with some of the bikes caused inaccurate recording of the time they were returned. In this exercise, you’ll replace erroneous data with the range limit (1440 minutes). # Load dataset bike_share_rides &lt;- read_csv(&quot;data/bike_share_rides_duration.csv&quot;) # Create breaks breaks &lt;- c(min(bike_share_rides$duration_min), 0, 1440, max(bike_share_rides$duration_min)) # Create a histogram of duration_min ggplot(bike_share_rides, aes(duration_min)) + geom_histogram(breaks = breaks) # duration_min_const: replace vals of duration_min &gt; 1440 with 1440 bike_share_rides &lt;- bike_share_rides %&gt;% mutate(duration_min_const = replace(duration_min, duration_min &gt; 1440, 1440)) # Make sure all values of duration_min_const are between 0 and 1440 range(bike_share_rides$duration_min_const) ## [1] 2.27 1440.00 10.1.2.2 Date constraints You’ll need to remove any rides from the dataset that have a date in the future. Before you can do this, the date column needs to be converted from a character to a Date. R makes it easy to check if one Date object is before (&lt;) or after (&gt;) another. # Load dataset again and create future data bike_share_rides &lt;- read_rds(&quot;data/bike_share_rides_ch1_1.rds&quot;) bike_share_rides$date &lt;- str_replace_all(bike_share_rides$date, &quot;2017-04-25&quot;, &quot;2043-02-17&quot;) # Convert date to Date type bike_share_rides &lt;- bike_share_rides %&gt;% mutate(date = as.Date(date)) # Make sure all dates are in the past bike_share_rides %&gt;% filter(date &gt; today()) ## # A tibble: 1,204 × 10 ## ride_id date duration station_A_id station_A_name station_B_id ## &lt;int&gt; &lt;date&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 69172 2043-02-17 12.68 minutes 16 Steuart St at Mar… 80 ## 2 63197 2043-02-17 4.07 minutes 67 San Francisco Cal… 321 ## 3 61811 2043-02-17 12.25 minutes 21 Montgomery St BAR… 74 ## 4 48844 2043-02-17 11 minutes 30 San Francisco Cal… 126 ## 5 79817 2043-02-17 12.02 minutes 15 San Francisco Fer… 3 ## 6 72592 2043-02-17 18 minutes 30 San Francisco Cal… 10 ## 7 66670 2043-02-17 7.23 minutes 16 Steuart St at Mar… 50 ## 8 66471 2043-02-17 13 minutes 5 Powell St BART St… 92 ## 9 79675 2043-02-17 7.95 minutes 3 Powell St BART St… 26 ## 10 86488 2043-02-17 3.5 minutes 67 San Francisco Cal… 90 ## # ℹ 1,194 more rows ## # ℹ 4 more variables: station_B_name &lt;chr&gt;, bike_id &lt;dbl&gt;, user_gender &lt;chr&gt;, ## # user_birth_year &lt;dbl&gt; # Filter for rides that occurred before or on today&#39;s date bike_share_rides_past &lt;- bike_share_rides %&gt;% filter(date &lt;= today()) # Make sure all dates from bike_share_rides_past are in the past max(bike_share_rides_past$date) ## [1] &quot;2017-04-30&quot; 10.1.3 Uniqueness constraints 10.1.3.1 Full duplicates A full duplicate occurs when there are multiple rows that have the same value in every column. Removing duplicates like this is important, since having the same value repeated multiple times can alter summary statistics like the mean and median. # count duplicate sum(duplicated(dataframe)) # Remove full duplicate dataframe %&gt;% distinct(.keep_all = TRUE) # Load dataset bike_share_rides &lt;- read_csv(&quot;data/bike_share_rides_full_duplicated.csv&quot;) # Count the number of full duplicates sum(duplicated(bike_share_rides)) ## [1] 2 # Remove duplicates bike_share_rides_unique &lt;- bike_share_rides %&gt;% distinct(.keep_all = TRUE) # Count the full duplicates in bike_share_rides_unique sum(duplicated(bike_share_rides_unique)) ## [1] 0 10.1.3.2 Partial duplicates Partial duplicates are rows of data that have much of the same information and refer to the same data point, but there may be some columns that differ between them. ride_id should be unique # Load dataset bike_share_rides &lt;- read_csv(&quot;data/bike_share_rides_partial_duplicated.csv&quot;) # Find duplicated ride_ids bike_share_rides %&gt;% # Count the number of occurrences of each ride_id count(ride_id) %&gt;% # Filter for rows with a count &gt; 1 filter(n &gt; 1) ## # A tibble: 2 × 2 ## ride_id n ## &lt;dbl&gt; &lt;int&gt; ## 1 41441 2 ## 2 87056 2 Removing partial duplicates # Remove partial duplicate dataframe %&gt;% distinct(column, .keep_all = TRUE) # Remove full and partial duplicates bike_share_rides_unique &lt;- bike_share_rides %&gt;% # Only based on ride_id instead of all cols distinct(ride_id, .keep_all = T) # Find duplicated ride_ids in bike_share_rides_unique bike_share_rides_unique %&gt;% # Count the number of occurrences of each ride_id count(ride_id) %&gt;% # Filter for rows with a count &gt; 1 filter(n &gt; 1) ## # A tibble: 0 × 2 ## # ℹ 2 variables: ride_id &lt;dbl&gt;, n &lt;int&gt; Aggregating partial duplicates Another way of handling partial duplicates is to compute a summary statistic of the values that differ between partial duplicates, such as mean, median, maximum, or minimum. This can come in handy when you’re not sure how your data was collected and want an average, or if based on domain knowledge, you’d rather have too high of an estimate than too low of an estimate (or vice versa). bike_share_rides %&gt;% # Group by ride_id and date group_by(ride_id, date) %&gt;% # Add duration_min_avg column mutate(duration_min_avg = mean(duration_min)) %&gt;% # Remove duplicates based on ride_id and date, keep all cols distinct(ride_id, date, .keep_all = T) %&gt;% # Remove duration_min column select(-duration_min) ## # A tibble: 35,229 × 6 ## # Groups: ride_id, date [35,229] ## ride_id date station_A_id station_A_name station_B_id duration_min_avg ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 52797 4/15/2017 67 &quot;San Francisco … 89 1316 ## 2 54540 4/19/2017 21 &quot;Montgomery St … 64 8.13 ## 3 87695 4/14/2017 16 &quot;Steuart St at … 355 24.8 ## 4 45619 4/3/2017 58 &quot;Market St at 1… 368 6.35 ## 5 70832 4/10/2017 16 &quot;Steuart St at … 81 9.8 ## 6 96135 4/18/2017 6 &quot;The Embarcader… 66 17.5 ## 7 29928 4/22/2017 5 &quot;Powell St BART… 350 16.5 ## 8 83331 4/11/2017 16 &quot;Steuart St at … 91 14.7 ## 9 72424 4/5/2017 5 &quot;Powell St BART… 62 4.12 ## 10 25910 4/20/2017 81 &quot;Berry St at 4t… 81 25.8 ## # ℹ 35,219 more rows 10.2 Categorical and Text Data 10.2.1 Checking membership Filtering joins: Keeps or removes observations from the first table without adding columns. Anti-joins: help you identify the rows that are causing issues semi-joins: remove the issue-causing rows. sfo_survey containing survey responses from passengers taking flights from San Francisco International Airport (SFO). dest_sizes is available that contains all the possible destination sizes. Your mission is to find rows with invalid dest_sizes and remove them from the data frame. # Load dataset sfo_survey &lt;- read_rds(&quot;data/sfo_survey_ch2_1.rds&quot;) str(sfo_survey) ## &#39;data.frame&#39;: 2809 obs. of 12 variables: ## $ id : int 1842 1844 1840 1837 1833 3010 1838 1845 2097 1846 ... ## $ day : chr &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; &quot;Monday&quot; ... ## $ airline : chr &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; ... ## $ destination : chr &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; ... ## $ dest_region : chr &quot;Middle East&quot; &quot;Middle East&quot; &quot;Middle East&quot; &quot;Middle East&quot; ... ## $ dest_size : chr &quot;Hub&quot; &quot;Hub&quot; &quot;Hub&quot; &quot;Hub&quot; ... ## $ boarding_area: chr &quot;Gates 91-102&quot; &quot;Gates 91-102&quot; &quot;Gates 91-102&quot; &quot;Gates 91-102&quot; ... ## $ dept_time : chr &quot;2018-12-31&quot; &quot;2018-12-31&quot; &quot;2018-12-31&quot; &quot;2018-12-31&quot; ... ## $ wait_min : num 255 315 165 225 175 ... ## $ cleanliness : chr &quot;Average&quot; &quot;Somewhat clean&quot; &quot;Average&quot; &quot;Somewhat clean&quot; ... ## $ safety : chr &quot;Neutral&quot; &quot;Somewhat safe&quot; &quot;Somewhat safe&quot; &quot;Somewhat safe&quot; ... ## $ satisfaction : chr &quot;Somewhat satsified&quot; &quot;Somewhat satsified&quot; &quot;Somewhat satsified&quot; &quot;Somewhat satsified&quot; ... # Create a dest_sizes dataframe for validation dest_size &lt;- c(&quot;Small&quot;, &quot;Medium&quot;, &quot;Large&quot;, &quot;Hub&quot;) passengers_per_day &lt;- c(&quot;0-20K&quot;, &quot;20K-70K&quot;, &quot;70K-100K&quot;, &quot;100K+&quot;) dest_sizes &lt;- data.frame(dest_size, passengers_per_day) str(dest_sizes) ## &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ dest_size : chr &quot;Small&quot; &quot;Medium&quot; &quot;Large&quot; &quot;Hub&quot; ## $ passengers_per_day: chr &quot;0-20K&quot; &quot;20K-70K&quot; &quot;70K-100K&quot; &quot;100K+&quot; # Count the number of occurrences of dest_size sfo_survey %&gt;% count(dest_size) ## dest_size n ## 1 Small 1 ## 2 Hub 1 ## 3 Hub 1756 ## 4 Large 143 ## 5 Large 1 ## 6 Medium 682 ## 7 Small 225 # Find bad dest_size rows sfo_survey %&gt;% # Join with dest_sizes data frame to get bad dest_size rows anti_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;% # Select id, airline, destination, and dest_size cols select(id, airline, destination, dest_size) ## id airline destination dest_size ## 1 982 LUFTHANSA MUNICH Hub ## 2 2063 AMERICAN PHILADELPHIA Large ## 3 777 UNITED INTL SAN JOSE DEL CABO Small # Remove bad dest_size rows sfo_survey %&gt;% # Join with dest_sizes semi_join(dest_sizes, by = &quot;dest_size&quot;) %&gt;% # Count the number of each dest_size count(dest_size) ## dest_size n ## 1 Hub 1756 ## 2 Large 143 ## 3 Medium 682 ## 4 Small 225 10.2.2 Categorical data problems Inconsistency within a category Case inconsistency: str_to_lower(), str_to_upper() Whitespace inconsistency: str_trim() Too many categories Collapsing categories: fct_collapse() 10.2.2.1 Identifying inconsistency You’ll examine the dest_size column again as well as the cleanliness column and determine what kind of issues. # Load uncleaned data sfo_survey &lt;- read_delim(&quot;data/sfo_survey_not_clean.txt&quot;, delim = &quot;,&quot;, col_select = 2:13) ## New names: ## Rows: 2809 Columns: 12 ## ── Column specification ## ────────────────────── Delimiter: &quot;,&quot; chr ## (9): day, airline, destination, dest_region, ## dest_size, boarding_area, ... dbl (2): id, ## wait_min date (1): dept_time ## ℹ Use `spec()` to retrieve the full column ## specification for this data. ℹ Specify the ## column types or set `show_col_types = FALSE` ## to quiet this message. ## • `` -&gt; `...1` # Count dest_size sfo_survey %&gt;% count(dest_size) ## # A tibble: 7 × 2 ## dest_size n ## &lt;chr&gt; &lt;int&gt; ## 1 &quot; Small &quot; 1 ## 2 &quot; Hub&quot; 1 ## 3 &quot;Hub&quot; 1756 ## 4 &quot;Large&quot; 143 ## 5 &quot;Large &quot; 1 ## 6 &quot;Medium&quot; 682 ## 7 &quot;Small&quot; 225 # Count cleanliness sfo_survey %&gt;% count(cleanliness) ## # A tibble: 9 × 2 ## cleanliness n ## &lt;chr&gt; &lt;int&gt; ## 1 AVERAGE 1 ## 2 Average 431 ## 3 Clean 970 ## 4 Dirty 2 ## 5 Somewhat clean 1253 ## 6 Somewhat dirty 30 ## 7 average 1 ## 8 somewhat clean 1 ## 9 &lt;NA&gt; 120 10.2.2.2 Correcting inconsistency Now that you’ve identified that dest_size has whitespace inconsistencies and cleanliness has capitalization inconsistencies, you’ll fix the inconsistent values in sfo_survey. # Add new columns to sfo_survey sfo_survey &lt;- sfo_survey %&gt;% # dest_size_trimmed: dest_size without whitespace mutate(dest_size_trimmed = str_trim(dest_size), # cleanliness_lower: cleanliness converted to lowercase cleanliness_lower = str_to_lower(cleanliness)) # Count values of dest_size_trimmed sfo_survey %&gt;% count(dest_size_trimmed) ## # A tibble: 4 × 2 ## dest_size_trimmed n ## &lt;chr&gt; &lt;int&gt; ## 1 Hub 1757 ## 2 Large 144 ## 3 Medium 682 ## 4 Small 226 # Count values of cleanliness_lower sfo_survey %&gt;% count(cleanliness_lower) ## # A tibble: 6 × 2 ## cleanliness_lower n ## &lt;chr&gt; &lt;int&gt; ## 1 average 433 ## 2 clean 970 ## 3 dirty 2 ## 4 somewhat clean 1254 ## 5 somewhat dirty 30 ## 6 &lt;NA&gt; 120 10.2.2.3 Collapsing categories Allowing the response for dest_region to be free text instead of a dropdown menu. This resulted in some inconsistencies in the dest_region variable # Count categories of dest_region sfo_survey %&gt;% count(dest_region) ## # A tibble: 12 × 2 ## dest_region n ## &lt;chr&gt; &lt;int&gt; ## 1 Asia 260 ## 2 Australia/New Zealand 66 ## 3 Canada/Mexico 220 ## 4 Central/South America 29 ## 5 EU 2 ## 6 East US 498 ## 7 Europ 1 ## 8 Europe 396 ## 9 Middle East 79 ## 10 Midwest US 281 ## 11 West US 975 ## 12 eur 2 # Categories to map to Europe europe_categories &lt;- c(&quot;EU&quot;, &quot;eur&quot;, &quot;Europ&quot;) # Add a new col dest_region_collapsed sfo_survey %&gt;% # Map all categories in europe_categories to Europe mutate(dest_region_collapsed = fct_collapse(dest_region, Europe = europe_categories)) %&gt;% # Count categories of dest_region_collapsed count(dest_region_collapsed) ## # A tibble: 9 × 2 ## dest_region_collapsed n ## &lt;fct&gt; &lt;int&gt; ## 1 Asia 260 ## 2 Australia/New Zealand 66 ## 3 Canada/Mexico 220 ## 4 Central/South America 29 ## 5 East US 498 ## 6 Europe 401 ## 7 Middle East 79 ## 8 Midwest US 281 ## 9 West US 975 You’ve reduced the number of categories from 12 to 9, and you can now be confident that 401 of the survey participants were heading to Europe. 10.2.3 Cleaning text data Unstructured data problems Formatting inconsistency \"6171679912\" vs. \"(868) 949-4489\" \"9239 5849 3712 0039\" vs. \"4490459957881031\" Information inconsistency +1 617-167-9912 vs. 617-167-9912 \"Veronica Hopkins\" vs. \"Josiah\" Invalid data Phone number \"0492\" is too short Zip code \"19888\" doesn’t exist Useful stringr function str_detect() str_replace_all() str_remove_all() str_length() 10.2.3.1 Detecting inconsistent text data sfo_survey &lt;- read_csv(&quot;data/sfo_survey_phone.csv&quot;) str(sfo_survey) ## spc_tbl_ [2,809 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ id : num [1:2809] 1842 1844 1840 1837 1833 ... ## $ airline : chr [1:2809] &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; &quot;TURKISH AIRLINES&quot; ... ## $ destination: chr [1:2809] &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; &quot;ISTANBUL&quot; ... ## $ phone : chr [1:2809] &quot;858 990 5153&quot; &quot;731-813-2043&quot; &quot;563-732-6802&quot; &quot;145 725 4021&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. id = col_double(), ## .. airline = col_character(), ## .. destination = col_character(), ## .. phone = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; You found that some phone numbers are written with hyphens (-) and some are written with parentheses (()). In this exercise, you’ll figure out which phone numbers have these issues so that you know which ones need fixing. # Filter for rows with &quot;-&quot; in the phone column sfo_survey %&gt;% filter(str_detect(phone, &quot;-&quot;)) ## # A tibble: 1,421 × 4 ## id airline destination phone ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1844 TURKISH AIRLINES ISTANBUL 731-813-2043 ## 2 1840 TURKISH AIRLINES ISTANBUL 563-732-6802 ## 3 3010 AMERICAN MIAMI (637) 782-6989 ## 4 2097 UNITED INTL MEXICO CITY (359) 803-9809 ## 5 1835 TURKISH AIRLINES ISTANBUL (416) 788-2844 ## 6 1849 TURKISH AIRLINES ISTANBUL 311-305-4367 ## 7 2289 QANTAS SYDNEY 817-400-0481 ## 8 105 UNITED WASHINGTON DC-DULLES (729) 609-4819 ## 9 1973 CATHAY PACIFIC HONG KONG (201) 737-4409 ## 10 2385 UNITED INTL SYDNEY (137) 611-3694 ## # ℹ 1,411 more rows Searching for special characters requires using fixed() : str_detect(column, fixed(\"$\")) # Filter for rows with &quot;(&quot; or &quot;)&quot; in the phone column sfo_survey %&gt;% filter(str_detect(phone, fixed(&quot;(&quot;)) | str_detect(phone, fixed(&quot;)&quot;))) ## # A tibble: 739 × 4 ## id airline destination phone ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3010 AMERICAN MIAMI (637) 782-6989 ## 2 2097 UNITED INTL MEXICO CITY (359) 803-9809 ## 3 1835 TURKISH AIRLINES ISTANBUL (416) 788-2844 ## 4 105 UNITED WASHINGTON DC-DULLES (729) 609-4819 ## 5 1973 CATHAY PACIFIC HONG KONG (201) 737-4409 ## 6 2385 UNITED INTL SYDNEY (137) 611-3694 ## 7 517 UNITED FT. LAUDERDALE (812) 869-6263 ## 8 2885 EVA AIR TAIPEI (194) 198-0504 ## 9 2128 FRONTIER DENVER (299) 137-6993 ## 10 2132 FRONTIER DENVER (739) 710-2966 ## # ℹ 729 more rows 10.2.3.2 Replacing and removing The customer support team has requested that all phone numbers be in the format \"123 456 7890\". # Remove parentheses from phone column phone_no_parens &lt;- sfo_survey$phone %&gt;% # Remove &quot;(&quot;s str_remove_all(fixed(&quot;(&quot;)) %&gt;% # Remove &quot;)&quot;s str_remove_all(fixed(&quot;)&quot;)) # Add phone_no_parens as column sfo_survey &lt;- sfo_survey %&gt;% mutate(phone_no_parens = phone_no_parens, # Replace all hyphens in phone_no_parens with spaces phone_clean = str_replace_all(phone_no_parens, &quot;-&quot;, &quot; &quot;)) sfo_survey ## # A tibble: 2,809 × 6 ## id airline destination phone phone_no_parens phone_clean ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1842 TURKISH AIRLINES ISTANBUL 858 990 5153 858 990 5153 858 990 51… ## 2 1844 TURKISH AIRLINES ISTANBUL 731-813-2043 731-813-2043 731 813 20… ## 3 1840 TURKISH AIRLINES ISTANBUL 563-732-6802 563-732-6802 563 732 68… ## 4 1837 TURKISH AIRLINES ISTANBUL 145 725 4021 145 725 4021 145 725 40… ## 5 1833 TURKISH AIRLINES ISTANBUL 931 311 5801 931 311 5801 931 311 58… ## 6 3010 AMERICAN MIAMI (637) 782-6989 637 782-6989 637 782 69… ## 7 1838 TURKISH AIRLINES ISTANBUL 172 990 3485 172 990 3485 172 990 34… ## 8 1845 TURKISH AIRLINES ISTANBUL 872 325 4341 872 325 4341 872 325 43… ## 9 2097 UNITED INTL MEXICO CITY (359) 803-9809 359 803-9809 359 803 98… ## 10 1846 TURKISH AIRLINES ISTANBUL 152 790 8238 152 790 8238 152 790 82… ## # ℹ 2,799 more rows 10.2.3.3 Invalid text data In this exercise, you’ll remove any rows with invalid phone numbers. # Check out the invalid numbers sfo_survey %&gt;% filter(str_length(phone_clean) != 12) ## # A tibble: 5 × 6 ## id airline destination phone phone_no_parens phone_clean ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2262 UNITED BAKERSFIELD 0244-5 0244-5 0244 5 ## 2 3081 COPA PANAMA CITY 925 8846 925 8846 925 8846 ## 3 340 SOUTHWEST PHOENIX 1623 1623 1623 ## 4 1128 DELTA MINNEAPOLIS-ST. PAUL 665-803 665-803 665 803 ## 5 373 ALASKA SAN JOSE DEL CABO 38515 38515 38515 # Remove rows with invalid numbers sfo_survey %&gt;% filter(str_length(phone_clean) == 12) ## # A tibble: 2,804 × 6 ## id airline destination phone phone_no_parens phone_clean ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1842 TURKISH AIRLINES ISTANBUL 858 990 5153 858 990 5153 858 990 51… ## 2 1844 TURKISH AIRLINES ISTANBUL 731-813-2043 731-813-2043 731 813 20… ## 3 1840 TURKISH AIRLINES ISTANBUL 563-732-6802 563-732-6802 563 732 68… ## 4 1837 TURKISH AIRLINES ISTANBUL 145 725 4021 145 725 4021 145 725 40… ## 5 1833 TURKISH AIRLINES ISTANBUL 931 311 5801 931 311 5801 931 311 58… ## 6 3010 AMERICAN MIAMI (637) 782-6989 637 782-6989 637 782 69… ## 7 1838 TURKISH AIRLINES ISTANBUL 172 990 3485 172 990 3485 172 990 34… ## 8 1845 TURKISH AIRLINES ISTANBUL 872 325 4341 872 325 4341 872 325 43… ## 9 2097 UNITED INTL MEXICO CITY (359) 803-9809 359 803-9809 359 803 98… ## 10 1846 TURKISH AIRLINES ISTANBUL 152 790 8238 152 790 8238 152 790 82… ## # ℹ 2,794 more rows 10.3 Advanced Data Problems 10.3.1 Uniformity continuous data points have different units or formats Temperature: °C vs. °F Weight: kg vs. g vs. lb Money: USD $ vs. GBP £ vs. JPY ¥ Date: DD-MM-YYYY vs. MM-DD-YYYY vs. YYYY-MM-DD Solution: Finding uniformity issues: plot or statistic Unit conversion ifelse(condition, value_if_true, value_if_false) parse_date_time(date_col, orders = c(\"format_1\", \"format_2\", …)) 10.3.1.1 Date uniformity The accounts dataset, which contains information about each customer, the amount in their account, and the date their account was opened. Your boss has asked you to calculate some summary statistics about the average value of each account and whether the age of the account is associated with a higher or lower account value. In this exercise, you’ll investigate the date_opened column and clean it up so that all the dates are in the same format. accounts &lt;- read_rds(&quot;data/ch3_1_accounts.rds&quot;) head(accounts) ## id date_opened total ## 1 A880C79F 2003-10-19 169305 ## 2 BE8222DF October 05, 2018 107460 ## 3 19F9E113 2008-07-29 15297152 ## 4 A2FE52A3 2005-06-09 14897272 ## 5 F6DC2C08 2012-03-31 124568 ## 6 D2E55799 2007-06-20 13635752 # Define the date formats formats &lt;- c(&quot;%Y-%m-%d&quot;, &quot;%B %d, %Y&quot;) # Convert dates to the same format accounts %&gt;% mutate(date_opened_clean = parse_date_time(date_opened, orders = formats)) ## id date_opened total date_opened_clean ## 1 A880C79F 2003-10-19 169305 2003-10-19 ## 2 BE8222DF October 05, 2018 107460 2018-10-05 ## 3 19F9E113 2008-07-29 15297152 2008-07-29 ## 4 A2FE52A3 2005-06-09 14897272 2005-06-09 ## 5 F6DC2C08 2012-03-31 124568 2012-03-31 ## 6 D2E55799 2007-06-20 13635752 2007-06-20 ## 7 53AE87EF December 01, 2017 15375984 2017-12-01 ## 8 3E97F253 2019-06-03 14515800 2019-06-03 ## 9 4AE79EA1 2011-05-07 23338536 2011-05-07 ## 10 2322DFB4 2018-04-07 189524 2018-04-07 ## 11 645335B2 2018-11-16 154001 2018-11-16 ## 12 D5EB0F00 2001-04-16 174576 2001-04-16 ## 13 1EB593F7 2005-04-21 191989 2005-04-21 ## 14 DDBA03D9 2006-06-13 9617192 2006-06-13 ## 15 40E4A2F4 2009-01-07 180547 2009-01-07 ## 16 39132EEA 2012-07-07 15611960 2012-07-07 ## 17 387F8E4D January 03, 2011 9402640 2011-01-03 ## 18 11C3C3C0 December 24, 2017 180003 2017-12-24 ## 19 C2FC91E1 2004-05-21 105722 2004-05-21 ## 20 FB8F01C1 2001-09-06 22575072 2001-09-06 ## 21 0128D2D0 2005-04-09 19179784 2005-04-09 ## 22 BE6E4B3F 2009-10-20 15679976 2009-10-20 ## 23 7C6E2ECC 2003-05-16 169814 2003-05-16 ## 24 02E63545 2015-10-25 125117 2015-10-25 ## 25 4399C98B May 19, 2001 130421 2001-05-19 ## 26 98F4CF0F May 27, 2014 14893944 2014-05-27 ## 27 247222A6 May 26, 2015 150372 2015-05-26 ## 28 420985EE 2008-12-27 123125 2008-12-27 ## 29 0E3903BA 2015-11-11 182668 2015-11-11 ## 30 64EF994F 2009-02-26 161141 2009-02-26 ## 31 CCF84EDB 2008-12-26 136128 2008-12-26 ## 32 51C21705 April 22, 2016 16191136 2016-04-22 ## 33 C868C6AD January 31, 2000 11733072 2000-01-31 ## 34 92C237C6 2005-12-13 11838528 2005-12-13 ## 35 9ECEADB2 May 17, 2018 146153 2018-05-17 ## 36 DF0AFE50 2004-12-03 15250040 2004-12-03 ## 37 5CD605B3 2016-10-19 87921 2016-10-19 ## 38 402839E2 September 14, 2019 163416 2019-09-14 ## 39 78286CE7 2009-10-05 15049216 2009-10-05 ## 40 168E071B 2013-07-11 87826 2013-07-11 ## 41 466CCDAA 2002-03-24 14981304 2002-03-24 ## 42 8DE1ECB9 2015-10-17 217975 2015-10-17 ## 43 E19FE6B5 June 06, 2009 101936 2009-06-06 ## 44 1240D39C September 07, 2011 15761824 2011-09-07 ## 45 A7BFAA72 2019-11-12 133790 2019-11-12 ## 46 C3D24436 May 24, 2002 101584 2002-05-24 ## 47 FAD92F0F September 13, 2007 17081064 2007-09-13 ## 48 236A1D51 2019-10-01 18486936 2019-10-01 ## 49 A6DDDC4C 2000-08-17 67962 2000-08-17 ## 50 DDFD0B3D 2001-04-11 15776384 2001-04-11 ## 51 D13375E9 November 01, 2005 13944632 2005-11-01 ## 52 AC50B796 2016-06-30 16111264 2016-06-30 ## 53 290319FD May 27, 2005 170178 2005-05-27 ## 54 FC71925A November 02, 2006 186281 2006-11-02 ## 55 7B0F3685 2013-05-23 179102 2013-05-23 ## 56 BE411172 2017-02-24 17689984 2017-02-24 ## 57 58066E39 September 16, 2015 17025632 2015-09-16 ## 58 EA7FF83A 2004-11-02 11598704 2004-11-02 ## 59 14A2DDB7 2019-03-06 12808952 2019-03-06 ## 60 305EEAA8 2018-09-01 14417728 2018-09-01 ## 61 8F25E54C November 24, 2008 189126 2008-11-24 ## 62 19DD73C6 2002-12-31 14692600 2002-12-31 ## 63 ACB8E6AF 2013-07-27 71359 2013-07-27 ## 64 91BFCC40 2014-01-10 132859 2014-01-10 ## 65 86ACAF81 2011-12-14 24533704 2011-12-14 ## 66 77E85C14 November 20, 2009 13868192 2009-11-20 ## 67 C5C6B79D 2008-03-01 188424 2008-03-01 ## 68 0E5B69F5 2018-05-07 18650632 2018-05-07 ## 69 5275B518 2017-11-23 71665 2017-11-23 ## 70 17217048 May 25, 2001 20111208 2001-05-25 ## 71 E7496A7F 2008-09-27 142669 2008-09-27 ## 72 41BBB7B4 February 22, 2005 144229 2005-02-22 ## 73 F6C7ABA1 2008-01-07 183440 2008-01-07 ## 74 E699DF01 February 17, 2008 199603 2008-02-17 ## 75 BACA7378 2005-05-11 204271 2005-05-11 ## 76 84A4302F 2003-08-12 19420648 2003-08-12 ## 77 F8A78C27 April 05, 2006 41164 2006-04-05 ## 78 8BADDF6A December 31, 2010 158203 2010-12-31 ## 79 9FB57E68 September 01, 2017 216352 2017-09-01 ## 80 5C98E8F5 2014-11-25 103200 2014-11-25 ## 81 6BB53C2A December 03, 2016 146394 2016-12-03 ## 82 E23F2505 October 15, 2017 121614 2017-10-15 ## 83 0C121914 June 21, 2017 227729 2017-06-21 ## 84 3627E08A 2008-04-01 238104 2008-04-01 ## 85 A94493B3 August 01, 2009 85975 2009-08-01 ## 86 0682E9DE 2002-10-01 72832 2002-10-01 ## 87 49931170 2011-03-25 14519856 2011-03-25 ## 88 A154F63B 2000-07-11 133800 2000-07-11 ## 89 3690CCED 2014-10-19 226595 2014-10-19 ## 90 48F5E6D8 February 16, 2020 135435 2020-02-16 ## 91 515FAD84 2013-06-20 98190 2013-06-20 ## 92 59794264 2008-01-16 157964 2008-01-16 ## 93 2038185B 2016-06-24 194662 2016-06-24 ## 94 65EAC615 February 20, 2004 140191 2004-02-20 ## 95 6C7509C9 September 16, 2000 212089 2000-09-16 ## 96 BD969A9D 2007-04-29 167238 2007-04-29 ## 97 B0CDCE3D May 28, 2014 145240 2014-05-28 ## 98 33A7F03E October 14, 2007 191839 2007-10-14 10.3.1.2 Currency uniformity When you first plot the data, you’ll notice that there’s a group of very high values, and a group of relatively lower values. The bank has two different offices - one in New York, and one in Tokyo, so you suspect that the accounts managed by the Tokyo office are in Japanese yen instead of U.S. dollars. accounts &lt;- accounts %&gt;% mutate(date_opened = parse_date_time(date_opened, orders = formats)) # Scatter plot of opening date and total amount accounts %&gt;% ggplot(aes(x = date_opened, y = total)) + geom_point() You have a data frame called account_offices that indicates which office manages each customer’s account, so you can use this information to figure out which totals need to be converted from yen to dollars. The formula to convert yen to dollars is USD = JPY / 104. account_offices &lt;- read_csv(&quot;data/account_offices.csv&quot;) ## Rows: 98 Columns: 2 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): id, office ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Left join accounts and account_offices by id accounts %&gt;% left_join(account_offices, by = &quot;id&quot;) %&gt;% # Convert totals from the Tokyo office to USD mutate(total_usd = ifelse(office == &quot;Tokyo&quot;, total/104, total)) ## id date_opened total office total_usd ## 1 A880C79F 2003-10-19 169305 New York 169305 ## 2 BE8222DF 2018-10-05 107460 New York 107460 ## 3 19F9E113 2008-07-29 15297152 Tokyo 147088 ## 4 A2FE52A3 2005-06-09 14897272 Tokyo 143243 ## 5 F6DC2C08 2012-03-31 124568 New York 124568 ## 6 D2E55799 2007-06-20 13635752 Tokyo 131113 ## 7 53AE87EF 2017-12-01 15375984 Tokyo 147846 ## 8 3E97F253 2019-06-03 14515800 Tokyo 139575 ## 9 4AE79EA1 2011-05-07 23338536 Tokyo 224409 ## 10 2322DFB4 2018-04-07 189524 New York 189524 ## 11 645335B2 2018-11-16 154001 New York 154001 ## 12 D5EB0F00 2001-04-16 174576 New York 174576 ## 13 1EB593F7 2005-04-21 191989 New York 191989 ## 14 DDBA03D9 2006-06-13 9617192 Tokyo 92473 ## 15 40E4A2F4 2009-01-07 180547 New York 180547 ## 16 39132EEA 2012-07-07 15611960 Tokyo 150115 ## 17 387F8E4D 2011-01-03 9402640 Tokyo 90410 ## 18 11C3C3C0 2017-12-24 180003 New York 180003 ## 19 C2FC91E1 2004-05-21 105722 New York 105722 ## 20 FB8F01C1 2001-09-06 22575072 Tokyo 217068 ## 21 0128D2D0 2005-04-09 19179784 Tokyo 184421 ## 22 BE6E4B3F 2009-10-20 15679976 Tokyo 150769 ## 23 7C6E2ECC 2003-05-16 169814 New York 169814 ## 24 02E63545 2015-10-25 125117 New York 125117 ## 25 4399C98B 2001-05-19 130421 New York 130421 ## 26 98F4CF0F 2014-05-27 14893944 Tokyo 143211 ## 27 247222A6 2015-05-26 150372 New York 150372 ## 28 420985EE 2008-12-27 123125 New York 123125 ## 29 0E3903BA 2015-11-11 182668 New York 182668 ## 30 64EF994F 2009-02-26 161141 New York 161141 ## 31 CCF84EDB 2008-12-26 136128 New York 136128 ## 32 51C21705 2016-04-22 16191136 Tokyo 155684 ## 33 C868C6AD 2000-01-31 11733072 Tokyo 112818 ## 34 92C237C6 2005-12-13 11838528 Tokyo 113832 ## 35 9ECEADB2 2018-05-17 146153 New York 146153 ## 36 DF0AFE50 2004-12-03 15250040 Tokyo 146635 ## 37 5CD605B3 2016-10-19 87921 New York 87921 ## 38 402839E2 2019-09-14 163416 &lt;NA&gt; NA ## 39 78286CE7 2009-10-05 15049216 Tokyo 144704 ## 40 168E071B 2013-07-11 87826 New York 87826 ## 41 466CCDAA 2002-03-24 14981304 Tokyo 144051 ## 42 8DE1ECB9 2015-10-17 217975 New York 217975 ## 43 E19FE6B5 2009-06-06 101936 New York 101936 ## 44 1240D39C 2011-09-07 15761824 Tokyo 151556 ## 45 A7BFAA72 2019-11-12 133790 New York 133790 ## 46 C3D24436 2002-05-24 101584 New York 101584 ## 47 FAD92F0F 2007-09-13 17081064 Tokyo 164241 ## 48 236A1D51 2019-10-01 18486936 Tokyo 177759 ## 49 A6DDDC4C 2000-08-17 67962 New York 67962 ## 50 DDFD0B3D 2001-04-11 15776384 Tokyo 151696 ## 51 D13375E9 2005-11-01 13944632 Tokyo 134083 ## 52 AC50B796 2016-06-30 16111264 Tokyo 154916 ## 53 290319FD 2005-05-27 170178 New York 170178 ## 54 FC71925A 2006-11-02 186281 New York 186281 ## 55 7B0F3685 2013-05-23 179102 New York 179102 ## 56 BE411172 2017-02-24 17689984 Tokyo 170096 ## 57 58066E39 2015-09-16 17025632 &lt;NA&gt; NA ## 58 EA7FF83A 2004-11-02 11598704 Tokyo 111526 ## 59 14A2DDB7 2019-03-06 12808952 Tokyo 123163 ## 60 305EEAA8 2018-09-01 14417728 Tokyo 138632 ## 61 8F25E54C 2008-11-24 189126 New York 189126 ## 62 19DD73C6 2002-12-31 14692600 Tokyo 141275 ## 63 ACB8E6AF 2013-07-27 71359 New York 71359 ## 64 91BFCC40 2014-01-10 132859 New York 132859 ## 65 86ACAF81 2011-12-14 24533704 Tokyo 235901 ## 66 77E85C14 2009-11-20 13868192 Tokyo 133348 ## 67 C5C6B79D 2008-03-01 188424 New York 188424 ## 68 0E5B69F5 2018-05-07 18650632 Tokyo 179333 ## 69 5275B518 2017-11-23 71665 New York 71665 ## 70 17217048 2001-05-25 20111208 Tokyo 193377 ## 71 E7496A7F 2008-09-27 142669 New York 142669 ## 72 41BBB7B4 2005-02-22 144229 New York 144229 ## 73 F6C7ABA1 2008-01-07 183440 New York 183440 ## 74 E699DF01 2008-02-17 199603 New York 199603 ## 75 BACA7378 2005-05-11 204271 New York 204271 ## 76 84A4302F 2003-08-12 19420648 Tokyo 186737 ## 77 F8A78C27 2006-04-05 41164 New York 41164 ## 78 8BADDF6A 2010-12-31 158203 New York 158203 ## 79 9FB57E68 2017-09-01 216352 New York 216352 ## 80 5C98E8F5 2014-11-25 103200 New York 103200 ## 81 6BB53C2A 2016-12-03 146394 New York 146394 ## 82 E23F2505 2017-10-15 121614 New York 121614 ## 83 0C121914 2017-06-21 227729 New York 227729 ## 84 3627E08A 2008-04-01 238104 New York 238104 ## 85 A94493B3 2009-08-01 85975 New York 85975 ## 86 0682E9DE 2002-10-01 72832 New York 72832 ## 87 49931170 2011-03-25 14519856 Tokyo 139614 ## 88 A154F63B 2000-07-11 133800 New York 133800 ## 89 3690CCED 2014-10-19 226595 New York 226595 ## 90 48F5E6D8 2020-02-16 135435 New York 135435 ## 91 515FAD84 2013-06-20 98190 New York 98190 ## 92 59794264 2008-01-16 157964 New York 157964 ## 93 2038185B 2016-06-24 194662 New York 194662 ## 94 65EAC615 2004-02-20 140191 New York 140191 ## 95 6C7509C9 2000-09-16 212089 New York 212089 ## 96 BD969A9D 2007-04-29 167238 New York 167238 ## 97 B0CDCE3D 2014-05-28 145240 New York 145240 ## 98 33A7F03E 2007-10-14 191839 New York 191839 accounts %&gt;% left_join(account_offices, by = &quot;id&quot;) %&gt;% mutate(total_usd = ifelse(office == &quot;Tokyo&quot;, total / 104, total)) %&gt;% # Scatter plot of opening date vs total_usd ggplot(aes(x = date_opened, y = total_usd)) + geom_point() ## Warning: Removed 2 rows containing missing values ## (`geom_point()`). 10.3.2 Cross field validation a sanity check on your data to check that one value makes sense based on other values in your dataset. 10.3.2.1 Validating totals You have a bit more information about each account. There are three different funds that account holders can store their money in. You’ll validate whether the total amount in each account is equal to the sum of the amount in fund_A, fund_B, and fund_C. accounts &lt;- read_delim(&quot;data/accounts_fund_ages.txt&quot;, delim = &quot;,&quot;) %&gt;% select(-1) %&gt;% mutate(date_opened = as.Date(date_opened, format = &quot;%m/%d/%Y&quot;)) head(accounts) ## # A tibble: 6 × 7 ## id date_opened total fund_A fund_B fund_C acct_age ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A880C79F 2003-10-19 169305 85018 75580 8707 20 ## 2 BE8222DF 2018-10-05 107460 64784 35194 7482 5 ## 3 19F9E113 2008-07-29 147088 64029 15300 67759 15 ## 4 A2FE52A3 2005-06-09 143243 63466 54053 25724 18 ## 5 F6DC2C08 2012-03-31 124568 21156 47935 55477 11 ## 6 D2E55799 2007-06-20 131113 79241 26800 25072 16 # Find invalid totals accounts %&gt;% # theoretical_total: sum of the three funds mutate(theoretical_total = fund_A + fund_B + fund_C) %&gt;% # Find accounts where total doesn&#39;t match theoretical_total filter(total != theoretical_total) ## # A tibble: 3 × 8 ## id date_opened total fund_A fund_B fund_C acct_age theoretical_total ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D5EB0F00 2001-04-16 130920 69487 48681 56408 22 174576 ## 2 92C237C6 2005-12-13 85362 72556 21739 19537 17 113832 ## 3 0E5B69F5 2018-05-07 134488 88475 44383 46475 5 179333 10.3.2.2 Validating age You’re suspicious that there may also be inconsistencies in the acct_age column, and you want to see if these inconsistencies are related. You’ll need to validate the age of each account and see if rows with inconsistent acct_ages are the same ones that had inconsistent totals. # Example date_difference &lt;- as.Date(&quot;2015-09-04&quot;) %--% today(); date_difference ## [1] 2015-09-04 UTC--2024-01-16 UTC as.numeric(date_difference, &quot;years&quot;) ## [1] 8.366872 floor(as.numeric(date_difference, &quot;years&quot;)) ## [1] 8 # Find invalid acct_age accounts %&gt;% # theoretical_age: age of acct based on date_opened mutate(theoretical_age = floor(as.numeric(date_opened %--% today(), &quot;years&quot;))) %&gt;% # Filter for rows where acct_age is different from theoretical_age filter(acct_age != theoretical_age) ## # A tibble: 24 × 8 ## id date_opened total fund_A fund_B fund_C acct_age theoretical_age ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 53AE87EF 2017-12-01 147846 38450 29185 80211 5 6 ## 2 645335B2 2018-11-16 154001 68691 56400 28910 4 5 ## 3 40E4A2F4 2009-01-07 180547 82564 68210 29773 14 15 ## 4 387F8E4D 2011-01-03 90410 7520 67142 15748 12 13 ## 5 11C3C3C0 2017-12-24 180003 84295 31591 64117 4 6 ## 6 420985EE 2008-12-27 123125 59390 27890 35845 14 15 ## 7 0E3903BA 2015-11-11 182668 47236 87437 47995 7 8 ## 8 CCF84EDB 2008-12-26 136128 33405 89016 13707 14 15 ## 9 92C237C6 2005-12-13 85362 72556 21739 19537 17 18 ## 10 DF0AFE50 2004-12-03 146635 67373 63443 15819 18 19 ## # ℹ 14 more rows There are three accounts that all have ages off by one year, but none of them are the same as the accounts that had total inconsistencies, so it looks like these two bookkeeping errors may not be related. 10.3.3 Completeness Types of missing data (“Missing at random” is actually a misleading name, since there’s nothing random about this type of missing data.) Dealing with missing data Simple approaches: Drop missing data Impute (fill in) with statistical measures (mean, median, mode..) or domain knowledge More complex approaches: Impute using an algorithmic approach Impute with machine learning models 10.3.3.1 Visualizing missing data You just received a new version of the accounts data frame containing data on the amount held and amount invested for new and existing customers. However, there are rows with missing inv_amount values. You know for a fact that most customers below 25 do not have investment accounts yet, and suspect it could be driving the missingness. accounts &lt;- read_csv(&quot;data/accounts_missing_values.csv&quot;) str(accounts) ## spc_tbl_ [97 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ cust_id : chr [1:97] &quot;8C35540A&quot; &quot;D5536652&quot; &quot;A631984D&quot; &quot;93F2F951&quot; ... ## $ age : num [1:97] 54 36 49 56 21 47 53 29 58 53 ... ## $ acct_amount : num [1:97] 44245 86507 77799 93875 99998 ... ## $ inv_amount : num [1:97] 35501 81922 46412 76563 NA ... ## $ account_opened : chr [1:97] &quot;3/5/2018&quot; &quot;21-01-18&quot; &quot;26-01-18&quot; &quot;21-08-17&quot; ... ## $ last_transaction: chr [1:97] &quot;30-09-19&quot; &quot;14-01-19&quot; &quot;6/10/2019&quot; &quot;10/7/2019&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. cust_id = col_character(), ## .. age = col_double(), ## .. acct_amount = col_double(), ## .. inv_amount = col_double(), ## .. account_opened = col_character(), ## .. last_transaction = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; library(visdat) ## Warning: package &#39;visdat&#39; was built under R version 4.3.2 vis_miss(accounts) Investigating summary statistics based on missingness is a great way to determine if data is missing completely at random or missing at random. accounts %&gt;% # missing_inv: Is inv_amount missing? mutate(missing_inv = is.na(inv_amount)) %&gt;% # Group by missing_inv group_by(missing_inv) %&gt;% # Calculate mean age for each missing_inv group summarise(avg_age = mean(age)) ## # A tibble: 2 × 2 ## missing_inv avg_age ## &lt;lgl&gt; &lt;dbl&gt; ## 1 FALSE 43.6 ## 2 TRUE 21.8 Since the average age for TRUE and FALSE groups, it is likely that the inv_amount is missing mostly in young customers. # Sort by age and visualize missing vals accounts %&gt;% arrange(age) %&gt;% vis_miss() 10.3.3.2 Treating missing data You’re working with another version of the accounts data that contains missing values for both the cust_id and acct_amount columns. accounts &lt;- read_delim(&quot;data/accounts_treat_missing_data.txt&quot;, delim = &quot;,&quot;, col_select = -1) str(accounts) ## tibble [97 × 5] (S3: tbl_df/tbl/data.frame) ## $ cust_id : chr [1:97] &quot;8C35540A&quot; &quot;D5536652&quot; &quot;A631984D&quot; &quot;93F2F951&quot; ... ## $ acct_amount : num [1:97] 44245 NA NA NA NA ... ## $ inv_amount : num [1:97] 35501 81922 46412 76563 18669 ... ## $ account_opened : chr [1:97] &quot;03-05-18&quot; &quot;21-01-18&quot; &quot;26-01-18&quot; &quot;21-08-17&quot; ... ## $ last_transaction: chr [1:97] &quot;30-09-19&quot; &quot;14-01-19&quot; &quot;06-10-19&quot; &quot;10-07-19&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ...1 = col_skip(), ## .. cust_id = col_character(), ## .. acct_amount = col_double(), ## .. inv_amount = col_double(), ## .. account_opened = col_character(), ## .. last_transaction = col_character() ## .. ) print(c(sum(is.na(accounts$cust_id)), sum(is.na(accounts$acct_amount)))) ## [1] 9 14 You want to figure out how many unique customers the bank has, as well as the average amount held by customers. You know that rows with missing cust_id don’t really help you, you will drop rows of accounts with missing cust_ids. # Create accounts_clean accounts_clean &lt;- accounts %&gt;% # Filter to remove rows with missing cust_id filter(!is.na(cust_id)) accounts_clean ## # A tibble: 88 × 5 ## cust_id acct_amount inv_amount account_opened last_transaction ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8C35540A 44245. 35500. 03-05-18 30-09-19 ## 2 D5536652 NA 81922. 21-01-18 14-01-19 ## 3 A631984D NA 46412. 26-01-18 06-10-19 ## 4 93F2F951 NA 76563. 21-08-17 10-07-19 ## 5 DE0A0882 NA 18669. 05-06-17 15-01-19 ## 6 25E68E1B 109738. 93553. 26-12-17 12-11-18 ## 7 3FA9296D NA 70358. 21-06-18 24-08-18 ## 8 984403B9 NA 14430. 07-10-17 18-05-18 ## 9 870A9281 63523. 51297. 02-09-18 22-02-19 ## 10 166B05B0 38175. 15053. 28-02-19 31-10-18 ## # ℹ 78 more rows And impute missing values of inv_amount with some domain knowledge, which is that on average, the acct_amount is usually 5 times the amount of inv_amount. # Create accounts_clean accounts_clean &lt;- accounts %&gt;% # Filter to remove rows with missing cust_id filter(!is.na(cust_id)) %&gt;% # Add new col acct_amount_filled with replaced NAs mutate(acct_amount_filled = ifelse(is.na(acct_amount), inv_amount * 5, acct_amount)) # Assert that cust_id has no missing vals sum(is.na(accounts_clean$cust_id)) ## [1] 0 # Assert that acct_amount_filled has no missing vals sum(is.na(accounts_clean$acct_amount_filled)) ## [1] 0 10.4 Record Linkage 10.4.1 Comparing strings Comparing strings to clean data In Chapter 2: \"EU\" , \"eur\" , \"Europ\" → \"Europe\" What if there are too many variations? \"EU\" , \"eur\" , \"Europ\" , \"Europa\" , \"Erope\" , \"Evropa\" , … → \"Europe\" ? Use string distance! Types of edit distance Damerau-Levenshtein (method = \"dl\") the minimum number of steps needed to get from String A to String B Insertion of a new character. Deletion of an existing character. Substitution of an existing character. Transposition of two existing consecutive characters. Levenshtein Considers only substitution, insertion, and deletion LCS (Longest Common Subsequence, method = \"lcs\") Considers only insertion and deletion Others Jaro-Winkler Jaccard (method = \"jaccard\") 10.4.1.1 Small distance, small difference Now you’ll practice using the stringdist package to compute string distances using various methods. It’s important to be familiar with different methods, as some methods work better on certain datasets, while others work better on other datasets. library(stringdist) ## Warning: package &#39;stringdist&#39; was built under R version 4.3.1 # Calculate Damerau-Levenshtein distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;dl&quot;) ## [1] 2 # Calculate LCS distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;lcs&quot;) ## [1] 4 # Calculate Jaccard distance stringdist(&quot;las angelos&quot;, &quot;los angeles&quot;, method = &quot;jaccard&quot;) ## [1] 0 10.4.1.2 Fixing typos with string distance zagat, is a set of restaurants in New York, Los Angeles, Atlanta, San Francisco, and Las Vegas. The data is from Zagat, a company that collects restaurant reviews, and includes the restaurant names, addresses, phone numbers, as well as other restaurant information. zagat &lt;- read_delim(&quot;data/zagat_unclean.txt&quot;, delim = &quot;,&quot;, col_select = -1) str(zagat) ## tibble [310 × 6] (S3: tbl_df/tbl/data.frame) ## $ id : num [1:310] 0 1 2 3 4 5 6 8 9 11 ... ## $ name : chr [1:310] &quot;apple pan the&quot; &quot;asahi ramen&quot; &quot;baja fresh&quot; &quot;belvedere the&quot; ... ## $ addr : chr [1:310] &quot;10801 w. pico blvd.&quot; &quot;2027 sawtelle blvd.&quot; &quot;3345 kimber dr.&quot; &quot;9882 little santa monica blvd.&quot; ... ## $ city : chr [1:310] &quot;llos angeles&quot; &quot;los angeles&quot; &quot;los angeles&quot; &quot;los angeles&quot; ... ## $ phone: chr [1:310] &quot;310-475-3585&quot; &quot;310-479-2231&quot; &quot;805-498-4049&quot; &quot;310-788-2306&quot; ... ## $ type : chr [1:310] &quot;american&quot; &quot;noodle shops&quot; &quot;mexican&quot; &quot;pacific new wave&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. ...1 = col_skip(), ## .. id = col_double(), ## .. name = col_character(), ## .. addr = col_character(), ## .. city = col_character(), ## .. phone = col_character(), ## .. type = col_character() ## .. ) The city column contains the name of the city that the restaurant is located in. However, there are a number of typos throughout the column. Your task is to map each city to one of the five correctly-spelled cities contained in the cities data frame. cities &lt;- data.frame(city_actual = c(&quot;new york&quot;, &quot;los angeles&quot;, &quot;atlanta&quot;, &quot;san francisco&quot;, &quot;las vegas&quot;)) # Count the number of each city variation zagat %&gt;% count(city) ## # A tibble: 63 × 2 ## city n ## &lt;chr&gt; &lt;int&gt; ## 1 aatlanta 3 ## 2 an francisco 1 ## 3 aotlanta 2 ## 4 atalanta 1 ## 5 atanta 1 ## 6 atlannta 2 ## 7 atlanta 48 ## 8 atlata 2 ## 9 ew york 2 ## 10 la vegas 2 ## # ℹ 53 more rows library(fuzzyjoin) ## Warning: package &#39;fuzzyjoin&#39; was built under R version 4.3.2 # Join zagat and cities and look at results zagat %&gt;% # Left join based on stringdist using city and city_actual cols stringdist_left_join(cities, by = c(&quot;city&quot; = &quot;city_actual&quot;)) %&gt;% # Select the name, city, and city_actual cols select(name, city, city_actual) ## # A tibble: 310 × 3 ## name city city_actual ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 apple pan the llos angeles los angeles ## 2 asahi ramen los angeles los angeles ## 3 baja fresh los angeles los angeles ## 4 belvedere the los angeles los angeles ## 5 benita&#39;s frites lo angeles los angeles ## 6 bernard&#39;s los angeles los angeles ## 7 bistro 45 lo angeles los angeles ## 8 brighton coffee shop los angeles los angeles ## 9 bristol farms market cafe los anegeles los angeles ## 10 cafe&#39;50s los angeles los angeles ## # ℹ 300 more rows 10.4.2 Generating and comparing pairs Record linkage involves linking data together that comes from multiple sources that don’t share a common identifier (a regular join won’t work), but contain data on the same entity. Steps : Clean the different datasets Generate pairs (rows) In order to figure out whether any of the rows are matches, we’ll need to compare every single row in table A with every single row in table B. reclin2 package, pair(df_A, df_B) But there may be too many pairs if we have thousand rows in each table. Solve: Blocking: Only consider pairs when they agree on the blocking variable pair_blocking(df_A, df_B, on = \"common_variable\") Compare pairs compare_pairs(on = \"variable\", default_comparator = method) on argument : indicate the column of each data frame should be compared (multiple columns by passing a vector of columns) default_comparator argument : indicate how strings should be compared (?cmp_identical) default_comparator = cmp_lcs() default_comparator = cmp_jaccard() default_comparator = cmp_jarowinkler() Score pairs based on how similar they are Link the most similar pairs together 10.4.2.1 Pair blocking The zagat and fodors datasets both contain information about various restaurants, including addresses, phone numbers, and cuisine types. Some restaurants appear in both datasets, but don’t necessarily have the same exact name or phone number written down. In this chapter, you’ll work towards figuring out which restaurants appear in both datasets. zagat &lt;- read_rds(&quot;data/zagat.rds&quot;) fodors &lt;- read_rds(&quot;data/fodors.rds&quot;) str(zagat) ## &#39;data.frame&#39;: 310 obs. of 7 variables: ## $ id : int 0 1 2 3 4 5 6 8 9 11 ... ## $ name : chr &quot;apple pan the&quot; &quot;asahi ramen&quot; &quot;baja fresh&quot; &quot;belvedere the&quot; ... ## $ addr : chr &quot;10801 w. pico blvd.&quot; &quot;2027 sawtelle blvd.&quot; &quot;3345 kimber dr.&quot; &quot;9882 little santa monica blvd.&quot; ... ## $ city : Factor w/ 26 levels &quot;atlanta&quot;,&quot;los angeles&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ phone: Factor w/ 329 levels &quot;100-813-8212&quot;,..: 143 145 312 155 139 96 323 125 150 132 ... ## $ type : chr &quot;american&quot; &quot;noodle shops&quot; &quot;mexican&quot; &quot;pacific new wave&quot; ... ## $ class: int 534 535 536 537 538 539 540 542 543 545 ... str(fodors) ## &#39;data.frame&#39;: 533 obs. of 7 variables: ## $ id : int 0 1 2 3 4 5 6 7 8 9 ... ## $ name : chr &quot;arnie morton&#39;s of chicago&quot; &quot;art&#39;s delicatessen&quot; &quot;hotel bel-air&quot; &quot;cafe bizou&quot; ... ## $ addr : chr &quot;435 s. la cienega blv .&quot; &quot;12224 ventura blvd.&quot; &quot;701 stone canyon rd.&quot; &quot;14016 ventura blvd.&quot; ... ## $ city : Factor w/ 5 levels &quot;atlanta&quot;,&quot;los angeles&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ phone: chr &quot;310-246-1501&quot; &quot;818-762-1221&quot; &quot;310-472-1211&quot; &quot;818-788-3536&quot; ... ## $ type : chr &quot;american&quot; &quot;american&quot; &quot;californian&quot; &quot;french&quot; ... ## $ class: int 0 1 2 3 4 5 6 7 8 9 ... The first step towards this goal is to generate pairs of records so that you can compare them. You’ll first generate all possible pairs, and then use your newly-cleaned city column as a blocking variable. # Load reclin library(reclin2) ## Warning: package &#39;reclin2&#39; was built under R version 4.3.2 # Generate all possible pairs pair(zagat, fodors) ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 165 230 pairs ## ## .x .y ## 1: 1 1 ## 2: 1 2 ## 3: 1 3 ## 4: 1 4 ## 5: 1 5 ## --- ## 165226: 310 529 ## 165227: 310 530 ## 165228: 310 531 ## 165229: 310 532 ## 165230: 310 533 # Generate pairs with same city pair_blocking(zagat, fodors, on = &quot;city&quot;) %&gt;% print() ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 40 532 pairs ## Blocking on: &#39;city&#39; ## ## .x .y ## 1: 125 75 ## 2: 125 76 ## 3: 125 77 ## 4: 125 78 ## 5: 125 79 ## --- ## 40528: 309 529 ## 40529: 309 530 ## 40530: 309 531 ## 40531: 309 532 ## 40532: 309 533 By using city as a blocking variable, you were able to reduce the number of pairs you’ll need to compare from 165,230 pairs to 40,532. 10.4.2.2 Comparing pairs Choosing a comparator and the columns to compare is highly dataset-dependent, so it’s best to try out different combinations to see which works best on the dataset you’re working with. # Generate pairs pair_blocking(zagat, fodors, on = &quot;city&quot;) %&gt;% # Compare pairs by name using lcs() compare_pairs(on = &quot;name&quot;, default_comparator = cmp_lcs()) %&gt;% print() ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 40 532 pairs ## Blocking on: &#39;city&#39; ## ## .x .y name ## 1: 125 75 0.0000000 ## 2: 125 76 0.0000000 ## 3: 125 77 0.2500000 ## 4: 125 78 0.2222222 ## 5: 125 79 0.1666667 ## --- ## 40528: 309 529 0.1904762 ## 40529: 309 530 0.2500000 ## 40530: 309 531 0.4516129 ## 40531: 309 532 0.4000000 ## 40532: 309 533 0.2758621 # Generate pairs pair_blocking(zagat, fodors, on = &quot;city&quot;) %&gt;% # Compare pairs by name, phone, addr compare_pairs(on = c(&quot;name&quot;, &quot;phone&quot;, &quot;addr&quot;), default_comparator = cmp_jarowinkler()) %&gt;% print() ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 40 532 pairs ## Blocking on: &#39;city&#39; ## ## .x .y name phone addr ## 1: 125 75 0.0000000 0.6666667 0.5618032 ## 2: 125 76 0.0000000 0.6746032 0.5598585 ## 3: 125 77 0.4888889 0.7361111 0.5367003 ## 4: 125 78 0.3810458 0.6666667 0.5530694 ## 5: 125 79 0.2809524 0.7962963 0.5067340 ## --- ## 40528: 309 529 0.4166667 0.6944444 0.5785247 ## 40529: 309 530 0.5000000 0.6111111 0.6500754 ## 40530: 309 531 0.5917398 0.6666667 0.5595777 ## 40531: 309 532 0.6111111 0.7152778 0.5492710 ## 40532: 309 533 0.4369281 0.6111111 0.5846405 10.4.3 Scoring and linking Score pairs based on how similar they are Link the most similar pairs together The end is near - all that’s left to do is: score and select pairs link the data together # Create pairs pairs &lt;- pair_blocking(zagat, fodors, on = &quot;city&quot;) %&gt;% # Compare pairs compare_pairs(on = c(&quot;name&quot;, &quot;addr&quot;), default_comparator = cmp_jarowinkler()) head(pairs) ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 6 pairs ## Blocking on: &#39;city&#39; ## ## .x .y name addr ## 1: 125 75 0.0000000 0.5618032 ## 2: 125 76 0.0000000 0.5598585 ## 3: 125 77 0.4888889 0.5367003 ## 4: 125 78 0.3810458 0.5530694 ## 5: 125 79 0.2809524 0.5067340 ## 6: 125 80 0.4666667 0.5193603 # Score pairs pairs &lt;- problink_em(~ name + addr, data = pairs) %&gt;% predict(pairs = pairs, add = TRUE) head(pairs) ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 6 pairs ## Blocking on: &#39;city&#39; ## ## .x .y name addr weights ## 1: 125 75 0.0000000 0.5618032 -1.34858419 ## 2: 125 76 0.0000000 0.5598585 -1.35576834 ## 3: 125 77 0.4888889 0.5367003 0.09666368 ## 4: 125 78 0.3810458 0.5530694 -0.17164700 ## 5: 125 79 0.2809524 0.5067340 -0.63511255 ## 6: 125 80 0.4666667 0.5193603 -0.03510377 # Select pairs pairs %&gt;% select_n_to_m(score = &quot;weights&quot;, variable = &quot;select&quot;, threshold = 0) ## First data set: 310 records ## Second data set: 533 records ## Total number of pairs: 40 532 pairs ## Blocking on: &#39;city&#39; ## ## .x .y name addr weights select ## 1: 125 75 0.0000000 0.5618032 -1.34858419 FALSE ## 2: 125 76 0.0000000 0.5598585 -1.35576834 FALSE ## 3: 125 77 0.4888889 0.5367003 0.09666368 FALSE ## 4: 125 78 0.3810458 0.5530694 -0.17164700 FALSE ## 5: 125 79 0.2809524 0.5067340 -0.63511255 FALSE ## --- ## 40528: 309 529 0.4166667 0.5785247 0.02930946 FALSE ## 40529: 309 530 0.5000000 0.6500754 0.56593082 FALSE ## 40530: 309 531 0.5917398 0.5595777 0.51978067 FALSE ## 40531: 309 532 0.6111111 0.5492710 0.55070344 FALSE ## 40532: 309 533 0.4369281 0.5846405 0.11358092 FALSE pairs %&gt;% select_n_to_m(score = &quot;weights&quot;, variable = &quot;select&quot;, threshold = 0) %&gt;% # Link data link(selection = &quot;select&quot;) ## Total number of pairs: 308 pairs ## ## .y .x id.x name.x addr.x ## 1: 1 200 218 arnie morton&#39;s of chicago 435 s. la cienega blvd. ## 2: 2 201 219 art&#39;s deli 12224 ventura blvd. ## 3: 3 202 220 bel-air hotel 701 stone canyon rd. ## 4: 4 36 46 mulberry st. 17040 ventura blvd. ## 5: 5 203 222 campanile 624 s. la brea ave. ## --- ## 304: 520 177 195 &#39;em erald garden restaurant 1550 california st. ## 305: 522 194 212 thep phanom 400 waller st. ## 306: 525 185 203 marnee thai 2225 irving st. ## 307: 530 176 194 ebisu 1283 ninth ave. ## 308: 531 189 207 roosevelt tamale parlor 2817 24th st. ## city.x phone.x type.x class.x id.y ## 1: los angeles 310-246-1501 steakhouses 0 0 ## 2: los angeles 818-762-1221 delis 1 1 ## 3: los angeles 310-472-1211 californian 2 2 ## 4: los angeles 818-906-8881 pizza 580 3 ## 5: los angeles 213-938-1447 californian 4 4 ## --- ## 304: san francisco 415-673-1155 vietnamese 730 519 ## 305: san francisco 415-431-2526 thai 747 521 ## 306: san francisco 415-665-9500 thai 738 524 ## 307: san francisco 415-566-1770 japanese 729 529 ## 308: san francisco 415-550-9213 mexican 742 530 ## name.y addr.y city.y ## 1: arnie morton&#39;s of chicago 435 s. la cienega blv . los angeles ## 2: art&#39;s delicatessen 12224 ventura blvd. los angeles ## 3: hotel bel-air 701 stone canyon rd. los angeles ## 4: cafe bizou 14016 ventura blvd. los angeles ## 5: campanile 624 s. la brea ave. los angeles ## --- ## 304: tadich grill 240 california st. san francisco ## 305: thepin 298 gough st. san francisco ## 306: vivande porta via 2125 fillmore st. san francisco ## 307: yaya cuisine 1220 9th ave. san francisco ## 308: yoyo tsumami bistro 1611 post st. san francisco ## phone.y type.y class.y ## 1: 310-246-1501 american 0 ## 2: 818-762-1221 american 1 ## 3: 310-472-1211 californian 2 ## 4: 818-788-3536 french 3 ## 5: 213-938-1447 american 4 ## --- ## 304: 415-391-2373 seafood 519 ## 305: 415-863-9335 asian 521 ## 306: 415-346-4430 italian 524 ## 307: 415-566-6966 greek and middle eastern 529 ## 308: 415-922-7788 french 530 # More clearly to see what happen # create and compare pairs pairs &lt;- pair_blocking(zagat, fodors, on = &quot;city&quot;) pairs &lt;- compare_pairs(pairs, on = c(&quot;name&quot;, &quot;addr&quot;), default_comparator = cmp_jarowinkler()) # Scoring probabilistically # m- and u-probabilities for each of the linkage variables m &lt;- problink_em(~ name + addr, data = pairs) print(m) # Then score the pairs pairs &lt;- predict(m, pairs = pairs, add = TRUE) # Select pairs pairs &lt;- select_n_to_m(pairs, score = &quot;weights&quot;, variable = &quot;select&quot;, threshold = 0) # link pairs linked_data_set &lt;- link(pairs, selection = &quot;select&quot;) linked_data_set "],["working-with-dates-and-times-in-r.html", "Chapter 11 Working with Dates and Times in R 11.1 Dates and Times in R 11.2 Parsing &amp; Manipulating with lubridate 11.3 Arithmetic with Dates and Times 11.4 Problems in practice", " Chapter 11 Working with Dates and Times in R 11.1 Dates and Times in R 11.1.1 Introduction to dates ISO 8601: YYYY-MM-DD Values ordered from the largest to smallest unit of time Each has a ,xed number of digits, must be padded with leading zeros Either, no separators for computers, or - in dates 1st of January 2011 -&gt; 2011-01-01 11.1.1.1 Specifying dates # The date R 3.0.0 was released x &lt;- &quot;2013-04-03&quot; # Examine structure of x str(x) ## chr &quot;2013-04-03&quot; # Use as.Date() to interpret x as a date x_date &lt;- as.Date(x) # Examine structure of x_date str(x_date) ## Date[1:1], format: &quot;2013-04-03&quot; # Store April 10 2014 as a Date april_10_2014 &lt;- as.Date(&quot;2014-04-10&quot;) 11.1.1.2 Automatic import anytime() function in the anytime package: automatically parse strings as dates regardless of the format. # Load the readr package library(readr) # Use read_csv() to import rversions.csv releases &lt;- read_csv(&quot;data/rversions.csv&quot;) # Examine the structure of the date column str(releases$date) ## Date[1:105], format: &quot;1997-12-04&quot; &quot;1997-12-21&quot; &quot;1998-01-10&quot; &quot;1998-03-14&quot; &quot;1998-05-02&quot; ... # Load the anytime package library(anytime) # Various ways of writing Sep 10 2009 sep_10_2009 &lt;- c(&quot;September 10 2009&quot;, &quot;2009-09-10&quot;, &quot;10 Sep 2009&quot;, &quot;09-10-2009&quot;) # Use anytime() to parse sep_10_2009 anytime(sep_10_2009) ## [1] &quot;2009-09-10 CST&quot; &quot;2009-09-10 CST&quot; &quot;2009-09-10 CST&quot; &quot;2009-09-10 CST&quot; 11.1.2 Why use dates? Dates act like numbers Plotting with dates str(releases) ## spc_tbl_ [105 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ major : num [1:105] 0 0 0 0 0 0 0 0 0 0 ... ## $ minor : num [1:105] 60 61 61 61 61 62 62 62 62 62 ... ## $ patch : num [1:105] NA NA 1 2 3 NA 1 2 3 4 ... ## $ date : Date[1:105], format: &quot;1997-12-04&quot; &quot;1997-12-21&quot; ... ## $ datetime: POSIXct[1:105], format: &quot;1997-12-04 08:47:58&quot; &quot;1997-12-21 13:09:22&quot; ... ## $ time : &#39;hms&#39; num [1:105] 08:47:58 13:09:22 00:31:55 19:25:55 ... ## ..- attr(*, &quot;units&quot;)= chr &quot;secs&quot; ## $ type : chr [1:105] &quot;patch&quot; &quot;minor&quot; &quot;patch&quot; &quot;patch&quot; ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. major = col_double(), ## .. minor = col_double(), ## .. patch = col_double(), ## .. date = col_date(format = &quot;&quot;), ## .. datetime = col_datetime(format = &quot;&quot;), ## .. time = col_time(format = &quot;&quot;), ## .. type = col_character() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 11.1.2.1 Plotting library(ggplot2) # Set the x axis to the date column ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) # Limit the axis to between 2010-01-01 and 2014-01-01 ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) + xlim(as.Date(&quot;2010-01-01&quot;), as.Date(&quot;2014-01-01&quot;)) ## Warning: Removed 87 rows containing missing values ## (`geom_line()`). # Specify breaks every ten years and labels with &quot;%Y&quot; ggplot(releases, aes(x = date, y = type)) + geom_line(aes(group = 1, color = factor(major))) + scale_x_date(date_breaks = &quot;10 years&quot;, date_labels = &quot;%Y&quot;) 11.1.2.2 Arithmetic &amp; logical operators Since Date objects are internally represented as the number of days since 1970-01-01, you can do basic math and comparisons with dates. You can compare dates with the usual logical operators (&lt;, ==, &gt; etc.), find extremes with min() and max(), and even subtract two dates to find out the time between them. Sys.date() -&gt; today’s date library(dplyr) # Find the largest date last_release_date &lt;- max(releases$date) # Filter row for last release last_release &lt;- releases %&gt;% filter(date == last_release_date) # Print last_release last_release ## # A tibble: 1 × 7 ## major minor patch date datetime time type ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;date&gt; &lt;dttm&gt; &lt;time&gt; &lt;chr&gt; ## 1 3 4 1 2017-06-30 2017-06-30 07:04:11 07:04:11 patch # How long since last release? Sys.Date() - last_release_date ## Time difference of 2391 days 11.1.3 What about times? ISO 8601: HH:MM:SS Largest unit to smallest Fixed digits Hours: 00 -- 24 Minutes: 00 -- 59 Seconds: 00 -- 60 (60 only for leap seconds) No separator or : Datetimes in R Two objects types: POSIXlt - list with named components POSIXct - seconds since 1970-01-01 00:00:00 POSIXct will go in a data frame as.POSIXct() turns a string into a POSIXct object Timezones as.POSIXct(\"2013-02-27T18:00:00Z\", tz = \"UTC\") 11.1.3.1 Getting datetimes as.POSIXct() expects strings to be in the format YYYY-MM-DD HH:MM:SS check your timezone with Sys.timezone() If you want the time to be interpreted in a different timezone, set the tz argument of as.POSIXct() # Check timezone Sys.timezone() ## [1] &quot;Asia/Taipei&quot; # Use as.POSIXct to enter the datetime as.POSIXct(&quot;2010-10-01 12:12:00&quot;) ## [1] &quot;2010-10-01 12:12:00 CST&quot; # Use as.POSIXct again but set the timezone to `&quot;America/Los_Angeles&quot;` as.POSIXct(&quot;2010-10-01 12:12:00&quot;, tz = &quot;America/Los_Angeles&quot;) ## [1] &quot;2010-10-01 12:12:00 PDT&quot; # Examine structure of datetime column str(releases$datetime) ## POSIXct[1:105], format: &quot;1997-12-04 08:47:58&quot; &quot;1997-12-21 13:09:22&quot; &quot;1998-01-10 00:31:55&quot; ... 11.1.3.2 Plot # Import &quot;cran-logs_2015-04-17.csv&quot; with read_csv() logs &lt;- read_csv(&quot;data/cran-logs_2015-04-17.csv&quot;) # Print logs print(logs) ## # A tibble: 100,000 × 3 ## datetime r_version country ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2015-04-16 22:40:19 3.1.3 CO ## 2 2015-04-16 09:11:04 3.1.3 GB ## 3 2015-04-16 17:12:37 3.1.3 DE ## 4 2015-04-18 12:34:43 3.2.0 GB ## 5 2015-04-16 04:49:18 3.1.3 PE ## 6 2015-04-16 06:40:44 3.1.3 TW ## 7 2015-04-16 00:21:36 3.1.3 US ## 8 2015-04-16 10:27:23 3.1.3 US ## 9 2015-04-16 01:59:43 3.1.3 SG ## 10 2015-04-18 15:41:32 3.2.0 CA ## # ℹ 99,990 more rows # Store the release time as a POSIXct object release_time &lt;- as.POSIXct(&quot;2015-04-16 07:13:33&quot;, tz = &quot;UTC&quot;) # When is the first download of 3.2.0? logs %&gt;% filter(datetime &gt; release_time, r_version == &quot;3.2.0&quot;) %&gt;% arrange(datetime) ## # A tibble: 35,826 × 3 ## datetime r_version country ## &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2015-04-16 07:20:06 3.2.0 GB ## 2 2015-04-16 07:30:18 3.2.0 ES ## 3 2015-04-16 07:36:04 3.2.0 US ## 4 2015-04-16 07:36:38 3.2.0 IN ## 5 2015-04-16 07:39:17 3.2.0 FI ## 6 2015-04-16 07:39:34 3.2.0 UA ## 7 2015-04-16 07:39:37 3.2.0 DE ## 8 2015-04-16 07:40:13 3.2.0 DE ## 9 2015-04-16 07:40:33 3.2.0 US ## 10 2015-04-16 07:43:29 3.2.0 KR ## # ℹ 35,816 more rows # Examine histograms of downloads by version ggplot(logs, aes(x = datetime)) + geom_histogram() + geom_vline(aes(xintercept = as.numeric(release_time)))+ facet_wrap(~ r_version, ncol = 1) 11.2 Parsing &amp; Manipulating with lubridate lubridate in tidyverse package 11.2.1 Parsing dates Parsing dates ymd() : year, then month, then day ydm() , mdy() , myd() , dmy() , dym(), dmy_hms(), dmy_hm(), dmy_h() … parse_date_time(\"date\", order = \"***\") e.g, parse_date_time(c(\"27-02-2013\", \"2013 Feb 27th\"), orders = c(\"dmy\", \"ymd\")) Formatting characters 11.2.1.1 Select the right parsing function library(lubridate) # Parse x x &lt;- &quot;2010 September 20th&quot; # 2010-09-20 ymd(x) ## [1] &quot;2010-09-20&quot; # Parse y y &lt;- &quot;02.01.2010&quot; # 2010-01-02 dmy(y) ## [1] &quot;2010-01-02&quot; # Parse z z &lt;- &quot;Sep, 12th 2010 14:00&quot; # 2010-09-12T14:00 mdy_hm(z) ## [1] &quot;2010-09-12 14:00:00 UTC&quot; 11.2.1.2 Specify an order with parse_date_time One advantage of parse_date_time() is that you can use more format characters. For example, you can specify weekday names with A, I for 12 hour time, am/pm indicators with p and many others. You can see a whole list on the help page ?parse_date_time. # Change to UTC timezone Sys.setlocale(&quot;LC_TIME&quot;, &quot;C&quot;) ## [1] &quot;C&quot; # Specify an order string to parse x x &lt;- &quot;Monday June 1st 2010 at 4pm&quot; parse_date_time(x, orders = &quot;AbdY_Ip&quot;) ## [1] &quot;2010-06-01 16:00:00 UTC&quot; # Specify order to include both &quot;mdy&quot; and &quot;dmy&quot; two_orders &lt;- c(&quot;October 7, 2001&quot;, &quot;October 13, 2002&quot;, &quot;April 13, 2003&quot;, &quot;17 April 2005&quot;, &quot;23 April 2017&quot;) parse_date_time(two_orders, orders = c(&quot;mdy&quot;, &quot;dmy&quot;)) ## [1] &quot;2001-10-07 UTC&quot; &quot;2002-10-13 UTC&quot; &quot;2003-04-13 UTC&quot; &quot;2005-04-17 UTC&quot; ## [5] &quot;2017-04-23 UTC&quot; # Specify order to include &quot;dOmY&quot;, &quot;OmY&quot; and &quot;Y&quot; short_dates &lt;- c(&quot;11 December 1282&quot;, &quot;May 1372&quot;, &quot;1253&quot;) parse_date_time(short_dates, orders = c(&quot;dOmY&quot;, &quot;OmY&quot;, &quot;Y&quot;)) ## [1] &quot;1282-12-11 UTC&quot; &quot;1372-05-01 UTC&quot; &quot;1253-01-01 UTC&quot; When a date component is missing, it’s just set to 1 11.2.2 Manipulating dates make_date has arguments year, month and day to allow you to specify a date from its individual components. make_date(year, month, day) Each argument can be vectors make_datetime adds hour, min and sec arguments to build a datetime. make_datetime(year, month, day, hour, min, sec) 11.2.2.1 Import daily weather data library(tidyverse) # Import CSV with read_csv() akl_daily_raw &lt;- read_csv(&quot;data/akl_weather_daily.csv&quot;) # Print akl_daily_raw print(akl_daily_raw) ## # A tibble: 3,661 × 7 ## date max_temp min_temp mean_temp mean_rh events cloud_cover ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2007-9-1 60 51 56 75 &lt;NA&gt; 4 ## 2 2007-9-2 60 53 56 82 Rain 4 ## 3 2007-9-3 57 51 54 78 &lt;NA&gt; 6 ## 4 2007-9-4 64 50 57 80 Rain 6 ## 5 2007-9-5 53 48 50 90 Rain 7 ## 6 2007-9-6 57 42 50 69 &lt;NA&gt; 1 ## 7 2007-9-7 59 41 50 77 &lt;NA&gt; 4 ## 8 2007-9-8 59 46 52 80 &lt;NA&gt; 5 ## 9 2007-9-9 55 50 52 88 Rain 7 ## 10 2007-9-10 59 50 54 82 Rain 4 ## # ℹ 3,651 more rows # Parse date akl_daily &lt;- akl_daily_raw %&gt;% mutate(date = ymd(date)) # Print akl_daily print(akl_daily) ## # A tibble: 3,661 × 7 ## date max_temp min_temp mean_temp mean_rh events cloud_cover ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2007-09-01 60 51 56 75 &lt;NA&gt; 4 ## 2 2007-09-02 60 53 56 82 Rain 4 ## 3 2007-09-03 57 51 54 78 &lt;NA&gt; 6 ## 4 2007-09-04 64 50 57 80 Rain 6 ## 5 2007-09-05 53 48 50 90 Rain 7 ## 6 2007-09-06 57 42 50 69 &lt;NA&gt; 1 ## 7 2007-09-07 59 41 50 77 &lt;NA&gt; 4 ## 8 2007-09-08 59 46 52 80 &lt;NA&gt; 5 ## 9 2007-09-09 55 50 52 88 Rain 7 ## 10 2007-09-10 59 50 54 82 Rain 4 ## # ℹ 3,651 more rows # Plot to check work ggplot(akl_daily, aes(x = date, y = max_temp)) + geom_line() ## Warning: Removed 1 row containing missing values ## (`geom_line()`). 11.2.2.2 Import hourly weather data The date information is spread over three columns year, month and mday, so you’ll need to use make_date() to combine them. time information is in a separate time column. One way to construct the datetimes is to paste the date and time together and then parse them. # Import &quot;akl_weather_hourly_2016.csv&quot; akl_hourly_raw &lt;- read_csv(&quot;data/akl_weather_hourly_2016.csv&quot;) # Print akl_hourly_raw print(akl_hourly_raw) ## # A tibble: 17,454 × 10 ## year month mday time temperature weather conditions events humidity ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;time&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2016 1 1 00:00 68 Clear Clear &lt;NA&gt; 68 ## 2 2016 1 1 00:30 68 Clear Clear &lt;NA&gt; 68 ## 3 2016 1 1 01:00 68 Clear Clear &lt;NA&gt; 73 ## 4 2016 1 1 01:30 68 Clear Clear &lt;NA&gt; 68 ## 5 2016 1 1 02:00 68 Clear Clear &lt;NA&gt; 68 ## 6 2016 1 1 02:30 68 Clear Clear &lt;NA&gt; 68 ## 7 2016 1 1 03:00 68 Clear Clear &lt;NA&gt; 68 ## 8 2016 1 1 03:30 68 Cloudy Partly Cloudy &lt;NA&gt; 68 ## 9 2016 1 1 04:00 68 Cloudy Scattered Clouds &lt;NA&gt; 68 ## 10 2016 1 1 04:30 66.2 Cloudy Partly Cloudy &lt;NA&gt; 73 ## # ℹ 17,444 more rows ## # ℹ 1 more variable: date_utc &lt;dttm&gt; # Use make_date() to combine year, month and mday akl_hourly &lt;- akl_hourly_raw %&gt;% mutate(date = make_date(year = year, month = month, day = mday)) # Parse datetime_string akl_hourly &lt;- akl_hourly %&gt;% mutate( datetime_string = paste(date, time, sep = &quot;T&quot;), datetime = ymd_hms(datetime_string) ) # Print date, time and datetime columns of akl_hourly akl_hourly %&gt;% select(date, time, datetime) ## # A tibble: 17,454 × 3 ## date time datetime ## &lt;date&gt; &lt;time&gt; &lt;dttm&gt; ## 1 2016-01-01 00:00 2016-01-01 00:00:00 ## 2 2016-01-01 00:30 2016-01-01 00:30:00 ## 3 2016-01-01 01:00 2016-01-01 01:00:00 ## 4 2016-01-01 01:30 2016-01-01 01:30:00 ## 5 2016-01-01 02:00 2016-01-01 02:00:00 ## 6 2016-01-01 02:30 2016-01-01 02:30:00 ## 7 2016-01-01 03:00 2016-01-01 03:00:00 ## 8 2016-01-01 03:30 2016-01-01 03:30:00 ## 9 2016-01-01 04:00 2016-01-01 04:00:00 ## 10 2016-01-01 04:30 2016-01-01 04:30:00 ## # ℹ 17,444 more rows # Plot to check work ggplot(akl_hourly, aes(x = datetime, y = temperature)) + geom_line() 11.2.3 Extracting parts of a datetime Function Extracts year() Year with century month() Month (1-12) day() Day of month (1-31) hour() Hour (0-23) min() Minute (0-59) second() Second (0-59) wday() Weekday (1-7) yday() Day of year a.k.a. Julian day (1-366) tz() Timezone # Example x &lt;- ymd(&quot;2013-02-23&quot;) # output 2013 year(x) # output 2 month(x) #output 23 day(x) Setting parts of a datetime # Example x &lt;- ymd(&quot;2013-02-23&quot;) # output &quot;2013-02-23&quot; x # Set new year, output &quot;2017-02-23&quot; year(x) &lt;- 2017x Other useful functions Function Extracts leap_year() In leap year ( 閏年, TRUE or FALSE ) am() In morning ( TRUE or FALSE ) pm() In afternoon ( TRUE or FALSE ) dst() During daylight savings ( TRUE or FALSE ) quarter() Quarter of year (1-4) semester() Half of year (1-2) 11.2.3.1 What can you extract release_time &lt;- releases$datetime # Examine the head() of release_time head(release_time) ## [1] &quot;1997-12-04 08:47:58 UTC&quot; &quot;1997-12-21 13:09:22 UTC&quot; ## [3] &quot;1998-01-10 00:31:55 UTC&quot; &quot;1998-03-14 19:25:55 UTC&quot; ## [5] &quot;1998-05-02 07:58:17 UTC&quot; &quot;1998-06-14 12:56:20 UTC&quot; # Examine the head() of the months of release_time head(month(release_time)) ## [1] 12 12 1 3 5 6 # Extract the month of releases month(release_time) %&gt;% table() ## . ## 1 2 3 4 5 6 7 8 9 10 11 12 ## 5 6 8 18 5 16 4 7 2 15 6 13 # Extract the year of releases year(release_time) %&gt;% table() ## . ## 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 ## 2 10 9 6 6 5 5 4 4 4 4 6 5 4 6 4 ## 2013 2014 2015 2016 2017 ## 4 4 5 5 3 # How often is the hour before 12 (noon)? mean(hour(release_time) &lt; 12) ## [1] 0.752381 # How often is the release in am? # Alternatively use am() to find out how often releases happen in the morning. mean(am(release_time)) ## [1] 0.752381 R versions have historically been released most in April, June, October and December, 1998 saw 10 releases and about 75% of releases happen in the morning (at least according to UTC). 11.2.3.2 Adding useful labels Both the month() and wday() (day of the week) functions have two additional arguments: label: Set label = TRUE to have the output labelled with month (or weekday) names abbr: Set abbr = FALSE for those names to be written in full rather than abbreviated. # Use wday() to tabulate release by day of the week # Do you know if 1 is Sunday or Monday? lubridate::wday(releases$datetime) %&gt;% table() ## . ## 1 2 3 4 5 6 7 ## 3 29 9 12 18 31 3 # Add label = TRUE to make table more readable lubridate::wday(releases$datetime, label = TRUE) %&gt;% table() ## . ## Sun Mon Tue Wed Thu Fri Sat ## 3 29 9 12 18 31 3 # Create column wday to hold labelled week days releases$wday &lt;- lubridate::wday(releases$datetime, label = TRUE) # Plot barchart of weekday by type of release ggplot(releases, aes(wday)) + geom_bar() + facet_wrap(~ type, ncol = 1, scale = &quot;free_y&quot;) Looks like not too many releases occur on the weekends, and there is quite a different weekday pattern between minor and patch releases. 11.2.3.3 Extracting for plotting Extracting components from a datetime is particularly useful when exploring data. Earlier in the chapter you imported daily data for weather in Auckland, and created a time series plot of ten years of daily maximum temperature. While that plot gives you a good overview of the whole ten years, it’s hard to see the annual pattern. In this exercise you’ll use components of the dates to help explore the pattern of maximum temperature over the year. head(akl_daily) ## # A tibble: 6 × 7 ## date max_temp min_temp mean_temp mean_rh events cloud_cover ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2007-09-01 60 51 56 75 &lt;NA&gt; 4 ## 2 2007-09-02 60 53 56 82 Rain 4 ## 3 2007-09-03 57 51 54 78 &lt;NA&gt; 6 ## 4 2007-09-04 64 50 57 80 Rain 6 ## 5 2007-09-05 53 48 50 90 Rain 7 ## 6 2007-09-06 57 42 50 69 &lt;NA&gt; 1 The first step is to create some new columns to hold the extracted pieces, then you’ll use them in a couple of plots. # Add columns for year, yday and month akl_daily &lt;- akl_daily %&gt;% mutate( year = lubridate::year(date), yday = lubridate::yday(date), month = lubridate::month(date, label = TRUE)) # Take a look of mutate outcome akl_daily %&gt;% select(date, year, yday, month) ## # A tibble: 3,661 × 4 ## date year yday month ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;ord&gt; ## 1 2007-09-01 2007 244 Sep ## 2 2007-09-02 2007 245 Sep ## 3 2007-09-03 2007 246 Sep ## 4 2007-09-04 2007 247 Sep ## 5 2007-09-05 2007 248 Sep ## 6 2007-09-06 2007 249 Sep ## 7 2007-09-07 2007 250 Sep ## 8 2007-09-08 2007 251 Sep ## 9 2007-09-09 2007 252 Sep ## 10 2007-09-10 2007 253 Sep ## # ℹ 3,651 more rows Each year is a line on this plot, with the x-axis running from Jan 1 to Dec 31. # Plot max_temp by yday for all years ggplot(akl_daily, aes(x = yday, y = max_temp)) + geom_line(aes(group = year), alpha = 0.5) ## Warning: Removed 1 row containing missing values ## (`geom_line()`). To take an alternate look, create a ridgeline plot(formerly known as a joyplot) with max_temp on the x-axis, month on the y-axis, using geom_density_ridges() from the ggridges package. library(ggridges) ## Warning: package &#39;ggridges&#39; was built under R version 4.3.2 # Examine distribution of max_temp by month ggplot(akl_daily, aes(x = max_temp, y = month, height = ..density..)) + geom_density_ridges(stat = &quot;density&quot;) ## Warning: Removed 10 rows containing non-finite values ## (`stat_density()`). Looks like Jan, Feb and Mar are great months to visit if you want warm temperatures. 11.2.3.4 Extracting for filter &amp; summarize Another reason to extract components is to help with filtering observations or creating summaries. For example, if you are only interested in observations made on weekdays (i.e. not on weekends) you could extract the weekdays then filter out weekends, e.g. wday(date) %in% 2:6. You’ll use the hourly data to calculate how many days in each month there was any rain during the day. str(akl_hourly) ## tibble [17,454 × 13] (S3: tbl_df/tbl/data.frame) ## $ year : num [1:17454] 2016 2016 2016 2016 2016 ... ## $ month : num [1:17454] 1 1 1 1 1 1 1 1 1 1 ... ## $ mday : num [1:17454] 1 1 1 1 1 1 1 1 1 1 ... ## $ time : &#39;hms&#39; num [1:17454] 00:00:00 00:30:00 01:00:00 01:30:00 ... ## ..- attr(*, &quot;units&quot;)= chr &quot;secs&quot; ## $ temperature : num [1:17454] 68 68 68 68 68 68 68 68 68 66.2 ... ## $ weather : chr [1:17454] &quot;Clear&quot; &quot;Clear&quot; &quot;Clear&quot; &quot;Clear&quot; ... ## $ conditions : chr [1:17454] &quot;Clear&quot; &quot;Clear&quot; &quot;Clear&quot; &quot;Clear&quot; ... ## $ events : chr [1:17454] NA NA NA NA ... ## $ humidity : num [1:17454] 68 68 73 68 68 68 68 68 68 73 ... ## $ date_utc : POSIXct[1:17454], format: &quot;2015-12-31 11:00:00&quot; &quot;2015-12-31 11:30:00&quot; ... ## $ date : Date[1:17454], format: &quot;2016-01-01&quot; &quot;2016-01-01&quot; ... ## $ datetime_string: chr [1:17454] &quot;2016-01-01T00:00:00&quot; &quot;2016-01-01T00:30:00&quot; &quot;2016-01-01T01:00:00&quot; &quot;2016-01-01T01:30:00&quot; ... ## $ datetime : POSIXct[1:17454], format: &quot;2016-01-01 00:00:00&quot; &quot;2016-01-01 00:30:00&quot; ... # Create new columns hour, month and rainy akl_hourly &lt;- akl_hourly %&gt;% mutate( hour = lubridate::hour(datetime), month = lubridate::month(datetime, label = TRUE), rainy = weather == &quot;Precipitation&quot; ) akl_hourly %&gt;% select(datetime, date, hour, month, rainy) ## # A tibble: 17,454 × 5 ## datetime date hour month rainy ## &lt;dttm&gt; &lt;date&gt; &lt;int&gt; &lt;ord&gt; &lt;lgl&gt; ## 1 2016-01-01 00:00:00 2016-01-01 0 Jan FALSE ## 2 2016-01-01 00:30:00 2016-01-01 0 Jan FALSE ## 3 2016-01-01 01:00:00 2016-01-01 1 Jan FALSE ## 4 2016-01-01 01:30:00 2016-01-01 1 Jan FALSE ## 5 2016-01-01 02:00:00 2016-01-01 2 Jan FALSE ## 6 2016-01-01 02:30:00 2016-01-01 2 Jan FALSE ## 7 2016-01-01 03:00:00 2016-01-01 3 Jan FALSE ## 8 2016-01-01 03:30:00 2016-01-01 3 Jan FALSE ## 9 2016-01-01 04:00:00 2016-01-01 4 Jan FALSE ## 10 2016-01-01 04:30:00 2016-01-01 4 Jan FALSE ## # ℹ 17,444 more rows Filter to just daytime observations. # Filter for hours between 8am and 10pm (inclusive) akl_day &lt;- akl_hourly %&gt;% filter(hour &gt;= 8, hour &lt;= 22) Group the observations first by month, then by date, and summarise by using any() on the rainy column. This results in one value per day. # Summarise for each date if there is any rain rainy_days &lt;- akl_day %&gt;% group_by(month, date) %&gt;% summarise( any_rain = any(rainy) ) ## `summarise()` has grouped output by &#39;month&#39;. ## You can override using the `.groups` ## argument. rainy_days ## # A tibble: 366 × 3 ## # Groups: month [12] ## month date any_rain ## &lt;ord&gt; &lt;date&gt; &lt;lgl&gt; ## 1 Jan 2016-01-01 TRUE ## 2 Jan 2016-01-02 TRUE ## 3 Jan 2016-01-03 TRUE ## 4 Jan 2016-01-04 FALSE ## 5 Jan 2016-01-05 FALSE ## 6 Jan 2016-01-06 FALSE ## 7 Jan 2016-01-07 TRUE ## 8 Jan 2016-01-08 TRUE ## 9 Jan 2016-01-09 FALSE ## 10 Jan 2016-01-10 FALSE ## # ℹ 356 more rows Summarise again by summing any_rain. This results in one value per month. # Summarise for each month, the number of days with rain rainy_days %&gt;% summarise( days_rainy = sum(any_rain) ) ## # A tibble: 12 × 2 ## month days_rainy ## &lt;ord&gt; &lt;int&gt; ## 1 Jan 15 ## 2 Feb 13 ## 3 Mar 12 ## 4 Apr 15 ## 5 May 21 ## 6 Jun 19 ## 7 Jul 22 ## 8 Aug 16 ## 9 Sep 25 ## 10 Oct 20 ## 11 Nov 19 ## 12 Dec 11 At least in 2016, it looks like you’ll still need to pack a raincoat if you visit in Jan, Feb or March. days_in_month get the number of days in the month of a date-time.(該月份有幾天) 11.2.4 Rounding datetimes Rounding in lubridate round_date() - round to nearest (四捨五入) ceiling_date() - round up (無條件進位) floor_date() - round down (無條件捨去) Possible values of unit : \"second\" , \"minute\" , \"hour\" , \"day\" , \"week\" , \"month\" , \"bimonth\" , \"quarter\" , \"halfyear\" , or \"year\" . Or multiples, e.g \"2 years\" , \"5 minutes\" 11.2.4.1 Practice rounding That last technique of subtracting a rounded datetime from an unrounded one is a really useful trick to remember. r_3_4_1 &lt;- ymd_hms(&quot;2016-05-03 07:13:28 UTC&quot;) # Round down to day floor_date(r_3_4_1, unit = &quot;day&quot;) ## [1] &quot;2016-05-03 UTC&quot; # Round to nearest 5 minutes round_date(r_3_4_1, unit = &quot;5 minutes&quot;) ## [1] &quot;2016-05-03 07:15:00 UTC&quot; # Round up to week ceiling_date(r_3_4_1, unit = &quot;week&quot;) ## [1] &quot;2016-05-08 UTC&quot; # Subtract r_3_4_1 rounded down to day r_3_4_1 - floor_date(r_3_4_1, unit = &quot;day&quot;) ## Time difference of 7.224444 hours 11.2.4.2 Rounding with the weather data The advantage of rounding over extracting is that it maintains the context of the unit. You’ll explore how many observations per hour there really are in the hourly Auckland weather data. # Create day_hour, datetime rounded down to hour akl_hourly &lt;- akl_hourly %&gt;% mutate( day_hour = floor_date(datetime, unit = &quot;hour&quot;) ) akl_hourly %&gt;% select(datetime, day_hour) ## # A tibble: 17,454 × 2 ## datetime day_hour ## &lt;dttm&gt; &lt;dttm&gt; ## 1 2016-01-01 00:00:00 2016-01-01 00:00:00 ## 2 2016-01-01 00:30:00 2016-01-01 00:00:00 ## 3 2016-01-01 01:00:00 2016-01-01 01:00:00 ## 4 2016-01-01 01:30:00 2016-01-01 01:00:00 ## 5 2016-01-01 02:00:00 2016-01-01 02:00:00 ## 6 2016-01-01 02:30:00 2016-01-01 02:00:00 ## 7 2016-01-01 03:00:00 2016-01-01 03:00:00 ## 8 2016-01-01 03:30:00 2016-01-01 03:00:00 ## 9 2016-01-01 04:00:00 2016-01-01 04:00:00 ## 10 2016-01-01 04:30:00 2016-01-01 04:00:00 ## # ℹ 17,444 more rows Count how many observations there are in each hour. What looks like the most common value? # Count observations per hour akl_hourly %&gt;% count(day_hour) ## # A tibble: 8,770 × 2 ## day_hour n ## &lt;dttm&gt; &lt;int&gt; ## 1 2016-01-01 00:00:00 2 ## 2 2016-01-01 01:00:00 2 ## 3 2016-01-01 02:00:00 2 ## 4 2016-01-01 03:00:00 2 ## 5 2016-01-01 04:00:00 2 ## 6 2016-01-01 05:00:00 2 ## 7 2016-01-01 06:00:00 2 ## 8 2016-01-01 07:00:00 2 ## 9 2016-01-01 08:00:00 2 ## 10 2016-01-01 09:00:00 2 ## # ℹ 8,760 more rows Filter for observations where n is not equal to 2. # Find day_hours with n != 2 akl_hourly %&gt;% count(day_hour) %&gt;% filter(n != 2) %&gt;% arrange(desc(n)) ## # A tibble: 92 × 2 ## day_hour n ## &lt;dttm&gt; &lt;int&gt; ## 1 2016-04-03 02:00:00 4 ## 2 2016-09-25 00:00:00 4 ## 3 2016-06-26 09:00:00 1 ## 4 2016-09-01 23:00:00 1 ## 5 2016-09-02 01:00:00 1 ## 6 2016-09-04 11:00:00 1 ## 7 2016-09-04 16:00:00 1 ## 8 2016-09-04 17:00:00 1 ## 9 2016-09-05 00:00:00 1 ## 10 2016-09-05 15:00:00 1 ## # ℹ 82 more rows 92 hours that don’t have two measurements. Interestingly there are four measurements on 2016-04-03 and 2016-09-25, they happen to be the days Daylight Saving starts and ends. 11.3 Arithmetic with Dates and Times 11.3.1 Taking differences of datetimes Subtraction of datetimes difftime(time1, time2) is the same as time1 - time2 takes an argument units which specifies the units for the difference. units = \"secs\" , \"mins\" , \"hours\" , \"days\" , or \"weeks\" lubridate functions today() and now(): return the current date and time in your system’s timezone. today() # [1] &quot;2023-11-10&quot; now() # [1] &quot;2023-11-10 10:53:08 CST&quot; # The date of landing and moment of step date_landing &lt;- mdy(&quot;July 20, 1969&quot;) moment_step &lt;- mdy_hms(&quot;July 20, 1969, 02:56:15&quot;, tz = &quot;UTC&quot;) # How many days since the first man on the moon? difftime(today(), date_landing, units = &quot;days&quot;) ## Time difference of 19903 days # How many seconds since the first man on the moon? difftime(now(), moment_step, units = &quot;secs&quot;) ## Time difference of 1719628984 secs How many seconds are in a day? There are 24 hours in a day, 60 minutes in an hour, and 60 seconds in a minute, so there should be 24*60*60 = 86400 seconds, right? — Not always! # Three dates mar_11 &lt;- ymd_hms(&quot;2017-03-11 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) mar_12 &lt;- ymd_hms(&quot;2017-03-12 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) mar_13 &lt;- ymd_hms(&quot;2017-03-13 12:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) # Difference between mar_13 and mar_12 in seconds # This should match your intuition. difftime(mar_13, mar_12, units = &quot;secs&quot;) ## Time difference of 86400 secs # Difference between mar_12 and mar_11 in seconds # Surprised? difftime(mar_12, mar_11, units = &quot;secs&quot;) ## Time difference of 82800 secs Why would a day only have 82800 seconds? At 2am on Mar 12th 2017, Daylight Savings started in the Pacific timezone. That means a whole hour of seconds gets skipped between noon on the 11th and noon on the 12th. 11.3.2 Time spans Time spans in lubridate Duration represent an exact number of seconds use case: only care about physical time (seconds elapsed) datetime + duration of one day = datetime + 86400 seconds Period represent human units use case: interested in human interpretaions of time datetime + period of one day = same time on the next date Interval represent a starting and ending point use case: when you have a start and end, and figure out how long a span is in human units Functions to create time spans 11.3.2.1 Adding or subtracting a time span Duration vs Period # Add a period of one week to mon_2pm mon_2pm &lt;- dmy_hm(&quot;27 Aug 2018 14:00&quot;) mon_2pm + weeks(1) ## [1] &quot;2018-09-03 14:00:00 UTC&quot; # Add a duration of 81 hours to tue_9am tue_9am &lt;- dmy_hm(&quot;28 Aug 2018 9:00&quot;) tue_9am + dhours(81) ## [1] &quot;2018-08-31 18:00:00 UTC&quot; # Subtract a period of five years from today() today() - years(5) ## [1] &quot;2019-01-16&quot; # Subtract a duration of five years from today() today() - dyears(5) ## [1] &quot;2019-01-15 18:00:00 UTC&quot; Why did subtracting a duration of five years from today, give a different answer to subtracting a period of five years? Periods know about leap years, and since five years ago includes at least one leap year, the period of five years is longer than the duration of 365*5 days. 11.3.2.2 Arithmetic with timespans You can add and subtract timespans to create different length timespans, and even multiply them by numbers. For example, to create a duration of three days and three hours: ddays(3) + dhours(3), 3*ddays(1) + 3*dhours(1) or 3*(ddays(1) + dhours(1)) There was an eclipse over North America on 2017-08-21 at 18:26:40. It’s possible to predict the next eclipse with similar geometry by calculating the time and date one Saros in the future. One Saros = 223 Synodic Months One Synodic Month = 29d 12h 44m 03s # Time of North American Eclipse 2017 eclipse_2017 &lt;- ymd_hms(&quot;2017-08-21 18:26:40&quot;) # Duration of 29 days, 12 hours, 44 mins and 3 secs synodic &lt;- ddays(29) + dhours(12) + dminutes(44) + dseconds(3) # 223 synodic months saros &lt;- 223 * synodic # Add saros to eclipse_2017 eclipse_2017 + saros ## [1] &quot;2035-09-02 02:09:49 UTC&quot; 11.3.2.3 Generate sequences of datetimes By combining addition and multiplication with sequences you can generate sequences of datetimes. 1:10 * days(1) ## [1] &quot;1d 0H 0M 0S&quot; &quot;2d 0H 0M 0S&quot; &quot;3d 0H 0M 0S&quot; &quot;4d 0H 0M 0S&quot; &quot;5d 0H 0M 0S&quot; ## [6] &quot;6d 0H 0M 0S&quot; &quot;7d 0H 0M 0S&quot; &quot;8d 0H 0M 0S&quot; &quot;9d 0H 0M 0S&quot; &quot;10d 0H 0M 0S&quot; Then by adding this sequence to a specific datetime, you can construct a sequence of datetimes from 1 day up to 10 days into the future. today() + 1:10 * days(1) ## [1] &quot;2024-01-17&quot; &quot;2024-01-18&quot; &quot;2024-01-19&quot; &quot;2024-01-20&quot; &quot;2024-01-21&quot; ## [6] &quot;2024-01-22&quot; &quot;2024-01-23&quot; &quot;2024-01-24&quot; &quot;2024-01-25&quot; &quot;2024-01-26&quot; You had a meeting this morning at 8am and you’d like to have that meeting at the same time and day every two weeks for a year. Generate the meeting times. # Add a period of 8 hours to today today_8am &lt;- today() + hours(8); today_8am ## [1] &quot;2024-01-16 08:00:00 UTC&quot; # Sequence of two weeks from 1 to 26 every_two_weeks &lt;- 1:26 * weeks(2); every_two_weeks ## [1] &quot;14d 0H 0M 0S&quot; &quot;28d 0H 0M 0S&quot; &quot;42d 0H 0M 0S&quot; &quot;56d 0H 0M 0S&quot; ## [5] &quot;70d 0H 0M 0S&quot; &quot;84d 0H 0M 0S&quot; &quot;98d 0H 0M 0S&quot; &quot;112d 0H 0M 0S&quot; ## [9] &quot;126d 0H 0M 0S&quot; &quot;140d 0H 0M 0S&quot; &quot;154d 0H 0M 0S&quot; &quot;168d 0H 0M 0S&quot; ## [13] &quot;182d 0H 0M 0S&quot; &quot;196d 0H 0M 0S&quot; &quot;210d 0H 0M 0S&quot; &quot;224d 0H 0M 0S&quot; ## [17] &quot;238d 0H 0M 0S&quot; &quot;252d 0H 0M 0S&quot; &quot;266d 0H 0M 0S&quot; &quot;280d 0H 0M 0S&quot; ## [21] &quot;294d 0H 0M 0S&quot; &quot;308d 0H 0M 0S&quot; &quot;322d 0H 0M 0S&quot; &quot;336d 0H 0M 0S&quot; ## [25] &quot;350d 0H 0M 0S&quot; &quot;364d 0H 0M 0S&quot; # Create datetime for every two weeks for a year today_8am + every_two_weeks ## [1] &quot;2024-01-30 08:00:00 UTC&quot; &quot;2024-02-13 08:00:00 UTC&quot; ## [3] &quot;2024-02-27 08:00:00 UTC&quot; &quot;2024-03-12 08:00:00 UTC&quot; ## [5] &quot;2024-03-26 08:00:00 UTC&quot; &quot;2024-04-09 08:00:00 UTC&quot; ## [7] &quot;2024-04-23 08:00:00 UTC&quot; &quot;2024-05-07 08:00:00 UTC&quot; ## [9] &quot;2024-05-21 08:00:00 UTC&quot; &quot;2024-06-04 08:00:00 UTC&quot; ## [11] &quot;2024-06-18 08:00:00 UTC&quot; &quot;2024-07-02 08:00:00 UTC&quot; ## [13] &quot;2024-07-16 08:00:00 UTC&quot; &quot;2024-07-30 08:00:00 UTC&quot; ## [15] &quot;2024-08-13 08:00:00 UTC&quot; &quot;2024-08-27 08:00:00 UTC&quot; ## [17] &quot;2024-09-10 08:00:00 UTC&quot; &quot;2024-09-24 08:00:00 UTC&quot; ## [19] &quot;2024-10-08 08:00:00 UTC&quot; &quot;2024-10-22 08:00:00 UTC&quot; ## [21] &quot;2024-11-05 08:00:00 UTC&quot; &quot;2024-11-19 08:00:00 UTC&quot; ## [23] &quot;2024-12-03 08:00:00 UTC&quot; &quot;2024-12-17 08:00:00 UTC&quot; ## [25] &quot;2024-12-31 08:00:00 UTC&quot; &quot;2025-01-14 08:00:00 UTC&quot; 11.3.2.4 Tricky thing about months In general lubridate returns the same day of the month in the next month, but since the 31st of February, April, June, September, November don’t exist, lubridate returns NAs. jan_31 &lt;- as.Date(&quot;2023-01-31&quot;) # A sequence of 1 to 12 periods of 1 month # output: &quot;1m 0d 0H 0M 0S&quot;, &quot;2m 0d 0H 0M 0S&quot;...&quot;12m 0d 0H 0M 0S&quot; month_seq &lt;- 1:12 * months(1) # Add 1 to 12 months to jan_31 jan_31 + month_seq ## [1] NA &quot;2023-03-31&quot; NA &quot;2023-05-31&quot; NA ## [6] &quot;2023-07-31&quot; &quot;2023-08-31&quot; NA &quot;2023-10-31&quot; NA ## [11] &quot;2023-12-31&quot; &quot;2024-01-31&quot; Alternative addition and subtraction operators: %m+% &amp; %m-% Rather than returning an NA for a non-existent date, they roll back to the last existing date. # Replace + with %m+% jan_31 %m+% month_seq ## [1] &quot;2023-02-28&quot; &quot;2023-03-31&quot; &quot;2023-04-30&quot; &quot;2023-05-31&quot; &quot;2023-06-30&quot; ## [6] &quot;2023-07-31&quot; &quot;2023-08-31&quot; &quot;2023-09-30&quot; &quot;2023-10-31&quot; &quot;2023-11-30&quot; ## [11] &quot;2023-12-31&quot; &quot;2024-01-31&quot; # Replace + with %m-% jan_31 %m-% month_seq ## [1] &quot;2022-12-31&quot; &quot;2022-11-30&quot; &quot;2022-10-31&quot; &quot;2022-09-30&quot; &quot;2022-08-31&quot; ## [6] &quot;2022-07-31&quot; &quot;2022-06-30&quot; &quot;2022-05-31&quot; &quot;2022-04-30&quot; &quot;2022-03-31&quot; ## [11] &quot;2022-02-28&quot; &quot;2022-01-31&quot; Use add_with_rollback(x, months(), roll_to_first = TRUE) to rollback to the first day of the month instead of the last day of the previous month. add_with_rollback(jan_31, months(1), roll_to_first = TRUE) ## [1] &quot;2023-03-01&quot; 11.3.3 Intervals Rather than representing an amount of time like a period or duration, interval have a specific start and end datetime. Creating intervals datetime1 %--% datetime2 interval(datetime1, datetime2) Operating on an interval int_start() : extract the start of the interval. int_end() : extract the end of the interval. int_length() : finds the length of the interval in seconds. as.period() as.duration() Comparing intervals date %within% interval : return TRUE if the datetime is inside the interval, vice versa. int_overlaps(int_1, int_2) : return TRUE if there are any times that are inside both intervals. 11.3.3.1 Examining intervals Practice by exploring the reigns of kings and queens of Britain (and its historical dominions). library(readxl) # Load dataset monarchs &lt;- read_excel(&quot;data/monarchs.xlsx&quot;) str(monarchs) ## tibble [131 × 4] (S3: tbl_df/tbl/data.frame) ## $ name : chr [1:131] &quot;Elizabeth II&quot; &quot;Victoria&quot; &quot;George V&quot; &quot;George III&quot; ... ## $ from : chr [1:131] &quot;1952-02-06 00:00:00&quot; &quot;1837-06-20 00:00:00&quot; &quot;1910-05-06 00:00:00&quot; &quot;1801-01-01 00:00:00&quot; ... ## $ to : chr [1:131] &quot;2022-07-04 00:00:00&quot; &quot;1901-01-22 00:00:00&quot; &quot;1936-01-20 00:00:00&quot; &quot;1820-01-29 00:00:00&quot; ... ## $ dominion: chr [1:131] &quot;United Ki&quot; &quot;United Ki&quot; &quot;United Ki&quot; &quot;United Ki&quot; ... # Convert data type monarchs &lt;- monarchs %&gt;% mutate(from = as.POSIXct(from, format = &quot;%Y-%m-%d %H:%M:%S&quot;), to = as.POSIXct(to, format = &quot;%Y-%m-%d %H:%M:%S&quot;)) monarchs ## # A tibble: 131 × 4 ## name from to dominion ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 Elizabeth II 1952-02-06 00:00:00 2022-07-04 00:00:00 United Ki ## 2 Victoria 1837-06-20 00:00:00 1901-01-22 00:00:00 United Ki ## 3 George V 1910-05-06 00:00:00 1936-01-20 00:00:00 United Ki ## 4 George III 1801-01-01 00:00:00 1820-01-29 00:00:00 United Ki ## 5 George VI 1936-12-11 00:00:00 1952-02-06 00:00:00 United Ki ## 6 George IV 1820-01-29 00:00:00 1830-06-26 00:00:00 United Ki ## 7 Edward VII 1901-01-22 00:00:00 1910-05-06 00:00:00 United Ki ## 8 William IV 1830-06-26 00:00:00 1837-06-20 00:00:00 United Ki ## 9 Edward VIII 1936-01-20 00:00:00 1936-12-11 00:00:00 United Ki ## 10 George III(also United ~ 1760-10-25 00:00:00 1801-01-01 00:00:00 Great Bri ## # ℹ 121 more rows # Create an interval for reign monarchs &lt;- monarchs %&gt;% mutate(reign = from %--% to) # Find the length of reign, and arrange monarchs %&gt;% mutate(length = int_length(reign)) %&gt;% arrange(desc(length)) %&gt;% select(name, length, dominion) ## # A tibble: 131 × 3 ## name length dominion ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Elizabeth II 2221862400 United Ki ## 2 Victoria 2006726760 United Ki ## 3 James VI 1820102400 Scotland ## 4 Gruffudd ap Cynan 1767139200 Gwynedd ## 5 Edward III 1590624000 England ## 6 William I 1545868800 Scotland ## 7 Llywelyn the Great 1428796800 Gwynedd ## 8 Elizabeth I 1399507200 England ## 9 Constantine II 1356912000 Scotland ## 10 David II 1316304000 Scotland ## # ℹ 121 more rows 11.3.3.2 Comparing intervals and datetimes A common task with intervals is to ask if a certain time is inside the interval or whether it overlaps with another interval. The operator %within% tests if the datetime (or interval) on the left hand side is within the interval of the right hand side. int_overlaps() performs a similar test, but will return true if two intervals overlap at all. halleys is a data set describing appearances of Halley’s comet. Practice to find out which monarchs saw Halley’s comet around 1066. halleys &lt;- read_excel(&quot;data/halleys.xlsx&quot;) str(halleys) ## tibble [27 × 6] (S3: tbl_df/tbl/data.frame) ## $ designation : chr [1:27] &quot;1P/66 B1, 66&quot; &quot;1P/141 F1, 141&quot; &quot;1P/218 H1, 218&quot; &quot;1P/295 J1, 295&quot; ... ## $ year : num [1:27] 66 141 218 295 374 451 530 607 684 760 ... ## $ perihelion_date: chr [1:27] &quot;1966-01-26&quot; &quot;141-03-25&quot; &quot;218-04-06&quot; &quot;295-04-07&quot; ... ## $ start_date : chr [1:27] &quot;1966-01-26&quot; &quot;141-03-22&quot; &quot;218-04-06&quot; &quot;295-04-07&quot; ... ## $ end_date : chr [1:27] &quot;1966-01-26&quot; &quot;141-03-25&quot; &quot;218-05-17&quot; &quot;295-04-20&quot; ... ## $ distance : chr [1:27] &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; &quot;NA&quot; ... perihelion_date is the date the Comet is closest to the Sun. start_date and end_date are the range of dates the comet is visible from Earth. # convert data type halleys &lt;- halleys %&gt;% mutate(perihelion_date = as.Date(perihelion_date, format = &quot;%Y-%m-%d&quot;), start_date = as.Date(start_date, format = &quot;%Y-%m-%d&quot;), end_date = as.Date(end_date, format = &quot;%Y-%m-%d&quot;)) halleys ## # A tibble: 27 × 6 ## designation year perihelion_date start_date end_date distance ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; ## 1 1P/66 B1, 66 66 1966-01-26 1966-01-26 1966-01-26 NA ## 2 1P/141 F1, 141 141 0141-03-25 0141-03-22 0141-03-25 NA ## 3 1P/218 H1, 218 218 0218-04-06 0218-04-06 0218-05-17 NA ## 4 1P/295 J1, 295 295 0295-04-07 0295-04-07 0295-04-20 NA ## 5 1P/374 E1, 374 374 0374-02-13 0374-02-13 0374-02-16 0.09 AU ## 6 1P/451 L1, 451 451 0451-07-03 0451-06-28 0451-07-03 NA ## 7 1P/530 Q1, 530 530 0530-11-15 0530-09-27 0530-11-15 NA ## 8 1P/607 H1, 607 607 0607-03-26 0607-03-15 0607-03-26 0.09 AU ## 9 1P/684 R1, 684 684 0684-11-26 0684-10-02 0684-11-26 NA ## 10 1P/760 K1, 760 760 0760-06-10 0760-05-20 0760-06-10 NA ## # ℹ 17 more rows Work with one appearance. # New column for interval from start to end date halleys &lt;- halleys %&gt;% mutate(visible = start_date %--% end_date) # The visitation of 1066 halleys_1066 &lt;- halleys[14, ]; halleys_1066 ## # A tibble: 1 × 7 ## designation year perihelion_date start_date end_date distance ## &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;date&gt; &lt;date&gt; &lt;chr&gt; ## 1 1P/1066 G1, 1066 1066 1066-03-25 1066-01-01 1066-03-25 0.10 AU ## # ℹ 1 more variable: visible &lt;Interval&gt; # Monarchs in power on perihelion date monarchs %&gt;% filter(halleys_1066$perihelion_date %within% reign) %&gt;% select(name, from, to, dominion) ## # A tibble: 2 × 4 ## name from to dominion ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 Harold II 1066-01-05 00:00:00 1066-10-14 00:00:00 England ## 2 Malcolm III 1058-03-17 00:00:00 1093-11-13 00:00:00 Scotland # Monarchs whose reign overlaps visible time monarchs %&gt;% filter(int_overlaps(reign, halleys_1066$visible)) %&gt;% select(name, from, to, dominion) ## # A tibble: 3 × 4 ## name from to dominion ## &lt;chr&gt; &lt;dttm&gt; &lt;dttm&gt; &lt;chr&gt; ## 1 Edward the Confessor 1042-06-08 00:00:00 1066-01-05 00:00:00 England ## 2 Harold II 1066-01-05 00:00:00 1066-10-14 00:00:00 England ## 3 Malcolm III 1058-03-17 00:00:00 1093-11-13 00:00:00 Scotland Looks like the Kings of England Edward the Confessor and Harold II would have been able to see the comet. 11.3.3.3 Converting to durations and periods Intervals are the most specific way to represent a span of time since they retain information about the exact start and end moments. They can be converted to periods (as.period()) and durations (as.duration) exactly: Calculate both the exact number of seconds elapsed between the start and end date, as well as the perceived change in clock time. Try to get better representations of the length of the monarchs reigns. # New columns for duration and period monarchs &lt;- monarchs %&gt;% mutate( duration = as.duration(reign), period = as.period(reign)) # Examine results monarchs %&gt;% select(name, duration, period) ## # A tibble: 131 × 3 ## name duration period ## &lt;chr&gt; &lt;Duration&gt; &lt;Period&gt; ## 1 Elizabeth II 2221862400s (~70.41 years) 70y 4m 28d 0H 0M 0S ## 2 Victoria 2006726760s (~63.59 years) 63y 7m 2d 0H 0M 0S ## 3 George V 811296000s (~25.71 years) 25y 8m 14d 0H 0M 0S ## 4 George III 601948800s (~19.07 years) 19y 0m 28d 0H 0M 0S ## 5 George VI 478224000s (~15.15 years) 15y 1m 26d 0H 0M 0S ## 6 George IV 328406400s (~10.41 years) 10y 4m 28d 0H 0M 0S ## 7 Edward VII 292982400s (~9.28 years) 9y 3m 14d 0H 0M 0S ## 8 William IV 220406400s (~6.98 years) 6y 11m 25d 0H 0M 0S ## 9 Edward VIII 28166400s (~46.57 weeks) 10m 21d 0H 0M 0S ## 10 George III(also United ~ 1268092800s (~40.18 years) 40y 2m 7d 0H 0M 0S ## # ℹ 121 more rows 11.4 Problems in practice 11.4.1 Time zones IANA Timezones OlsonNames() ## [1] &quot;Africa/Abidjan&quot; &quot;Africa/Accra&quot; ## [3] &quot;Africa/Addis_Ababa&quot; &quot;Africa/Algiers&quot; ## [5] &quot;Africa/Asmara&quot; &quot;Africa/Asmera&quot; ## [7] &quot;Africa/Bamako&quot; &quot;Africa/Bangui&quot; ## [9] &quot;Africa/Banjul&quot; &quot;Africa/Bissau&quot; ## [11] &quot;Africa/Blantyre&quot; &quot;Africa/Brazzaville&quot; ## [13] &quot;Africa/Bujumbura&quot; &quot;Africa/Cairo&quot; ## [15] &quot;Africa/Casablanca&quot; &quot;Africa/Ceuta&quot; ## [17] &quot;Africa/Conakry&quot; &quot;Africa/Dakar&quot; ## [19] &quot;Africa/Dar_es_Salaam&quot; &quot;Africa/Djibouti&quot; ## [21] &quot;Africa/Douala&quot; &quot;Africa/El_Aaiun&quot; ## [23] &quot;Africa/Freetown&quot; &quot;Africa/Gaborone&quot; ## [25] &quot;Africa/Harare&quot; &quot;Africa/Johannesburg&quot; ## [27] &quot;Africa/Juba&quot; &quot;Africa/Kampala&quot; ## [29] &quot;Africa/Khartoum&quot; &quot;Africa/Kigali&quot; ## [31] &quot;Africa/Kinshasa&quot; &quot;Africa/Lagos&quot; ## [33] &quot;Africa/Libreville&quot; &quot;Africa/Lome&quot; ## [35] &quot;Africa/Luanda&quot; &quot;Africa/Lubumbashi&quot; ## [37] &quot;Africa/Lusaka&quot; &quot;Africa/Malabo&quot; ## [39] &quot;Africa/Maputo&quot; &quot;Africa/Maseru&quot; ## [41] &quot;Africa/Mbabane&quot; &quot;Africa/Mogadishu&quot; ## [43] &quot;Africa/Monrovia&quot; &quot;Africa/Nairobi&quot; ## [45] &quot;Africa/Ndjamena&quot; &quot;Africa/Niamey&quot; ## [47] &quot;Africa/Nouakchott&quot; &quot;Africa/Ouagadougou&quot; ## [49] &quot;Africa/Porto-Novo&quot; &quot;Africa/Sao_Tome&quot; ## [51] &quot;Africa/Timbuktu&quot; &quot;Africa/Tripoli&quot; ## [53] &quot;Africa/Tunis&quot; &quot;Africa/Windhoek&quot; ## [55] &quot;America/Adak&quot; &quot;America/Anchorage&quot; ## [57] &quot;America/Anguilla&quot; &quot;America/Antigua&quot; ## [59] &quot;America/Araguaina&quot; &quot;America/Argentina/Buenos_Aires&quot; ## [61] &quot;America/Argentina/Catamarca&quot; &quot;America/Argentina/ComodRivadavia&quot; ## [63] &quot;America/Argentina/Cordoba&quot; &quot;America/Argentina/Jujuy&quot; ## [65] &quot;America/Argentina/La_Rioja&quot; &quot;America/Argentina/Mendoza&quot; ## [67] &quot;America/Argentina/Rio_Gallegos&quot; &quot;America/Argentina/Salta&quot; ## [69] &quot;America/Argentina/San_Juan&quot; &quot;America/Argentina/San_Luis&quot; ## [71] &quot;America/Argentina/Tucuman&quot; &quot;America/Argentina/Ushuaia&quot; ## [73] &quot;America/Aruba&quot; &quot;America/Asuncion&quot; ## [75] &quot;America/Atikokan&quot; &quot;America/Atka&quot; ## [77] &quot;America/Bahia&quot; &quot;America/Bahia_Banderas&quot; ## [79] &quot;America/Barbados&quot; &quot;America/Belem&quot; ## [81] &quot;America/Belize&quot; &quot;America/Blanc-Sablon&quot; ## [83] &quot;America/Boa_Vista&quot; &quot;America/Bogota&quot; ## [85] &quot;America/Boise&quot; &quot;America/Buenos_Aires&quot; ## [87] &quot;America/Cambridge_Bay&quot; &quot;America/Campo_Grande&quot; ## [89] &quot;America/Cancun&quot; &quot;America/Caracas&quot; ## [91] &quot;America/Catamarca&quot; &quot;America/Cayenne&quot; ## [93] &quot;America/Cayman&quot; &quot;America/Chicago&quot; ## [95] &quot;America/Chihuahua&quot; &quot;America/Ciudad_Juarez&quot; ## [97] &quot;America/Coral_Harbour&quot; &quot;America/Cordoba&quot; ## [99] &quot;America/Costa_Rica&quot; &quot;America/Creston&quot; ## [101] &quot;America/Cuiaba&quot; &quot;America/Curacao&quot; ## [103] &quot;America/Danmarkshavn&quot; &quot;America/Dawson&quot; ## [105] &quot;America/Dawson_Creek&quot; &quot;America/Denver&quot; ## [107] &quot;America/Detroit&quot; &quot;America/Dominica&quot; ## [109] &quot;America/Edmonton&quot; &quot;America/Eirunepe&quot; ## [111] &quot;America/El_Salvador&quot; &quot;America/Ensenada&quot; ## [113] &quot;America/Fort_Nelson&quot; &quot;America/Fort_Wayne&quot; ## [115] &quot;America/Fortaleza&quot; &quot;America/Glace_Bay&quot; ## [117] &quot;America/Godthab&quot; &quot;America/Goose_Bay&quot; ## [119] &quot;America/Grand_Turk&quot; &quot;America/Grenada&quot; ## [121] &quot;America/Guadeloupe&quot; &quot;America/Guatemala&quot; ## [123] &quot;America/Guayaquil&quot; &quot;America/Guyana&quot; ## [125] &quot;America/Halifax&quot; &quot;America/Havana&quot; ## [127] &quot;America/Hermosillo&quot; &quot;America/Indiana/Indianapolis&quot; ## [129] &quot;America/Indiana/Knox&quot; &quot;America/Indiana/Marengo&quot; ## [131] &quot;America/Indiana/Petersburg&quot; &quot;America/Indiana/Tell_City&quot; ## [133] &quot;America/Indiana/Vevay&quot; &quot;America/Indiana/Vincennes&quot; ## [135] &quot;America/Indiana/Winamac&quot; &quot;America/Indianapolis&quot; ## [137] &quot;America/Inuvik&quot; &quot;America/Iqaluit&quot; ## [139] &quot;America/Jamaica&quot; &quot;America/Jujuy&quot; ## [141] &quot;America/Juneau&quot; &quot;America/Kentucky/Louisville&quot; ## [143] &quot;America/Kentucky/Monticello&quot; &quot;America/Knox_IN&quot; ## [145] &quot;America/Kralendijk&quot; &quot;America/La_Paz&quot; ## [147] &quot;America/Lima&quot; &quot;America/Los_Angeles&quot; ## [149] &quot;America/Louisville&quot; &quot;America/Lower_Princes&quot; ## [151] &quot;America/Maceio&quot; &quot;America/Managua&quot; ## [153] &quot;America/Manaus&quot; &quot;America/Marigot&quot; ## [155] &quot;America/Martinique&quot; &quot;America/Matamoros&quot; ## [157] &quot;America/Mazatlan&quot; &quot;America/Mendoza&quot; ## [159] &quot;America/Menominee&quot; &quot;America/Merida&quot; ## [161] &quot;America/Metlakatla&quot; &quot;America/Mexico_City&quot; ## [163] &quot;America/Miquelon&quot; &quot;America/Moncton&quot; ## [165] &quot;America/Monterrey&quot; &quot;America/Montevideo&quot; ## [167] &quot;America/Montreal&quot; &quot;America/Montserrat&quot; ## [169] &quot;America/Nassau&quot; &quot;America/New_York&quot; ## [171] &quot;America/Nipigon&quot; &quot;America/Nome&quot; ## [173] &quot;America/Noronha&quot; &quot;America/North_Dakota/Beulah&quot; ## [175] &quot;America/North_Dakota/Center&quot; &quot;America/North_Dakota/New_Salem&quot; ## [177] &quot;America/Nuuk&quot; &quot;America/Ojinaga&quot; ## [179] &quot;America/Panama&quot; &quot;America/Pangnirtung&quot; ## [181] &quot;America/Paramaribo&quot; &quot;America/Phoenix&quot; ## [183] &quot;America/Port-au-Prince&quot; &quot;America/Port_of_Spain&quot; ## [185] &quot;America/Porto_Acre&quot; &quot;America/Porto_Velho&quot; ## [187] &quot;America/Puerto_Rico&quot; &quot;America/Punta_Arenas&quot; ## [189] &quot;America/Rainy_River&quot; &quot;America/Rankin_Inlet&quot; ## [191] &quot;America/Recife&quot; &quot;America/Regina&quot; ## [193] &quot;America/Resolute&quot; &quot;America/Rio_Branco&quot; ## [195] &quot;America/Rosario&quot; &quot;America/Santa_Isabel&quot; ## [197] &quot;America/Santarem&quot; &quot;America/Santiago&quot; ## [199] &quot;America/Santo_Domingo&quot; &quot;America/Sao_Paulo&quot; ## [201] &quot;America/Scoresbysund&quot; &quot;America/Shiprock&quot; ## [203] &quot;America/Sitka&quot; &quot;America/St_Barthelemy&quot; ## [205] &quot;America/St_Johns&quot; &quot;America/St_Kitts&quot; ## [207] &quot;America/St_Lucia&quot; &quot;America/St_Thomas&quot; ## [209] &quot;America/St_Vincent&quot; &quot;America/Swift_Current&quot; ## [211] &quot;America/Tegucigalpa&quot; &quot;America/Thule&quot; ## [213] &quot;America/Thunder_Bay&quot; &quot;America/Tijuana&quot; ## [215] &quot;America/Toronto&quot; &quot;America/Tortola&quot; ## [217] &quot;America/Vancouver&quot; &quot;America/Virgin&quot; ## [219] &quot;America/Whitehorse&quot; &quot;America/Winnipeg&quot; ## [221] &quot;America/Yakutat&quot; &quot;America/Yellowknife&quot; ## [223] &quot;Antarctica/Casey&quot; &quot;Antarctica/Davis&quot; ## [225] &quot;Antarctica/DumontDUrville&quot; &quot;Antarctica/Macquarie&quot; ## [227] &quot;Antarctica/Mawson&quot; &quot;Antarctica/McMurdo&quot; ## [229] &quot;Antarctica/Palmer&quot; &quot;Antarctica/Rothera&quot; ## [231] &quot;Antarctica/South_Pole&quot; &quot;Antarctica/Syowa&quot; ## [233] &quot;Antarctica/Troll&quot; &quot;Antarctica/Vostok&quot; ## [235] &quot;Arctic/Longyearbyen&quot; &quot;Asia/Aden&quot; ## [237] &quot;Asia/Almaty&quot; &quot;Asia/Amman&quot; ## [239] &quot;Asia/Anadyr&quot; &quot;Asia/Aqtau&quot; ## [241] &quot;Asia/Aqtobe&quot; &quot;Asia/Ashgabat&quot; ## [243] &quot;Asia/Ashkhabad&quot; &quot;Asia/Atyrau&quot; ## [245] &quot;Asia/Baghdad&quot; &quot;Asia/Bahrain&quot; ## [247] &quot;Asia/Baku&quot; &quot;Asia/Bangkok&quot; ## [249] &quot;Asia/Barnaul&quot; &quot;Asia/Beirut&quot; ## [251] &quot;Asia/Bishkek&quot; &quot;Asia/Brunei&quot; ## [253] &quot;Asia/Calcutta&quot; &quot;Asia/Chita&quot; ## [255] &quot;Asia/Choibalsan&quot; &quot;Asia/Chongqing&quot; ## [257] &quot;Asia/Chungking&quot; &quot;Asia/Colombo&quot; ## [259] &quot;Asia/Dacca&quot; &quot;Asia/Damascus&quot; ## [261] &quot;Asia/Dhaka&quot; &quot;Asia/Dili&quot; ## [263] &quot;Asia/Dubai&quot; &quot;Asia/Dushanbe&quot; ## [265] &quot;Asia/Famagusta&quot; &quot;Asia/Gaza&quot; ## [267] &quot;Asia/Harbin&quot; &quot;Asia/Hebron&quot; ## [269] &quot;Asia/Ho_Chi_Minh&quot; &quot;Asia/Hong_Kong&quot; ## [271] &quot;Asia/Hovd&quot; &quot;Asia/Irkutsk&quot; ## [273] &quot;Asia/Istanbul&quot; &quot;Asia/Jakarta&quot; ## [275] &quot;Asia/Jayapura&quot; &quot;Asia/Jerusalem&quot; ## [277] &quot;Asia/Kabul&quot; &quot;Asia/Kamchatka&quot; ## [279] &quot;Asia/Karachi&quot; &quot;Asia/Kashgar&quot; ## [281] &quot;Asia/Kathmandu&quot; &quot;Asia/Katmandu&quot; ## [283] &quot;Asia/Khandyga&quot; &quot;Asia/Kolkata&quot; ## [285] &quot;Asia/Krasnoyarsk&quot; &quot;Asia/Kuala_Lumpur&quot; ## [287] &quot;Asia/Kuching&quot; &quot;Asia/Kuwait&quot; ## [289] &quot;Asia/Macao&quot; &quot;Asia/Macau&quot; ## [291] &quot;Asia/Magadan&quot; &quot;Asia/Makassar&quot; ## [293] &quot;Asia/Manila&quot; &quot;Asia/Muscat&quot; ## [295] &quot;Asia/Nicosia&quot; &quot;Asia/Novokuznetsk&quot; ## [297] &quot;Asia/Novosibirsk&quot; &quot;Asia/Omsk&quot; ## [299] &quot;Asia/Oral&quot; &quot;Asia/Phnom_Penh&quot; ## [301] &quot;Asia/Pontianak&quot; &quot;Asia/Pyongyang&quot; ## [303] &quot;Asia/Qatar&quot; &quot;Asia/Qostanay&quot; ## [305] &quot;Asia/Qyzylorda&quot; &quot;Asia/Rangoon&quot; ## [307] &quot;Asia/Riyadh&quot; &quot;Asia/Saigon&quot; ## [309] &quot;Asia/Sakhalin&quot; &quot;Asia/Samarkand&quot; ## [311] &quot;Asia/Seoul&quot; &quot;Asia/Shanghai&quot; ## [313] &quot;Asia/Singapore&quot; &quot;Asia/Srednekolymsk&quot; ## [315] &quot;Asia/Taipei&quot; &quot;Asia/Tashkent&quot; ## [317] &quot;Asia/Tbilisi&quot; &quot;Asia/Tehran&quot; ## [319] &quot;Asia/Tel_Aviv&quot; &quot;Asia/Thimbu&quot; ## [321] &quot;Asia/Thimphu&quot; &quot;Asia/Tokyo&quot; ## [323] &quot;Asia/Tomsk&quot; &quot;Asia/Ujung_Pandang&quot; ## [325] &quot;Asia/Ulaanbaatar&quot; &quot;Asia/Ulan_Bator&quot; ## [327] &quot;Asia/Urumqi&quot; &quot;Asia/Ust-Nera&quot; ## [329] &quot;Asia/Vientiane&quot; &quot;Asia/Vladivostok&quot; ## [331] &quot;Asia/Yakutsk&quot; &quot;Asia/Yangon&quot; ## [333] &quot;Asia/Yekaterinburg&quot; &quot;Asia/Yerevan&quot; ## [335] &quot;Atlantic/Azores&quot; &quot;Atlantic/Bermuda&quot; ## [337] &quot;Atlantic/Canary&quot; &quot;Atlantic/Cape_Verde&quot; ## [339] &quot;Atlantic/Faeroe&quot; &quot;Atlantic/Faroe&quot; ## [341] &quot;Atlantic/Jan_Mayen&quot; &quot;Atlantic/Madeira&quot; ## [343] &quot;Atlantic/Reykjavik&quot; &quot;Atlantic/South_Georgia&quot; ## [345] &quot;Atlantic/St_Helena&quot; &quot;Atlantic/Stanley&quot; ## [347] &quot;Australia/ACT&quot; &quot;Australia/Adelaide&quot; ## [349] &quot;Australia/Brisbane&quot; &quot;Australia/Broken_Hill&quot; ## [351] &quot;Australia/Canberra&quot; &quot;Australia/Currie&quot; ## [353] &quot;Australia/Darwin&quot; &quot;Australia/Eucla&quot; ## [355] &quot;Australia/Hobart&quot; &quot;Australia/LHI&quot; ## [357] &quot;Australia/Lindeman&quot; &quot;Australia/Lord_Howe&quot; ## [359] &quot;Australia/Melbourne&quot; &quot;Australia/North&quot; ## [361] &quot;Australia/NSW&quot; &quot;Australia/Perth&quot; ## [363] &quot;Australia/Queensland&quot; &quot;Australia/South&quot; ## [365] &quot;Australia/Sydney&quot; &quot;Australia/Tasmania&quot; ## [367] &quot;Australia/Victoria&quot; &quot;Australia/West&quot; ## [369] &quot;Australia/Yancowinna&quot; &quot;Brazil/Acre&quot; ## [371] &quot;Brazil/DeNoronha&quot; &quot;Brazil/East&quot; ## [373] &quot;Brazil/West&quot; &quot;Canada/Atlantic&quot; ## [375] &quot;Canada/Central&quot; &quot;Canada/Eastern&quot; ## [377] &quot;Canada/Mountain&quot; &quot;Canada/Newfoundland&quot; ## [379] &quot;Canada/Pacific&quot; &quot;Canada/Saskatchewan&quot; ## [381] &quot;Canada/Yukon&quot; &quot;CET&quot; ## [383] &quot;Chile/Continental&quot; &quot;Chile/EasterIsland&quot; ## [385] &quot;CST6CDT&quot; &quot;Cuba&quot; ## [387] &quot;EET&quot; &quot;Egypt&quot; ## [389] &quot;Eire&quot; &quot;EST&quot; ## [391] &quot;EST5EDT&quot; &quot;Etc/GMT&quot; ## [393] &quot;Etc/GMT-0&quot; &quot;Etc/GMT-1&quot; ## [395] &quot;Etc/GMT-10&quot; &quot;Etc/GMT-11&quot; ## [397] &quot;Etc/GMT-12&quot; &quot;Etc/GMT-13&quot; ## [399] &quot;Etc/GMT-14&quot; &quot;Etc/GMT-2&quot; ## [401] &quot;Etc/GMT-3&quot; &quot;Etc/GMT-4&quot; ## [403] &quot;Etc/GMT-5&quot; &quot;Etc/GMT-6&quot; ## [405] &quot;Etc/GMT-7&quot; &quot;Etc/GMT-8&quot; ## [407] &quot;Etc/GMT-9&quot; &quot;Etc/GMT+0&quot; ## [409] &quot;Etc/GMT+1&quot; &quot;Etc/GMT+10&quot; ## [411] &quot;Etc/GMT+11&quot; &quot;Etc/GMT+12&quot; ## [413] &quot;Etc/GMT+2&quot; &quot;Etc/GMT+3&quot; ## [415] &quot;Etc/GMT+4&quot; &quot;Etc/GMT+5&quot; ## [417] &quot;Etc/GMT+6&quot; &quot;Etc/GMT+7&quot; ## [419] &quot;Etc/GMT+8&quot; &quot;Etc/GMT+9&quot; ## [421] &quot;Etc/GMT0&quot; &quot;Etc/Greenwich&quot; ## [423] &quot;Etc/UCT&quot; &quot;Etc/Universal&quot; ## [425] &quot;Etc/UTC&quot; &quot;Etc/Zulu&quot; ## [427] &quot;Europe/Amsterdam&quot; &quot;Europe/Andorra&quot; ## [429] &quot;Europe/Astrakhan&quot; &quot;Europe/Athens&quot; ## [431] &quot;Europe/Belfast&quot; &quot;Europe/Belgrade&quot; ## [433] &quot;Europe/Berlin&quot; &quot;Europe/Bratislava&quot; ## [435] &quot;Europe/Brussels&quot; &quot;Europe/Bucharest&quot; ## [437] &quot;Europe/Budapest&quot; &quot;Europe/Busingen&quot; ## [439] &quot;Europe/Chisinau&quot; &quot;Europe/Copenhagen&quot; ## [441] &quot;Europe/Dublin&quot; &quot;Europe/Gibraltar&quot; ## [443] &quot;Europe/Guernsey&quot; &quot;Europe/Helsinki&quot; ## [445] &quot;Europe/Isle_of_Man&quot; &quot;Europe/Istanbul&quot; ## [447] &quot;Europe/Jersey&quot; &quot;Europe/Kaliningrad&quot; ## [449] &quot;Europe/Kiev&quot; &quot;Europe/Kirov&quot; ## [451] &quot;Europe/Kyiv&quot; &quot;Europe/Lisbon&quot; ## [453] &quot;Europe/Ljubljana&quot; &quot;Europe/London&quot; ## [455] &quot;Europe/Luxembourg&quot; &quot;Europe/Madrid&quot; ## [457] &quot;Europe/Malta&quot; &quot;Europe/Mariehamn&quot; ## [459] &quot;Europe/Minsk&quot; &quot;Europe/Monaco&quot; ## [461] &quot;Europe/Moscow&quot; &quot;Europe/Nicosia&quot; ## [463] &quot;Europe/Oslo&quot; &quot;Europe/Paris&quot; ## [465] &quot;Europe/Podgorica&quot; &quot;Europe/Prague&quot; ## [467] &quot;Europe/Riga&quot; &quot;Europe/Rome&quot; ## [469] &quot;Europe/Samara&quot; &quot;Europe/San_Marino&quot; ## [471] &quot;Europe/Sarajevo&quot; &quot;Europe/Saratov&quot; ## [473] &quot;Europe/Simferopol&quot; &quot;Europe/Skopje&quot; ## [475] &quot;Europe/Sofia&quot; &quot;Europe/Stockholm&quot; ## [477] &quot;Europe/Tallinn&quot; &quot;Europe/Tirane&quot; ## [479] &quot;Europe/Tiraspol&quot; &quot;Europe/Ulyanovsk&quot; ## [481] &quot;Europe/Uzhgorod&quot; &quot;Europe/Vaduz&quot; ## [483] &quot;Europe/Vatican&quot; &quot;Europe/Vienna&quot; ## [485] &quot;Europe/Vilnius&quot; &quot;Europe/Volgograd&quot; ## [487] &quot;Europe/Warsaw&quot; &quot;Europe/Zagreb&quot; ## [489] &quot;Europe/Zaporozhye&quot; &quot;Europe/Zurich&quot; ## [491] &quot;GB&quot; &quot;GB-Eire&quot; ## [493] &quot;GMT&quot; &quot;GMT-0&quot; ## [495] &quot;GMT+0&quot; &quot;GMT0&quot; ## [497] &quot;Greenwich&quot; &quot;Hongkong&quot; ## [499] &quot;HST&quot; &quot;Iceland&quot; ## [501] &quot;Indian/Antananarivo&quot; &quot;Indian/Chagos&quot; ## [503] &quot;Indian/Christmas&quot; &quot;Indian/Cocos&quot; ## [505] &quot;Indian/Comoro&quot; &quot;Indian/Kerguelen&quot; ## [507] &quot;Indian/Mahe&quot; &quot;Indian/Maldives&quot; ## [509] &quot;Indian/Mauritius&quot; &quot;Indian/Mayotte&quot; ## [511] &quot;Indian/Reunion&quot; &quot;Iran&quot; ## [513] &quot;Israel&quot; &quot;Jamaica&quot; ## [515] &quot;Japan&quot; &quot;Kwajalein&quot; ## [517] &quot;Libya&quot; &quot;MET&quot; ## [519] &quot;Mexico/BajaNorte&quot; &quot;Mexico/BajaSur&quot; ## [521] &quot;Mexico/General&quot; &quot;MST&quot; ## [523] &quot;MST7MDT&quot; &quot;Navajo&quot; ## [525] &quot;NZ&quot; &quot;NZ-CHAT&quot; ## [527] &quot;Pacific/Apia&quot; &quot;Pacific/Auckland&quot; ## [529] &quot;Pacific/Bougainville&quot; &quot;Pacific/Chatham&quot; ## [531] &quot;Pacific/Chuuk&quot; &quot;Pacific/Easter&quot; ## [533] &quot;Pacific/Efate&quot; &quot;Pacific/Enderbury&quot; ## [535] &quot;Pacific/Fakaofo&quot; &quot;Pacific/Fiji&quot; ## [537] &quot;Pacific/Funafuti&quot; &quot;Pacific/Galapagos&quot; ## [539] &quot;Pacific/Gambier&quot; &quot;Pacific/Guadalcanal&quot; ## [541] &quot;Pacific/Guam&quot; &quot;Pacific/Honolulu&quot; ## [543] &quot;Pacific/Johnston&quot; &quot;Pacific/Kanton&quot; ## [545] &quot;Pacific/Kiritimati&quot; &quot;Pacific/Kosrae&quot; ## [547] &quot;Pacific/Kwajalein&quot; &quot;Pacific/Majuro&quot; ## [549] &quot;Pacific/Marquesas&quot; &quot;Pacific/Midway&quot; ## [551] &quot;Pacific/Nauru&quot; &quot;Pacific/Niue&quot; ## [553] &quot;Pacific/Norfolk&quot; &quot;Pacific/Noumea&quot; ## [555] &quot;Pacific/Pago_Pago&quot; &quot;Pacific/Palau&quot; ## [557] &quot;Pacific/Pitcairn&quot; &quot;Pacific/Pohnpei&quot; ## [559] &quot;Pacific/Ponape&quot; &quot;Pacific/Port_Moresby&quot; ## [561] &quot;Pacific/Rarotonga&quot; &quot;Pacific/Saipan&quot; ## [563] &quot;Pacific/Samoa&quot; &quot;Pacific/Tahiti&quot; ## [565] &quot;Pacific/Tarawa&quot; &quot;Pacific/Tongatapu&quot; ## [567] &quot;Pacific/Truk&quot; &quot;Pacific/Wake&quot; ## [569] &quot;Pacific/Wallis&quot; &quot;Pacific/Yap&quot; ## [571] &quot;Poland&quot; &quot;Portugal&quot; ## [573] &quot;PRC&quot; &quot;PST8PDT&quot; ## [575] &quot;ROC&quot; &quot;ROK&quot; ## [577] &quot;Singapore&quot; &quot;Turkey&quot; ## [579] &quot;UCT&quot; &quot;Universal&quot; ## [581] &quot;US/Alaska&quot; &quot;US/Aleutian&quot; ## [583] &quot;US/Arizona&quot; &quot;US/Central&quot; ## [585] &quot;US/East-Indiana&quot; &quot;US/Eastern&quot; ## [587] &quot;US/Hawaii&quot; &quot;US/Indiana-Starke&quot; ## [589] &quot;US/Michigan&quot; &quot;US/Mountain&quot; ## [591] &quot;US/Pacific&quot; &quot;US/Samoa&quot; ## [593] &quot;UTC&quot; &quot;W-SU&quot; ## [595] &quot;WET&quot; &quot;Zulu&quot; ## attr(,&quot;Version&quot;) ## [1] &quot;2023c&quot; Setting and extracting timezone setting: set tz = \"timezone_name\" argument extracting: function tz() Manipulating timezones force_tz(): change the timezone without changing the clock time with_tz() - view the same instant in a different timezone 11.4.1.1 Setting the timezone If you import a datetime and it has the wrong timezone, you can set it with force_tz(date, tzone = \"tz\"). To watch New Zealand in the Women’s World Cup Soccer games in 2015, but the times listed on the FIFA website were all in times local to the venues. In this exercise you’ll help to set the timezones. # Game2: CAN vs NZL in Edmonton game2 &lt;- mdy_hm(&quot;June 11 2015 19:00&quot;) # Game3: CHN vs NZL in Winnipeg game3 &lt;- mdy_hm(&quot;June 15 2015 18:30&quot;) Game 2 was played in Edmonton. # Set the timezone to &quot;America/Edmonton&quot; game2_local &lt;- force_tz(game2, tzone = &quot;America/Edmonton&quot;) game2_local ## [1] &quot;2015-06-11 19:00:00 MDT&quot; Game 3 was played in Winnipeg. # Set the timezone to &quot;America/Winnipeg&quot; game3_local &lt;- force_tz(game3, tzone = &quot;America/Winnipeg&quot;) game3_local ## [1] &quot;2015-06-15 18:30:00 CDT&quot; Find out how long the team had to rest between the two games. # How long does the team have to rest? as.period(interval(game2_local, game3_local)) ## [1] &quot;3d 22H 30M 0S&quot; Edmonton and Winnipeg are in different timezones, so even though the start times of the games only look 30 minutes apart, they are in fact 1 hour and 30 minutes apart, and the team only has 3 days, 22 hours and 30 minutes to prepare. 11.4.1.2 Viewing in a timezone with_tz(date, tzone = \"tz\") isn’t changing the underlying moment of time. For example, the difference between now() displayed in the “America/Los_Angeles” timezone and “Pacific/Auckland” timezone is 0. (同一時間，只是在不同時區) Now, figure out when to tune into the games from the previous exercise. Most fans will tune in from New Zealand. Usewith_tz() to display game2_local in New Zealand time. New Zealand is in the \"Pacific/Auckland\" timezone. # What time is game2_local in NZ? with_tz(game2_local, tzone = &quot;Pacific/Auckland&quot;) ## [1] &quot;2015-06-12 13:00:00 NZST&quot; In Corvallis, Corvallis is in the “America/Los_Angeles” timezone. # What time is game2_local in Corvallis, Oregon? with_tz(game2_local, tzone = &quot;America/Los_Angeles&quot;) ## [1] &quot;2015-06-11 18:00:00 PDT&quot; # What time is game3_local in NZ? with_tz(game3_local, tzone = &quot;Pacific/Auckland&quot;) ## [1] &quot;2015-06-16 11:30:00 NZST&quot; 11.4.1.3 Timezones in the weather data glimpse(akl_hourly) ## Rows: 17,454 ## Columns: 16 ## $ year &lt;dbl&gt; 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, 2016, … ## $ month &lt;ord&gt; Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan, Jan,… ## $ mday &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ time &lt;time&gt; 00:00:00, 00:30:00, 01:00:00, 01:30:00, 02:00:00, 02:… ## $ temperature &lt;dbl&gt; 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, 68.0, … ## $ weather &lt;chr&gt; &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, … ## $ conditions &lt;chr&gt; &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, &quot;Clear&quot;, … ## $ events &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ humidity &lt;dbl&gt; 68, 68, 73, 68, 68, 68, 68, 68, 68, 73, 73, 73, 68, 73… ## $ date_utc &lt;dttm&gt; 2015-12-31 11:00:00, 2015-12-31 11:30:00, 2015-12-31 … ## $ date &lt;date&gt; 2016-01-01, 2016-01-01, 2016-01-01, 2016-01-01, 2016-… ## $ datetime_string &lt;chr&gt; &quot;2016-01-01T00:00:00&quot;, &quot;2016-01-01T00:30:00&quot;, &quot;2016-01… ## $ datetime &lt;dttm&gt; 2016-01-01 00:00:00, 2016-01-01 00:30:00, 2016-01-01 … ## $ hour &lt;int&gt; 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, … ## $ rainy &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE… ## $ day_hour &lt;dttm&gt; 2016-01-01 00:00:00, 2016-01-01 00:00:00, 2016-01-01 … The datetime column you created represented local time in Auckland, NZ. Suspect date_utc column represents the observation time in UTC (the name seems a big clue). But does it really? What timezone are datetime and date_utc currently in? # Examine datetime columns head(akl_hourly$datetime) ## [1] &quot;2016-01-01 00:00:00 UTC&quot; &quot;2016-01-01 00:30:00 UTC&quot; ## [3] &quot;2016-01-01 01:00:00 UTC&quot; &quot;2016-01-01 01:30:00 UTC&quot; ## [5] &quot;2016-01-01 02:00:00 UTC&quot; &quot;2016-01-01 02:30:00 UTC&quot; # Examine date_utc columns head(akl_hourly$date_utc) ## [1] &quot;2015-12-31 11:00:00 UTC&quot; &quot;2015-12-31 11:30:00 UTC&quot; ## [3] &quot;2015-12-31 12:00:00 UTC&quot; &quot;2015-12-31 12:30:00 UTC&quot; ## [5] &quot;2015-12-31 13:00:00 UTC&quot; &quot;2015-12-31 13:30:00 UTC&quot; Fix datetime to have the timezone for \"Pacific/Auckland\". # Force datetime to Pacific/Auckland akl_hourly &lt;- akl_hourly %&gt;% mutate(datetime = force_tz(datetime, tzone = &quot;Pacific/Auckland&quot;)) # Reexamine datetime head(akl_hourly$datetime) ## [1] &quot;2016-01-01 00:00:00 NZDT&quot; &quot;2016-01-01 00:30:00 NZDT&quot; ## [3] &quot;2016-01-01 01:00:00 NZDT&quot; &quot;2016-01-01 01:30:00 NZDT&quot; ## [5] &quot;2016-01-01 02:00:00 NZDT&quot; &quot;2016-01-01 02:30:00 NZDT&quot; Now tabulate up the difference between the datetime and date_utc. It should be zero if our hypothesis was correct. # Are datetime and date_utc the same moments table(akl_hourly$datetime - akl_hourly$date_utc) ## ## -82800 0 3600 ## 2 17450 2 Looks like for 17,450 rows datetime and date_utc describe the same moment. But for 4 rows they are different. Because it’s the times where DST kicks in. 11.4.1.4 Times without dates Sometimes you just have a time without a date. The hms package provides an hms class of object for holding times without dates, and the best place to start would be with as.hms(). # Import auckland hourly data akl_hourly &lt;- read_csv(&quot;data/akl_weather_hourly_2016.csv&quot;) # Examine structure of time column # verify it has the class hms str(akl_hourly$time) ## &#39;hms&#39; num [1:17454] 00:00:00 00:30:00 01:00:00 01:30:00 ... ## - attr(*, &quot;units&quot;)= chr &quot;secs&quot; # Examine head of time column head(akl_hourly$time) ## 00:00:00 ## 00:30:00 ## 01:00:00 ## 01:30:00 ## 02:00:00 ## 02:30:00 Using time without date is a great way to examine daily patterns. # A plot using just time ggplot(akl_hourly, aes(x = time, y = temperature)) + geom_line(aes(group = make_date(year, month, mday)), alpha = 0.2) 11.4.2 More on import and export datetimes 11.4.2.1 Fast parsing with fasttime The fasttime package provides a single function fastPOSIXct(), designed to read in datetimes formatted according to ISO 8601 (YYYY-MM-DD HH:MM:SS). Because it only reads in one format, and doesn’t have to guess a format, it is really fast! You’ll see how fast in this exercise by comparing how fast it reads in the dates from the Auckland hourly weather data (over 17,000 dates) to lubridates ymd_hms(). To compare run times you’ll use the microbenchmark() function from the package of the same name. library(microbenchmark) ## Warning: package &#39;microbenchmark&#39; was built under R version 4.3.2 library(fasttime) ## Warning: package &#39;fasttime&#39; was built under R version 4.3.1 # Use make_date() to combine year, month and mday akl_hourly &lt;- akl_hourly_raw %&gt;% mutate(date = make_date(year = year, month = month, day = mday), datetime_string = paste(date, &quot;T&quot;, time, &quot;Z&quot;, sep = &quot;&quot;), datetime = ymd_hms(datetime_string)) dates &lt;- akl_hourly$datetime_string # Examine structure of dates str(dates) ## chr [1:17454] &quot;2016-01-01T00:00:00Z&quot; &quot;2016-01-01T00:30:00Z&quot; ... # Use fastPOSIXct() to parse dates fastPOSIXct(dates) %&gt;% str() ## POSIXct[1:17454], format: &quot;2016-01-01 08:00:00&quot; &quot;2016-01-01 08:30:00&quot; &quot;2016-01-01 09:00:00&quot; ... Now to compare timing # Compare speed of fastPOSIXct() to ymd_hms() microbenchmark( ymd_hms = ymd_hms(dates), fasttime = fastPOSIXct(dates), times = 20) ## Unit: milliseconds ## expr min lq mean median uq max neval ## ymd_hms 46.1705 46.87105 48.993385 49.42365 50.28835 52.8446 20 ## fasttime 2.1090 2.15855 2.275015 2.24135 2.35460 2.5627 20 11.4.2.2 Fast parsing with lubridate lubridate provides its own fast datetime parser: fast_strptime(). It takes a format argument and the format must comply with the strptime() style. e.g, fast_strptime(x, format = \"%Y-%m-%d\") # Head of dates head(dates) ## [1] &quot;2016-01-01T00:00:00Z&quot; &quot;2016-01-01T00:30:00Z&quot; &quot;2016-01-01T01:00:00Z&quot; ## [4] &quot;2016-01-01T01:30:00Z&quot; &quot;2016-01-01T02:00:00Z&quot; &quot;2016-01-01T02:30:00Z&quot; Parse dates with fast_strptime() by specifying the appropriate format string. # Parse dates with fast_strptime fast_strptime(dates, format = &quot;%Y-%m-%dT%H:%M:%SZ&quot;) %&gt;% str() ## POSIXlt[1:17454], format: &quot;2016-01-01 00:00:00&quot; &quot;2016-01-01 00:30:00&quot; &quot;2016-01-01 01:00:00&quot; ... Compare the timing of fast_strptime() to fasttime and ymd_hms(). # Comparse speed to ymd_hms() and fasttime microbenchmark( ymd_hms = ymd_hms(dates), fasttime = fastPOSIXct(dates), fast_strptime = fast_strptime(dates, format = &quot;%Y-%m-%dT%H:%M:%SZ&quot;), times = 20) ## Unit: milliseconds ## expr min lq mean median uq max neval ## ymd_hms 47.0874 48.56200 49.476860 48.83850 49.62405 60.4190 20 ## fasttime 2.1689 2.19825 2.294765 2.25925 2.33565 2.6500 20 ## fast_strptime 2.5011 2.54600 2.636500 2.62385 2.71275 2.8521 20 11.4.2.3 Outputting pretty dates and times An easy way to output dates is to use the stamp() function in lubridate. stamp() takes a string which should be an example of how the date should be formatted, and returns a function that can be used to format dates. Notice date_stamp is a function. # Create a stamp based on &quot;Saturday, Jan 1, 2000&quot; date_stamp &lt;- stamp(&quot;Saturday, Jan 1, 2000&quot;) ## Multiple formats matched: &quot;%A, %b %d, %Y&quot;(1), &quot;Saturday, %Om %d, %Y&quot;(1), &quot;Saturday, Jan %Om, %Y&quot;(1), &quot;Saturday, %b %d, %Y&quot;(1), &quot;Saturday, Jan %m, %Y&quot;(1), &quot;%A, %Om %d, %Y&quot;(0), &quot;%A, Jan %Om, %Y&quot;(0), &quot;%A, Jan %m, %Y&quot;(0) ## Using: &quot;%A, %b %d, %Y&quot; # Print date_stamp print(date_stamp) ## function (x, locale = &quot;C&quot;) ## { ## { ## old_lc_time &lt;- Sys.getlocale(&quot;LC_TIME&quot;) ## if (old_lc_time != locale) { ## on.exit(Sys.setlocale(&quot;LC_TIME&quot;, old_lc_time)) ## Sys.setlocale(&quot;LC_TIME&quot;, locale) ## } ## } ## format(x, format = &quot;%A, %b %d, %Y&quot;) ## } ## &lt;environment: 0x000001cc6ad52668&gt; Format today’s date by date_stamp. # Call date_stamp on today() date_stamp(today()) ## [1] &quot;Tuesday, Jan 16, 2024&quot; Now output today’s date in American style MM/DD/YYYY. # Create and call a stamp based on &quot;12/31/1999&quot; stamp(&quot;12/31/1999&quot;)(today()) ## Multiple formats matched: &quot;%Om/%d/%Y&quot;(1), &quot;%m/%d/%Y&quot;(1) ## Using: &quot;%Om/%d/%Y&quot; ## [1] &quot;01/16/2024&quot; finished &lt;- &quot;I finished &#39;Dates and Times in R&#39; on Thursday, September 4, 2017!&quot; # Use string finished for stamp() stamp(finished)(today()) ## Multiple formats matched: &quot;I finished &#39;Dates and Times in R&#39; on %A, %B %d, %Y!&quot;(1), &quot;I finished &#39;Dates and Times in R&#39; on Thursday, %Om %d, %Y!&quot;(1), &quot;I finished &#39;Dates and Times in R&#39; on Thursday, September %Om, %Y!&quot;(1), &quot;I finished &#39;Dates and Times in R&#39; on Thursday, %B %d, %Y!&quot;(1), &quot;I finished &#39;Dates and Times in R&#39; on Thursday, September %m, %Y!&quot;(1), &quot;I finished &#39;Dates and Times in R&#39; on %A, %Om %d, %Y!&quot;(0), &quot;I finished &#39;Dates and Times in R&#39; on %A, September %Om, %Y!&quot;(0), &quot;I finished &#39;Dates and Times in R&#39; on %A, September %m, %Y!&quot;(0) ## Using: &quot;I finished &#39;Dates and Times in R&#39; on %A, %B %d, %Y!&quot; ## [1] &quot;I finished &#39;Dates and Times in R&#39; on Tuesday, January 16, 2024!&quot; "],["introduction-to-writing-functions.html", "Chapter 12 Introduction to Writing Functions 12.1 Write a Function 12.2 All About Arguments 12.3 Return Values and Scope 12.4 Case Study on Grain Yields", " Chapter 12 Introduction to Writing Functions 12.1 Write a Function 12.1.1 Why write function? Benefits of writing functions: Functions eliminate repetition from your code, which can reduce your workload, and help avoid errors. Functions also allow code reuse and sharing. 12.1.1.1 Calling functions Best practice for calling functions is to include them in the order shown by args(), and to only name rare arguments. args(rank) ## function (x, na.last = TRUE, ties.method = c(&quot;average&quot;, &quot;first&quot;, ## &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;)) ## NULL 12.1.2 Convert scripts into functions Make a template Paste your script into the body Choose the arguments Replace specific values with arguments Generalize variable names Remove the final assignment 12.1.2.1 First function # Your script, from a previous step # Sample from coin_sides once coin_sides &lt;- c(&quot;head&quot;, &quot;tail&quot;) sample(coin_sides, 1) ## [1] &quot;tail&quot; # Paste your script into the function body toss_coin &lt;- function() { coin_sides &lt;- c(&quot;head&quot;, &quot;tail&quot;) sample(coin_sides, 1) } # Call your function toss_coin() ## [1] &quot;head&quot; 12.1.2.2 Inputs to functions The inputs to functions are called arguments. # Update the function to return n coin tosses toss_coin &lt;- function(n_flips) { coin_sides &lt;- c(&quot;head&quot;, &quot;tail&quot;) sample(coin_sides, size = n_flips, replace = TRUE) } # Generate 10 coin tosses toss_coin(10) ## [1] &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;tail&quot; &quot;head&quot; 12.1.2.3 Multiple inputs to functions If a function should have more than one argument, list them in the function signature, separated by commas. Bias the coin by weighting the sampling. Specify the prob argument so that heads are sampled with probability p_head (and tails are sampled with probability 1 - p_head). coin_sides &lt;- c(&quot;head&quot;, &quot;tail&quot;) n_flips &lt;- 10 p_head &lt;- 0.8 # Define a vector of weights weights &lt;- c(p_head, 1 - p_head) # Update so that heads are sampled with prob p_head sample(coin_sides, n_flips, replace = TRUE, prob = weights) ## [1] &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; Update the definition of toss_coin() so it accepts an argument, p_head, and weights the samples using the code you wrote in the previous step. # Update the function so heads have probability p_head toss_coin &lt;- function(n_flips, p_head) { coin_sides &lt;- c(&quot;head&quot;, &quot;tail&quot;) # Define a vector of weights weights &lt;- c(p_head, 1 - p_head) # Modify the sampling to be weighted sample(coin_sides, n_flips, replace = TRUE, prob = weights) } # Generate 10 coin tosses with an 80% chance of each head toss_coin(10, 0.8) ## [1] &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; &quot;head&quot; &quot;head&quot; &quot;tail&quot; &quot;head&quot; 12.1.3 Readable code Function names should contain a verb Readability vs. typeability Understanding code &gt;&gt; typing code Code editors have autocomplete You can alias common functions Types of argument Data arguments: what you compute on Detail arguments: how you perform the computation Data args should precede detail args Renaming GLM R’s generalized linear regression function, glm(), suffers the same usability problems as lm(): its name is an acronym, and its formula and data arguments are in the wrong order. To solve this exercise, you need to know two things about generalized linear regression: glm() formulas are specified like lm() formulas: response is on the left, and explanatory variables are added on the right. To model count data, set glm()’s family argument to poisson, making it a Poisson regression. library(tidyverse) snake_river_visits &lt;- read_rds(&quot;data/snake_river_visits.rds&quot;) # Run a generalized linear regression glm( # Model no. of visits vs. gender, income, travel n_visits ~ gender + income + travel, # Use the snake_river_visits dataset data = snake_river_visits, # Make it a Poisson regression family = poisson ) ## ## Call: glm(formula = n_visits ~ gender + income + travel, family = poisson, ## data = snake_river_visits) ## ## Coefficients: ## (Intercept) genderfemale income($25k,$55k] income($55k,$95k] ## 4.0864 0.3740 -0.0199 -0.5807 ## income($95k,$Inf) travel(0.25h,4h] travel(4h,Infh) ## -0.5782 -0.6271 -2.4230 ## ## Degrees of Freedom: 345 Total (i.e. Null); 339 Residual ## (64 observations deleted due to missingness) ## Null Deviance: 18850 ## Residual Deviance: 11530 AIC: 12860 Define a function, run_poisson_regression(), to run a Poisson regression. This should take two arguments: data and formula, and call glm() . # Write a function to run a Poisson regression run_poisson_regression &lt;- function(data, formula) { glm(formula, data, family = poisson) } # Re-run the Poisson regression, using your function model &lt;- snake_river_visits %&gt;% run_poisson_regression(n_visits ~ gender + income + travel) # Run this to see the predictions # snake_river_explanatory %&gt;% # mutate(predicted_n_visits = predict(model, ., type = &quot;response&quot;))%&gt;% # arrange(desc(predicted_n_visits)) 12.2 All About Arguments 12.2.1 Default arguments 12.2.1.1 Numeric defaults Only set defaults for numeric detail arguments, not data arguments. cut_by_quantile() converts a numeric vector into a categorical variable where quantiles define the cut points. By specifying default arguments, you can make it easier to use. Let’s start with n, which specifies how many categories to cut x into. # A numeric vector of the number of visits to Snake River n_visits &lt;- snake_river_visits$n_visits # Set the default for n to 5 cut_by_quantile &lt;- function(x, n = 5, na.rm, labels, interval_type) { probs &lt;- seq(0, 1, length.out = n + 1) qtiles &lt;- quantile(x, probs, na.rm = na.rm, names = FALSE) right &lt;- switch(interval_type, &quot;(lo, hi]&quot; = TRUE, &quot;[lo, hi)&quot; = FALSE) cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE) } # Remove the n argument from the call cut_by_quantile( n_visits, na.rm = FALSE, labels = c(&quot;very low&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;very high&quot;), interval_type = &quot;(lo, hi]&quot; ) ## [1] very low very low very low very low very low very low very low ## [8] very low very low very low very low very low very low very low ## [15] very low very low very low very low very low high very high ## [22] high very low medium low very low very low very low ## [29] very low very low very low very high very high very high very high ## [36] very high high very high very high very high very high very high ## [43] medium very high very high very high medium medium low ## [50] high high high very high very high high high ## [57] very high medium very high high medium high very high ## [64] very high very high very high high high very high high ## [71] very low very high high high medium high high ## [78] high medium very high very high very high high high ## [85] high very low very high medium high very high high ## [92] high very high high very low very low medium very low ## [99] medium medium very high medium medium medium high ## [106] low high very high medium very high medium very high ## [113] low very high low very high high very low very low ## [120] very low very low low very low very low very low very low ## [127] very low very low medium very low very low low low ## [134] very low very low low very low very low very low low ## [141] low medium medium medium medium medium very low ## [148] very low low very low low medium very low very low ## [155] very low very low very high high very high high medium ## [162] very high medium very low high medium high high ## [169] very high high high very high very high high very high ## [176] high high medium very high high high high ## [183] very high very high very low high very high high high ## [190] medium very high high very high high very high high ## [197] very high high very high very low high very high very high ## [204] very low very low medium very high medium low medium ## [211] high medium very low medium very high high very high ## [218] high very high high low high medium very high ## [225] medium high high high very low high high ## [232] high very high high medium medium very low very low ## [239] very low very low medium low very low very low very low ## [246] medium high very low very low medium very low very low ## [253] very low very low very low very low very low very low very low ## [260] very low very high medium very low very high medium very high ## [267] medium low very high medium medium medium low ## [274] high medium high very high medium very high very high ## [281] medium medium very high high medium very high high ## [288] medium low very low medium very low very low very low ## [295] very low very low low very low very low very low very low ## [302] very low very low very low very low low very low very low ## [309] very low very low low very low very low low very low ## [316] very low very low very low low very low very low very low ## [323] very low very low low very low very low very low very low ## [330] very low very low very low very low very low very low very low ## [337] very low very low very low very low very low very low very low ## [344] very low very low medium very low very low very low very low ## [351] very low very low very low very low very low very low very low ## [358] very low low very low very low very low very low very low ## [365] very low very low very low very low very low very low low ## [372] very low very low very low very high high very high very high ## [379] very high high very high very high very high very high medium ## [386] medium medium high very high high high high ## [393] high high high high very high high very high ## [400] medium high low high very high low very low ## [407] medium very low medium low ## Levels: very low low medium high very high 12.2.1.2 Logical defaults cut_by_quantile() is now slightly easier to use, but you still always have to specify the na.rm argument. This removes missing values. Where functions have an argument for removing missing values, the best practice is to not remove them by default (in case you hadn’t spotted that you had missing values). That means that the default for na.rm should be FALSE. # Set the default for na.rm to FALSE cut_by_quantile &lt;- function(x, n = 5, na.rm = FALSE, labels, interval_type) { probs &lt;- seq(0, 1, length.out = n + 1) qtiles &lt;- quantile(x, probs, na.rm = na.rm, names = FALSE) right &lt;- switch(interval_type, &quot;(lo, hi]&quot; = TRUE, &quot;[lo, hi)&quot; = FALSE) cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE) } # Remove the na.rm argument from the call cut_by_quantile( n_visits, labels = c(&quot;very low&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;very high&quot;), interval_type = &quot;(lo, hi]&quot; ) ## [1] very low very low very low very low very low very low very low ## [8] very low very low very low very low very low very low very low ## [15] very low very low very low very low very low high very high ## [22] high very low medium low very low very low very low ## [29] very low very low very low very high very high very high very high ## [36] very high high very high very high very high very high very high ## [43] medium very high very high very high medium medium low ## [50] high high high very high very high high high ## [57] very high medium very high high medium high very high ## [64] very high very high very high high high very high high ## [71] very low very high high high medium high high ## [78] high medium very high very high very high high high ## [85] high very low very high medium high very high high ## [92] high very high high very low very low medium very low ## [99] medium medium very high medium medium medium high ## [106] low high very high medium very high medium very high ## [113] low very high low very high high very low very low ## [120] very low very low low very low very low very low very low ## [127] very low very low medium very low very low low low ## [134] very low very low low very low very low very low low ## [141] low medium medium medium medium medium very low ## [148] very low low very low low medium very low very low ## [155] very low very low very high high very high high medium ## [162] very high medium very low high medium high high ## [169] very high high high very high very high high very high ## [176] high high medium very high high high high ## [183] very high very high very low high very high high high ## [190] medium very high high very high high very high high ## [197] very high high very high very low high very high very high ## [204] very low very low medium very high medium low medium ## [211] high medium very low medium very high high very high ## [218] high very high high low high medium very high ## [225] medium high high high very low high high ## [232] high very high high medium medium very low very low ## [239] very low very low medium low very low very low very low ## [246] medium high very low very low medium very low very low ## [253] very low very low very low very low very low very low very low ## [260] very low very high medium very low very high medium very high ## [267] medium low very high medium medium medium low ## [274] high medium high very high medium very high very high ## [281] medium medium very high high medium very high high ## [288] medium low very low medium very low very low very low ## [295] very low very low low very low very low very low very low ## [302] very low very low very low very low low very low very low ## [309] very low very low low very low very low low very low ## [316] very low very low very low low very low very low very low ## [323] very low very low low very low very low very low very low ## [330] very low very low very low very low very low very low very low ## [337] very low very low very low very low very low very low very low ## [344] very low very low medium very low very low very low very low ## [351] very low very low very low very low very low very low very low ## [358] very low low very low very low very low very low very low ## [365] very low very low very low very low very low very low low ## [372] very low very low very low very high high very high very high ## [379] very high high very high very high very high very high medium ## [386] medium medium high very high high high high ## [393] high high high high very high high very high ## [400] medium high low high very high low very low ## [407] medium very low medium low ## Levels: very low low medium high very high 12.2.1.3 NULL defaults The cut() function used by cut_by_quantile() can automatically provide sensible labels for each category. The code to generate these labels is pretty complicated, so rather than appearing in the function signature directly, its labels argument defaults to NULL, and the calculation details are shown on the ?cut help page. (If you use this capability, make sure to document how the argument behaves in the function’s help page.) # Set the default for labels to NULL cut_by_quantile &lt;- function(x, n = 5, na.rm = FALSE, labels = NULL, interval_type) { probs &lt;- seq(0, 1, length.out = n + 1) qtiles &lt;- quantile(x, probs, na.rm = na.rm, names = FALSE) right &lt;- switch(interval_type, &quot;(lo, hi]&quot; = TRUE, &quot;[lo, hi)&quot; = FALSE) cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE) } # Remove the labels argument from the call cut_by_quantile( n_visits, interval_type = &quot;(lo, hi]&quot; ) ## [1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [9] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [17] [0,1] [0,1] [0,1] (10,35] (35,350] (10,35] [0,1] (2,10] ## [25] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] (35,350] ## [33] (35,350] (35,350] (35,350] (35,350] (10,35] (35,350] (35,350] (35,350] ## [41] (35,350] (35,350] (2,10] (35,350] (35,350] (35,350] (2,10] (2,10] ## [49] (1,2] (10,35] (10,35] (10,35] (35,350] (35,350] (10,35] (10,35] ## [57] (35,350] (2,10] (35,350] (10,35] (2,10] (10,35] (35,350] (35,350] ## [65] (35,350] (35,350] (10,35] (10,35] (35,350] (10,35] [0,1] (35,350] ## [73] (10,35] (10,35] (2,10] (10,35] (10,35] (10,35] (2,10] (35,350] ## [81] (35,350] (35,350] (10,35] (10,35] (10,35] [0,1] (35,350] (2,10] ## [89] (10,35] (35,350] (10,35] (10,35] (35,350] (10,35] [0,1] [0,1] ## [97] (2,10] [0,1] (2,10] (2,10] (35,350] (2,10] (2,10] (2,10] ## [105] (10,35] (1,2] (10,35] (35,350] (2,10] (35,350] (2,10] (35,350] ## [113] (1,2] (35,350] (1,2] (35,350] (10,35] [0,1] [0,1] [0,1] ## [121] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [129] (2,10] [0,1] [0,1] (1,2] (1,2] [0,1] [0,1] (1,2] ## [137] [0,1] [0,1] [0,1] (1,2] (1,2] (2,10] (2,10] (2,10] ## [145] (2,10] (2,10] [0,1] [0,1] (1,2] [0,1] (1,2] (2,10] ## [153] [0,1] [0,1] [0,1] [0,1] (35,350] (10,35] (35,350] (10,35] ## [161] (2,10] (35,350] (2,10] [0,1] (10,35] (2,10] (10,35] (10,35] ## [169] (35,350] (10,35] (10,35] (35,350] (35,350] (10,35] (35,350] (10,35] ## [177] (10,35] (2,10] (35,350] (10,35] (10,35] (10,35] (35,350] (35,350] ## [185] [0,1] (10,35] (35,350] (10,35] (10,35] (2,10] (35,350] (10,35] ## [193] (35,350] (10,35] (35,350] (10,35] (35,350] (10,35] (35,350] [0,1] ## [201] (10,35] (35,350] (35,350] [0,1] [0,1] (2,10] (35,350] (2,10] ## [209] (1,2] (2,10] (10,35] (2,10] [0,1] (2,10] (35,350] (10,35] ## [217] (35,350] (10,35] (35,350] (10,35] (1,2] (10,35] (2,10] (35,350] ## [225] (2,10] (10,35] (10,35] (10,35] [0,1] (10,35] (10,35] (10,35] ## [233] (35,350] (10,35] (2,10] (2,10] [0,1] [0,1] [0,1] [0,1] ## [241] (2,10] (1,2] [0,1] [0,1] [0,1] (2,10] (10,35] [0,1] ## [249] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [257] [0,1] [0,1] [0,1] [0,1] (35,350] (2,10] [0,1] (35,350] ## [265] (2,10] (35,350] (2,10] (1,2] (35,350] (2,10] (2,10] (2,10] ## [273] (1,2] (10,35] (2,10] (10,35] (35,350] (2,10] (35,350] (35,350] ## [281] (2,10] (2,10] (35,350] (10,35] (2,10] (35,350] (10,35] (2,10] ## [289] (1,2] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] ## [297] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [305] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [313] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [321] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] [0,1] [0,1] ## [329] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [337] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [345] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [353] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [361] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [369] [0,1] [0,1] (1,2] [0,1] [0,1] [0,1] (35,350] (10,35] ## [377] (35,350] (35,350] (35,350] (10,35] (35,350] (35,350] (35,350] (35,350] ## [385] (2,10] (2,10] (2,10] (10,35] (35,350] (10,35] (10,35] (10,35] ## [393] (10,35] (10,35] (10,35] (10,35] (35,350] (10,35] (35,350] (2,10] ## [401] (10,35] (1,2] (10,35] (35,350] (1,2] [0,1] (2,10] [0,1] ## [409] (2,10] (1,2] ## Levels: [0,1] (1,2] (2,10] (10,35] (35,350] 12.2.1.4 Categorical defaults Pass a character vector in the signature. Call match.arg() in the body. The pattern for categorical defaults is: function(cat_arg = c(&quot;choice1&quot;, &quot;choice2&quot;)) { cat_arg &lt;- match.arg(cat_arg) } When cutting up a numeric vector, you need to worry about what happens if a value lands exactly on a boundary. You can either put this value into a category of the lower interval or the higher interval. That is, you can choose your intervals to include values at the top boundary but not the bottom. In mathematical terminology, “open on the left, closed on the right” = (lo, hi] “closed on the left, open on the right” = [lo, hi) cut_by_quantile() should allow these two choices. # Take &quot;rank()&quot; for example, look at the ties.method argument head(rank, 7) ## ## 1 function (x, na.last = TRUE, ties.method = c(&quot;average&quot;, &quot;first&quot;, ## 2 &quot;last&quot;, &quot;random&quot;, &quot;max&quot;, &quot;min&quot;)) ## 3 { ## 4 stopifnot(length(na.last) == 1L) ## 5 nas &lt;- is.na(x) ## 6 nm &lt;- names(x) ## 7 ties.method &lt;- match.arg(ties.method) # Set the categories for interval_type to &quot;(lo, hi]&quot; and &quot;[lo, hi)&quot; cut_by_quantile &lt;- function(x, n = 5, na.rm = FALSE, labels = NULL, interval_type = c(&quot;(lo, hi]&quot;, &quot;[lo, hi)&quot;)) { # Match the interval_type argument interval_type &lt;- match.arg(interval_type) probs &lt;- seq(0, 1, length.out = n + 1) qtiles &lt;- quantile(x, probs, na.rm = na.rm, names = FALSE) right &lt;- switch(interval_type, &quot;(lo, hi]&quot; = TRUE, &quot;[lo, hi)&quot; = FALSE) cut(x, qtiles, labels = labels, right = right, include.lowest = TRUE) } # Remove the interval_type argument from the call cut_by_quantile(n_visits) ## [1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [9] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [17] [0,1] [0,1] [0,1] (10,35] (35,350] (10,35] [0,1] (2,10] ## [25] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] (35,350] ## [33] (35,350] (35,350] (35,350] (35,350] (10,35] (35,350] (35,350] (35,350] ## [41] (35,350] (35,350] (2,10] (35,350] (35,350] (35,350] (2,10] (2,10] ## [49] (1,2] (10,35] (10,35] (10,35] (35,350] (35,350] (10,35] (10,35] ## [57] (35,350] (2,10] (35,350] (10,35] (2,10] (10,35] (35,350] (35,350] ## [65] (35,350] (35,350] (10,35] (10,35] (35,350] (10,35] [0,1] (35,350] ## [73] (10,35] (10,35] (2,10] (10,35] (10,35] (10,35] (2,10] (35,350] ## [81] (35,350] (35,350] (10,35] (10,35] (10,35] [0,1] (35,350] (2,10] ## [89] (10,35] (35,350] (10,35] (10,35] (35,350] (10,35] [0,1] [0,1] ## [97] (2,10] [0,1] (2,10] (2,10] (35,350] (2,10] (2,10] (2,10] ## [105] (10,35] (1,2] (10,35] (35,350] (2,10] (35,350] (2,10] (35,350] ## [113] (1,2] (35,350] (1,2] (35,350] (10,35] [0,1] [0,1] [0,1] ## [121] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [129] (2,10] [0,1] [0,1] (1,2] (1,2] [0,1] [0,1] (1,2] ## [137] [0,1] [0,1] [0,1] (1,2] (1,2] (2,10] (2,10] (2,10] ## [145] (2,10] (2,10] [0,1] [0,1] (1,2] [0,1] (1,2] (2,10] ## [153] [0,1] [0,1] [0,1] [0,1] (35,350] (10,35] (35,350] (10,35] ## [161] (2,10] (35,350] (2,10] [0,1] (10,35] (2,10] (10,35] (10,35] ## [169] (35,350] (10,35] (10,35] (35,350] (35,350] (10,35] (35,350] (10,35] ## [177] (10,35] (2,10] (35,350] (10,35] (10,35] (10,35] (35,350] (35,350] ## [185] [0,1] (10,35] (35,350] (10,35] (10,35] (2,10] (35,350] (10,35] ## [193] (35,350] (10,35] (35,350] (10,35] (35,350] (10,35] (35,350] [0,1] ## [201] (10,35] (35,350] (35,350] [0,1] [0,1] (2,10] (35,350] (2,10] ## [209] (1,2] (2,10] (10,35] (2,10] [0,1] (2,10] (35,350] (10,35] ## [217] (35,350] (10,35] (35,350] (10,35] (1,2] (10,35] (2,10] (35,350] ## [225] (2,10] (10,35] (10,35] (10,35] [0,1] (10,35] (10,35] (10,35] ## [233] (35,350] (10,35] (2,10] (2,10] [0,1] [0,1] [0,1] [0,1] ## [241] (2,10] (1,2] [0,1] [0,1] [0,1] (2,10] (10,35] [0,1] ## [249] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [257] [0,1] [0,1] [0,1] [0,1] (35,350] (2,10] [0,1] (35,350] ## [265] (2,10] (35,350] (2,10] (1,2] (35,350] (2,10] (2,10] (2,10] ## [273] (1,2] (10,35] (2,10] (10,35] (35,350] (2,10] (35,350] (35,350] ## [281] (2,10] (2,10] (35,350] (10,35] (2,10] (35,350] (10,35] (2,10] ## [289] (1,2] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] ## [297] (1,2] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [305] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [313] [0,1] (1,2] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [321] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] [0,1] [0,1] ## [329] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [337] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [345] [0,1] (2,10] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [353] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] (1,2] [0,1] ## [361] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] [0,1] ## [369] [0,1] [0,1] (1,2] [0,1] [0,1] [0,1] (35,350] (10,35] ## [377] (35,350] (35,350] (35,350] (10,35] (35,350] (35,350] (35,350] (35,350] ## [385] (2,10] (2,10] (2,10] (10,35] (35,350] (10,35] (10,35] (10,35] ## [393] (10,35] (10,35] (10,35] (10,35] (35,350] (10,35] (35,350] (2,10] ## [401] (10,35] (1,2] (10,35] (35,350] (1,2] [0,1] (2,10] [0,1] ## [409] (2,10] (1,2] ## Levels: [0,1] (1,2] (2,10] (10,35] (35,350] match.arg() handles throwing an error if the user types a value that wasn’t specified. 12.2.2 Pass arguments between functions 12.2.2.1 Harmonic mean The harmonic mean is the reciprocal of the arithmetic mean of the reciprocal of the data. The harmonic mean is often used to average ratio data. You’ll be using it on the price/earnings ratio of stocks in the Standard and Poor’s 500 index, provided as std_and_poor500. Price/earnings ratio is a measure of how expensive a stock is. std_and_poor500 &lt;- read_rds(&quot;data/std_and_poor500_with_pe_2019-06-21.rds&quot;) glimpse(std_and_poor500) ## Rows: 505 ## Columns: 5 ## $ symbol &lt;chr&gt; &quot;MMM&quot;, &quot;ABT&quot;, &quot;ABBV&quot;, &quot;ABMD&quot;, &quot;ACN&quot;, &quot;ATVI&quot;, &quot;ADBE&quot;, &quot;AMD&quot;, &quot;… ## $ company &lt;chr&gt; &quot;3M Company&quot;, &quot;Abbott Laboratories&quot;, &quot;AbbVie Inc.&quot;, &quot;ABIOMED … ## $ sector &lt;chr&gt; &quot;Industrials&quot;, &quot;Health Care&quot;, &quot;Health Care&quot;, &quot;Health Care&quot;, &quot;… ## $ industry &lt;chr&gt; &quot;Industrial Conglomerates&quot;, &quot;Health Care Equipment&quot;, &quot;Pharmac… ## $ pe_ratio &lt;dbl&gt; 18.31678, 57.66621, 22.43805, 45.63993, 27.00233, 20.13596, 5… # Write a function to calculate the reciprocal get_reciprocal &lt;- function(x) { 1/x } # Write a function to calculate the harmonic mean calc_harmonic_mean &lt;- function(x) { x %&gt;% get_reciprocal() %&gt;% mean() %&gt;% get_reciprocal() } # Calculate each sector Price/earnings ratio std_and_poor500 %&gt;% # Group by sector group_by(sector) %&gt;% # Summarize, calculating harmonic mean of P/E ratio summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio)) ## # A tibble: 11 × 2 ## sector hmean_pe_ratio ## &lt;chr&gt; &lt;dbl&gt; ## 1 Communication Services NA ## 2 Consumer Discretionary NA ## 3 Consumer Staples NA ## 4 Energy NA ## 5 Financials NA ## 6 Health Care NA ## 7 Industrials NA ## 8 Information Technology NA ## 9 Materials NA ## 10 Real Estate 32.5 ## 11 Utilities NA It looks like we have a problem though: most sectors have missing values. 12.2.2.2 Handling missing values It would be useful for your function to be able to remove missing values before calculating. Rather than writing your own code for this, you can outsource this functionality to mean(). # Add an na.rm arg with a default, and pass it to mean() calc_harmonic_mean &lt;- function(x, na.rm = FALSE) { x %&gt;% get_reciprocal() %&gt;% mean(na.rm = na.rm) %&gt;% get_reciprocal() } std_and_poor500 %&gt;% # Group by sector group_by(sector) %&gt;% # Summarize, calculating harmonic mean of P/E ratio, and remove missing value summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE)) ## # A tibble: 11 × 2 ## sector hmean_pe_ratio ## &lt;chr&gt; &lt;dbl&gt; ## 1 Communication Services 17.5 ## 2 Consumer Discretionary 15.2 ## 3 Consumer Staples 19.8 ## 4 Energy 13.7 ## 5 Financials 12.9 ## 6 Health Care 26.6 ## 7 Industrials 18.2 ## 8 Information Technology 21.6 ## 9 Materials 16.3 ## 10 Real Estate 32.5 ## 11 Utilities 23.9 12.2.2.3 Using ... The tradeoff Benefits Less typing for you No need to match signatures Drawbacks You need to trust the inner function The interface is not as obvious to users Rather than explicitly giving calc_harmonic_mean() and na.rm argument, you can use ... to simply “pass other arguments” to mean(). # Swap na.rm arg for ... in signature and body calc_harmonic_mean &lt;- function(x, ...) { x %&gt;% get_reciprocal() %&gt;% mean(...) %&gt;% get_reciprocal() } std_and_poor500 %&gt;% # Group by sector group_by(sector) %&gt;% # Summarize, calculating harmonic mean of P/E ratio, and remove missing value summarise(hmean_pe_ratio = calc_harmonic_mean(pe_ratio, na.rm = TRUE)) ## # A tibble: 11 × 2 ## sector hmean_pe_ratio ## &lt;chr&gt; &lt;dbl&gt; ## 1 Communication Services 17.5 ## 2 Consumer Discretionary 15.2 ## 3 Consumer Staples 19.8 ## 4 Energy 13.7 ## 5 Financials 12.9 ## 6 Health Care 26.6 ## 7 Industrials 18.2 ## 8 Information Technology 21.6 ## 9 Materials 16.3 ## 10 Real Estate 32.5 ## 11 Utilities 23.9 Using ... doesn’t change how people use your function; it just means the function is more flexible. 12.2.3 Checking arguments 12.2.3.1 Throwing errors If a user provides a bad input to a function, the best course of action is to throw an error letting them know. The two rules are Throw the error message as soon as you realize there is a problem (typically at the start of the function). Make the error message easily understandable. You can use the assert() functions from assert package to check inputs and throw errors when they fail. (no output means no problem) library(assert) calc_harmonic_mean &lt;- function(x, na.rm = FALSE) { # Assert that x is numeric assert(is.numeric(x)) x %&gt;% get_reciprocal() %&gt;% mean(na.rm = na.rm) %&gt;% get_reciprocal() } # See what happens when you pass it strings calc_harmonic_mean(std_and_poor500$sector) Error: in calc_harmonic_mean(x = std_and_poor500$sector) Failed checks: is.numeric(x) 12.2.3.2 Custom error logic Sometimes the assert() functions in assert don’t give the most informative error message. For example, won’t say why that’s a problem. In that case, you can use the is_*() functions in conjunction with messages, warnings, or errors to define custom feedback. The harmonic mean only makes sense when x has all positive values. calc_harmonic_mean &lt;- function(x, na.rm = FALSE) { assert(is.numeric(x)) # Check if any values of x are non-positive if(any(x &lt; 0, na.rm = TRUE)) { # Throw an error stop(&quot;x contains non-positive values, so the harmonic mean makes no sense.&quot;) } x %&gt;% get_reciprocal() %&gt;% mean(na.rm = na.rm) %&gt;% get_reciprocal() } # See what happens when you pass it negative numbers calc_harmonic_mean(std_and_poor500$pe_ratio - 20) Error in calc_harmonic_mean(std_and_poor500$pe_ratio - 20) : x contains non-positive values, so the harmonic mean makes no sense. Explaining what went wrong is helpful to users. Explaining why it is wrong is even better! 12.2.3.3 Fixing function arguments You still need to provide some checks on the na.rm argument. This time, rather than throwing errors when the input is in an incorrect form, you are going to try to fix it. na.rm should be a logical vector with one element. Fix the na.rm argument by using use_first() to select the first na.rm element, and coerce_to() to change it to logical. library(assert) ## Warning: package &#39;assert&#39; was built under R version 4.3.1 library(assertive.base) ## Warning: package &#39;assertive.base&#39; was built under R version 4.3.2 # Update the function definition to fix the na.rm argument calc_harmonic_mean &lt;- function(x, na.rm = FALSE) { assert(is.numeric(x)) if(any(x &lt; 0, na.rm = TRUE)) { stop(&quot;x contains non-positive values, so the harmonic mean makes no sense.&quot;) } # Use the first value of na.rm, and coerce to logical na.rm &lt;- coerce_to(use_first(na.rm), target_class = &quot;logical&quot;) x %&gt;% get_reciprocal() %&gt;% mean(na.rm = na.rm) %&gt;% get_reciprocal() } # See what happens when you pass it malformed na.rm calc_harmonic_mean(std_and_poor500$pe_ratio, na.rm = 1:5) ## Warning: Only the first value of na.rm (= 1) will be used. ## Warning: Coercing use_first(na.rm) to class &#39;logical&#39;. ## [1] 18.23871 12.3 Return Values and Scope 12.3.1 Returning values Reasons for returning early You already know the answer. The input is an edge case. 12.3.1.1 Returning early Sometimes, you don’t need to run through the whole body of a function to get the answer. In that case you can return early from that function using return(). Returning early can often improve the performance of your functions considerably for some input values. You need to know that a leap year is every 400th year (like the year 2000) or every 4th year that isn’t a century (like 1904 but not 1900 or 1905). # checking for the cases of year being divisible by 400, then 100, then 4, returning early from the function in each case. is_leap_year &lt;- function(year) { # If year is div. by 400 return TRUE if(year %% 400 == 0) { return(TRUE) } # If year is div. by 100 return FALSE if(year %% 100 == 0) { return(FALSE) } # If year is div. by 4 return TRUE if(year %% 4 == 0) { return(TRUE) } # Otherwise return FALSE else { return(FALSE) } } 12.3.1.2 Returning invisibly When the main purpose of a function is to generate output, like drawing a plot or printing something in the console, you may not want a return value to be printed as well. In that case, the value should be invisibly returned: invisible() The base R plot function returns NULL, since its main purpose is to draw a plot. Recall that plot() has a formula interface: plot(y ~ x, data = data) # Using cars, draw a scatter plot of dist vs. speed plt_dist_vs_speed &lt;- base::plot(dist ~ speed, data = cars) # Oh no! The plot object is NULL plt_dist_vs_speed This isn’t helpful if you want to use it in piped code: instead it should invisibly return the plot data to be piped on to the next step. # Define a pipeable plot fn with data and formula args pipeable_plot &lt;- function(data, formula) { # Call plot() with the formula interface base::plot(formula, data) # Invisibly return the input dataset invisible(data) } # Draw the scatter plot of dist vs. speed again plt_dist_vs_speed &lt;- cars %&gt;% pipeable_plot(dist ~ speed) # Now the plot object has a value plt_dist_vs_speed 12.3.2 Return multiple values R functions can only return a single value, but there are two ways to get around this rule: return several objects in a list store objects as attributes When to use each technique: If you need the result to have a particular type, add additional return values as attributes. Otherwise, collect all return values into a list 12.3.2.1 Returning many things Functions can only return one value. If you want to return multiple things, then you can store them all in a list. Multi-assignment If users want to have the list items as separate variables, they can assign each list element to its own variable. Using zeallot’s multi-assignment operator, %&lt;-%, e.g, c(a, b) %&lt;-% list(2 elements) broom package: glance(): model tidy(): coefficient augment(): observation library(broom) library(zeallot) ## Warning: package &#39;zeallot&#39; was built under R version 4.3.2 # Look at the structure of model (it&#39;s a mess!) str(model) ## List of 31 ## $ coefficients : Named num [1:7] 4.0864 0.374 -0.0199 -0.5807 -0.5782 ... ## ..- attr(*, &quot;names&quot;)= chr [1:7] &quot;(Intercept)&quot; &quot;genderfemale&quot; &quot;income($25k,$55k]&quot; &quot;income($55k,$95k]&quot; ... ## $ residuals : Named num [1:346] -0.535 -0.768 -0.944 -0.662 -0.767 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ fitted.values : Named num [1:346] 4.3 4.3 17.83 2.96 4.29 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ effects : Named num [1:346] -360 -29.2 20.3 -10 23.4 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;(Intercept)&quot; &quot;genderfemale&quot; &quot;income($25k,$55k]&quot; &quot;income($55k,$95k]&quot; ... ## $ R : num [1:7, 1:7] -97.4 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:7] &quot;(Intercept)&quot; &quot;genderfemale&quot; &quot;income($25k,$55k]&quot; &quot;income($55k,$95k]&quot; ... ## .. ..$ : chr [1:7] &quot;(Intercept)&quot; &quot;genderfemale&quot; &quot;income($25k,$55k]&quot; &quot;income($55k,$95k]&quot; ... ## $ rank : int 7 ## $ qr :List of 5 ## ..$ qr : num [1:346, 1:7] -97.3861 0.0213 0.0434 0.0177 0.0213 ... ## .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. ..$ : chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## .. .. ..$ : chr [1:7] &quot;(Intercept)&quot; &quot;genderfemale&quot; &quot;income($25k,$55k]&quot; &quot;income($55k,$95k]&quot; ... ## ..$ rank : int 7 ## ..$ qraux: num [1:7] 1.02 1.02 1.04 1.01 1 ... ## ..$ pivot: int [1:7] 1 2 3 4 5 6 7 ## ..$ tol : num 1e-11 ## ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot; ## $ family :List of 13 ## ..$ family : chr &quot;poisson&quot; ## ..$ link : chr &quot;log&quot; ## ..$ linkfun :function (mu) ## ..$ linkinv :function (eta) ## ..$ variance :function (mu) ## ..$ dev.resids:function (y, mu, wt) ## ..$ aic :function (y, n, mu, wt, dev) ## ..$ mu.eta :function (eta) ## ..$ initialize: expression({ if (any(y &lt; 0)) stop(&quot;negative values not allowed for the &#39;Poisson&#39; family&quot;) n &lt;- rep.int(1, nobs| __truncated__ ## ..$ validmu :function (mu) ## ..$ valideta :function (eta) ## ..$ simulate :function (object, nsim) ## ..$ dispersion: num 1 ## ..- attr(*, &quot;class&quot;)= chr &quot;family&quot; ## $ linear.predictors: Named num [1:346] 1.46 1.46 2.88 1.09 1.46 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ deviance : num 11529 ## $ aic : num 12864 ## $ null.deviance : num 18850 ## $ iter : int 6 ## $ weights : Named num [1:346] 4.3 4.3 17.83 2.96 4.29 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ prior.weights : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ df.residual : int 339 ## $ df.null : int 345 ## $ y : Named num [1:346] 2 1 1 1 1 1 80 104 55 350 ... ## ..- attr(*, &quot;names&quot;)= chr [1:346] &quot;25&quot; &quot;26&quot; &quot;27&quot; &quot;29&quot; ... ## $ converged : logi TRUE ## $ boundary : logi FALSE ## $ model :&#39;data.frame&#39;: 346 obs. of 4 variables: ## ..$ n_visits: num [1:346] 2 1 1 1 1 1 80 104 55 350 ... ## ..$ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 2 1 2 2 1 2 ... ## ..$ income : Factor w/ 4 levels &quot;[$0,$25k]&quot;,&quot;($25k,$55k]&quot;,..: 4 4 4 4 3 1 1 4 2 2 ... ## ..$ travel : Factor w/ 3 levels &quot;[0h,0.25h]&quot;,&quot;(0.25h,4h]&quot;,..: 3 3 2 3 3 1 1 1 2 1 ... ## ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39; language n_visits ~ gender + income + travel ## .. .. ..- attr(*, &quot;variables&quot;)= language list(n_visits, gender, income, travel) ## .. .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... ## .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. .. ..$ : chr [1:4] &quot;n_visits&quot; &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. .. .. .. ..$ : chr [1:3] &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 ## .. .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. .. ..- attr(*, &quot;response&quot;)= int 1 ## .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x000001cc54816658&gt; ## .. .. ..- attr(*, &quot;predvars&quot;)= language list(n_visits, gender, income, travel) ## .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;n_visits&quot; &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## ..- attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:64] 1 2 3 4 5 6 7 8 9 10 ... ## .. ..- attr(*, &quot;names&quot;)= chr [1:64] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ na.action : &#39;omit&#39; Named int [1:64] 1 2 3 4 5 6 7 8 9 10 ... ## ..- attr(*, &quot;names&quot;)= chr [1:64] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ... ## $ call : language glm(formula = formula, family = poisson, data = data) ## $ formula :Class &#39;formula&#39; language n_visits ~ gender + income + travel ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x000001cc54816658&gt; ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language n_visits ~ gender + income + travel ## .. ..- attr(*, &quot;variables&quot;)= language list(n_visits, gender, income, travel) ## .. ..- attr(*, &quot;factors&quot;)= int [1:4, 1:3] 0 1 0 0 0 0 1 0 0 0 ... ## .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. .. .. ..$ : chr [1:4] &quot;n_visits&quot; &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. .. .. ..$ : chr [1:3] &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. ..- attr(*, &quot;term.labels&quot;)= chr [1:3] &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## .. ..- attr(*, &quot;order&quot;)= int [1:3] 1 1 1 ## .. ..- attr(*, &quot;intercept&quot;)= int 1 ## .. ..- attr(*, &quot;response&quot;)= int 1 ## .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: 0x000001cc54816658&gt; ## .. ..- attr(*, &quot;predvars&quot;)= language list(n_visits, gender, income, travel) ## .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:4] &quot;numeric&quot; &quot;factor&quot; &quot;factor&quot; &quot;factor&quot; ## .. .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;n_visits&quot; &quot;gender&quot; &quot;income&quot; &quot;travel&quot; ## $ data :&#39;data.frame&#39;: 410 obs. of 4 variables: ## ..$ n_visits: num [1:410] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 1 1 1 2 1 2 2 2 1 1 ... ## ..$ income : Factor w/ 4 levels &quot;[$0,$25k]&quot;,&quot;($25k,$55k]&quot;,..: 4 2 4 2 4 2 4 4 4 4 ... ## ..$ travel : Factor w/ 3 levels &quot;[0h,0.25h]&quot;,&quot;(0.25h,4h]&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## $ offset : NULL ## $ control :List of 3 ## ..$ epsilon: num 1e-08 ## ..$ maxit : num 25 ## ..$ trace : logi FALSE ## $ method : chr &quot;glm.fit&quot; ## $ contrasts :List of 3 ## ..$ gender: chr &quot;contr.treatment&quot; ## ..$ income: chr &quot;contr.treatment&quot; ## ..$ travel: chr &quot;contr.treatment&quot; ## $ xlevels :List of 3 ## ..$ gender: chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ income: chr [1:4] &quot;[$0,$25k]&quot; &quot;($25k,$55k]&quot; &quot;($55k,$95k]&quot; &quot;($95k,$Inf)&quot; ## ..$ travel: chr [1:3] &quot;[0h,0.25h]&quot; &quot;(0.25h,4h]&quot; &quot;(4h,Infh)&quot; ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;glm&quot; &quot;lm&quot; # Use broom tools to get a list of 3 data frames list( # Get model-level values model = glance(model), # Get coefficient-level values coefficients = tidy(model), # Get observation-level values observations = augment(model) ) ## $model ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 18850. 345 -6425. 12864. 12891. 11529. 339 346 ## ## $coefficients ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.09 0.0279 146. 0 ## 2 genderfemale 0.374 0.0212 17.6 2.18e- 69 ## 3 income($25k,$55k] -0.0199 0.0267 -0.746 4.56e- 1 ## 4 income($55k,$95k] -0.581 0.0343 -16.9 3.28e- 64 ## 5 income($95k,$Inf) -0.578 0.0310 -18.7 6.88e- 78 ## 6 travel(0.25h,4h] -0.627 0.0217 -28.8 5.40e-183 ## 7 travel(4h,Infh) -2.42 0.0492 -49.3 0 ## ## $observations ## # A tibble: 346 × 11 ## .rownames n_visits gender income travel .fitted .resid .hat .sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 2 female ($95k,$Inf) (4h,Inf… 1.46 -1.24 0.0109 5.84 ## 2 26 1 female ($95k,$Inf) (4h,Inf… 1.46 -1.92 0.0109 5.84 ## 3 27 1 male ($95k,$Inf) (0.25h,… 2.88 -5.28 0.0129 5.83 ## 4 29 1 male ($95k,$Inf) (4h,Inf… 1.09 -1.32 0.00711 5.84 ## 5 30 1 female ($55k,$95k] (4h,Inf… 1.46 -1.92 0.0121 5.84 ## 6 31 1 male [$0,$25k] [0h,0.2… 4.09 -10.4 0.0465 5.81 ## 7 33 80 female [$0,$25k] [0h,0.2… 4.46 -0.710 0.0479 5.84 ## 8 34 104 female ($95k,$Inf) [0h,0.2… 3.88 6.90 0.0332 5.83 ## 9 35 55 male ($25k,$55k] (0.25h,… 3.44 3.85 0.0153 5.84 ## 10 36 350 female ($25k,$55k] [0h,0.2… 4.44 21.5 0.0360 5.72 ## # ℹ 336 more rows ## # ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; # Wrap this code into a function, groom_model groom_model &lt;- function(model) { list( model = glance(model), coefficients = tidy(model), observations = augment(model) ) } # Call groom_model on model, assigning to 3 variables c(mdl, cff, obs) %&lt;-% groom_model(model) # See these individual variables mdl; cff; obs ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 18850. 345 -6425. 12864. 12891. 11529. 339 346 ## # A tibble: 7 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 4.09 0.0279 146. 0 ## 2 genderfemale 0.374 0.0212 17.6 2.18e- 69 ## 3 income($25k,$55k] -0.0199 0.0267 -0.746 4.56e- 1 ## 4 income($55k,$95k] -0.581 0.0343 -16.9 3.28e- 64 ## 5 income($95k,$Inf) -0.578 0.0310 -18.7 6.88e- 78 ## 6 travel(0.25h,4h] -0.627 0.0217 -28.8 5.40e-183 ## 7 travel(4h,Infh) -2.42 0.0492 -49.3 0 ## # A tibble: 346 × 11 ## .rownames n_visits gender income travel .fitted .resid .hat .sigma ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25 2 female ($95k,$Inf) (4h,Inf… 1.46 -1.24 0.0109 5.84 ## 2 26 1 female ($95k,$Inf) (4h,Inf… 1.46 -1.92 0.0109 5.84 ## 3 27 1 male ($95k,$Inf) (0.25h,… 2.88 -5.28 0.0129 5.83 ## 4 29 1 male ($95k,$Inf) (4h,Inf… 1.09 -1.32 0.00711 5.84 ## 5 30 1 female ($55k,$95k] (4h,Inf… 1.46 -1.92 0.0121 5.84 ## 6 31 1 male [$0,$25k] [0h,0.2… 4.09 -10.4 0.0465 5.81 ## 7 33 80 female [$0,$25k] [0h,0.2… 4.46 -0.710 0.0479 5.84 ## 8 34 104 female ($95k,$Inf) [0h,0.2… 3.88 6.90 0.0332 5.83 ## 9 35 55 male ($25k,$55k] (0.25h,… 3.44 3.85 0.0153 5.84 ## 10 36 350 female ($25k,$55k] [0h,0.2… 4.44 21.5 0.0360 5.72 ## # ℹ 336 more rows ## # ℹ 2 more variables: .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt; 12.3.2.2 Returning metadata Sometimes you want to return multiple things from a function, but you want the result to have a particular class (for example, a data frame or a numeric vector), so returning a list isn’t appropriate. This is common when you have a result plus metadata about the result. (Metadata is “data about the data”. For example, it could be the file a dataset was loaded from) In that case, you can store the metadata in attributes. attr(object, &quot;attribute_name&quot;) &lt;- attribute_value pipeable_plot &lt;- function(data, formula) { plot(formula, data) # Add a &quot;formula&quot; attribute to data attr(data, &quot;formula&quot;) &lt;- formula invisible(data) } # From previous exercise plt_dist_vs_speed &lt;- cars %&gt;% pipeable_plot(dist ~ speed) # Examine the structure of the result str(plt_dist_vs_speed) 12.3.3 Environments Environments are a type of variable that is used to store other variables. Mostly, you can think of them as lists. Every environment has a parent environment (except the empty environment, at the root of the environment tree). This determines which variables R know about at different places in your code. 12.3.3.1 Create &amp; explore environments ls.str(list): list the elements, give a compact summary of multiple variables at once. list2env(list): convert the list to an environment. environmentName(parent.env(env)): find the parent of an environment, and display it. Facts about the Republic of South Africa are contained in capitals, national_parks, and population. # create capitals city &lt;- c(&quot;Cape Town&quot;, &quot;Bloemfontein&quot;, &quot;Pretoria&quot;) type_of_capital &lt;- c(&quot;Legislative&quot;, &quot;Judicial&quot;, &quot;Administrative&quot;) capitals &lt;- data.frame(city, type_of_capital) # create national_parks national_parks &lt;- c( &quot;Addo Elephant National Park&quot;, &quot;Agulhas National Park&quot;, &quot;Ai-Ais/Richtersveld Transfrontier Park&quot;, &quot;Augrabies Falls National Park&quot;, &quot;Bontebok National Park&quot;, &quot;Camdeboo National Park&quot;, &quot;Golden Gate Highlands National Park&quot;, &quot;Hluhluwe–Imfolozi Park&quot;, &quot;Karoo National Park&quot;, &quot;Kgalagadi Transfrontier Park&quot;, &quot;Knysna National Lake Area&quot;, &quot;Kruger National Park&quot;, &quot;Mapungubwe National Park&quot;, &quot;Marakele National Park&quot;, &quot;Mokala National Park&quot;, &quot;Mountain Zebra National Park&quot;, &quot;Namaqua National Park&quot;, &quot;Table Mountain National Park&quot;, &quot;Tankwa Karoo National Park&quot;, &quot;Tsitsikamma National Park&quot;, &quot;West Coast National Park&quot;, &quot;Wilderness National Park&quot;) # create population population &lt;- ts(c(40583573, 44819778, 47390900, 51770560, 55908900), start = 1996, end = 2016, frequency = 0.2); population ## Time Series: ## Start = 1996 ## End = 2016 ## Frequency = 0.2 ## [1] 40583573 44819778 47390900 51770560 55908900 # Add capitals, national_parks, &amp; population to a named list rsa_lst &lt;- list( capitals = capitals, national_parks = national_parks, population = population ) # List the structure of each element of rsa_lst ls.str(rsa_lst) ## capitals : &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ city : chr &quot;Cape Town&quot; &quot;Bloemfontein&quot; &quot;Pretoria&quot; ## $ type_of_capital: chr &quot;Legislative&quot; &quot;Judicial&quot; &quot;Administrative&quot; ## national_parks : chr [1:22] &quot;Addo Elephant National Park&quot; &quot;Agulhas National Park&quot; ... ## population : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900 # Convert the list to an environment rsa_env &lt;- list2env(rsa_lst) # List the structure of each variable ls.str(rsa_env) ## capitals : &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ city : chr &quot;Cape Town&quot; &quot;Bloemfontein&quot; &quot;Pretoria&quot; ## $ type_of_capital: chr &quot;Legislative&quot; &quot;Judicial&quot; &quot;Administrative&quot; ## national_parks : chr [1:22] &quot;Addo Elephant National Park&quot; &quot;Agulhas National Park&quot; ... ## population : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900 # Find the parent environment of rsa_env parent &lt;- parent.env(rsa_env) # Print its name environmentName(parent) ## [1] &quot;R_GlobalEnv&quot; 12.3.3.2 Do variables exist? If R cannot find a variable in the current environment, it will look in the parent environment, then the grandparent environment, and so on until it finds it. exists(\"variable\", envir = env) You can force R to only look in the environment you asked for by setting inherits to FALSE. exists(\"variable\", envir = env, inherits = FALSE) # Compare the contents of the global environment and rsa_env ls.str(globalenv()) ## a : num 5 ## account_offices : spc_tbl_ [98 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## accounts : tibble [97 × 5] (S3: tbl_df/tbl/data.frame) ## accounts_clean : tibble [88 × 6] (S3: tbl_df/tbl/data.frame) ## airquality : spc_tbl_ [153 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## akl_daily : tibble [3,661 × 10] (S3: tbl_df/tbl/data.frame) ## akl_daily_raw : spc_tbl_ [3,661 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## akl_day : tibble [10,905 × 15] (S3: tbl_df/tbl/data.frame) ## akl_hourly : tibble [17,454 × 13] (S3: tbl_df/tbl/data.frame) ## akl_hourly_raw : spc_tbl_ [17,454 × 10] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## all_deals : &#39;data.frame&#39;: 10548 obs. of 2 variables: ## $ product : int 3544 5073 6149 7863 14 10247 3544 5073 6149 7863 ... ## $ num_users: int 19 43 87 83 17 2 29 13 80 23 ... ## all_wars_matrix : num [1:6, 1:2] 461 290 309 474 311 ... ## amir_deals : &#39;data.frame&#39;: 178 obs. of 5 variables: ## $ product : Factor w/ 14 levels &quot;Product A&quot;,&quot;Product B&quot;,..: 6 3 2 9 5 2 3 13 6 2 ... ## $ client : Factor w/ 3 levels &quot;Current&quot;,&quot;New&quot;,..: 1 2 2 1 1 2 1 1 1 1 ... ## $ status : Factor w/ 2 levels &quot;Lost&quot;,&quot;Won&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ amount : num 7390 4493 5738 2591 6623 ... ## $ num_users: num 19 43 87 83 17 2 29 13 80 23 ... ## animals_vector : chr [1:4] &quot;Elephant&quot; &quot;Giraffe&quot; &quot;Donkey&quot; &quot;Horse&quot; ## answer_counts : spc_tbl_ [227,452 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## answers : spc_tbl_ [451,118 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## answers_with_tags : spc_tbl_ [800,378 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## april_10_2014 : Date[1:1], format: &quot;2014-04-10&quot; ## astro : Named chr [1:4] &quot;20-Mar-2015&quot; &quot;25-Jun-2015&quot; &quot;23-Sep-2015&quot; &quot;22-Dec-2015&quot; ## astro_dates : Date[1:4], format: &quot;2015-03-20&quot; &quot;2015-06-25&quot; &quot;2015-09-23&quot; &quot;2015-12-22&quot; ## avg_review : num 4.72 ## avg_sum : num 22.3 ## avg_sum_trimmed : num 22.6 ## awards : chr [1:6] &quot;Won 1 Oscar.&quot; &quot;Won 1 Oscar. Another 9 wins &amp; 24 nominations.&quot; ... ## babynames_fraction : tibble [1,924,665 × 7] (S3: tbl_df/tbl/data.frame) ## babynames_ratios_filtered : gropd_df [541,903 × 8] (S3: grouped_df/tbl_df/tbl/data.frame) ## basics : function (x) ## batman : tibble [10,111 × 13] (S3: tbl_df/tbl/data.frame) ## batman_colors : tibble [56 × 3] (S3: tbl_df/tbl/data.frame) ## batman_parts : tibble [3,455 × 3] (S3: tbl_df/tbl/data.frame) ## batmobile : tibble [173 × 5] (S3: tbl_df/tbl/data.frame) ## batwing : tibble [323 × 5] (S3: tbl_df/tbl/data.frame) ## below_zero : function (x) ## bike_share_rides : spc_tbl_ [35,231 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## bike_share_rides_past : tibble [34,025 × 10] (S3: tbl_df/tbl/data.frame) ## bike_share_rides_unique : tibble [35,229 × 6] (S3: tbl_df/tbl/data.frame) ## blue_range : function (n) ## blues : chr [1:9] &quot;#F7FBFF&quot; &quot;#DEEBF7&quot; &quot;#C6DBEF&quot; &quot;#9ECAE1&quot; &quot;#6BAED6&quot; &quot;#4292C6&quot; ... ## box_office : num [1:6] 461 314 290 248 309 ... ## breaks : num [1:4] 2.27 0 1440 1668 ## by_continent : tibble [5 × 2] (S3: tbl_df/tbl/data.frame) ## by_continent_2007 : tibble [5 × 3] (S3: tbl_df/tbl/data.frame) ## by_type_year_tag : tibble [57,331 × 4] (S3: tbl_df/tbl/data.frame) ## by_type_year_tag_filtered : tibble [38 × 4] (S3: tbl_df/tbl/data.frame) ## by_year : tibble [12 × 2] (S3: tbl_df/tbl/data.frame) ## by_year_continent : gropd_df [60 × 3] (S3: grouped_df/tbl_df/tbl/data.frame) ## c : int 3 ## calc_harmonic_mean : function (x, na.rm = FALSE) ## capitals : &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ city : chr &quot;Cape Town&quot; &quot;Bloemfontein&quot; &quot;Pretoria&quot; ## $ type_of_capital: chr &quot;Legislative&quot; &quot;Judicial&quot; &quot;Administrative&quot; ## cff : tibble [7 × 5] (S3: tbl_df/tbl/data.frame) ## char : chr &quot;u&quot; ## chars : chr [1:40] &quot;r&quot; &quot;&#39;&quot; &quot;s&quot; &quot; &quot; &quot;i&quot; &quot;n&quot; &quot;t&quot; &quot;e&quot; &quot;r&quot; &quot;n&quot; &quot;a&quot; &quot;l&quot; &quot;s&quot; &quot; &quot; &quot;a&quot; &quot;r&quot; ... ## cities : &#39;data.frame&#39;: 5 obs. of 1 variable: ## $ city_actual: chr &quot;new york&quot; &quot;los angeles&quot; &quot;atlanta&quot; &quot;san francisco&quot; ... ## city : chr [1:3] &quot;Cape Town&quot; &quot;Bloemfontein&quot; &quot;Pretoria&quot; ## coin_sides : chr [1:2] &quot;head&quot; &quot;tail&quot; ## color_palette : Named chr [1:42] &quot;#05131D&quot; &quot;#0055BF&quot; &quot;#237841&quot; &quot;#C91A09&quot; &quot;#F2CD37&quot; ... ## colors : spc_tbl_ [262 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## colors_joined : tibble [42 × 10] (S3: tbl_df/tbl/data.frame) ## cols : chr [1:8] &quot;country&quot; &quot;year_1960&quot; &quot;year_1961&quot; &quot;year_1962&quot; &quot;year_1963&quot; ... ## comments : chr [1:5] &quot;I would watch it again&quot; &quot;Amazing!&quot; &quot;I liked it&quot; ... ## counties : spc_tbl_ [3,138 × 40] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## counties_selected : tibble [3,138 × 5] (S3: tbl_df/tbl/data.frame) ## countries : &#39;data.frame&#39;: 209 obs. of 1 variable: ## $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## cut_by_quantile : function (x, n = 5, na.rm = FALSE, labels = NULL, interval_type = c(&quot;(lo, hi]&quot;, ## &quot;[lo, hi)&quot;)) ## da2 : Ord.factor w/ 3 levels &quot;slow&quot;&lt;&quot;medium&quot;&lt;..: 1 ## da5 : Ord.factor w/ 3 levels &quot;slow&quot;&lt;&quot;medium&quot;&lt;..: 3 ## datacamp_light_blue : chr &quot;#51A8C9&quot; ## date_difference : Formal class &#39;Interval&#39; [package &quot;lubridate&quot;] with 3 slots ## date_landing : Date[1:1], format: &quot;1969-07-20&quot; ## date_stamp : function (x, locale = &quot;C&quot;) ## date1 : Date[1:1], format: &quot;1996-05-23&quot; ## date2 : Date[1:1], format: &quot;2012-03-15&quot; ## date3 : Date[1:1], format: &quot;2006-01-30&quot; ## dates : chr [1:17454] &quot;2016-01-01T00:00:00Z&quot; &quot;2016-01-01T00:30:00Z&quot; ... ## day_diff : &#39;difftime&#39; num [1:4] 2 5 6 5 ## day1 : Date[1:1], format: &quot;2023-09-15&quot; ## day2 : Date[1:1], format: &quot;2023-09-17&quot; ## day3 : Date[1:1], format: &quot;2023-09-22&quot; ## day4 : Date[1:1], format: &quot;2023-09-28&quot; ## day5 : Date[1:1], format: &quot;2023-10-03&quot; ## days_vector : chr [1:5] &quot;Monday&quot; &quot;Tuesday&quot; &quot;Wednesday&quot; &quot;Thursday&quot; &quot;Friday&quot; ## deals : int [1:52] 1 1 0 0 0 0 1 1 1 1 ... ## departed_list : List of 4 ## $ title : chr &quot;The Departed&quot; ## $ actors : chr [1:6] &quot;Leonardo DiCaprio&quot; &quot;Matt Damon&quot; &quot;Jack Nicholson&quot; &quot;Mark Wahlberg&quot; ... ## $ reviews:&#39;data.frame&#39;: 5 obs. of 2 variables: ## $ average: num 4.72 ## dest_size : chr [1:4] &quot;Small&quot; &quot;Medium&quot; &quot;Large&quot; &quot;Hub&quot; ## dest_sizes : &#39;data.frame&#39;: 4 obs. of 2 variables: ## $ dest_size : chr &quot;Small&quot; &quot;Medium&quot; &quot;Large&quot; &quot;Hub&quot; ## $ passengers_per_day: chr &quot;0-20K&quot; &quot;20K-70K&quot; &quot;70K-100K&quot; &quot;100K+&quot; ## diameter : num [1:8] 0.382 0.949 1 0.532 11.209 ... ## dims : int [1:2, 1:3] 209 8 209 9 209 38 ## eclipse_2017 : POSIXct[1:1], format: &quot;2017-08-21 18:26:40&quot; ## emails : chr [1:6] &quot;john.doe@ivyleague.edu&quot; &quot;education@world.gov&quot; ... ## emissions_by_country : tibble [130 × 2] (S3: tbl_df/tbl/data.frame) ## empire_strikes : num [1:2] 290 248 ## errors : num [1:6] 1.9 -2.6 4 -9.5 -3.4 7.3 ## europe_categories : chr [1:3] &quot;EU&quot; &quot;eur&quot; &quot;Europ&quot; ## every_two_weeks : Formal class &#39;Period&#39; [package &quot;lubridate&quot;] with 6 slots ## expected_val : num 2.9 ## extremes : function (x) ## extremes_avg : function (x) ## fac : List of 3 ## $ levels : chr [1:3] &quot;Beef&quot; &quot;Meat&quot; &quot;Poultry&quot; ## $ ordered : logi FALSE ## $ include_na: logi FALSE ## facebook : List of 7 ## $ : num 17 ## $ : num 7 ## $ : num 5 ## $ : num 16 ## $ : num 8 ## $ : num 13 ## $ : num 14 ## factor_animals_vector : Factor w/ 4 levels &quot;Donkey&quot;,&quot;Elephant&quot;,..: 2 3 1 4 ## factor_sex_vector : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 ## factor_speed_vector : Ord.factor w/ 3 levels &quot;slow&quot;&lt;&quot;medium&quot;&lt;..: 2 1 1 2 3 ## factor_survey_vector : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 1 1 2 2 ## factor_temperature_vector : Ord.factor w/ 3 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 3 1 3 1 2 ## fb : num 9 ## fb_vec : num [1:7] 17 7 5 16 8 13 14 ## finished : chr &quot;I finished &#39;Dates and Times in R&#39; on Thursday, September 4, 2017!&quot; ## fish : chr [1:3] &quot;fish.species&quot; &quot;fish.year&quot; &quot;fish.tidy&quot; ## fish.species : &#39;data.frame&#39;: 61 obs. of 8 variables: ## $ Year : int 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 ... ## $ Pink : int 100600 259000 132600 235900 123400 244400 203400 270119 200798 200085 ... ## $ Chum : int 139300 155900 113800 99800 148700 143700 158480 125377 132407 113114 ... ## $ Sockeye : int 64100 51200 58200 66100 83800 72000 84800 69676 100520 62472 ... ## $ Coho : int 30500 40900 33600 32400 38300 45100 40000 39900 39200 32865 ... ## $ Rainbow : int 0 100 100 100 100 100 100 100 100 100 ... ## $ Chinook : int 23200 25500 24900 25300 24500 27700 25300 21200 20900 20335 ... ## $ Atlantic: int 10800 9701 9800 8800 9600 7800 8100 9000 8801 8700 ... ## fish.tidy : &#39;data.frame&#39;: 427 obs. of 3 variables: ## $ Species: Factor w/ 7 levels &quot;Pink&quot;,&quot;Chum&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Year : int 1950 1951 1952 1953 1954 1955 1956 1957 1958 1959 ... ## $ Capture: int 100600 259000 132600 235900 123400 244400 203400 270119 200798 200085 ... ## fish.year : &#39;data.frame&#39;: 7 obs. of 62 variables: ## $ Species: Factor w/ 7 levels &quot;Pink&quot;,&quot;Chum&quot;,..: 1 2 3 4 5 6 7 ## $ 1950 : int 100600 139300 64100 30500 0 23200 10800 ## $ 1951 : int 259000 155900 51200 40900 100 25500 9701 ## $ 1952 : int 132600 113800 58200 33600 100 24900 9800 ## $ 1953 : int 235900 99800 66100 32400 100 25300 8800 ## $ 1954 : int 123400 148700 83800 38300 100 24500 9600 ## $ 1955 : int 244400 143700 72000 45100 100 27700 7800 ## $ 1956 : int 203400 158480 84800 40000 100 25300 8100 ## $ 1957 : int 270119 125377 69676 39900 100 21200 9000 ## $ 1958 : int 200798 132407 100520 39200 100 20900 8801 ## $ 1959 : int 200085 113114 62472 32865 100 20335 8700 ## $ 1960 : int 111880 120760 83171 26898 100 17412 8900 ## $ 1961 : int 178133 109053 93311 37524 200 18130 9400 ## $ 1962 : int 163558 110707 66126 39883 300 17489 11500 ## $ 1963 : int 222477 106491 53140 41547 1600 18928 10766 ## $ 1964 : int 143623 94498 56189 38948 2300 20932 14342 ## $ 1965 : int 161131 76503 94286 44207 1500 21197 12363 ## $ 1966 : int 174844 93949 81080 46874 900 21549 12337 ## $ 1967 : int 168960 88027 75582 43396 1700 21375 14411 ## $ 1968 : int 155729 95624 113332 46463 11200 20720 12823 ## $ 1969 : int 195169 67888 65106 30973 3000 21915 13316 ## $ 1970 : int 133466 113650 106436 44050 4400 24000 13457 ## $ 1971 : int 178823 105879 79160 43110 4600 25800 12397 ## $ 1972 : int 93852 137655 43932 33880 4900 24000 12665 ## $ 1973 : int 151180 125364 55637 38550 6900 27400 15387 ## $ 1974 : int 94026 121052 53027 42631 8099 24148 15158 ## $ 1975 : int 171012 127561 38615 31782 10302 24137 14736 ## $ 1976 : int 146895 125110 60357 36965 9492 24984 11372 ## $ 1977 : int 224475 118677 64651 30777 6166 26351 11491 ## $ 1978 : int 174314 129513 75737 30712 7219 25417 8948 ## $ 1979 : int 249558 150176 109439 34279 7036 25452 10259 ## $ 1980 : int 226191 166803 111830 30557 9027 23023 12965 ## $ 1981 : int 264855 186550 132651 28702 8819 22789 12425 ## $ 1982 : int 170373 182561 128176 42281 6839 25147 10326 ## $ 1983 : int 255129 196414 163790 30291 7473 18926 10862 ## $ 1984 : int 210511 210507 126834 40834 7977 18449 10562 ## $ 1985 : int 300987 267794 150860 38823 9373 20318 12422 ## $ 1986 : int 211685 239065 136452 42825 9709 21187 11739 ## $ 1987 : int 218121 217129 131135 28173 12196 25475 11413 ## $ 1988 : int 165236 286572 107361 32245 11872 31338 10093 ## $ 1989 : int 363456 243728 169410 31614 12249 20725 15795 ## $ 1990 : int 235190 299438 198168 34635 12529 18249 10860 ## $ 1991 : int 438998 266940 161218 38078 12545 15373 9793 ## $ 1992 : int 216110 238427 199709 38433 9731 15093 9308 ## $ 1993 : int 302568 286996 242613 25451 19091 15036 8161 ## $ 1994 : int 326065 328765 183686 45602 12624 13623 7231 ## $ 1995 : int 394735 424595 189612 28922 13450 13801 6819 ## $ 1996 : int 294915 411395 188584 28201 12407 10509 6493 ## $ 1997 : int 318717 347560 132075 13191 13147 13000 5579 ## $ 1998 : int 371552 311965 78972 19386 18940 9840 4744 ## $ 1999 : int 386928 281260 130128 15449 15874 8735 4107 ## $ 2000 : int 285338 276355 124782 18035 14918 8437 4710 ## $ 2001 : int 360973 307662 108618 20006 14927 8771 4772 ## $ 2002 : int 268544 314098 103325 20757 12241 13911 4139 ## $ 2003 : int 377749 360429 109822 16995 17128 15046 3648 ## $ 2004 : int 266554 351188 142385 24546 16601 15899 4082 ## $ 2005 : int 456350 318389 147151 18791 16886 13571 3727 ## $ 2006 : int 316205 361561 151523 18275 17079 10497 3087 ## $ 2007 : int 506343 331266 164609 17228 14844 8900 3014 ## $ 2008 : int 294876 295819 138896 21280 16819 6443 3002 ## $ 2009 : int 591654 359908 150040 19725 17884 6303 2373 ## $ 2010 : int 370044 143959 170972 20770 20754 7968 2363 ## fodors : &#39;data.frame&#39;: 533 obs. of 7 variables: ## $ id : int 0 1 2 3 4 5 6 7 8 9 ... ## $ name : chr &quot;arnie morton&#39;s of chicago&quot; &quot;art&#39;s delicatessen&quot; &quot;hotel bel-air&quot; &quot;cafe bizou&quot; ... ## $ addr : chr &quot;435 s. la cienega blv .&quot; &quot;12224 ventura blvd.&quot; &quot;701 stone canyon rd.&quot; &quot;14016 ventura blvd.&quot; ... ## $ city : Factor w/ 5 levels &quot;atlanta&quot;,&quot;los angeles&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ phone: chr &quot;310-246-1501&quot; &quot;818-762-1221&quot; &quot;310-472-1211&quot; &quot;818-788-3536&quot; ... ## $ type : chr &quot;american&quot; &quot;american&quot; &quot;californian&quot; &quot;french&quot; ... ## $ class: int 0 1 2 3 4 5 6 7 8 9 ... ## food_consumption : spc_tbl_ [1,430 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## formats : chr [1:2] &quot;%Y-%m-%d&quot; &quot;%B %d, %Y&quot; ## freezing_l : List of 7 ## $ : num -1 ## $ : num(0) ## $ : num [1:2] -1 -3 ## $ : num -2 ## $ : num(0) ## $ : num -3 ## $ : num(0) ## freezing_s : List of 7 ## $ : num -1 ## $ : num(0) ## $ : num [1:2] -1 -3 ## $ : num -2 ## $ : num(0) ## $ : num -3 ## $ : num(0) ## game2 : POSIXct[1:1], format: &quot;2015-06-11 19:00:00&quot; ## game2_local : POSIXct[1:1], format: &quot;2015-06-11 19:00:00&quot; ## game3 : POSIXct[1:1], format: &quot;2015-06-15 18:30:00&quot; ## game3_local : POSIXct[1:1], format: &quot;2015-06-15 18:30:00&quot; ## gapminder_1952 : tibble [142 × 6] (S3: tbl_df/tbl/data.frame) ## get_reciprocal : function (x) ## global_mean : num 67 ## gm2007 : spc_tbl_ [20 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## gm2007_full : spc_tbl_ [142 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## groom_model : function (model) ## group_id : chr [1:10] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; ## group_size : num [1:10] 2 4 6 2 2 2 3 2 4 2 ## growth_by_dose : List of 9 ## $ data : spc_tbl_ [60 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ layers :List of 2 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme :List of 97 ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## halleys : tibble [27 × 7] (S3: tbl_df/tbl/data.frame) ## halleys_1066 : tibble [1 × 7] (S3: tbl_df/tbl/data.frame) ## hello : function () ## hits : int [1:2] 1 5 ## hits_w : int [1:4] 1 2 4 5 ## hotdogs : &#39;data.frame&#39;: 54 obs. of 3 variables: ## $ type : chr &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; &quot;Beef&quot; ... ## $ calories: int 186 181 176 149 184 190 158 139 175 148 ... ## $ sodium : int 495 477 425 322 482 587 370 322 479 375 ... ## hotdogs_factor : spc_tbl_ [54 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## hotdogs2 : &#39;data.frame&#39;: 54 obs. of 2 variables: ## $ type : Factor w/ 3 levels &quot;Beef&quot;,&quot;Meat&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ sodium: num 495 477 425 322 482 587 370 322 479 375 ... ## i : int 6 ## index2 : int 7 ## int : list() ## interpret : function (num_views) ## interpret_all : function (views, return_sum = T) ## inventories : spc_tbl_ [37,233 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## inventory_parts : spc_tbl_ [1,179,869 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## inventory_parts_joined : tibble [1,179,869 × 6] (S3: tbl_df/tbl/data.frame) ## inventory_parts_themes : tibble [1,113,729 × 13] (S3: tbl_df/tbl/data.frame) ## inventory_sets_themes : tibble [1,113,729 × 13] (S3: tbl_df/tbl/data.frame) ## inventory_version_1 : spc_tbl_ [35,612 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## iqr : Named num 664 ## is_leap_year : function (year) ## jan_31 : Date[1:1], format: &quot;2023-01-31&quot; ## last : num 14 ## last_release : spc_tbl_ [1 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## last_release_date : Date[1:1], format: &quot;2017-06-30&quot; ## last2 : num [1:2] 17 14 ## li : num 17 ## li_vec : num [1:7] 16 9 13 5 2 17 14 ## lily : &#39;data.frame&#39;: 1 obs. of 3 variables: ## $ type : chr &quot;Poultry&quot; ## $ calories: int 86 ## $ sodium : int 358 ## linkedin : List of 7 ## $ : num 16 ## $ : num 9 ## $ : num 13 ## $ : num 5 ## $ : num 2 ## $ : num 17 ## $ : num 14 ## login : POSIXct[1:5], format: &quot;2023-09-19 10:18:04&quot; &quot;2023-09-24 09:14:18&quot; &quot;2023-09-24 12:21:51&quot; ... ## logout : POSIXct[1:5], format: &quot;2023-09-19 10:56:29&quot; &quot;2023-09-24 09:14:52&quot; &quot;2023-09-24 12:35:48&quot; ... ## logs : spc_tbl_ [100,000 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## lower : Named num -550 ## mar_11 : POSIXct[1:1], format: &quot;2017-03-11 12:00:00&quot; ## mar_12 : POSIXct[1:1], format: &quot;2017-03-12 12:00:00&quot; ## mar_13 : POSIXct[1:1], format: &quot;2017-03-13 12:00:00&quot; ## matrix2_rowname : chr [1:3] &quot;The Phantom Menace&quot; &quot;Attack of the Clones&quot; ... ## max : num 30 ## mdl : tibble [1 × 8] (S3: tbl_df/tbl/data.frame) ## medium : chr &quot;LinkedIn&quot; ## meteo : Named chr [1:4] &quot;March 1, 15&quot; &quot;June 1, 15&quot; &quot;September 1, 15&quot; ... ## meteo_dates : Date[1:4], format: &quot;2015-03-01&quot; &quot;2015-06-01&quot; &quot;2015-09-01&quot; &quot;2015-12-01&quot; ## millennium_falcon : tibble [239 × 6] (S3: tbl_df/tbl/data.frame) ## millennium_falcon_colors : tibble [19 × 2] (S3: tbl_df/tbl/data.frame) ## min : num 0 ## model : List of 31 ## $ coefficients : Named num [1:7] 4.0864 0.374 -0.0199 -0.5807 -0.5782 ... ## $ residuals : Named num [1:346] -0.535 -0.768 -0.944 -0.662 -0.767 ... ## $ fitted.values : Named num [1:346] 4.3 4.3 17.83 2.96 4.29 ... ## $ effects : Named num [1:346] -360 -29.2 20.3 -10 23.4 ... ## $ R : num [1:7, 1:7] -97.4 0 0 0 0 ... ## $ rank : int 7 ## $ qr :List of 5 ## $ family :List of 13 ## $ linear.predictors: Named num [1:346] 1.46 1.46 2.88 1.09 1.46 ... ## $ deviance : num 11529 ## $ aic : num 12864 ## $ null.deviance : num 18850 ## $ iter : int 6 ## $ weights : Named num [1:346] 4.3 4.3 17.83 2.96 4.29 ... ## $ prior.weights : Named num [1:346] 1 1 1 1 1 1 1 1 1 1 ... ## $ df.residual : int 339 ## $ df.null : int 345 ## $ y : Named num [1:346] 2 1 1 1 1 1 80 104 55 350 ... ## $ converged : logi TRUE ## $ boundary : logi FALSE ## $ model :&#39;data.frame&#39;: 346 obs. of 4 variables: ## $ na.action : &#39;omit&#39; Named int [1:64] 1 2 3 4 5 6 7 8 9 10 ... ## $ call : language glm(formula = formula, family = poisson, data = data) ## $ formula :Class &#39;formula&#39; language n_visits ~ gender + income + travel ## $ terms :Classes &#39;terms&#39;, &#39;formula&#39; language n_visits ~ gender + income + travel ## $ data :&#39;data.frame&#39;: 410 obs. of 4 variables: ## $ offset : NULL ## $ control :List of 3 ## $ method : chr &quot;glm.fit&quot; ## $ contrasts :List of 3 ## $ xlevels :List of 3 ## moment_step : POSIXct[1:1], format: &quot;1969-07-20 02:56:15&quot; ## mon_2pm : POSIXct[1:1], format: &quot;2018-08-27 14:00:00&quot; ## monarchs : tibble [131 × 7] (S3: tbl_df/tbl/data.frame) ## month_seq : Formal class &#39;Period&#39; [package &quot;lubridate&quot;] with 6 slots ## movie_actors : chr [1:6] &quot;Leonardo DiCaprio&quot; &quot;Matt Damon&quot; &quot;Jack Nicholson&quot; ... ## movie_title : chr &quot;The Departed&quot; ## mtcars : &#39;data.frame&#39;: 32 obs. of 16 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp : num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat : num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec : num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear : num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb : num 4 4 1 1 2 1 4 2 2 4 ... ## $ fcyl : Factor w/ 3 levels &quot;4&quot;,&quot;6&quot;,&quot;8&quot;: 2 2 1 2 3 2 3 1 1 2 ... ## $ fam : Factor w/ 2 levels &quot;manual&quot;,&quot;automatic&quot;: 1 1 1 2 2 2 2 2 2 2 ... ## $ car : chr &quot;Mazda RX4&quot; &quot;Mazda RX4 Wag&quot; &quot;Datsun 710&quot; &quot;Hornet 4 Drive&quot; ... ## $ fcyl_fam: Factor w/ 6 levels &quot;4:0&quot;,&quot;6:0&quot;,&quot;8:0&quot;,..: 5 5 4 2 3 2 3 1 1 2 ... ## $ fvs : Factor w/ 2 levels &quot;V-shaped&quot;,&quot;straight&quot;: 1 1 2 2 1 2 1 2 2 2 ... ## mtcars_by_cyl : tibble [3 × 5] (S3: tbl_df/tbl/data.frame) ## mtcars_by_cyl1 : tibble [3 × 3] (S3: tbl_df/tbl/data.frame) ## mtcars_by_cyl2 : &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ cyl : num 4 6 8 ## $ n_wt: int 11 7 14 ## $ prop: num 0.344 0.219 0.438 ## my_blue : chr &quot;#4ABEFF&quot; ## my_book : Formal class &#39;workbook&#39; [package &quot;XLConnect&quot;] with 2 slots ## my_character : chr &quot;universe&quot; ## my_df : &#39;data.frame&#39;: 10 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ## $ am : num 1 1 1 0 0 0 0 0 0 0 ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ## my_list : List of 3 ## $ vec: int [1:10] 1 2 3 4 5 6 7 8 9 10 ## $ mat: int [1:3, 1:3] 1 2 3 4 5 6 7 8 9 ## $ df :&#39;data.frame&#39;: 10 obs. of 11 variables: ## my_logical : logi FALSE ## my_matrix : int [1:3, 1:3] 1 2 3 4 5 6 7 8 9 ## my_numeric : num 42 ## my_vector : int [1:10] 1 2 3 4 5 6 7 8 9 10 ## n_flips : num 10 ## n_visits : num [1:410] 0 0 0 0 0 0 0 0 0 0 ... ## name : chr [1:8] &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; &quot;Jupiter&quot; &quot;Saturn&quot; &quot;Uranus&quot; ... ## names : List of 4 ## $ : chr &quot;gauss&quot; ## $ : chr &quot;bayes&quot; ## $ : chr &quot;pascal&quot; ## $ : chr &quot;pearson&quot; ## names_ad : chr [1:4] &quot;gauss&quot; &quot;bayes&quot; &quot;pascal&quot; &quot;pearson&quot; ## names_an : chr [1:4] &quot;gauss&quot; &quot;bayes&quot; &quot;pascal&quot; &quot;pearson&quot; ## names_filtered : tibble [414 × 8] (S3: tbl_df/tbl/data.frame) ## names_normalized : tibble [1,924,665 × 8] (S3: tbl_df/tbl/data.frame) ## national_parks : chr [1:22] &quot;Addo Elephant National Park&quot; &quot;Agulhas National Park&quot; ... ## new_hope : num [1:2] 461 314 ## new_mean : num 6000 ## new_sales : &#39;data.frame&#39;: 36 obs. of 2 variables: ## $ sale_num: int 1 2 3 4 5 6 7 8 9 10 ... ## $ amount : num 3732 5735 5340 1180 5797 ... ## new_sd : num 2600 ## non_us_all : Named num [1:6] 314 248 166 552 339 ... ## non_us_some : Named num [1:2] 314 248 ## now : POSIXct[1:1], format: &quot;2024-01-16 13:33:14&quot; ## num : num 2500 ## num_chars : int [1:6] 8 5 6 5 14 9 ## num_views : num 14 ## number : num 2500 ## nyc : List of 3 ## $ pop : num 8405837 ## $ boroughs: chr [1:5] &quot;Manhattan&quot; &quot;Bronx&quot; &quot;Brooklyn&quot; &quot;Queens&quot; ... ## $ capital : logi FALSE ## nyc_ind : int 3 ## nyc_val : logi FALSE ## obs : tibble [346 × 11] (S3: tbl_df/tbl/data.frame) ## oceania_1952 : tibble [2 × 6] (S3: tbl_df/tbl/data.frame) ## p_head : num 0.8 ## p_wt_vs_fcyl_by_fam : List of 9 ## $ data :&#39;data.frame&#39;: 32 obs. of 13 variables: ## $ layers : list() ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: NULL ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## p_wt_vs_fcyl_by_fam_jit : List of 9 ## $ data :&#39;data.frame&#39;: 32 obs. of 13 variables: ## $ layers :List of 1 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## pairs : Classes &#39;pairs&#39;, &#39;data.table&#39; and &#39;data.frame&#39;: 40532 obs. of 5 variables: ## $ .x : int 125 125 125 125 125 125 125 125 125 125 ... ## $ .y : int 75 76 77 78 79 80 81 82 83 84 ... ## $ name : num 0 0 0.489 0.381 0.281 ... ## $ addr : num 0.562 0.56 0.537 0.553 0.507 ... ## $ weights: num -1.3486 -1.3558 0.0967 -0.1716 -0.6351 ... ## palette : chr [1:2] &quot;#D7191C&quot; &quot;#2C7BB6&quot; ## parent : &lt;environment: R_GlobalEnv&gt; ## part_categories : spc_tbl_ [66 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## parts : spc_tbl_ [52,600 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## parts_joined : tibble [9,964 × 4] (S3: tbl_df/tbl/data.frame) ## passengers_per_day : chr [1:4] &quot;0-20K&quot; &quot;20K-70K&quot; &quot;70K-100K&quot; &quot;100K+&quot; ## path : chr &quot;data/hotdogs.txt&quot; ## phone_no_parens : chr [1:2809] &quot;858 990 5153&quot; &quot;731-813-2043&quot; &quot;563-732-6802&quot; &quot;145 725 4021&quot; ... ## pioneers : chr [1:4] &quot;GAUSS:1777&quot; &quot;BAYES:1702&quot; &quot;PASCAL:1623&quot; &quot;PEARSON:1857&quot; ## pizza : Date[1:5], format: &quot;2023-09-15&quot; &quot;2023-09-17&quot; &quot;2023-09-22&quot; &quot;2023-09-28&quot; &quot;2023-10-03&quot; ## planets_df : &#39;data.frame&#39;: 8 obs. of 5 variables: ## $ name : chr &quot;Mercury&quot; &quot;Venus&quot; &quot;Earth&quot; &quot;Mars&quot; ... ## $ type : chr &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; ... ## $ diameter: num 0.382 0.949 1 0.532 11.209 ... ## $ rotation: num 58.64 -243.02 1 1.03 0.41 ... ## $ rings : logi FALSE FALSE FALSE FALSE TRUE TRUE ... ## plt_country_vs_lifeExp : List of 9 ## $ data : spc_tbl_ [20 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ layers :List of 3 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 8 ## plt_mpg_vs_fcyl_by_fam : List of 9 ## $ data :&#39;data.frame&#39;: 32 obs. of 13 variables: ## $ layers : list() ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: NULL ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## plt_mpg_vs_wt : List of 9 ## $ data :&#39;data.frame&#39;: 32 obs. of 13 variables: ## $ layers : list() ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: NULL ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 2 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 2 ## plt_mpg_vs_wt_by_cyl : List of 9 ## $ data :&#39;data.frame&#39;: 32 obs. of 13 variables: ## $ layers :List of 1 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme :List of 3 ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## plt_price_vs_carat : List of 9 ## $ data : tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) ## $ layers : list() ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: NULL ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 2 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 2 ## plt_price_vs_carat_by_clarity : List of 9 ## $ data : tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) ## $ layers : list() ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: NULL ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 3 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 3 ## plt_price_vs_carat_transparent : List of 9 ## $ data : tibble [53,940 × 10] (S3: tbl_df/tbl/data.frame) ## $ layers :List of 1 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 2 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 2 ## plt_prop_unemployed_over_time : List of 9 ## $ data : spc_tbl_ [574 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ layers :List of 1 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 2 ## $ theme :List of 1 ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 4 ## poker_midweek : Named num [1:3] -50 20 -120 ## poker_midweek_name : Named num [1:3] -50 20 -120 ## poker_two : Named num [1:2] 140 240 ## poker_two_name : Named num [1:2] 140 240 ## poker_vector : Named num [1:5] 140 -50 20 -120 240 ## poker_wednesday : Named num 20 ## poker_winning_days : Named num [1:3] 140 20 240 ## pools : &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... ## $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... ## pop_1 : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## pop_2 : tibble [209 × 9] (S3: tbl_df/tbl/data.frame) ## pop_3 : tibble [209 × 38] (S3: tbl_df/tbl/data.frame) ## pop_a : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## pop_b : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## pop_list : List of 3 ## $ : tibble [209 × 8] (S3: tbl_df/tbl/data.frame) ## $ : tibble [209 × 9] (S3: tbl_df/tbl/data.frame) ## $ : tibble [209 × 38] (S3: tbl_df/tbl/data.frame) ## population : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900 ## positions : int [1:8] 1 4 2 3 8 7 6 5 ## posn_d : Classes &#39;PositionDodge&#39;, &#39;Position&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class PositionDodge, Position, gg&gt; ## compute_layer: function ## compute_panel: function ## preserve: total ## required_aes: ## setup_data: function ## setup_params: function ## width: 0.9 ## super: &lt;ggproto object: Class PositionDodge, Position, gg&gt; ## posn_j : Classes &#39;PositionJitter&#39;, &#39;Position&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class PositionJitter, Position, gg&gt; ## compute_layer: function ## compute_panel: function ## height: NULL ## required_aes: x y ## seed: NA ## setup_data: function ## setup_params: function ## width: 0.2 ## super: &lt;ggproto object: Class PositionJitter, Position, gg&gt; ## posn_jd : Classes &#39;PositionJitterdodge&#39;, &#39;Position&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class PositionJitterdodge, Position, gg&gt; ## compute_layer: function ## compute_panel: function ## dodge.width: 0.1 ## jitter.height: 0 ## jitter.width: 0.2 ## required_aes: x y ## seed: 1840911448 ## setup_data: function ## setup_params: function ## super: &lt;ggproto object: Class PositionJitterdodge, Position, gg&gt; ## posts_with_tags : tibble [1,546,174 × 7] (S3: tbl_df/tbl/data.frame) ## potatoes : Classes &#39;data.table&#39; and &#39;data.frame&#39;: 160 obs. of 8 variables: ## $ area : int 1 1 1 1 1 1 1 1 1 1 ... ## $ temp : int 1 1 1 1 1 1 1 1 1 1 ... ## $ size : int 1 1 1 1 1 1 1 1 1 1 ... ## $ storage : int 1 1 1 1 1 2 2 2 2 2 ... ## $ method : int 1 2 3 4 5 1 2 3 4 5 ... ## $ texture : num 2.9 2.3 2.5 2.1 1.9 1.8 2.6 3 2.2 2 ... ## $ flavor : num 3.2 2.5 2.8 2.9 2.8 3 3.1 3 3.2 2.8 ... ## $ moistness: num 3 2.6 2.8 2.4 2.2 1.7 2.4 2.9 2.5 1.9 ... ## potatoes_char : spc_tbl_ [160 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## potatoes_fragment : spc_tbl_ [5 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## potatoes_fread : Classes &#39;data.table&#39; and &#39;data.frame&#39;: 160 obs. of 2 variables: ## $ texture : num 2.9 2.3 2.5 2.1 1.9 1.8 2.6 3 2.2 2 ... ## $ moistness: num 3 2.6 2.8 2.4 2.2 1.7 2.4 2.9 2.5 1.9 ... ## pow_two : function (x, print_info = T) ## print_info : function (x) ## prob_between_10_and_20 : num 0.333 ## prob_greater_than_5 : num 0.833 ## prob_less_than_5 : num 0.167 ## properties : chr [1:8] &quot;area&quot; &quot;temp&quot; &quot;size&quot; &quot;storage&quot; &quot;method&quot; &quot;texture&quot; &quot;flavor&quot; ... ## q1 : Named num 447 ## q3 : Named num 1111 ## question_answer_counts : spc_tbl_ [394,732 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## question_tags : spc_tbl_ [572,153 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## questions : spc_tbl_ [394,732 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## questions_with_tags : spc_tbl_ [745,796 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## r : int 3 ## r_3_4_1 : POSIXct[1:1], format: &quot;2016-05-03 07:13:28&quot; ## rainy_days : gropd_df [366 × 3] (S3: grouped_df/tbl_df/tbl/data.frame) ## rcount : num 5 ## red_brewer_palette : chr [1:9] &quot;#FFF5F0&quot; &quot;#FEE0D2&quot; &quot;#FCBBA1&quot; &quot;#FC9272&quot; &quot;#FB6A4A&quot; &quot;#EF3B2C&quot; ... ## region : chr [1:2] &quot;US&quot; &quot;non-US&quot; ## release_time : POSIXct[1:105], format: &quot;1997-12-04 08:47:58&quot; &quot;1997-12-21 13:09:22&quot; &quot;1998-01-10 00:31:55&quot; ... ## releases : spc_tbl_ [105 × 8] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## restaurant_groups : &#39;data.frame&#39;: 10 obs. of 2 variables: ## $ group_id : chr &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## $ group_size: num 2 4 6 2 2 2 3 2 4 2 ## result : chr &quot;large&quot; ## return_jedi : num [1:2] 309 166 ## reviews_df : &#39;data.frame&#39;: 5 obs. of 2 variables: ## $ scores : num 4.2 4.6 4.8 5 5 ## $ comments: chr &quot;Fascinating plot&quot; &quot;I would watch it again&quot; &quot;I liked it&quot; &quot;Amazing!&quot; ... ## rings : logi [1:8] FALSE FALSE FALSE FALSE TRUE TRUE ... ## rings_vector : logi [1:8] FALSE FALSE FALSE FALSE TRUE TRUE ... ## rotation : num [1:8] 58.64 -243.02 1 1.03 0.41 ... ## roulette_vector : Named num [1:5] -24 -50 100 -350 10 ## rquote : chr &quot;r&#39;s internals are irrefutably intriguing&quot; ## rsa_env : &lt;environment: 0x000001cc592b4790&gt; ## rsa_lst : List of 3 ## $ capitals :&#39;data.frame&#39;: 3 obs. of 2 variables: ## $ national_parks: chr [1:22] &quot;Addo Elephant National Park&quot; &quot;Agulhas National Park&quot; &quot;Ai-Ais/Richtersveld Transfrontier Park&quot; &quot;Augrabies Falls National Park&quot; ... ## $ population : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900 ## run_poisson_regression : function (data, formula) ## sample_means : num [1:30] 49.2 43 26.6 45.3 41 ... ## samples : &#39;data.frame&#39;: 100 obs. of 1 variable: ## $ mean: num 39.6 42.4 27.5 46.2 37.4 ... ## saros : Formal class &#39;Duration&#39; [package &quot;lubridate&quot;] with 1 slot ## scores : num [1:5] 4.6 5 4.8 5 4.2 ## secondary_y_axis : Classes &#39;AxisSecondary&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class AxisSecondary, gg&gt; ## axis: NULL ## break_info: function ## breaks: 59 68 77 86 95 104 ## create_scale: function ## detail: 1000 ## empty: function ## guide: waiver ## init: function ## labels: 15 20 25 30 35 40 ## make_title: function ## mono_test: function ## name: Celsius ## trans: function ## transform_range: function ## super: &lt;ggproto object: Class AxisSecondary, gg&gt; ## select_el : function (x, index) ## select_first : function (x) ## select_second : function (x) ## selected_names : tibble [281 × 5] (S3: tbl_df/tbl/data.frame) ## selection : &#39;data.frame&#39;: 209 obs. of 4 variables: ## $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## $ X1968 : num 1182159 639964 5017299 17996 16727 ... ## $ X1969 : num 1248901 658853 5219332 18619 18088 ... ## $ X1970 : num 1319849 677839 5429743 19206 19529 ... ## selection_vector : Named logi [1:5] TRUE FALSE TRUE FALSE TRUE ## sep_10_2009 : chr [1:4] &quot;September 10 2009&quot; &quot;2009-09-10&quot; &quot;10 Sep 2009&quot; &quot;09-10-2009&quot; ## seq1 : num [1:167] 1 4 7 10 13 16 19 22 25 28 ... ## seq2 : num [1:43] 1200 1193 1186 1179 1172 ... ## sets : spc_tbl_ [21,857 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## sex_vector : chr [1:5] &quot;Male&quot; &quot;Female&quot; &quot;Female&quot; &quot;Male&quot; &quot;Male&quot; ## sfo_survey : tibble [2,809 × 6] (S3: tbl_df/tbl/data.frame) ## sheets : chr [1:3] &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; ## short_dates : chr [1:3] &quot;11 December 1282&quot; &quot;May 1372&quot; &quot;1253&quot; ## size_distribution : &#39;data.frame&#39;: 4 obs. of 3 variables: ## $ group_size : num 2 3 4 6 ## $ n : int 6 1 2 1 ## $ probability: num 0.6 0.1 0.2 0.1 ## sms : num 24 ## snake_river_visits : &#39;data.frame&#39;: 410 obs. of 4 variables: ## $ n_visits: num 0 0 0 0 0 0 0 0 0 0 ... ## $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 1 1 1 2 1 2 2 2 1 1 ... ## $ income : Factor w/ 4 levels &quot;[$0,$25k]&quot;,&quot;($25k,$55k]&quot;,..: 4 2 4 2 4 2 4 4 4 4 ... ## $ travel : Factor w/ 3 levels &quot;[0h,0.25h]&quot;,&quot;(0.25h,4h]&quot;,..: NA NA NA NA NA NA NA NA NA NA ... ## social_vec : num [1:14] 16 9 13 5 2 17 14 17 7 5 ... ## speed : num 88 ## speed_vector : chr [1:5] &quot;medium&quot; &quot;slow&quot; &quot;slow&quot; &quot;medium&quot; &quot;fast&quot; ## split_low : List of 4 ## $ : chr [1:2] &quot;gauss&quot; &quot;1777&quot; ## $ : chr [1:2] &quot;bayes&quot; &quot;1702&quot; ## $ : chr [1:2] &quot;pascal&quot; &quot;1623&quot; ## $ : chr [1:2] &quot;pearson&quot; &quot;1857&quot; ## split_math : List of 4 ## $ : chr [1:2] &quot;GAUSS&quot; &quot;1777&quot; ## $ : chr [1:2] &quot;BAYES&quot; &quot;1702&quot; ## $ : chr [1:2] &quot;PASCAL&quot; &quot;1623&quot; ## $ : chr [1:2] &quot;PEARSON&quot; &quot;1857&quot; ## star_destroyer : tibble [293 × 6] (S3: tbl_df/tbl/data.frame) ## star_destroyer_colors : tibble [16 × 2] (S3: tbl_df/tbl/data.frame) ## star_wars : tibble [80,843 × 13] (S3: tbl_df/tbl/data.frame) ## star_wars_colors : tibble [81 × 3] (S3: tbl_df/tbl/data.frame) ## star_wars_matrix : num [1:3, 1:2] 461 290 309 314 248 ... ## star_wars_matrix_dim : num [1:3, 1:2] 461 290 309 314 248 ... ## star_wars_matrix2 : num [1:3, 1:2] 474 311 380 552 339 ... ## star_wars_parts : tibble [9,081 × 3] (S3: tbl_df/tbl/data.frame) ## std_and_poor500 : &#39;data.frame&#39;: 505 obs. of 5 variables: ## $ symbol : chr &quot;MMM&quot; &quot;ABT&quot; &quot;ABBV&quot; &quot;ABMD&quot; ... ## $ company : chr &quot;3M Company&quot; &quot;Abbott Laboratories&quot; &quot;AbbVie Inc.&quot; &quot;ABIOMED Inc&quot; ... ## $ sector : chr &quot;Industrials&quot; &quot;Health Care&quot; &quot;Health Care&quot; &quot;Health Care&quot; ... ## $ industry: chr &quot;Industrial Conglomerates&quot; &quot;Health Care Equipment&quot; &quot;Pharmaceuticals&quot; &quot;Health Care Equipment&quot; ... ## $ pe_ratio: num 18.3 57.7 22.4 45.6 27 ... ## str1 : chr &quot;May 23, &#39;96 hours:23 minutes:01 seconds:45&quot; ## str2 : chr &quot;2012-3-12 14:23:08&quot; ## str3 : chr &quot;30/January/2006&quot; ## sum_abs : function (b, c) ## summ : &#39;data.frame&#39;: 3 obs. of 3 variables: ## $ sheets: chr &quot;1960-1966&quot; &quot;1967-1974&quot; &quot;1975-2011&quot; ## $ nrows : int 209 209 209 ## $ ncols : int 8 9 38 ## sun_plot : List of 9 ## $ data :&#39;data.frame&#39;: 3177 obs. of 2 variables: ## $ layers :List of 1 ## $ scales :Classes &#39;ScalesList&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class ScalesList, gg&gt; ## add: function ## clone: function ## find: function ## get_scales: function ## has_scale: function ## input: function ## n: function ## non_position_scales: function ## scales: list ## super: &lt;ggproto object: Class ScalesList, gg&gt; ## $ mapping :List of 2 ## $ theme : list() ## $ coordinates:Classes &#39;CoordCartesian&#39;, &#39;Coord&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## aspect: function ## backtransform_range: function ## clip: on ## default: TRUE ## distance: function ## expand: TRUE ## is_free: function ## is_linear: function ## labels: function ## limits: list ## modify_scales: function ## range: function ## render_axis_h: function ## render_axis_v: function ## render_bg: function ## render_fg: function ## setup_data: function ## setup_layout: function ## setup_panel_guides: function ## setup_panel_params: function ## setup_params: function ## train_panel_guides: function ## transform: function ## super: &lt;ggproto object: Class CoordCartesian, Coord, gg&gt; ## $ facet :Classes &#39;FacetNull&#39;, &#39;Facet&#39;, &#39;ggproto&#39;, &#39;gg&#39; &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## compute_layout: function ## draw_back: function ## draw_front: function ## draw_labels: function ## draw_panels: function ## finish_data: function ## init_scales: function ## map_data: function ## params: list ## setup_data: function ## setup_params: function ## shrink: TRUE ## train_scales: function ## vars: function ## super: &lt;ggproto object: Class FacetNull, Facet, gg&gt; ## $ plot_env :&lt;environment: R_GlobalEnv&gt; ## $ labels :List of 2 ## sunspots.m : &#39;data.frame&#39;: 3177 obs. of 2 variables: ## $ year : num 1749 1749 1749 1749 1749 ... ## $ value: Time-Series from 1749 to 2014: 58 62.6 70 55.7 85 83.5 94.8 66.3 75.9 75.5 ... ## survey_vector : chr [1:5] &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; &quot;M&quot; ## synodic : Formal class &#39;Duration&#39; [package &quot;lubridate&quot;] with 1 slot ## tagged_answers : spc_tbl_ [745,796 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## tags : spc_tbl_ [48,299 × 2] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## temp : List of 7 ## $ : num [1:5] 3 7 9 6 -1 ## $ : num [1:5] 6 9 12 13 5 ## $ : num [1:5] 4 8 3 -1 -3 ## $ : num [1:5] 1 4 7 2 -2 ## $ : num [1:5] 5 7 9 4 2 ## $ : num [1:5] -3 5 8 9 4 ## $ : num [1:5] 3 6 9 4 1 ## temperature_vector : chr [1:5] &quot;High&quot; &quot;Low&quot; &quot;High&quot; &quot;Low&quot; &quot;Medium&quot; ## TG : spc_tbl_ [60 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## theme_recession : List of 8 ## $ rect :List of 5 ## $ axis.text :List of 11 ## $ axis.ticks : list() ## $ legend.key :List of 5 ## $ legend.position : num [1:2] 0.6 0.1 ## $ panel.grid : list() ## $ panel.grid.major.y:List of 6 ## $ plot.title :List of 11 ## theme_tufte_recession : List of 97 ## $ line :List of 6 ## $ rect :List of 5 ## $ text :List of 11 ## $ title : NULL ## $ aspect.ratio : NULL ## $ axis.title : NULL ## $ axis.title.x :List of 11 ## $ axis.title.x.top :List of 11 ## $ axis.title.x.bottom : NULL ## $ axis.title.y :List of 11 ## $ axis.title.y.left : NULL ## $ axis.title.y.right :List of 11 ## $ axis.text :List of 11 ## $ axis.text.x :List of 11 ## $ axis.text.x.top :List of 11 ## $ axis.text.x.bottom : NULL ## $ axis.text.y :List of 11 ## $ axis.text.y.left : NULL ## $ axis.text.y.right :List of 11 ## $ axis.ticks : list() ## $ axis.ticks.x : NULL ## $ axis.ticks.x.top : NULL ## $ axis.ticks.x.bottom : NULL ## $ axis.ticks.y : NULL ## $ axis.ticks.y.left : NULL ## $ axis.ticks.y.right : NULL ## $ axis.ticks.length : &#39;simpleUnit&#39; num 2.75points ## $ axis.ticks.length.x : NULL ## $ axis.ticks.length.x.top : NULL ## $ axis.ticks.length.x.bottom: NULL ## $ axis.ticks.length.y : NULL ## $ axis.ticks.length.y.left : NULL ## $ axis.ticks.length.y.right : NULL ## $ axis.line : list() ## $ axis.line.x : NULL ## $ axis.line.x.top : NULL ## $ axis.line.x.bottom : NULL ## $ axis.line.y : NULL ## $ axis.line.y.left : NULL ## $ axis.line.y.right : NULL ## $ legend.background : list() ## $ legend.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## $ legend.spacing : &#39;simpleUnit&#39; num 11points ## $ legend.spacing.x : NULL ## $ legend.spacing.y : NULL ## $ legend.key :List of 5 ## $ legend.key.size : &#39;simpleUnit&#39; num 1.2lines ## $ legend.key.height : NULL ## $ legend.key.width : NULL ## $ legend.text :List of 11 ## $ legend.text.align : NULL ## $ legend.title :List of 11 ## $ legend.title.align : NULL ## $ legend.position : num [1:2] 0.6 0.1 ## $ legend.direction : NULL ## $ legend.justification : chr &quot;center&quot; ## $ legend.box : NULL ## $ legend.box.just : NULL ## $ legend.box.margin : &#39;margin&#39; num [1:4] 0cm 0cm 0cm 0cm ## $ legend.box.background : list() ## $ legend.box.spacing : &#39;simpleUnit&#39; num 11points ## $ panel.background : list() ## $ panel.border : list() ## $ panel.spacing : &#39;simpleUnit&#39; num 5.5points ## $ panel.spacing.x : NULL ## $ panel.spacing.y : NULL ## $ panel.grid : list() ## $ panel.grid.major : NULL ## $ panel.grid.minor :List of 6 ## $ panel.grid.major.x : NULL ## $ panel.grid.major.y :List of 6 ## $ panel.grid.minor.x : NULL ## $ panel.grid.minor.y : NULL ## $ panel.ontop : logi FALSE ## $ plot.background : list() ## $ plot.title :List of 11 ## $ plot.title.position : chr &quot;panel&quot; ## $ plot.subtitle :List of 11 ## $ plot.caption :List of 11 ## $ plot.caption.position : chr &quot;panel&quot; ## $ plot.tag :List of 11 ## $ plot.tag.position : chr &quot;topleft&quot; ## $ plot.margin : &#39;margin&#39; num [1:4] 5.5points 5.5points 5.5points 5.5points ## $ strip.background : list() ## $ strip.background.x : NULL ## $ strip.background.y : NULL ## $ strip.clip : chr &quot;inherit&quot; ## $ strip.placement : chr &quot;inside&quot; ## $ strip.text :List of 11 ## $ strip.text.x : NULL ## $ strip.text.x.bottom : NULL ## $ strip.text.x.top : NULL ## $ strip.text.y :List of 11 ## $ strip.text.y.left :List of 11 ## $ strip.text.y.right : NULL ## $ strip.switch.pad.grid : &#39;simpleUnit&#39; num 2.75points ## $ strip.switch.pad.wrap : &#39;simpleUnit&#39; num 2.75points ## themes : spc_tbl_ [467 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## throw_die : function () ## ticket_prices_matrix : num [1:6, 1:2] 5 6 7 4 4.5 4.9 5 6 7 4 ... ## ticket_rowname : chr [1:6] &quot;A New Hope&quot; &quot;The Empire Strikes Back&quot; &quot;Return of the Jedi&quot; ... ## time_online : &#39;difftime&#39; num [1:5] 2305 34 837 2398 ... ## time1 : POSIXct[1:1], format: &quot;1996-05-23 23:01:45&quot; ## time2 : POSIXct[1:1], format: &quot;2012-03-12 14:23:08&quot; ## titles : chr [1:3] &quot;A New Hope&quot; &quot;The Empire Strikes Back&quot; &quot;Return of the Jedi&quot; ## today : Date[1:1], format: &quot;2024-01-16&quot; ## today_8am : POSIXct[1:1], format: &quot;2024-01-16 08:00:00&quot; ## tom : &#39;data.frame&#39;: 1 obs. of 3 variables: ## $ type : chr &quot;Beef&quot; ## $ calories: int 190 ## $ sodium : int 645 ## toss_coin : function (n_flips, p_head) ## total_poker : num 230 ## total_roulette : num -314 ## total_week : num -84 ## triple : function (x) ## ttt : chr [1:3, 1:3] &quot;O&quot; NA &quot;X&quot; NA &quot;O&quot; NA &quot;X&quot; &quot;O&quot; &quot;X&quot; ## tue_9am : POSIXct[1:1], format: &quot;2018-08-28 09:00:00&quot; ## two_orders : chr [1:5] &quot;October 7, 2001&quot; &quot;October 13, 2002&quot; &quot;April 13, 2003&quot; ... ## type : chr [1:8] &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; &quot;Terrestrial planet&quot; ... ## type_of_capital : chr [1:3] &quot;Legislative&quot; &quot;Judicial&quot; &quot;Administrative&quot; ## upper : Named num 2108 ## urban : &#39;data.frame&#39;: 209 obs. of 53 variables: ## $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;American Samoa&quot; ... ## $ 1960 : num 769308 494443 3293999 NA NA ... ## $ 1961 : num 814923 511803 3515148 13660 8724 ... ## $ 1962 : num 858522 529439 3739963 14166 9700 ... ## $ 1963 : num 903914 547377 3973289 14759 10748 ... ## $ 1964 : num 951226 565572 4220987 15396 11866 ... ## $ 1965 : num 1000582 583983 4488176 16045 13053 ... ## $ 1966 : num 1058743 602512 4649105 16693 14217 ... ## $ 1967 : num 1119067 621180 4826104 17349 15440 ... ## $ 1968 : num 1182159 639964 5017299 17996 16727 ... ## $ 1969 : num 1248901 658853 5219332 18619 18088 ... ## $ 1970 : num 1319849 677839 5429743 19206 19529 ... ## $ 1971 : num 1409001 698932 5619042 19752 20929 ... ## $ 1972 : num 1502402 720207 5815734 20263 22406 ... ## $ 1973 : num 1598835 741681 6020647 20742 23937 ... ## $ 1974 : num 1696445 763385 6235114 21194 25482 ... ## $ 1975 : num 1793266 785350 6460138 21632 27019 ... ## $ 1976 : num 1905033 807990 6774099 22047 28366 ... ## $ 1977 : num 2021308 830959 7102902 22452 29677 ... ## $ 1978 : num 2142248 854262 7447728 22899 31037 ... ## $ 1979 : num 2268015 877898 7810073 23457 32572 ... ## $ 1980 : num 2398775 901884 8190772 24177 34366 ... ## $ 1981 : num 2493265 927224 8637724 25173 36356 ... ## $ 1982 : num 2590846 952447 9105820 26342 38618 ... ## $ 1983 : num 2691612 978476 9591900 27655 40983 ... ## $ 1984 : num 2795656 1006613 10091289 29062 43207 ... ## $ 1985 : num 2903078 1037541 10600112 30524 45119 ... ## $ 1986 : num 3006983 1072365 11101757 32014 46254 ... ## $ 1987 : num 3113957 1109954 11609104 33548 47019 ... ## $ 1988 : num 3224082 1146633 12122941 35095 47669 ... ## $ 1989 : num 3337444 1177286 12645263 36618 48577 ... ## $ 1990 : num 3454129 1198293 13177079 38088 49982 ... ## $ 1991 : num 3617842 1215445 13708813 39600 51972 ... ## $ 1992 : num 3788685 1222544 14248297 41049 54469 ... ## $ 1993 : num 3966956 1222812 14789176 42443 57079 ... ## $ 1994 : num 4152960 1221364 15322651 43798 59243 ... ## $ 1995 : num 4347018 1222234 15842442 45129 60598 ... ## $ 1996 : num 4531285 1228760 16395553 46343 60927 ... ## $ 1997 : num 4722603 1238090 16935451 47527 60462 ... ## $ 1998 : num 4921227 1250366 17469200 48705 59685 ... ## $ 1999 : num 5127421 1265195 18007937 49906 59281 ... ## $ 2000 : num 5341456 1282223 18560597 51151 59719 ... ## $ 2001 : num 5564492 1315690 19198872 52341 61062 ... ## $ 2002 : num 5795940 1352278 19854835 53583 63212 ... ## $ 2003 : num 6036100 1391143 20529356 54864 65802 ... ## $ 2004 : num 6285281 1430918 21222198 56166 68301 ... ## $ 2005 : num 6543804 1470488 21932978 57474 70329 ... ## $ 2006 : num 6812538 1512255 22625052 58679 71726 ... ## $ 2007 : num 7091245 1553491 23335543 59894 72684 ... ## $ 2008 : num 7380272 1594351 24061749 61118 73335 ... ## $ 2009 : num 7679982 1635262 24799591 62357 73897 ... ## $ 2010 : num 7990746 1676545 25545622 63616 74525 ... ## $ 2011 : num 8316976 1716842 26216968 64817 75207 ... ## urban_clean : &#39;data.frame&#39;: 197 obs. of 53 variables: ## $ country: chr &quot;Afghanistan&quot; &quot;Albania&quot; &quot;Algeria&quot; &quot;Angola&quot; ... ## $ 1960 : num 769308 494443 3293999 521205 21699 ... ## $ 1961 : num 814923 511803 3515148 548265 21635 ... ## $ 1962 : num 858522 529439 3739963 579695 21664 ... ## $ 1963 : num 903914 547377 3973289 612087 21741 ... ## $ 1964 : num 951226 565572 4220987 645262 21830 ... ## $ 1965 : num 1000582 583983 4488176 679109 21909 ... ## $ 1966 : num 1058743 602512 4649105 717833 22003 ... ## $ 1967 : num 1119067 621180 4826104 757496 22086 ... ## $ 1968 : num 1182159 639964 5017299 798459 22149 ... ## $ 1969 : num 1248901 658853 5219332 841262 22183 ... ## $ 1970 : num 1319849 677839 5429743 886402 22181 ... ## $ 1971 : num 1409001 698932 5619042 955010 22561 ... ## $ 1972 : num 1502402 720207 5815734 1027397 22908 ... ## $ 1973 : num 1598835 741681 6020647 1103830 23221 ... ## $ 1974 : num 1696445 763385 6235114 1184486 23503 ... ## $ 1975 : num 1793266 785350 6460138 1269606 23753 ... ## $ 1976 : num 1905033 807990 6774099 1372800 23977 ... ## $ 1977 : num 2021308 830959 7102902 1481598 24168 ... ## $ 1978 : num 2142248 854262 7447728 1597267 24304 ... ## $ 1979 : num 2268015 877898 7810073 1721590 24358 ... ## $ 1980 : num 2398775 901884 8190772 1855983 24312 ... ## $ 1981 : num 2493265 927224 8637724 2015833 24164 ... ## $ 1982 : num 2590846 952447 9105820 2188351 23929 ... ## $ 1983 : num 2691612 978476 9591900 2371047 23632 ... ## $ 1984 : num 2795656 1006613 10091289 2559552 23306 ... ## $ 1985 : num 2903078 1037541 10600112 2750310 22979 ... ## $ 1986 : num 3006983 1072365 11101757 2955174 22654 ... ## $ 1987 : num 3113957 1109954 11609104 3161499 22345 ... ## $ 1988 : num 3224082 1146633 12122941 3373207 22100 ... ## $ 1989 : num 3337444 1177286 12645263 3597090 21982 ... ## $ 1990 : num 3454129 1198293 13177079 3838852 22036 ... ## $ 1991 : num 3617842 1215445 13708813 4102967 22047 ... ## $ 1992 : num 3788685 1222544 14248297 4388137 22229 ... ## $ 1993 : num 3966956 1222812 14789176 4690892 22538 ... ## $ 1994 : num 4152960 1221364 15322651 5004756 22914 ... ## $ 1995 : num 4347018 1222234 15842442 5324794 23312 ... ## $ 1996 : num 4531285 1228760 16395553 5602207 23667 ... ## $ 1997 : num 4722603 1238090 16935451 5882843 24031 ... ## $ 1998 : num 4921227 1250366 17469200 6173329 24379 ... ## $ 1999 : num 5127421 1265195 18007937 6483827 24690 ... ## $ 2000 : num 5341456 1282223 18560597 6822112 24949 ... ## $ 2001 : num 5564492 1315690 19198872 7190985 25197 ... ## $ 2002 : num 5795940 1352278 19854835 7589585 25383 ... ## $ 2003 : num 6036100 1391143 20529356 8012295 25520 ... ## $ 2004 : num 6285281 1430918 21222198 8449762 25623 ... ## $ 2005 : num 6543804 1470488 21932978 8894673 25706 ... ## $ 2006 : num 6812538 1512255 22625052 9326818 25896 ... ## $ 2007 : num 7091245 1553491 23335543 9764679 26066 ... ## $ 2008 : num 7380272 1594351 24061749 10210317 26223 ... ## $ 2009 : num 7679982 1635262 24799591 10667670 26369 ... ## $ 2010 : num 7990746 1676545 25545622 11139829 26508 ... ## $ 2011 : num 8316976 1716842 26216968 11602851 26771 ... ## urbanpop_sel : &#39;data.frame&#39;: 209 obs. of 3 variables: ## $ X1968: num 1182159 639964 5017299 17996 16727 ... ## $ X1969: num 1248901 658853 5219332 18619 18088 ... ## $ X1970: num 1319849 677839 5429743 19206 19529 ... ## us_visitors : Named num [1:6] 92.2 48.4 44.2 118.6 69 ... ## val : num 14 ## version_1_inventories : spc_tbl_ [35,612 × 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## view_true : logi [1:2, 1:7] FALSE FALSE FALSE FALSE TRUE FALSE ... ## views : num [1:2, 1:7] 16 17 9 7 13 5 5 16 2 8 ... ## visitors : num [1:6, 1:2] 92.2 48.4 44.2 118.6 69 ... ## Vocab : tibble [21,638 × 5] (S3: tbl_df/tbl/data.frame) ## wait_times : &#39;data.frame&#39;: 1000 obs. of 1 variable: ## $ simulation_nb: int 1 2 3 4 5 6 7 8 9 10 ... ## weights : num [1:2] 0.8 0.2 ## wind : &#39;data.frame&#39;: 8753 obs. of 3 variables: ## $ date: POSIXct, format: &quot;2003-01-01 00:00:00&quot; &quot;2003-01-01 01:00:00&quot; ... ## $ ws : Factor w/ 7 levels &quot;0 - 2&quot;,&quot;10 - 12&quot;,..: 5 5 4 5 5 5 5 5 4 4 ... ## $ wd : Factor w/ 16 levels &quot;E&quot;,&quot;ENE&quot;,&quot;ESE&quot;,..: 11 10 10 10 10 10 11 11 11 11 ... ## won_25pct : num 0.75 ## won_30pct : num 0.9 ## won_35pct : num 1.05 ## world_happiness : &#39;data.frame&#39;: 133 obs. of 10 variables: ## $ country : chr &quot;Finland&quot; &quot;Denmark&quot; &quot;Norway&quot; &quot;Iceland&quot; ... ## $ social_support : int 2 4 3 1 15 13 25 5 20 31 ... ## $ freedom : int 5 6 3 7 19 11 10 8 9 26 ... ## $ corruption : int 4 3 8 45 12 7 6 5 11 19 ... ## $ generosity : int 47 22 11 3 7 16 17 8 14 25 ... ## $ gdp_per_cap : int 42400 48300 66300 47900 50500 59000 47200 36500 44200 46900 ... ## $ life_exp : num 81.8 81 82.6 83 81.8 84.3 82.8 81.9 82.2 82 ... ## $ happiness_score : num 155 154 153 152 151 150 149 148 147 146 ... ## $ grams_sugar_per_day: num 86.8 152 120 132 122 166 115 162 132 124 ... ## $ log_gdp_per_cap : num 10.7 10.8 11.1 10.8 10.8 ... ## worldwide_vector : Named num [1:3] 775 538 475 ## worldwide_vector_col : Named num [1:2] 1061 728 ## x : chr &quot;Monday June 1st 2010 at 4pm&quot; ## x_date : Date[1:1], format: &quot;2013-04-03&quot; ## x_end : num 67 ## x_start : num 71 ## y : chr &quot;02.01.2010&quot; ## y_breaks : num [1:6] 59 68 77 86 95 104 ## y_end : num 7.5 ## y_labels : num [1:6] 15 20 25 30 35 40 ## y_start : num 5.5 ## years : List of 4 ## $ : chr &quot;1777&quot; ## $ : chr &quot;1702&quot; ## $ : chr &quot;1623&quot; ## $ : chr &quot;1857&quot; ## years_ad : chr [1:4] &quot;1777&quot; &quot;1702&quot; &quot;1623&quot; &quot;1857&quot; ## years_an : chr [1:4] &quot;1777&quot; &quot;1702&quot; &quot;1623&quot; &quot;1857&quot; ## z : chr &quot;Sep, 12th 2010 14:00&quot; ## zagat : &#39;data.frame&#39;: 310 obs. of 7 variables: ## $ id : int 0 1 2 3 4 5 6 8 9 11 ... ## $ name : chr &quot;apple pan the&quot; &quot;asahi ramen&quot; &quot;baja fresh&quot; &quot;belvedere the&quot; ... ## $ addr : chr &quot;10801 w. pico blvd.&quot; &quot;2027 sawtelle blvd.&quot; &quot;3345 kimber dr.&quot; &quot;9882 little santa monica blvd.&quot; ... ## $ city : Factor w/ 26 levels &quot;atlanta&quot;,&quot;los angeles&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ phone: Factor w/ 329 levels &quot;100-813-8212&quot;,..: 143 145 312 155 139 96 323 125 150 132 ... ## $ type : chr &quot;american&quot; &quot;noodle shops&quot; &quot;mexican&quot; &quot;pacific new wave&quot; ... ## $ class: int 534 535 536 537 538 539 540 542 543 545 ... ls.str(rsa_env) ## capitals : &#39;data.frame&#39;: 3 obs. of 2 variables: ## $ city : chr &quot;Cape Town&quot; &quot;Bloemfontein&quot; &quot;Pretoria&quot; ## $ type_of_capital: chr &quot;Legislative&quot; &quot;Judicial&quot; &quot;Administrative&quot; ## national_parks : chr [1:22] &quot;Addo Elephant National Park&quot; &quot;Agulhas National Park&quot; ... ## population : Time-Series [1:5] from 1996 to 2016: 40583573 44819778 47390900 51770560 55908900 # Does groom_model exist in rsa_env? exists(&quot;groom_model&quot;, envir = rsa_env) ## [1] TRUE # Does groom_model exist in rsa_env, ignoring inheritance? exists(&quot;groom_model&quot;, envir = rsa_env, inherits = FALSE) ## [1] FALSE R searches for variables in all the parent environments, unless you explicitly tell it not to (inherits = FALSE). 12.3.4 Scope and precedence When you call a function, R gives it an environment to store its variables. Accessing variables outside functions When R couldn’t find variable inside the function’s environment, it looked in the parent environment. And variable can still be used inside the function. x_times_y &lt;- function(x) { x * y } x_times_y(10) Error in x_times_y(10) : object &#39;y&#39; not found x_times_y &lt;- function(x) { x * y } y &lt;- 4 x_times_y(10) ## [1] 40 Accessing function variables from outside But you can’t look inside the function’s environment from outside. x_times_y &lt;- function(x) { x * y } y &lt;- 4 x_times_y(10) print(x) Error: object &#39;x&#39; not found Although x is defined inside the function, you can’t access it from outside the function. Variable precedence Variables inside functions take precedence over variables outside functions. So, R will search for variables inside the function first. If it finds y there, it doesn’t need to look in the parent environment. x_times_y &lt;- function(x) { y &lt;- 6 x * y } y &lt;- 4 x_times_y(10) ## [1] 60 Values defined inside the function take precedence over values passed into the function. x_times_y &lt;- function(x) { x &lt;- 9 y &lt;- 6 x * y } y &lt;- 4 x_times_y(10) ## [1] 54 12.4 Case Study on Grain Yields 12.4.1 Grain yields &amp; unit conversion 12.4.1.1 Convert areas to metric 1 In this chapter, you’ll be working with grain yield data from the United States Department of Agriculture, National Agricultural Statistics Service. Unfortunately, they report all areas in acres. So, the first thing you need to do is write some utility functions to convert areas in acres to areas in hectares. To solve this exercise, you need to know the following: There are 4840 square yards in an acre. There are 36 inches in a yard and one inch is 0.0254 meters. There are 10000 square meters in a hectare. # convert areas in acres to areas in square yards. # Write a function to convert acres to sq. yards acres_to_sq_yards &lt;- function(acres) { acres * 4840 } # convert distances in yards to distances in meters. # Write a function to convert yards to meters yards_to_meters &lt;- function(yards) { yards * 36 * 0.0254 } # convert areas in square meters to areas in hectares. # Write a function to convert sq. meters to hectares sq_meters_to_hectares &lt;- function(sq_meters) { sq_meters / 10000 } 12.4.1.2 Convert areas to metric 2 You need another utility function to deal with getting from square yards to square meters. Then, you can bring everything together to write the overall acres-to-hectares conversion function. magrittr’s pipeable operator replacements x * y = x %&gt;% multiply_by(y) x ^ y = x %&gt;% raise_to_power(y) x[y] = x %&gt;% extract2(\"y\") library(magrittr) # Write a function to convert distance in square yards to square meters sq_yards_to_sq_meters &lt;- function(sq_yards) { sq_yards %&gt;% # Take the square root sqrt() %&gt;% # Convert yards to meters yards_to_meters() %&gt;% # Square it raise_to_power(2) } # Write a function to convert areas in acres to hectares acres_to_hectares &lt;- function(acres) { acres %&gt;% # Convert acres to sq yards acres_to_sq_yards() %&gt;% # Convert sq yards to sq meters sq_yards_to_sq_meters() %&gt;% # Convert sq meters to hectares sq_meters_to_hectares() } # harmonically convert areas in acres to hectares. # Define a harmonic acres to hectares function harmonic_acres_to_hectares &lt;- function(acres) { acres %&gt;% # Get the reciprocal get_reciprocal() %&gt;% # Convert acres to hectares acres_to_hectares() %&gt;% # Get the reciprocal again get_reciprocal() } By breaking down this conversion into lots of simple functions, you have easy to read code, which helps guard against bugs. 12.4.1.3 Convert yields to metric The yields in the NASS corn data are also given in US units, namely bushels per acre. You’ll need to write some more utility functions to convert this unit to the metric unit of kg per hectare. Bushels historically meant a volume of 8 gallons, but in the context of grain, they are now defined as masses. This mass differs for each grain! To solve this exercise, you need to know these facts. One pound (lb) is 0.45359237 kilograms (kg). One bushel is 48 lbs of barley, 56 lbs of corn, or 60 lbs of wheat. # Write a function to convert lb to kg lbs_to_kgs &lt;- function(lbs) { lbs * 0.45359237 } # Write a function to convert bushels to lbs bushels_to_lbs &lt;- function(bushels, crop) { # Define a lookup table of scale factors c(barley = 48, corn = 56, wheat = 60) %&gt;% # Extract the value for the crop extract2(crop) %&gt;% # Multiply by the no. of bushels multiply_by(bushels) } # Write a function to convert bushels to kg bushels_to_kgs &lt;- function(bushels, crop) { bushels %&gt;% # Convert bushels to lbs for this crop bushels_to_lbs(crop) %&gt;% # Convert lbs to kgs lbs_to_kgs() } # Write a function to convert bushels/acre to kg/ha bushels_per_acre_to_kgs_per_hectare &lt;- function(bushels_per_acre, crop = c(&quot;barley&quot;, &quot;corn&quot;, &quot;wheat&quot;)) { # Match the crop argument crop &lt;- match.arg(crop) bushels_per_acre %&gt;% # Convert bushels to kgs for this crop bushels_to_kgs(crop) %&gt;% # Convert harmonic acres to ha harmonic_acres_to_hectares() } You now have a full stack of functions for converting the units. 12.4.1.4 Apply the unit conversion Now that you’ve written some functions, it’s time to apply them! The NASS corn dataset is available, and you can fortify it (jargon for “adding new columns”) with metrics areas and yields. This fortification process can also be turned into a function, so you’ll define a function for this, and test it on the NASS wheat dataset. corn &lt;- read_rds(&quot;data/nass.corn.rds&quot;) wheat &lt;- read_rds(&quot;data/nass.wheat.rds&quot;) # View the corn dataset glimpse(corn) ## Rows: 6,381 ## Columns: 4 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Connectic… ## $ farmed_area_acres &lt;dbl&gt; 1050000, 280000, 42000, 57000, 200000, 125000, … ## $ yield_bushels_per_acre &lt;dbl&gt; 9.0, 18.0, 28.0, 34.0, 23.0, 9.0, 6.0, 29.0, 36… corn &lt;- corn %&gt;% # Add some columns mutate( # Convert farmed area from acres to ha farmed_area_ha = acres_to_hectares(farmed_area_acres), # Convert yield from bushels/acre to kg/ha yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare( yield_bushels_per_acre, crop = &quot;corn&quot; ) ) glimpse(corn) ## Rows: 6,381 ## Columns: 6 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Connectic… ## $ farmed_area_acres &lt;dbl&gt; 1050000, 280000, 42000, 57000, 200000, 125000, … ## $ yield_bushels_per_acre &lt;dbl&gt; 9.0, 18.0, 28.0, 34.0, 23.0, 9.0, 6.0, 29.0, 36… ## $ farmed_area_ha &lt;dbl&gt; 424919.92, 113311.98, 16996.80, 23067.08, 80937… ## $ yield_kg_per_ha &lt;dbl&gt; 564.9090, 1129.8180, 1757.4946, 2134.1006, 1443… # Wrap this code into a function fortify_with_metric_units &lt;- function(data, crop) { data %&gt;% mutate( farmed_area_ha = acres_to_hectares(farmed_area_acres), yield_kg_per_ha = bushels_per_acre_to_kgs_per_hectare( yield_bushels_per_acre, crop = crop ) ) } # Try it on the wheat dataset wheat &lt;- fortify_with_metric_units(wheat, &quot;wheat&quot;) glimpse(wheat) ## Rows: 5,963 ## Columns: 6 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Connectic… ## $ farmed_area_acres &lt;dbl&gt; 125000, 50000, 650000, 2000, 59000, 245000, 230… ## $ yield_bushels_per_acre &lt;dbl&gt; 5.0, 6.5, 18.0, 17.5, 11.0, 4.0, 10.5, 10.0, 13… ## $ farmed_area_ha &lt;dbl&gt; 50585.7053, 20234.2821, 263045.6675, 809.3713, … ## $ yield_kg_per_ha &lt;dbl&gt; 336.2553, 437.1320, 1210.5192, 1176.8937, 739.7… 12.4.2 Visualizing grain yields 12.4.2.1 Plot yields over time An obvious question to ask about each crop is, “how do the yields change over time in each US state?” # Using corn, plot yield (kg/ha) vs. year ggplot(corn, aes(year, yield_kg_per_ha)) + # Add a line layer, grouped by state geom_line(aes(group = state)) + # Add a smooth trend layer geom_smooth() ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Turn the plotting code into a function. # Wrap this plotting code into a function plot_yield_vs_year &lt;- function(data) { ggplot(data, aes(year, yield_kg_per_ha)) + geom_line(aes(group = state)) + geom_smooth() } # Test it on the wheat dataset plot_yield_vs_year(wheat) ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Look at the huge increase in yields from the time of the Green Revolution in the 1950s. 12.4.2.2 A nation divided The USA has a varied climate, so we might expect yields to differ between states. Rather than trying to reason about 50 states separately, we can use the USA Census Regions to get 9 groups. The “Corn Belt”, where most US corn is grown is in the “West North Central” and “East North Central” regions. The “Wheat Belt” is in the “West South Central” region. usa_census_regions &lt;- read_delim(&quot;data/usa_census_regions.txt&quot;, delim = &quot;,&quot;) usa_census_regions ## # A tibble: 51 × 2 ## census_region state ## &lt;chr&gt; &lt;chr&gt; ## 1 New England Connecticut ## 2 New England Maine ## 3 New England Massachusetts ## 4 New England New Hampshire ## 5 New England Rhode Island ## 6 New England Vermont ## 7 Mid-Atlantic New Jersey ## 8 Mid-Atlantic New York ## 9 Mid-Atlantic Pennsylvania ## 10 East North Central Illinois ## # ℹ 41 more rows # Inner join the corn dataset to usa_census_regions by state corn &lt;- corn %&gt;% inner_join(usa_census_regions, by = &quot;state&quot;) glimpse(corn) ## Rows: 6,381 ## Columns: 7 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Connectic… ## $ farmed_area_acres &lt;dbl&gt; 1050000, 280000, 42000, 57000, 200000, 125000, … ## $ yield_bushels_per_acre &lt;dbl&gt; 9.0, 18.0, 28.0, 34.0, 23.0, 9.0, 6.0, 29.0, 36… ## $ farmed_area_ha &lt;dbl&gt; 424919.92, 113311.98, 16996.80, 23067.08, 80937… ## $ yield_kg_per_ha &lt;dbl&gt; 564.9090, 1129.8180, 1757.4946, 2134.1006, 1443… ## $ census_region &lt;chr&gt; &quot;East South Central&quot;, &quot;West South Central&quot;, &quot;Pa… Turn the code into a function. # Wrap this code into a function fortify_with_census_region &lt;- function(data) { data %&gt;% inner_join(usa_census_regions, by = &quot;state&quot;) } # Try it on the wheat dataset wheat &lt;- fortify_with_census_region(wheat) glimpse(wheat) ## Rows: 5,963 ## Columns: 7 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Arkansas&quot;, &quot;California&quot;, &quot;Connectic… ## $ farmed_area_acres &lt;dbl&gt; 125000, 50000, 650000, 2000, 59000, 245000, 230… ## $ yield_bushels_per_acre &lt;dbl&gt; 5.0, 6.5, 18.0, 17.5, 11.0, 4.0, 10.5, 10.0, 13… ## $ farmed_area_ha &lt;dbl&gt; 50585.7053, 20234.2821, 263045.6675, 809.3713, … ## $ yield_kg_per_ha &lt;dbl&gt; 336.2553, 437.1320, 1210.5192, 1176.8937, 739.7… ## $ census_region &lt;chr&gt; &quot;East South Central&quot;, &quot;West South Central&quot;, &quot;Pa… With the census data incorporated into the crop datasets, you can now look at yield differences between the regions. 12.4.2.3 Plot yields over time by region Now you are ready to look at how the yields change over time in each region of the USA. Use the function you wrote to plot yield versus year for the corn dataset, then facet the plot, wrapped by census_region. # Plot yield vs. year for the corn dataset plot_yield_vs_year(corn) + # Facet, wrapped by census region facet_wrap(vars(census_region)) ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; The corn yields are highest in the West North Central region, the heart of the Corn Belt. Turn the code into a function. # Wrap this code into a function plot_yield_vs_year_by_region &lt;- function(data) { plot_yield_vs_year(data) + facet_wrap(vars(census_region)) } # Try it on the wheat dataset plot_yield_vs_year_by_region(wheat) ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; For wheat, it looks like the yields are highest in the Wheat Belt (West South Central region) have been overtaken by some other regions. 12.4.3 Modeling grain yields 12.4.3.1 Running a model Notice that the lines on the plot aren’t straight. This means that we need to choose a model that handles nonlinear responses. geom_smooth creates the smooth trend lines by using generalized additive models (gam). generalized additive model mgcv package GAM syntax: gam(response ~ s(explanatory_var1) + explanatory_var2, data = dataset) Here, s() means “make the variable smooth”, where smooth very roughly means nonlinear. library(mgcv) # Run a generalized additive model of yield vs. smoothed year and census region corn_model &lt;- gam(yield_kg_per_ha ~ s(year) + census_region, data = corn) corn_model ## ## Family: gaussian ## Link function: identity ## ## Formula: ## yield_kg_per_ha ~ s(year) + census_region ## ## Estimated degrees of freedom: ## 7.64 total = 16.64 ## ## GCV score: 836963 Wrap the modeling code into a function. # Wrap the model code into a function run_gam_yield_vs_year_by_region &lt;- function(data) { gam(yield_kg_per_ha ~ s(year) + census_region, data = data) } # Try it on the wheat dataset wheat_model &lt;- run_gam_yield_vs_year_by_region(wheat) wheat_model ## ## Family: gaussian ## Link function: identity ## ## Formula: ## yield_kg_per_ha ~ s(year) + census_region ## ## Estimated degrees of freedom: ## 7.03 total = 16.03 ## ## GCV score: 316685.5 12.4.3.2 Making yield predictions Using the models to make predictions. You can do this using a call to predict(), in the following form: predict(model, cases_to_predict, type = \"response\") GAMs of the corn and wheat datasets are available as corn_model and wheat_model. A character vector of census regions is stored as census_regions. census_regions &lt;- c(&quot;New England&quot;, &quot;Mid-Atlantic&quot;, &quot;East North Central&quot;, &quot;West North Central&quot;, &quot;South Atlantic&quot;, &quot;East South Central&quot;, &quot;West South Central&quot;, &quot;Mountain&quot;, &quot;Pacific&quot; ) # Make predictions in 2050 predict_this &lt;- data.frame( year = 2050, census_region = census_regions ) # Predict the yield pred_yield_kg_per_ha &lt;- predict(corn_model, predict_this, type = &quot;response&quot;) predict_this %&gt;% # Add the prediction as a column of predict_this mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha) %&gt;% arrange(desc(pred_yield_kg_per_ha)) ## year census_region pred_yield_kg_per_ha ## 1 2050 Pacific 14396.33 ## 2 2050 New England 13971.80 ## 3 2050 East North Central 13858.74 ## 4 2050 Mid-Atlantic 13483.19 ## 5 2050 Mountain 13360.67 ## 6 2050 West North Central 13262.68 ## 7 2050 West South Central 12616.83 ## 8 2050 South Atlantic 12517.50 ## 9 2050 East South Central 12481.83 Wrap the script into a function. # Wrap this prediction code into a function predict_yields &lt;- function(model, year) { predict_this &lt;- data.frame( year = year, census_region = census_regions ) pred_yield_kg_per_ha &lt;- predict(model, predict_this, type = &quot;response&quot;) predict_this %&gt;% mutate(pred_yield_kg_per_ha = pred_yield_kg_per_ha) %&gt;% arrange(desc(pred_yield_kg_per_ha)) } # Try it on the wheat dataset predict_yields(wheat_model, 2050) ## year census_region pred_yield_kg_per_ha ## 1 2050 Pacific 5707.724 ## 2 2050 New England 5506.308 ## 3 2050 East North Central 5470.007 ## 4 2050 Mountain 5457.741 ## 5 2050 Mid-Atlantic 5425.554 ## 6 2050 South Atlantic 5079.523 ## 7 2050 East South Central 4985.757 ## 8 2050 West North Central 4984.050 ## 9 2050 West South Central 4764.289 The models predict that in 2050, the highest yields will be in the Pacific region for both corn and wheat. 12.4.3.3 Do it all over again Now you are going to rerun the whole analysis from this chapter on a new crop, barley. Barley prefers a cooler climate compared to corn and wheat and is commonly grown in the US mountain states of Idaho and Montana. Since all the infrastructure is in place, fortify_with_metric_units(), fortify_with_census_region(), plot_yield_vs_year_by_region(), run_gam_yield_vs_year_by_region(), and predict_yields() that’s less effort than it sounds! barley &lt;- read_rds(&quot;data/nass.barley.rds&quot;) head(barley) ## year state farmed_area_acres yield_bushels_per_acre ## 1 1866 Connecticut 1000 22.5 ## 2 1866 Illinois 96000 23.4 ## 3 1866 Indiana 11000 23.0 ## 4 1866 Iowa 66000 22.0 ## 5 1866 Kansas 2000 23.0 ## 6 1866 Kentucky 10000 23.5 Fortify the barley data with metric units, then with census regions. fortified_barley &lt;- barley %&gt;% # Fortify with metric units fortify_with_metric_units(&quot;barley&quot;) %&gt;% # Fortify with census regions fortify_with_census_region() # See the result glimpse(fortified_barley) ## Rows: 4,839 ## Columns: 7 ## $ year &lt;int&gt; 1866, 1866, 1866, 1866, 1866, 1866, 1866, 1866,… ## $ state &lt;chr&gt; &quot;Connecticut&quot;, &quot;Illinois&quot;, &quot;Indiana&quot;, &quot;Iowa&quot;, &quot;… ## $ farmed_area_acres &lt;dbl&gt; 1000, 96000, 11000, 66000, 2000, 10000, 34000, … ## $ yield_bushels_per_acre &lt;dbl&gt; 22.5, 23.4, 23.0, 22.0, 23.0, 23.5, 21.5, 25.5,… ## $ farmed_area_ha &lt;dbl&gt; 404.6856, 38849.8217, 4451.5421, 26709.2524, 80… ## $ yield_kg_per_ha &lt;dbl&gt; 1210.5192, 1258.9400, 1237.4197, 1183.6188, 123… ## $ census_region &lt;chr&gt; &quot;New England&quot;, &quot;East North Central&quot;, &quot;East Nort… Plot the yield versus year by census region. # Plot yield vs. year by region fortified_barley %&gt;% plot_yield_vs_year_by_region() ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Run a GAM of yield versus year by census region, then predict the yields in 2050. fortified_barley %&gt;% # Run a GAM of yield vs. year by region run_gam_yield_vs_year_by_region() %&gt;% # Make predictions of yields in 2050 predict_yields(2050) ## year census_region pred_yield_kg_per_ha ## 1 2050 Mountain 4638.785 ## 2 2050 Pacific 4267.937 ## 3 2050 South Atlantic 4248.425 ## 4 2050 Mid-Atlantic 4247.439 ## 5 2050 New England 4229.927 ## 6 2050 East North Central 4210.503 ## 7 2050 East South Central 4015.834 ## 8 2050 West North Central 3843.945 ## 9 2050 West South Central 3638.759 Since all your analysis code was contained in functions, it was really simple to apply it to another dataset. Here you can see that yields are highest in the Mountain region, and the model predicts that this will still be the case in 2050. "],["exploratory-data-analysis-in-r.html", "Chapter 13 Exploratory Data Analysis in R 13.1 Exploring Categorical Data 13.2 Exploring numerical data 13.3 Numerical Summaries 13.4 Case Study", " Chapter 13 Exploratory Data Analysis in R 13.1 Exploring Categorical Data 13.1.1 Categorical data 13.1.1.1 Contingency table Create a contingency table by table(), is a useful way to represent the total counts of observations that fall into each combination of the levels of categorical variables. comics dataset is a collection of characteristics on all of the superheroes created by Marvel and DC comics in the last 80 years. library(tidyverse) comics &lt;- read_csv(&quot;data/comics.csv&quot;, col_types = &quot;ffffffffiff&quot;) glimpse(comics) ## Rows: 23,272 ## Columns: 11 ## $ name &lt;fct&gt; &quot;Spider-Man (Peter Parker)&quot;, &quot;Captain America (Steven Rog… ## $ id &lt;fct&gt; Secret, Public, Public, Public, No Dual, Public, Public, … ## $ align &lt;fct&gt; Good, Good, Neutral, Good, Good, Good, Good, Good, Neutra… ## $ eye &lt;fct&gt; Hazel Eyes, Blue Eyes, Blue Eyes, Blue Eyes, Blue Eyes, B… ## $ hair &lt;fct&gt; Brown Hair, White Hair, Black Hair, Black Hair, Blond Hai… ## $ gender &lt;fct&gt; Male, Male, Male, Male, Male, Male, Male, Male, Male, Mal… ## $ gsm &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ alive &lt;fct&gt; Living Characters, Living Characters, Living Characters, … ## $ appearances &lt;int&gt; 4043, 3360, 3061, 2961, 2258, 2255, 2072, 2017, 1955, 193… ## $ first_appear &lt;fct&gt; &quot;Aug-62&quot;, &quot;Mar-41&quot;, &quot;Oct-74&quot;, &quot;Mar-63&quot;, &quot;Nov-50&quot;, &quot;Nov-61… ## $ publisher &lt;fct&gt; marvel, marvel, marvel, marvel, marvel, marvel, marvel, m… # Check levels of align levels(comics$align) ## [1] &quot;Good&quot; &quot;Neutral&quot; &quot;Bad&quot; ## [4] &quot;Reformed Criminals&quot; # Check the levels of gender levels(comics$gender) ## [1] &quot;Male&quot; &quot;Female&quot; &quot;Other&quot; # Create a 2-way contingency table tab &lt;- table(comics$gender, comics$align); tab ## ## Good Neutral Bad Reformed Criminals ## Male 4809 1799 7561 2 ## Female 2490 836 1573 1 ## Other 17 17 32 0 13.1.1.2 Dropping levels The contingency table from the last exercise revealed that there are some levels that have very low counts. To simplify the analysis, it often helps to drop such levels. Two steps: filtering out any rows with the levels that have very low counts, removing these levels from the factor variable with droplevels(). (This is because the droplevels() would keep levels that have just 1 or 2 counts; it only drops levels that don’t exist in a dataset.) Find out which level of align has the fewest total entries. Then filter out all rows of comics with that level, then drop the unused level with droplevels(). # Remove align level comics_filtered &lt;- comics %&gt;% filter(align != &quot;Reformed Criminals&quot;) %&gt;% droplevels() # See the result comics_filtered ## # A tibble: 19,856 × 11 ## name id align eye hair gender gsm alive appearances first_appear ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;fct&gt; ## 1 &quot;Spider-… Secr… Good Haze… Brow… Male &lt;NA&gt; Livi… 4043 Aug-62 ## 2 &quot;Captain… Publ… Good Blue… Whit… Male &lt;NA&gt; Livi… 3360 Mar-41 ## 3 &quot;Wolveri… Publ… Neut… Blue… Blac… Male &lt;NA&gt; Livi… 3061 Oct-74 ## 4 &quot;Iron Ma… Publ… Good Blue… Blac… Male &lt;NA&gt; Livi… 2961 Mar-63 ## 5 &quot;Thor (T… No D… Good Blue… Blon… Male &lt;NA&gt; Livi… 2258 Nov-50 ## 6 &quot;Benjami… Publ… Good Blue… No H… Male &lt;NA&gt; Livi… 2255 Nov-61 ## 7 &quot;Reed Ri… Publ… Good Brow… Brow… Male &lt;NA&gt; Livi… 2072 Nov-61 ## 8 &quot;Hulk (R… Publ… Good Brow… Brow… Male &lt;NA&gt; Livi… 2017 May-62 ## 9 &quot;Scott S… Publ… Neut… Brow… Brow… Male &lt;NA&gt; Livi… 1955 Sep-63 ## 10 &quot;Jonatha… Publ… Good Blue… Blon… Male &lt;NA&gt; Livi… 1934 Nov-61 ## # ℹ 19,846 more rows ## # ℹ 1 more variable: publisher &lt;fct&gt; str(comics_filtered$align) ## Factor w/ 3 levels &quot;Good&quot;,&quot;Neutral&quot;,..: 1 1 2 1 1 1 1 1 2 1 ... 13.1.1.3 Side-by-side bar charts While a contingency table represents the counts numerically, it’s often more useful to represent them graphically. Here you’ll construct two side-by-side bar charts of the comics data. This shows that there can often be two or more options for presenting the same data. # Create side-by-side bar chart of gender by alignment ggplot(comics, aes(x = align, fill = gender)) + geom_bar(position = &quot;dodge&quot;) # Create side-by-side bar chart of alignment by gender ggplot(comics, aes(x = gender, fill = align)) + geom_bar(position = &quot;dodge&quot;) + theme(axis.text.x = element_text(angle = 90)) 13.1.2 Counts vs. proportions 13.1.2.1 Proportions # Simplify display format options(scipen = 999, digits = 3) tab_cnt &lt;- table(comics$id, comics$align) tab_cnt ## ## Good Neutral Bad Reformed Criminals ## Secret 2475 959 4493 1 ## Public 2930 965 2172 1 ## No Dual 647 390 474 0 ## Unknown 0 2 7 0 Compute the proportions by prop.table(table). prop.table(tab_cnt) ## ## Good Neutral Bad Reformed Criminals ## Secret 0.1595128 0.0618072 0.2895721 0.0000644 ## Public 0.1888373 0.0621939 0.1399845 0.0000644 ## No Dual 0.0416989 0.0251353 0.0305491 0.0000000 ## Unknown 0.0000000 0.0001289 0.0004511 0.0000000 Note that because these are all proportions out of the whole dataset, the sum of all of these proportions is 1. sum(prop.table(tab_cnt)) ## [1] 1 13.1.2.2 Conditional proportions condition on the rows: prop.table(table, 1) prop.table(tab_cnt, 1) ## ## Good Neutral Bad Reformed Criminals ## Secret 0.312185 0.120964 0.566726 0.000126 ## Public 0.482861 0.159031 0.357943 0.000165 ## No Dual 0.428193 0.258107 0.313700 0.000000 ## Unknown 0.000000 0.222222 0.777778 0.000000 Because we’re conditioning on row, it’s every row that now sums to one. rowSums(prop.table(tab_cnt, 1)) ## Secret Public No Dual Unknown ## 1 1 1 1 condition on the columns: prop.table(table, 2) prop.table(tab_cnt, 2) ## ## Good Neutral Bad Reformed Criminals ## Secret 0.408956 0.414076 0.628743 0.500000 ## Public 0.484137 0.416667 0.303946 0.500000 ## No Dual 0.106907 0.168394 0.066331 0.000000 ## Unknown 0.000000 0.000864 0.000980 0.000000 Because we’re conditioning on column, it’s every column that now sums to one. colSums(prop.table(tab_cnt, 2)) ## Good Neutral Bad Reformed Criminals ## 1 1 1 1 Approximately 41% of all good characters are secret. 13.1.2.3 Conditional bar chart Bar charts can tell dramatically different stories depending on whether they represent counts or proportions and, if proportions, what the proportions are conditioned on. By adding position = \"fill\" to geom_bar(), you are saying you want the bars to fill the entire height of the plotting window, thus displaying proportions and not raw counts. Create a stacked bar chart of gender counts. # Plot of gender by align ggplot(comics, aes(x = align, fill = gender)) + geom_bar() Create a stacked bar chart of gender proportions. # Plot proportion of gender, conditional on align ggplot(comics, aes(x = align, fill = gender)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;proportion&quot;) 13.1.3 Distribution of one variable 13.1.3.1 Marginal bar chart If you are interested in the distribution of alignment of all superheroes, it makes sense to construct a bar chart for just that single variable. You can improve the interpretability of the plot, though, by implementing some sensible ordering. # Change the order of the levels in align comics$align &lt;- factor(comics$align, levels = c(&quot;Bad&quot;, &quot;Neutral&quot;, &quot;Good&quot;)) # Create plot of align ggplot(comics, aes(x = align)) + geom_bar() 13.1.3.2 Conditional bar chart Now, if you want to break down the distribution of alignment based on gender, you’re looking for conditional distributions. You could make these by creating multiple filtered datasets (one for each gender) or by faceting the plot of alignment based on gender. Create a bar chart of align faceted by gender. # Plot of alignment broken down by gender ggplot(comics, aes(x = align)) + geom_bar() + facet_wrap(~ gender) 13.1.3.3 Improve pie chart The pie chart is a very common way to represent the distribution of a single categorical variable, but they can be more difficult to interpret than bar charts. This is a pie chart of a dataset called pies that contains the favorite pie flavors of 98 people. pies &lt;- read_delim(&quot;data/pies.txt&quot;, delim = &quot;,&quot;) ## Rows: 98 Columns: 1 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): flavor ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. pies$flavor &lt;- as.factor(pies$flavor) # pie chart, 很難看出數量很難看出數量高低 ggplot(pies, aes(x = factor(1), fill = flavor)) + geom_bar(width = 1) + coord_polar(theta = &quot;y&quot;) + theme_void() Improve the representation of these data by constructing a bar chart that is ordered in descending order of count. # Put levels of flavor in descending order lev &lt;- c(&quot;apple&quot;, &quot;key lime&quot;, &quot;boston creme&quot;, &quot;blueberry&quot;, &quot;cherry&quot;, &quot;pumpkin&quot;, &quot;strawberry&quot;) pies$flavor &lt;- factor(pies$flavor, levels = lev) # Create bar chart of flavor ggplot(pies, aes(x = flavor)) + geom_bar(fill = &quot;chartreuse&quot;) + theme(axis.text.x = element_text(angle = 90)) 13.2 Exploring numerical data 13.2.1 Numerical Data 13.2.1.1 Faceted histogram cars dataset, which records characteristics on all of the new models of cars for sale in the US in a certain year. You will investigate the distribution of mileage across a categorical variable, cars &lt;- read_csv(&quot;data/cars04.csv&quot;) # Learn data structure str(cars, give.attr = FALSE) ## spc_tbl_ [428 × 19] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ name : chr [1:428] &quot;Chevrolet Aveo 4dr&quot; &quot;Chevrolet Aveo LS 4dr hatch&quot; &quot;Chevrolet Cavalier 2dr&quot; &quot;Chevrolet Cavalier 4dr&quot; ... ## $ sports_car : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ suv : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ wagon : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ minivan : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ pickup : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ all_wheel : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ rear_wheel : logi [1:428] FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ msrp : num [1:428] 11690 12585 14610 14810 16385 ... ## $ dealer_cost: num [1:428] 10965 11802 13697 13884 15357 ... ## $ eng_size : num [1:428] 1.6 1.6 2.2 2.2 2.2 2 2 2 2 2 ... ## $ ncyl : num [1:428] 4 4 4 4 4 4 4 4 4 4 ... ## $ horsepwr : num [1:428] 103 103 140 140 140 132 132 130 110 130 ... ## $ city_mpg : num [1:428] 28 28 26 26 26 29 29 26 27 26 ... ## $ hwy_mpg : num [1:428] 34 34 37 37 37 36 36 33 36 33 ... ## $ weight : num [1:428] 2370 2348 2617 2676 2617 ... ## $ wheel_base : num [1:428] 98 98 104 104 104 105 105 103 103 103 ... ## $ length : num [1:428] 167 153 183 183 183 174 174 168 168 168 ... ## $ width : num [1:428] 66 66 69 68 69 67 67 67 67 67 ... Plot a histogram of city_mpg faceted by suv, a logical variable indicating whether the car is an SUV or not. # Create faceted histogram ggplot(cars, aes(x = city_mpg)) + geom_histogram() + facet_wrap(~ suv) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. ## Warning: Removed 14 rows containing non-finite values ## (`stat_bin()`). you can facet a plot by any categorical variable using facet_wrap(). 13.2.1.2 Boxplots &amp; density plots The mileage of a car tends to be associated with the size of its engine (as measured by the number of cylinders). To explore the relationship between these two variables, you’ll try your hand at two alternatives: the box plot and the density plot. A quick look at ncyl shows that there are more possible levels of ncyl than you might think. Here, restrict attention to the most common levels. unique(cars$ncyl) ## [1] 4 6 3 8 5 12 10 -1 # Filter cars with 4, 6, 8 cylinders common_cyl &lt;- filter(cars, cars$ncyl %in% c(4, 6, 8)) Create side-by-side box plots. # Create box plots of city mpg by ncyl ggplot(common_cyl, aes(x = as.factor(ncyl), y = city_mpg)) + geom_boxplot() ## Warning: Removed 11 rows containing non-finite values ## (`stat_boxplot()`). Create overlaid density plots. # Create overlaid density plots for same data ggplot(common_cyl, aes(x = city_mpg, fill = as.factor(ncyl))) + geom_density(alpha = .3) ## Warning: Removed 11 rows containing non-finite values ## (`stat_density()`). 13.2.2 Distribution of one variable 13.2.2.1 Marginal &amp; conditional hist Now, turn your attention to a new variable: horsepwr. The goal is to get a sense of the marginal distribution of this variable and then compare it to the distribution of horsepower conditional on the price of the car being less than $25,000. # Create hist of horsepwr cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram() + ggtitle(&quot;marginal distribution of horsepower&quot;) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. # Create hist of horsepwr for affordable cars cars %&gt;% filter(msrp &lt; 25000) %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram() + xlim(c(90, 550)) + ggtitle(&quot;distribution of horsepower conditional on the price less than $25000&quot;) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. ## Warning: Removed 1 rows containing non-finite values ## (`stat_bin()`). ## Warning: Removed 2 rows containing missing values ## (`geom_bar()`). 13.2.2.2 Binwidths The binwidth determines how smooth your distribution will appear: the smaller the binwidth, the more jagged your distribution becomes. It’s good practice to consider several binwidths in order to detect different types of structure in your data. # Create hist of horsepwr with binwidth of 3 plot_A &lt;- cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 3) + ggtitle(&quot;Plot A, binwidth = 3&quot;) # Create hist of horsepwr with binwidth of 30 plot_B &lt;- cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 30) + ggtitle(&quot;Plot B, binwidth = 30&quot;) # Create hist of horsepwr with binwidth of 60 plot_C &lt;- cars %&gt;% ggplot(aes(x = horsepwr)) + geom_histogram(binwidth = 60) + ggtitle(&quot;Plot C, binwidth = 60&quot;) # put all plot together to compare gridExtra::grid.arrange(plot_A, plot_B, plot_C, nrow = 1) Plot A is the only histogram that shows the count for cars with exactly 200 and 300 horsepower. 13.2.3 Box plots 13.2.3.1 Box plots for outliers In addition to indicating the center and spread of a distribution, a box plot provides a graphical means to detect outliers. You can apply this method to the msrp column (manufacturer’s suggested retail price) to detect if there are unusually expensive or cheap cars. # Construct box plot of msrp cars %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() # Exclude outliers from data cars_no_out &lt;- cars %&gt;% filter(msrp &lt; 100000) # Construct box plot of msrp using the reduced dataset cars_no_out %&gt;% ggplot(aes(x = 1, y = msrp)) + geom_boxplot() 13.2.3.2 Plot selection Both density plots and box plots display the central tendency and spread of the data, but the box plot is more robust to outliers. Consider two other columns in the cars dataset: city_mpg and width. Which is the most appropriate plot for displaying the important features of their distributions? For each variable, try both plots and decide which one is better at capturing the important structure. # Create plot of city_mpg # box plot city_mpg_boxplot &lt;- cars %&gt;% ggplot(aes(x = 1, y = city_mpg)) + geom_boxplot() # density plot city_mpg_density &lt;- cars %&gt;% ggplot(aes(x = city_mpg)) + geom_density() # Create plot of width # box plot width_boxplot &lt;- cars %&gt;% ggplot(aes(x = 1, y = width)) + geom_boxplot() # density plot width_density &lt;- cars %&gt;% ggplot(aes(x = width)) + geom_density() gridExtra::grid.arrange(city_mpg_boxplot, city_mpg_density, width_boxplot, width_density, nrow = 2) ## Warning: Removed 14 rows containing non-finite values ## (`stat_boxplot()`). ## Warning: Removed 14 rows containing non-finite values ## (`stat_density()`). ## Warning: Removed 28 rows containing non-finite values ## (`stat_boxplot()`). ## Warning: Removed 28 rows containing non-finite values ## (`stat_density()`). Because the city_mpg variable has a much wider range with its outliers, it’s best to display its distribution as a box plot. 13.2.4 Visual in higher dimensions Higher dimensional plots: Shape, Size, Color, Pattern, Movement, x-coordinate, y-coordinate 13.2.4.1 three variables plot Faceting is a valuable technique for looking at several conditional distributions at the same time. If the faceted distributions are laid out in a grid, you can consider the association between a variable and two others, one on the rows of the grid and the other on the columns. Using common_cyl, create a histogram of hwy_mpg. Grid-facet the plot rowwise by ncyl and columnwise by suv. # Facet hists using hwy mileage and ncyl common_cyl %&gt;% ggplot(aes(x = hwy_mpg)) + geom_histogram() + facet_grid(ncyl ~ suv) + ggtitle(&quot;hwy_mpg faceted by ncyl and suv&quot;) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. ## Warning: Removed 11 rows containing non-finite values ## (`stat_bin()`). 13.3 Numerical Summaries Characteristics of a distribution Center Variability Shape Outliers 13.3.1 Measures of center Center: mean, median, mode The choice of measure for center can have a dramatic impact on what we consider to be a typical observation, so it is important that you consider the shape of the distribution before deciding on the measure. 13.3.1.1 Calculate center measures You will use data from gapminder, which tracks demographic data in countries of the world over time. How the life expectancy differs from continent to continent? # Create dataset of 2007 data gap2007 &lt;- filter(gapminder::gapminder, year == 2007) # Compute groupwise mean and median lifeExp gap2007 %&gt;% group_by(continent) %&gt;% summarize(mean(lifeExp), median(lifeExp)) ## # A tibble: 5 × 3 ## continent `mean(lifeExp)` `median(lifeExp)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 54.8 52.9 ## 2 Americas 73.6 72.9 ## 3 Asia 70.7 72.4 ## 4 Europe 77.6 78.6 ## 5 Oceania 80.7 80.7 # Generate box plots of lifeExp for each continent gap2007 %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() 13.3.2 Measures of variability Variability: variance, standard deviation, interquartile range(IQR), range The choice of measure for spread can dramatically impact how variable we consider our data to be, so it is important that you consider the shape of the distribution before deciding on the measure. IQR is useful when your dataset is heavily skewed or has extreme observations. 13.3.2.1 Calculate spread measures summarize life expectancies using the sd(), the IQR(), and the count of countries, n(). # Compute groupwise measures of spread gap2007 %&gt;% group_by(continent) %&gt;% summarize(sd(lifeExp), IQR(lifeExp), n()) ## # A tibble: 5 × 4 ## continent `sd(lifeExp)` `IQR(lifeExp)` `n()` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 9.63 11.6 52 ## 2 Americas 4.44 4.63 25 ## 3 Asia 7.96 10.2 33 ## 4 Europe 2.98 4.78 30 ## 5 Oceania 0.729 0.516 2 # Graphically compare the spread of these distributions # Generate overlaid density plots gap2007 %&gt;% ggplot(aes(x = lifeExp, fill = continent)) + geom_density(alpha = 0.3) 13.3.2.2 Choose measures for center &amp; spread Using the shapes of the density plots, calculate the most appropriate measures of center and spread. # The distribution of life expectancy in the countries of the Americas. gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% ggplot(aes(x = lifeExp)) + geom_density() # Compute stats for lifeExp in Americas gap2007 %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% summarize(mean(lifeExp), sd(lifeExp)) ## # A tibble: 1 × 2 ## `mean(lifeExp)` `sd(lifeExp)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 73.6 4.44 # The distribution of country populations across the entire gap2007 dataset. gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() median and IQR measure the central tendency and spread, respectively, but are robust to outliers and non-normal data. # Compute stats for population gap2007 %&gt;% summarize(median(pop), IQR(pop)) ## # A tibble: 1 × 2 ## `median(pop)` `IQR(pop)` ## &lt;dbl&gt; &lt;dbl&gt; ## 1 10517531 26702008. 13.3.3 Shape &amp; transformations Modality The modality of a distribution is the number of prominent humps that show up in the distribution. Skew The skew is where the long tail is. 13.3.3.1 Transformations Highly skewed distributions can make it very difficult to learn anything from a visualization. Transformations can be helpful in revealing the more subtle structure. # Original pop shape pop_nontrans_plot &lt;- gap2007 %&gt;% ggplot(aes(x = pop)) + geom_density() + ggtitle(&quot;population without transform&quot;) # Transform the skewed pop variable gap2007 &lt;- gap2007 %&gt;% mutate(log_pop = log(pop)) # Create density plot of new variable pop_trans_plot &lt;- gap2007 %&gt;% ggplot(aes(x = log_pop)) + geom_density() + ggtitle(&quot;population with log transform&quot;) # compare transforming gridExtra::grid.arrange(pop_nontrans_plot, pop_trans_plot, nrow = 1) 13.3.4 Outliers 13.3.4.1 Identify outliers Consider the distribution, shown here, of the life expectancies of the countries in Asia. The box plot identifies one clear outlier: a country with a notably low life expectancy. # box plot for Asia lifeExp gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() # which country with lowest lifeExp gap2007[which.min(gap2007$lifeExp), c(&quot;country&quot;, &quot;lifeExp&quot;)] ## # A tibble: 1 × 2 ## country lifeExp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Swaziland 39.6 Building a plot with that country removed. # Filter for Asia, add column indicating outliers gap_asia &lt;- gap2007 %&gt;% filter(continent == &quot;Asia&quot;) %&gt;% mutate(is_outliner = lifeExp &lt; 50) # Remove outliers, create box plot of lifeExp gap_asia %&gt;% filter(!is_outliner) %&gt;% ggplot(aes(x = 1, y = lifeExp)) + geom_boxplot() 13.4 Case Study 13.4.1 New data What characteristics of an email are associated with it being spam? Here, you’ll use the email dataset to settle that question. library(openintro) ## Loading required package: airports ## Loading required package: cherryblossom ## Loading required package: usdata ## ## Attaching package: &#39;openintro&#39; ## The following objects are masked from &#39;package:lattice&#39;: ## ## ethanol, lsegments ## The following object is masked from &#39;package:babynames&#39;: ## ## births str(email) ## tibble [3,921 × 21] (S3: tbl_df/tbl/data.frame) ## $ spam : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ to_multiple : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 2 2 1 1 ... ## $ from : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ cc : int [1:3921] 0 0 0 0 0 0 0 1 0 0 ... ## $ sent_email : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 2 2 1 1 ... ## $ time : POSIXct[1:3921], format: &quot;2012-01-01 14:16:41&quot; &quot;2012-01-01 15:03:59&quot; ... ## $ image : num [1:3921] 0 0 0 0 0 0 0 1 0 0 ... ## $ attach : num [1:3921] 0 0 0 0 0 0 0 1 0 0 ... ## $ dollar : num [1:3921] 0 0 4 0 0 0 0 0 0 0 ... ## $ winner : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ inherit : num [1:3921] 0 0 1 0 0 0 0 0 0 0 ... ## $ viagra : num [1:3921] 0 0 0 0 0 0 0 0 0 0 ... ## $ password : num [1:3921] 0 0 0 0 2 2 0 0 0 0 ... ## $ num_char : num [1:3921] 11.37 10.5 7.77 13.26 1.23 ... ## $ line_breaks : int [1:3921] 202 202 192 255 29 25 193 237 69 68 ... ## $ format : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 2 2 2 2 1 1 2 2 1 2 ... ## $ re_subj : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ exclaim_subj: num [1:3921] 0 0 0 0 0 0 0 0 0 0 ... ## $ urgent_subj : Factor w/ 2 levels &quot;0&quot;,&quot;1&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ exclaim_mess: num [1:3921] 0 1 6 48 1 1 1 18 1 0 ... ## $ number : Factor w/ 3 levels &quot;none&quot;,&quot;small&quot;,..: 3 2 2 2 1 1 3 2 2 2 ... # label factor for easier to read email$spam &lt;- factor(email$spam, labels = c(&quot;non-spam&quot;, &quot;spam&quot;)) str(email$spam) ## Factor w/ 2 levels &quot;non-spam&quot;,&quot;spam&quot;: 1 1 1 1 1 1 1 1 1 1 ... 13.4.1.1 Spam and num_char Is there an association between spam and the length of an email? # distribution of num_char ggplot(email, aes(x = num_char)) + geom_density() Compute appropriate measures of the center and spread of num_char for both spam and not-spam. # Compute summary statistics email %&gt;% group_by(spam) %&gt;% summarise(median(num_char), IQR(num_char)) ## # A tibble: 2 × 3 ## spam `median(num_char)` `IQR(num_char)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 non-spam 6.83 13.6 ## 2 spam 1.05 2.82 Construct side-by-side box plots to visualize the association between the same two variables. # Create plot email %&gt;% mutate(log_num_char = log(num_char)) %&gt;% ggplot(aes(x = spam, y = log_num_char)) + geom_boxplot() Interpretation: the median length of not-spam emails is greater than that of spam emails. 13.4.1.2 Spam and !!! Let’s look at a more obvious indicator of spam: exclamation marks. exclaim_mess contains the number of exclamation marks in each message. Using summary statistics and visualization, see if there is a relationship between this variable and whether or not a message is spam. Side-by-side box plots Faceted histograms Overlaid density plots # distribution of exclaim_mess ggplot(email, aes(x = exclaim_mess)) + geom_histogram() + facet_wrap(~ spam) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. Calculate appropriate measures of the center and spread of exclaim_mess for both spam and not-spam. # Compute center and spread for exclaim_mess by spam email %&gt;% group_by(spam) %&gt;% summarise(median(exclaim_mess), IQR(exclaim_mess)) ## # A tibble: 2 × 3 ## spam `median(exclaim_mess)` `IQR(exclaim_mess)` ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 non-spam 1 5 ## 2 spam 0 1 Construct an appropriate plot to visualize the association between the same two variables, adding in a log-transformation step if necessary. log(0) is -Inf in R, which isn’t a very useful value! You can get around this by adding a small number (like 0.01) to the quantity inside the log() function. # Create plot for spam and exclaim_mess email %&gt;% mutate(log_exclaim_mess = log(0.01 + exclaim_mess)) %&gt;% ggplot(aes(x = log_exclaim_mess)) + geom_histogram() + facet_wrap(~ spam) ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. The dominant feature of the exclaim mess variable, though, is the large proportion of cases that report zero or on this log scale, -4 (point) 6 exclamation marks. This is a common occurrence in data analysis that is often termed “zero-inflation”. 13.4.2 Check-in 1 Zero inflation strategies Analyze the two components separately (zero / non-zero) Collapse into two-level categorical variable (zero / non-zero) 13.4.2.1 Collapsing levels The number of images attached to each email (image) poses even more of a challenge. # get a sense of image&#39;s distribution table(email$image) ## ## 0 1 2 3 4 5 9 20 ## 3811 76 17 11 2 2 1 1 Given the very low counts at the higher number of images, let’s collapse image into a categorical variable that indicates whether or not the email had at least one image. # Create plot of proportion of spam by image email %&gt;% mutate(has_image = image &gt; 0) %&gt;% ggplot(aes(x = has_image, fill = spam)) + geom_bar(position = &quot;fill&quot;) An email without an image is more likely to be not-spam than spam. 13.4.2.2 Data Integrity In the process of exploring a dataset, you’ll sometimes come across something that will lead you to question how the data were compiled. Consider the variables image and attach. Do attached images count as attached files in this dataset? # Test if images count as attachments sum(email$image &gt; email$attach) ## [1] 0 Since image is never greater than attach, we can infer that images are counted as attachments. 13.4.2.3 Answer questions with chains Build a chain to answer each of the following questions, both about the variable dollar. For emails containing the word “dollar”, does the typical spam email contain a greater number of occurrences of the word than the typical non-spam email? # Question 1 email %&gt;% filter(dollar &gt; 0) %&gt;% group_by(spam) %&gt;% summarize(median(num_char)) ## # A tibble: 2 × 2 ## spam `median(num_char)` ## &lt;fct&gt; &lt;dbl&gt; ## 1 non-spam 12.3 ## 2 spam 1.41 If you encounter an email with greater than 10 occurrences of the word dollar, is it more likely to be spam or not-spam? # Question 2 email %&gt;% filter(dollar &gt; 10) %&gt;% ggplot(aes(x = spam)) + geom_bar() 13.4.3 Check-in 2 13.4.3.1 What’s in a number? Turn your attention to the variable called number. It’s a factor variable saying whether there was no number, a small number (under 1 million), or a big number. To explore the association between this variable and spam, select and construct an informative plot. Faceted bar charts Side-by-side bar charts Stacked and normalized bar charts. # Reorder the levels of number so that they preserve the natural ordering. email$number_reordered &lt;- factor(email$number, levels = c(&quot;none&quot;, &quot;small&quot;, &quot;big&quot;)) # Construct a faceted bar chart of the association between number_reordered and spam. ggplot(email, aes(x = number_reordered)) + geom_bar() + facet_wrap(~ spam) "],["introduction-to-regression.html", "Chapter 14 Introduction to Regression 14.1 Simple Linear Regression 14.2 Predictions &amp; model objects 14.3 Assessing model fit 14.4 Simple logistic regression", " Chapter 14 Introduction to Regression 14.1 Simple Linear Regression 14.1.1 Tale of two variables Regression Statistical models to explore the relationship a response (dependent) variable and some explanatory (independent) variables. Given values of explanatory variables, you can predict the values of the response variable. Linear regression: The response variable is numeric. Logistic regression: The response variable is logical. Simple linear/logistic regression: There is only one explanatory variable. 14.1.1.1 Visualize two variables Scatter plots are the standard way to visualize the relationship between two numeric variables You’ll explore a Taiwan real estate dataset with 4 variables. dist_to_mrt_station_m: Distance to nearest MRT metro station, in meters. n_convenience: No. of convenience stores in walking distance. house_age_years: The age of the house, in years, in 3 groups. price_twd_msq:House price per unit area, in New Taiwan dollars per meter squared. Here, we’ll look at the relationship between house price per area and the number of nearby convenience stores. library(tidyverse) library(fst) ## Warning: package &#39;fst&#39; was built under R version 4.3.2 # load dataset taiwan_real_estate &lt;- read_fst(&quot;data/taiwan_real_estate.fst&quot;) ## Warning: package &#39;fstcore&#39; was built under R version 4.3.2 str(taiwan_real_estate) ## &#39;data.frame&#39;: 414 obs. of 4 variables: ## $ dist_to_mrt_m : num 84.9 306.6 562 562 390.6 ... ## $ n_convenience : num 10 9 5 5 5 3 7 6 1 3 ... ## $ house_age_years: Ord.factor w/ 3 levels &quot;0 to 15&quot;&lt;&quot;15 to 30&quot;&lt;..: 3 2 1 1 1 1 3 2 3 2 ... ## $ price_twd_msq : num 11.5 12.8 14.3 16.6 13 ... # Draw a scatter plot of n_convenience vs. price_twd_msq ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) + # Make points 50% transparent geom_point(alpha = 0.5) + # Add a linear trend line without a confidence ribbon geom_smooth(method = &quot;lm&quot;, se = FALSE) 14.1.2 Fitting a linear regression Regression lines Equation y = intercept + slope ∗ x Slope The amount the y value increases if you increase x by one. Intercept The y value at the point when x is zero. Syntax lm(y ~ x, data) Run a linear regression with price_twd_msq as the response variable, n_convenience as the explanatory variable. # Run a linear regression of price_twd_msq vs. n_convenience lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 8.224 0.798 Interpretation: price_twd_msq = 8.2242 + 0.7981 ∗ n_convenience On average, a house with zero convenience stores nearby had a price of 8.2242 TWD per square meter. If you increase the number of nearby convenience stores by one, then the expected increase in house price is 0.7981 TWD per square meter. 14.1.3 Categorical explanatory variables 14.1.3.1 Visualizing categorical If the explanatory variable is categorical, the scatter plot that you used before to visualize the data doesn’t make sense. Instead, a good option is to draw a histogram for each category. # Using taiwan_real_estate, plot price_twd_msq ggplot(taiwan_real_estate, aes(price_twd_msq)) + # Make it a histogram with 10 bins geom_histogram(bins = 10) + # Facet the plot so each house age group gets its own panel facet_wrap(~ house_age_years) It appears that new houses are the most expensive on average, and the medium aged ones (15 to 30 years) are the cheapest. 14.1.3.2 Calculating means by category A good way to explore categorical variables is to calculate summary statistics such as the mean for each category. summary_stats &lt;- taiwan_real_estate %&gt;% # Group by house age group_by(house_age_years) %&gt;% # Summarize to calculate the mean house price/area summarise(mean_by_group = mean(price_twd_msq)) # See the result summary_stats ## # A tibble: 3 × 2 ## house_age_years mean_by_group ## &lt;ord&gt; &lt;dbl&gt; ## 1 0 to 15 12.6 ## 2 15 to 30 9.88 ## 3 30 to 45 11.4 14.1.3.3 lm() &amp; categorical explanatory variable Linear regressions also work with categorical explanatory variables. In this case, the code to run the model is the same, but the coefficients returned by the model are different. Run a linear regression with price_twd_msq as the response variable, house_age_years as the explanatory variable. # Run a linear regression of price_twd_msq vs. house_age_years mdl_price_vs_age &lt;- lm(price_twd_msq ~ house_age_years, taiwan_real_estate) # See the result mdl_price_vs_age ## ## Call: ## lm(formula = price_twd_msq ~ house_age_years, data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) house_age_years.L house_age_years.Q ## 11.30 -0.88 1.75 The intercept is the mean of the first group. The coefficients for each category are calculated relative to the intercept. Update the model formula so that no intercept is included in the model. # Update the model formula to remove the intercept mdl_price_vs_age_no_intercept &lt;- lm( price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate ) # See the result mdl_price_vs_age_no_intercept ## ## Call: ## lm(formula = price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate) ## ## Coefficients: ## house_age_years0 to 15 house_age_years15 to 30 house_age_years30 to 45 ## 12.64 9.88 11.39 After adding 0 to intercept, the coefficients of the model are just the means of each category you calculated previously. 14.2 Predictions &amp; model objects 14.2.1 Making predictions Data on explanatory values to predict If I set the explanatory variables to these values, what value would the response variable have? explanatory_data &lt;- tibble( explanatory_var = some_values ) Predicting inside a data frame explanatory_data %&gt;% mutate( response_var = predict(model, explanatory_data) ) Extrapolating Making predictions outside the range of observed data. 14.2.1.1 Predicting Specify values for each of the explanatory variables, feed them to the model, and you get a prediction for the corresponding response variable. Here, you’ll make predictions for the house prices versus number of convenience stores. # fit model mdl_price_vs_conv &lt;- lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate) # Create a tibble with n_convenience column from zero to ten explanatory_data &lt;- tibble(n_convenience = 0:10) # Use mdl_price_vs_conv to predict with explanatory_data predict(mdl_price_vs_conv, explanatory_data) ## 1 2 3 4 5 6 7 8 9 10 11 ## 8.22 9.02 9.82 10.62 11.42 12.21 13.01 13.81 14.61 15.41 16.21 # Edit this, so predictions are stored in prediction_data prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_conv, explanatory_data)) # See the result prediction_data ## # A tibble: 11 × 2 ## n_convenience price_twd_msq ## &lt;int&gt; &lt;dbl&gt; ## 1 0 8.22 ## 2 1 9.02 ## 3 2 9.82 ## 4 3 10.6 ## 5 4 11.4 ## 6 5 12.2 ## 7 6 13.0 ## 8 7 13.8 ## 9 8 14.6 ## 10 9 15.4 ## 11 10 16.2 14.2.1.2 Visualizing predictions The prediction data you calculated contains a column of explanatory variable values and a column of response variable values. That means you can plot it on the same scatter plot of response versus explanatory data values. # Add to the plot ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add a point layer of prediction data, colored yellow geom_point(data = prediction_data, color = &quot;yellow&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 14.2.1.3 The limits of prediction To test the limits of the model’s ability to predict, try some impossible situations. When there are -1, 2.5 convenience stores. minus_one &lt;- tibble(n_convenience = -1) two_pt_five &lt;- tibble(n_convenience = 2.5) c(predict(mdl_price_vs_conv, minus_one), predict(mdl_price_vs_conv, two_pt_five)) ## 1 1 ## 7.43 10.22 Linear models don’t know what is possible or not in real life. That means that they can give you predictions that don’t make any sense when applied to your data. You need to understand what your data means in order to determine whether a prediction is nonsense or not. 14.2.2 Working with model objects 14.2.2.1 Extracting model elements The variable returned by lm() that contains the model object has many elements. In order to perform further analysis on the model results, you need to extract the useful bits of it. The model coefficients, the fitted values, and the residuals are perhaps the most important bits of the linear model object. coefficients(model) fitted(model): predictions on the original dataset. residuals(model): actual response values minus predicted response values. summary(model) broom package: convert model objects to data frames for easier programming. tidy(): returns the coefficient level results. glance(): returns model-level results. augment(): returns observation level results. # Get the model coefficients of mdl_price_vs_conv coefficients(mdl_price_vs_conv) ## (Intercept) n_convenience ## 8.224 0.798 # Get the fitted values of mdl_price_vs_conv fitted(mdl_price_vs_conv) ## 1 2 3 4 5 6 7 8 9 10 11 12 13 ## 16.21 15.41 12.21 12.21 12.21 10.62 13.81 13.01 9.02 10.62 9.02 15.41 12.21 ## 14 15 16 17 18 19 20 21 22 23 24 25 26 ## 11.42 11.42 9.82 13.01 9.02 14.61 13.81 10.62 13.81 9.02 13.81 11.42 9.82 ## 27 28 29 30 31 32 33 34 35 36 37 38 39 ## 12.21 12.21 11.42 12.21 8.22 13.81 9.02 13.01 13.81 8.22 9.82 9.02 13.01 ## 40 41 42 43 44 45 46 47 48 49 50 51 52 ## 12.21 8.22 8.22 12.21 13.01 11.42 14.61 15.41 10.62 8.22 9.02 11.42 9.02 ## 53 54 55 56 57 58 59 60 61 62 63 64 65 ## 10.62 12.21 12.21 8.22 14.61 13.81 9.02 12.21 9.82 13.01 10.62 11.42 8.22 ## 66 67 68 69 70 71 72 73 74 75 76 77 78 ## 14.61 13.01 12.21 13.01 12.21 15.41 10.62 14.61 8.22 16.21 9.02 10.62 10.62 ## 79 80 81 82 83 84 85 86 87 88 89 90 91 ## 9.82 9.02 11.42 13.01 13.81 10.62 13.81 15.41 9.02 8.22 8.22 8.22 9.02 ## 92 93 94 95 96 97 98 99 100 101 102 103 104 ## 8.22 11.42 8.22 12.21 12.21 15.41 10.62 12.21 15.41 11.42 9.02 13.01 13.01 ## 105 106 107 108 109 110 111 112 113 114 115 116 117 ## 13.01 13.01 14.61 9.02 9.82 10.62 12.21 13.81 10.62 13.01 14.61 9.82 9.02 ## 118 119 120 121 122 123 124 125 126 127 128 129 130 ## 8.22 10.62 12.21 12.21 12.21 11.42 8.22 13.81 13.01 11.42 12.21 13.01 13.81 ## 131 132 133 134 135 136 137 138 139 140 141 142 143 ## 14.61 10.62 12.21 14.61 13.01 8.22 12.21 13.01 8.22 12.21 12.21 10.62 12.21 ## 144 145 146 147 148 149 150 151 152 153 154 155 156 ## 12.21 9.02 12.21 8.22 14.61 8.22 14.61 13.81 15.41 9.02 13.01 8.22 8.22 ## 157 158 159 160 161 162 163 164 165 166 167 168 169 ## 8.22 11.42 12.21 11.42 14.61 10.62 8.22 12.21 8.22 9.02 13.01 14.61 12.21 ## 170 171 172 173 174 175 176 177 178 179 180 181 182 ## 9.02 8.22 12.21 15.41 11.42 13.81 10.62 8.22 15.41 11.42 9.02 8.22 14.61 ## 183 184 185 186 187 188 189 190 191 192 193 194 195 ## 10.62 8.22 8.22 10.62 10.62 8.22 14.61 8.22 14.61 9.82 13.81 12.21 8.22 ## 196 197 198 199 200 201 202 203 204 205 206 207 208 ## 12.21 9.82 14.61 13.81 14.61 8.22 12.21 10.62 12.21 9.02 10.62 16.21 10.62 ## 209 210 211 212 213 214 215 216 217 218 219 220 221 ## 9.02 14.61 12.21 9.02 9.82 15.41 10.62 14.61 9.02 13.01 12.21 14.61 15.41 ## 222 223 224 225 226 227 228 229 230 231 232 233 234 ## 8.22 16.21 8.22 13.01 13.01 8.22 14.61 8.22 8.22 10.62 8.22 9.02 15.41 ## 235 236 237 238 239 240 241 242 243 244 245 246 247 ## 11.42 13.81 16.21 8.22 8.22 8.22 9.82 13.81 10.62 14.61 10.62 12.21 13.01 ## 248 249 250 251 252 253 254 255 256 257 258 259 260 ## 8.22 8.22 9.02 13.81 8.22 15.41 9.82 12.21 9.02 9.02 9.02 13.01 8.22 ## 261 262 263 264 265 266 267 268 269 270 271 272 273 ## 11.42 10.62 12.21 10.62 13.81 11.42 10.62 12.21 12.21 8.22 9.02 14.61 12.21 ## 274 275 276 277 278 279 280 281 282 283 284 285 286 ## 9.02 13.81 13.81 12.21 10.62 13.01 10.62 13.01 15.41 9.02 9.82 13.81 10.62 ## 287 288 289 290 291 292 293 294 295 296 297 298 299 ## 15.41 12.21 13.01 12.21 8.22 13.81 12.21 13.81 13.01 10.62 11.42 11.42 8.22 ## 300 301 302 303 304 305 306 307 308 309 310 311 312 ## 16.21 11.42 8.22 10.62 8.22 10.62 12.21 9.02 8.22 12.21 8.22 9.82 11.42 ## 313 314 315 316 317 318 319 320 321 322 323 324 325 ## 15.41 12.21 13.01 9.82 13.81 9.82 16.21 12.21 8.22 11.42 9.02 13.01 9.82 ## 326 327 328 329 330 331 332 333 334 335 336 337 338 ## 14.61 13.81 10.62 10.62 8.22 8.22 8.22 9.82 12.21 12.21 13.01 9.82 12.21 ## 339 340 341 342 343 344 345 346 347 348 349 350 351 ## 15.41 12.21 8.22 9.82 15.41 14.61 8.22 8.22 9.82 9.02 13.01 12.21 12.21 ## 352 353 354 355 356 357 358 359 360 361 362 363 364 ## 10.62 10.62 10.62 9.02 12.21 9.02 15.41 13.01 8.22 16.21 14.61 11.42 16.21 ## 365 366 367 368 369 370 371 372 373 374 375 376 377 ## 13.81 11.42 9.02 9.82 9.02 10.62 12.21 12.21 13.81 9.02 12.21 8.22 9.82 ## 378 379 380 381 382 383 384 385 386 387 388 389 390 ## 14.61 14.61 13.01 12.21 15.41 8.22 11.42 8.22 16.21 8.22 10.62 11.42 14.61 ## 391 392 393 394 395 396 397 398 399 400 401 402 403 ## 15.41 9.02 13.01 11.42 9.02 11.42 9.02 11.42 9.82 9.02 12.21 10.62 9.02 ## 404 405 406 407 408 409 410 411 412 413 414 ## 15.41 12.21 13.01 13.81 8.22 10.62 8.22 15.41 13.81 12.21 15.41 # Get the residuals of mdl_price_vs_conv residuals(mdl_price_vs_conv) ## 1 2 3 4 5 6 7 8 ## -4.7376 -2.6384 2.0970 4.3663 0.8262 -0.9059 -1.6171 1.1174 ## 9 10 11 12 13 14 15 16 ## -3.3340 -3.9316 3.5042 2.1725 -0.3236 -4.2153 -1.0383 5.4595 ## 17 18 19 20 21 22 23 24 ## 8.1976 2.2939 -1.8101 0.6219 -1.7531 1.8019 -1.5790 0.6824 ## 25 26 27 28 29 30 31 32 ## 0.3232 -1.6510 4.7899 -2.0482 2.8043 5.0622 -1.5374 -6.2465 ## 33 34 35 36 37 38 39 40 ## 1.3256 1.9041 2.8609 0.0360 -2.8915 -1.3672 1.4200 1.7642 ## 41 42 43 44 45 46 47 48 ## -3.4133 -2.7174 -1.7154 -2.6950 4.8921 -3.0204 -2.6989 7.9897 ## 49 50 51 52 53 54 55 56 ## -4.1698 -5.0284 1.9571 -2.7591 -2.4490 -0.4446 3.4283 -4.0790 ## 57 58 59 60 61 62 63 64 ## -1.9311 2.3768 -2.1842 0.6144 -3.3756 6.1098 -2.2372 5.2249 ## 65 66 67 68 69 70 71 72 ## -0.5692 -1.2049 2.3277 4.9714 -2.0596 0.4934 2.4448 1.7265 ## 73 74 75 76 77 78 79 80 ## -3.6255 -2.1728 0.2549 -0.0964 0.5162 -2.8726 -0.8038 -1.0042 ## 81 82 83 84 85 86 87 88 ## 0.7771 -1.8781 0.7429 -5.2630 -0.5884 -0.0363 -0.8529 -2.6872 ## 89 90 91 92 93 94 95 96 ## 6.2992 -0.5692 4.7144 4.8469 -4.8205 -3.3528 0.1908 3.4586 ## 97 98 99 100 101 102 103 104 ## 2.5961 -0.1495 3.2165 3.4130 0.1417 0.9323 3.4472 0.8148 ## 105 106 107 108 109 110 111 112 ## -3.7843 8.4699 -0.3577 -0.9739 0.4973 -2.0254 3.3981 -1.8895 ## 113 114 115 116 117 118 119 120 ## -3.6291 -10.7132 1.5182 4.2189 -5.3309 -4.2908 -1.3598 5.8186 ## 121 122 123 124 125 126 127 128 ## -2.7441 2.3088 -1.5830 5.5428 3.5568 1.6923 7.6152 4.4268 ## 129 130 131 132 133 134 135 136 ## 5.3534 -1.4053 -3.2624 -1.3295 -0.8682 -2.6573 -0.2442 -1.9307 ## 137 138 139 140 141 142 143 144 ## 1.9457 1.3292 4.9376 0.6447 3.3376 -1.8741 -0.8682 -0.0815 ## 145 146 147 148 149 150 151 152 ## -0.4293 1.5524 7.5700 -1.5378 5.4218 -2.5968 0.8639 -1.8820 ## 153 154 155 156 157 158 159 160 ## -0.2780 -0.6375 -1.9610 -3.5041 -2.6872 -0.6450 -0.2933 -0.1004 ## 161 162 163 164 165 166 167 168 ## 2.8798 1.3634 -4.7144 4.5781 8.4777 0.2364 9.2566 -1.4773 ## 169 170 171 172 173 174 175 176 ## -0.8984 -1.9119 -3.8672 5.5766 2.1725 -0.7963 -0.1345 0.4254 ## 177 178 179 180 181 182 183 184 ## -2.4149 -2.6989 -0.3122 3.8672 -3.5344 2.3049 -3.4778 -2.5359 ## 185 186 187 188 189 190 191 192 ## -1.6282 -4.1132 -2.8424 -1.5677 -1.2049 -2.0215 -1.8101 1.6168 ## 193 194 195 196 197 198 199 200 ## -0.8910 2.7022 0.6411 -1.7456 1.2537 -0.0249 -1.9802 -5.0476 ## 201 202 203 204 205 206 207 208 ## -0.5087 1.6734 -1.0875 1.7339 -0.9739 -4.1434 -2.8919 -0.2705 ## 209 210 211 212 213 214 215 216 ## -1.0949 -2.2337 3.5796 4.1396 -0.4104 2.1422 -4.2947 -0.0552 ## 217 218 219 220 221 222 223 224 ## 2.9898 -0.6678 1.0380 -2.4455 8.2844 3.4248 -1.5303 4.5746 ## 225 226 227 228 229 230 231 232 ## 0.9056 1.8133 -4.3513 -2.4455 5.8756 -2.4754 -0.5126 -3.7764 ## 233 234 235 236 237 238 239 240 ## -3.7576 -5.6036 -4.1851 -1.9197 2.5242 3.5761 4.0602 0.7621 ## 241 242 243 244 245 246 247 248 ## -1.1063 -1.2843 -0.5126 -0.0249 -4.0527 0.1303 -0.7283 -1.2348 ## 249 250 251 252 253 254 255 256 ## -1.4769 -4.4837 -4.7336 -4.0487 0.5386 -1.9838 3.4586 -3.7576 ## 257 258 259 260 261 262 263 264 ## -1.0042 4.2606 6.1401 0.4898 -2.1276 -3.2357 3.8217 -1.0269 ## 265 266 267 268 269 270 271 272 ## -1.5264 0.1114 -3.4475 0.2211 -0.0815 -1.2651 26.5299 -6.5907 ## 273 274 275 276 277 278 279 280 ## 0.0395 -0.1570 -1.4053 1.2270 -1.9272 -2.2372 0.3004 -1.2085 ## 281 282 283 284 285 286 287 288 ## 0.7240 -1.8517 -1.2765 -2.7100 -3.4023 6.1137 1.6278 -2.2600 ## 289 290 291 292 293 294 295 296 ## 2.4184 1.2498 2.9709 2.6491 -4.8016 -0.9515 -1.4847 -4.0224 ## 297 298 299 300 301 302 303 304 ## -1.0989 -2.7933 -3.1713 -2.2565 -0.2517 2.5776 -3.5988 3.3945 ## 305 306 307 308 309 310 311 312 ## -1.7229 4.4268 6.1668 -0.7507 3.8217 -2.4451 -2.3469 1.3520 ## 313 314 315 316 317 318 319 320 ## 8.1937 0.7354 -0.4257 -1.5602 -1.1028 1.5260 -1.1370 -4.0755 ## 321 322 323 324 325 326 327 328 ## -2.5964 -0.0096 0.9928 -0.1534 -0.3499 -3.0809 4.9789 0.4859 ## 329 330 331 332 333 334 335 336 ## -3.4778 -2.4149 -4.3513 -3.5041 2.1614 -0.5959 -5.3160 -1.9688 ## 337 338 339 340 341 342 343 344 ## 0.9512 -2.8652 -4.4236 3.0350 4.7561 1.3748 0.7806 -0.5090 ## 345 346 347 348 349 350 351 352 ## 4.2417 3.2432 -0.5012 -5.6335 3.2354 2.0062 0.5842 -1.9649 ## 353 354 355 356 357 358 359 360 ## -2.8424 -1.1480 0.0851 6.1515 4.6842 -1.8215 0.6333 -0.7507 ## 361 362 363 364 365 366 367 368 ## -1.9539 4.5439 0.6863 -1.6816 -3.7957 -2.4907 -1.5185 -3.4966 ## 369 370 371 372 373 374 375 376 ## 4.0185 -3.7198 0.5236 3.4283 -1.2541 6.7719 2.7627 -1.0230 ## 377 378 379 380 381 382 383 384 ## -0.5920 2.5772 -3.2927 8.0765 3.9124 -1.0953 0.6411 0.7771 ## 385 386 387 388 389 390 391 392 ## -4.3211 -2.1052 8.5080 -2.8726 -3.1563 5.8752 -3.7277 0.4482 ## 393 394 395 396 397 398 399 400 ## -2.3319 0.7771 -1.5488 1.4427 0.6297 -1.6737 -2.8612 2.2636 ## 401 402 403 404 405 406 407 408 ## -1.4733 -2.2372 -0.3990 -3.3949 0.2513 -1.7570 -1.5566 -1.4769 ## 409 410 411 412 413 414 ## -2.1162 -3.5646 -0.2784 -1.5264 3.6704 3.9274 # Print a summary of mdl_price_vs_conv summary(mdl_price_vs_conv) ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.713 -2.221 -0.541 1.810 26.530 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 8.2242 0.2850 28.9 &lt;0.0000000000000002 *** ## n_convenience 0.7981 0.0565 14.1 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.38 on 412 degrees of freedom ## Multiple R-squared: 0.326, Adjusted R-squared: 0.324 ## F-statistic: 199 on 1 and 412 DF, p-value: &lt;0.0000000000000002 Working with individual pieces of the model is often more useful than working with the whole model object at once. 14.2.2.2 Manually predicting You can manually calculate the predictions from the model coefficients. When making predictions in real life, it is better to use predict(). simple linear regression: response = intercept + slope * explanatory # Get the coefficients of mdl_price_vs_conv coeffs &lt;- coefficients(mdl_price_vs_conv) # Get the intercept intercept &lt;- coeffs[1] # Get the slope slope &lt;- coeffs[2] explanatory_data %&gt;% mutate( # Manually calculate the predictions price_twd_msq = intercept + slope * n_convenience ) ## # A tibble: 11 × 2 ## n_convenience price_twd_msq ## &lt;int&gt; &lt;dbl&gt; ## 1 0 8.22 ## 2 1 9.02 ## 3 2 9.82 ## 4 3 10.6 ## 5 4 11.4 ## 6 5 12.2 ## 7 6 13.0 ## 8 7 13.8 ## 9 8 14.6 ## 10 9 15.4 ## 11 10 16.2 # Compare to the results from predict() predict(mdl_price_vs_conv, explanatory_data) ## 1 2 3 4 5 6 7 8 9 10 11 ## 8.22 9.02 9.82 10.62 11.42 12.21 13.01 13.81 14.61 15.41 16.21 14.2.2.3 Using broom library(broom) # Get the coefficient-level elements of the model tidy(mdl_price_vs_conv) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 8.22 0.285 28.9 5.81e-101 ## 2 n_convenience 0.798 0.0565 14.1 3.41e- 37 # Get the observation-level elements of the model augment(mdl_price_vs_conv) ## # A tibble: 414 × 8 ## price_twd_msq n_convenience .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 11.5 10 16.2 -4.74 0.0121 3.38 1.22e-2 -1.41 ## 2 12.8 9 15.4 -2.64 0.00913 3.39 2.83e-3 -0.783 ## 3 14.3 5 12.2 2.10 0.00264 3.39 5.10e-4 0.621 ## 4 16.6 5 12.2 4.37 0.00264 3.38 2.21e-3 1.29 ## 5 13.0 5 12.2 0.826 0.00264 3.39 7.92e-5 0.244 ## 6 9.71 3 10.6 -0.906 0.00275 3.39 9.91e-5 -0.268 ## 7 12.2 7 13.8 -1.62 0.00477 3.39 5.50e-4 -0.479 ## 8 14.1 6 13.0 1.12 0.00343 3.39 1.88e-4 0.331 ## 9 5.69 1 9.02 -3.33 0.00509 3.38 2.49e-3 -0.988 ## 10 6.69 3 10.6 -3.93 0.00275 3.38 1.87e-3 -1.16 ## # ℹ 404 more rows # Get the model-level elements of the model glance(mdl_price_vs_conv) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.326 0.324 3.38 199. 3.41e-37 1 -1091. 2188. 2200. ## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; 14.2.3 Regression to the mean Response value = fitted value + residual “The stuff you explained” + “the stuff you couldn’t explain” Residuals exist due to problems in the model and fundamental randomness Extreme cases are often due to randomness Regression to the mean means extreme cases don’t persist over time 14.2.3.1 Plotting Here you’ll look at the annual returns from investing in companies in the Standard and Poor 500 index (S&amp;P 500), in 2018 and 2019. variable meaning symbol Stock ticker symbol uniquely identifying the company. return_2018 A measure of investment performance in 2018. return_2019 A measure of investment performance in 2019. A positive number for the return means the investment increased in value; negative means it lost value. A naive prediction might be that the investment performance stays the same from year to year, lying on the “y equals x” line. sp500_yearly_returns &lt;- read_tsv(&quot;data/sp500_yearly_returns.txt&quot;) sp500_yearly_returns ## # A tibble: 384 × 3 ## symbol return_2018 return_2019 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 MSFT 0.208 0.576 ## 2 AAPL -0.0539 0.890 ## 3 AMZN 0.284 0.230 ## 4 FB -0.257 0.566 ## 5 GOOG -0.0103 0.291 ## 6 JNJ -0.0513 0.162 ## 7 V 0.165 0.433 ## 8 JPM -0.0662 0.473 ## 9 INTC 0.0423 0.307 ## 10 MA 0.253 0.592 ## # ℹ 374 more rows geom_abline(): ab means a and b in the syntax of a line: y = a + b*x # Using sp500_yearly_returns, plot return_2019 vs. return_2018 ggplot(sp500_yearly_returns, aes(return_2018, return_2019)) + # Make it a scatter plot geom_point() + # Add a line at y = x, colored green, size 1 geom_abline(color = &quot;green&quot;, linewidth = 1) + # Add a linear regression trend line, no std. error ribbon geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Fix the coordinate ratio, so distances along the x and y axes appear same. coord_fixed() ## `geom_smooth()` using formula = &#39;y ~ x&#39; The regression trend line looks very different to the y equals x line. As the financial advisors say, “Past performance is no guarantee of future results.” 14.2.3.2 Modeling Let’s quantify the relationship between returns in 2019 and 2018 by running a linear regression and making predictions. By looking at companies with extremely high or extremely low returns in 2018, we can see if their performance was similar in 2019. # Run a linear regression on return_2019 vs. return_2018 using sp500_yearly_returns mdl_returns &lt;- lm( return_2019 ~ return_2018, data = sp500_yearly_returns ) mdl_returns ## ## Call: ## lm(formula = return_2019 ~ return_2018, data = sp500_yearly_returns) ## ## Coefficients: ## (Intercept) return_2018 ## 0.3113 0.0469 # Create a data frame with return_2018 at -1, 0, and 1 explanatory_data &lt;- tibble(return_2018 = c(-1, 0, 1)) # Use mdl_returns to predict with explanatory_data predict_invest &lt;- explanatory_data %&gt;% mutate(predict_return_2019 = predict(mdl_returns, explanatory_data)) predict_invest ## # A tibble: 3 × 2 ## return_2018 predict_return_2019 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1 0.264 ## 2 0 0.311 ## 3 1 0.358 Investments that gained a lot in value in 2018 on average gained only a small amount in 2019. Similarly, investments that lost a lot of value in 2018 on average also gained a small amount in 2019. 14.2.4 Transforming variables 14.2.4.1 Transform explanatory variable If there is no straight line relationship between the response variable and the explanatory variable, it is sometimes possible to create one by transforming one or both of the variables. You’ll take another look at taiwan_real_estate, this time using the distance to the nearest MRT (metro) station as the explanatory variable. Shortening the distance to the metro station by taking the square root. # Run the code to see the plot ggplot(taiwan_real_estate, aes(dist_to_mrt_m, price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Notice how the numbers on the x-axis have changed. # Edit so x-axis is square root of dist_to_mrt_m ggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Run a linear regression of price_twd_msq vs. square root of dist_to_mrt_m using taiwan_real_estate mdl_price_vs_dist &lt;- lm( price_twd_msq ~ sqrt(dist_to_mrt_m), data = taiwan_real_estate ) # See the result mdl_price_vs_dist ## ## Call: ## lm(formula = price_twd_msq ~ sqrt(dist_to_mrt_m), data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) sqrt(dist_to_mrt_m) ## 16.710 -0.183 explanatory_data &lt;- tibble( dist_to_mrt_m = seq(0, 80, 10) ^ 2 ) # Use mdl_price_vs_dist to predict explanatory_data prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_dist, explanatory_data)) # See the result prediction_data ## # A tibble: 9 × 2 ## dist_to_mrt_m price_twd_msq ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.7 ## 2 100 14.9 ## 3 400 13.1 ## 4 900 11.2 ## 5 1600 9.40 ## 6 2500 7.57 ## 7 3600 5.74 ## 8 4900 3.91 ## 9 6400 2.08 ggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), price_twd_msq)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add points from prediction_data, colored green, size 5 geom_point(data = prediction_data, color = &quot;green&quot;, size = 5) ## `geom_smooth()` using formula = &#39;y ~ x&#39; By transforming the explanatory variable, the relationship with the response variable became linear, and so a linear regression became an appropriate model. 14.2.4.2 Transform response variable The response variable can be transformed too, but this means you need an extra step at the end to undo that transformation. Undoing the transformation of the response is called backtransformation the predictions. Determining how many people click on the advert after seeing it in ad_conversion. ad_conversion &lt;- read_fst(&quot;data/ad_conversion.fst&quot;) glimpse(ad_conversion) ## Rows: 936 ## Columns: 3 ## $ spent_usd &lt;dbl&gt; 1.43, 1.82, 1.25, 1.29, 4.77, 1.27, 1.50, 3.16, 10.28, 0… ## $ n_impressions &lt;dbl&gt; 7350, 17861, 4259, 4133, 15615, 10951, 2355, 9502, 14669… ## $ n_clicks &lt;dbl&gt; 1, 2, 1, 1, 3, 1, 1, 3, 7, 1, 1, 4, 2, 2, 2, 2, 4, 2, 7,… # Run the code to see the plot ggplot(ad_conversion, aes(n_impressions, n_clicks)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Edit to raise x, y aesthetics to power 0.25 ggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Each variable in the formula needs to be specified “as is”, using I(). # Run a linear regression of n_clicks to the power 0.25 vs. n_impressions to the power 0.25 using ad_conversion mdl_click_vs_impression &lt;- lm( I(n_clicks^0.25) ~ I(n_impressions^0.25), data = ad_conversion ) mdl_click_vs_impression ## ## Call: ## lm(formula = I(n_clicks^0.25) ~ I(n_impressions^0.25), data = ad_conversion) ## ## Coefficients: ## (Intercept) I(n_impressions^0.25) ## 0.0717 0.1115 Back transform by raising n_clicks_025 to the power 4 to get n_clicks. explanatory_data &lt;- tibble( n_impressions = seq(0, 3e6, 5e5) ) prediction_data &lt;- explanatory_data %&gt;% mutate( # Use mdl_click_vs_impression to predict n_clicks ^ 0.25 n_clicks_025 = predict(mdl_click_vs_impression, explanatory_data), # Back transform to get n_clicks n_clicks = n_clicks_025 ^ 4 ) prediction_data ## # A tibble: 7 × 3 ## n_impressions n_clicks_025 n_clicks ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0.0717 0.0000265 ## 2 500000 3.04 85.1 ## 3 1000000 3.60 168. ## 4 1500000 3.97 250. ## 5 2000000 4.27 331. ## 6 2500000 4.51 413. ## 7 3000000 4.71 494. ggplot(ad_conversion, aes(n_impressions ^ 0.25, n_clicks ^ 0.25)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add points from prediction_data, colored green geom_point(data = prediction_data, color = &quot;green&quot;, size = 3) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 14.3 Assessing model fit 14.3.1 Quantifying model fit Metrics to know whether or not predictions from your model are nonsense: r-squared / Coefficient of determination The proportion of the variance in the response variable that is predictable from the explanatory variable. 0 = perfect fit ~ 1 = worst possible fit Is correlation squared Residual standard error (RSE) A “typical” di(erence between a prediction and an observed response. It has the same unit as the response variable. Root-mean-square error (RMSE) It performs the same task as RSE, namely quantifying how inaccurate the model predictions are, but is worse for comparisons between models. You need to be aware that RMSE exists, but typically you should use RSE instead. 14.3.1.1 Coefficient of determination The coefficient of determination is a measure of how well the linear regression line fits the observed values. For simple linear regression, it is equal to the square of the correlation between the explanatory and response variables. # modeling the click response to impressions mdl_click_vs_impression_orig &lt;- lm(formula = n_clicks ~ n_impressions, data = ad_conversion) # modeling the click response to impressions, with transform mdl_click_vs_impression_trans &lt;- lm(formula = I(n_clicks^0.25) ~ I(n_impressions^0.25), data = ad_conversion) # Print a summary of mdl_click_vs_impression_orig summary(mdl_click_vs_impression_orig) ## ## Call: ## lm(formula = n_clicks ~ n_impressions, data = ad_conversion) ## ## Residuals: ## Min 1Q Median 3Q Max ## -186.10 -5.39 -1.42 2.07 119.88 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.68289597 0.78881511 2.13 0.033 * ## n_impressions 0.00017183 0.00000196 87.65 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.9 on 934 degrees of freedom ## Multiple R-squared: 0.892, Adjusted R-squared: 0.891 ## F-statistic: 7.68e+03 on 1 and 934 DF, p-value: &lt;0.0000000000000002 # Print a summary of mdl_click_vs_impression_trans summary(mdl_click_vs_impression_trans) ## ## Call: ## lm(formula = I(n_clicks^0.25) ~ I(n_impressions^0.25), data = ad_conversion) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.5706 -0.1323 0.0058 0.1449 0.4689 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.071748 0.017202 4.17 0.000033 *** ## I(n_impressions^0.25) 0.111533 0.000884 126.11 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.197 on 934 degrees of freedom ## Multiple R-squared: 0.945, Adjusted R-squared: 0.944 ## F-statistic: 1.59e+04 on 1 and 934 DF, p-value: &lt;0.0000000000000002 Use dplyr’s pull() function to pull out specific value. Get the coefficient of determination by glancing at the model, then pulling the r.squared value. # Get coeff of determination for mdl_click_vs_impression_orig mdl_click_vs_impression_orig %&gt;% # Get the model-level details glance() %&gt;% # Pull out r.squared pull(r.squared) ## [1] 0.892 # Do the same for the transformed model mdl_click_vs_impression_trans %&gt;% # Get the model-level details glance() %&gt;% # Pull out r.squared pull(r.squared) ## [1] 0.945 The number of impressions explains 89% of the variability in the number of clicks. The transformed model has a higher coefficient of determination that the original model, suggesting that it gives a better fit to the data. 14.3.1.2 Residual standard error Residual standard error (RSE) is a measure of the typical size of the residuals. Equivalently, it’s a measure of how badly wrong you can expect predictions to be. Smaller numbers are better, with zero being a perfect fit to the data. # Get RSE for mdl_click_vs_impression_orig mdl_click_vs_impression_orig %&gt;% # Get the model-level details glance() %&gt;% # Pull out sigma pull(sigma) ## [1] 19.9 # Do the same for the transformed model mdl_click_vs_impression_trans %&gt;% # Get the model-level details glance() %&gt;% # Pull out sigma pull(sigma) ## [1] 0.197 The typical difference between predicted number of clicks and observed number of clicks is 20. RSE is a measure of accuracy for regression models, so you can compare accuracy across different classes of models. 14.3.2 Visualizing model fit Hoped for properties of residuals Residuals are normally distributed The mean of the residuals is zero Diagnostic plots Residuals vs. fitted values In a good model, the residuals should have a trend line close to zero. Q-Q plot Shows whether or not the residuals follow a normal distribution. If the residuals from the model are normally distributed, then the points will track the line on the Q-Q plot. Scale-location Show the size of residuals versus fitted values. In a good model, the size of the residuals shouldn’t change much as the fitted values change. 14.3.2.1 Drawing diagnostic plots autoplot(model, which = int) lets you specify which diagnostic plots you are interested in. 1 residuals vs. fitted values 2 Q-Q plot 3 scale-location These three diagnostic plots are excellent for sanity-checking the quality of your models. library(ggfortify) ## Warning: package &#39;ggfortify&#39; was built under R version 4.3.2 # Plot the three diagnostics for mdl_price_vs_conv autoplot(mdl_price_vs_conv, which = 1:3, nrow = 1, ncol = 3) 14.3.3 Outliers, leverage, influence Leverage and influence are important concepts for determining your model is overly affected by some unusual data points. Leverage A measure of how extreme the explanatory variable values are. Highly leveraged points are the ones with explanatory variables that are furthest away from the others. The .hat column (in augment()) or hatvalues() Influence Measures how much the model would change if you left the observation out of the dataset when modeling. It measures how different the prediction line would look if you ran a linear regression on all data points except that point, compared to running a linear regression on the whole dataset. Cook’s distance is the standard metric for influence which calculates influence based on the size of the residual and the leverage of the point. cooks.distance() or .cooksd column. Outlier diagnostic plots autoplot(model, which = 4:6) 14.3.3.1 Extracting leverage &amp; influence Now you’ll extract those values from an augmented version of the model. # leverage mdl_price_vs_dist %&gt;% # Augment the model augment() %&gt;% # Arrange rows by descending leverage arrange(desc(.hat)) %&gt;% # Get the head of the dataset head() ## # A tibble: 6 × 8 ## price_twd_msq `sqrt(dist_to_mrt_m)` .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.39 80.5 1.98 1.41 0.0267 2.82 0.00351 ## 2 3.69 80.0 2.09 1.60 0.0261 2.82 0.00447 ## 3 4.54 79.4 2.19 2.35 0.0256 2.82 0.00937 ## 4 5.69 74.2 3.13 2.55 0.0211 2.82 0.00906 ## 5 5.26 74.2 3.13 2.13 0.0211 2.82 0.00630 ## 6 4.05 67.9 4.30 -0.247 0.0163 2.82 0.0000644 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; # influence, cook distance mdl_price_vs_dist %&gt;% # Augment the model augment() %&gt;% # Arrange rows by descending Cook&#39;s distance arrange(desc(.cooksd)) %&gt;% # Get the head of the dataset head() ## # A tibble: 6 × 8 ## price_twd_msq `sqrt(dist_to_mrt_m)` .fitted .resid .hat .sigma .cooksd ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 35.6 15.9 13.8 21.7 0.00385 2.61 0.116 ## 2 13.6 61.5 5.47 8.18 0.0121 2.79 0.0524 ## 3 14.1 56.3 6.41 7.69 0.00933 2.80 0.0354 ## 4 23.7 13.7 14.2 9.48 0.00440 2.78 0.0251 ## 5 2.30 19.8 13.1 -10.8 0.00310 2.77 0.0228 ## 6 23.6 17.8 13.4 10.2 0.00344 2.78 0.0225 ## # ℹ 1 more variable: .std.resid &lt;dbl&gt; Plot the three outlier diagnostic plots. # Plot the three outlier diagnostics for mdl_price_vs_dist autoplot(mdl_price_vs_dist, which = 4:6, nrow = 1, ncol = 3) 14.4 Simple logistic regression 14.4.1 Why need logistic regression Another type of generalized linear model. Used when the response variable is logical. The responses follow logistic (S-shaped) curve. glm() with binomial family glm(y \\~ x, data, family = binomial) Linear regression using glm(): glm(y \\~ x, data, family = gaussian) 14.4.1.1 Exploring explanatory variables Use a histogram of the explanatory variable, faceted on the response. churn &lt;- read_fst(&quot;data/churn.fst&quot;) str(churn) ## &#39;data.frame&#39;: 400 obs. of 3 variables: ## $ has_churned : int 0 0 0 0 0 0 0 0 0 0 ... ## $ time_since_first_purchase: num -1.0892 1.183 -0.8462 0.0869 -1.1666 ... ## $ time_since_last_purchase : num -0.721 3.634 -0.428 -0.536 -0.673 ... # Using churn, plot time_since_last_purchase ggplot(churn, aes(time_since_last_purchase)) + # as a histogram with binwidth 0.25 geom_histogram(binwidth = 0.25) + # faceted in a grid with has_churned on each row facet_grid(vars(has_churned)) The distribution of churned customers was further right than the distribution of non-churned customers (churners typically have a longer time since their last purchase). # Redraw the plot with time_since_first_purchase ggplot(churn, aes(time_since_first_purchase)) + geom_histogram(binwidth = 0.25) + facet_grid(vars(has_churned)) churners have a shorter length of relationship. 14.4.1.2 Visualizing logistic models To see how the predictions differ for linear and logistic regressions, try drawing both trend lines side by side. You should see a linear (straight line) trend from the linear model, and a logistic (S-shaped) trend from the logistic model. geom_smooth(method = \"glm\", method.args = list(family = \"binomial\")) # Using churn plot has_churned vs. time_since_first_purchase ggplot(churn, aes(time_since_first_purchase, has_churned)) + # Make it a scatter plot geom_point() + # Add an lm trend line, no std error ribbon, colored red geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;red&quot;) + # Add a glm trend line, no std error ribbon, binomial family geom_smooth(method = &quot;glm&quot;, method.args = list(family = &quot;binomial&quot;), se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## `geom_smooth()` using formula = &#39;y ~ x&#39; The two models give similar predictions in some places, but notice the slight curve in the logistic model trend. 14.4.1.3 Logistic regression with glm() Linear regression and logistic regression are special cases of a broader type of models called generalized linear models (“GLMs”). A linear regression makes the assumption that the residuals follow a Gaussian (normal) distribution. By contrast, a logistic regression assumes that residuals follow a binomial distribution. # Fit a logistic regression of churn vs. length of relationship using the churn dataset mdl_churn_vs_relationship &lt;- glm( has_churned ~ time_since_first_purchase, churn, family = binomial) # See the result mdl_churn_vs_relationship ## ## Call: glm(formula = has_churned ~ time_since_first_purchase, family = binomial, ## data = churn) ## ## Coefficients: ## (Intercept) time_since_first_purchase ## -0.0152 -0.3548 ## ## Degrees of Freedom: 399 Total (i.e. Null); 398 Residual ## Null Deviance: 555 ## Residual Deviance: 544 AIC: 548 14.4.2 Predictions &amp; odds ratios Making predictions You also need to set the type argument to \"response\" to get the probabilities of response. predict(mdl_recency, explanatory_data, type = \"response\") There are four main ways of expressing the prediction from a logistic regression model: 14.4.2.1 Probabilities Firstly, since the response variable is either “yes” or “no”, you can make a prediction of the probability of a “yes”. Here, you’ll calculate and visualize these probabilities. # A data frame of explanatory values explanatory_data &lt;- tibble(time_since_first_purchase = seq(-1.5, 4, 0.25)) # a scatter plot of has_churned versus time_since_first_purchase with a smooth glm line plt_churn_vs_relationship &lt;- ggplot(churn, aes(time_since_first_purchase, has_churned)) + geom_point() + geom_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = F) Predict the probability of churning. Remember to set the prediction type. # Make a data frame of predicted probabilities prediction_data &lt;- explanatory_data %&gt;% mutate(has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = &quot;response&quot;)) # See the result prediction_data ## # A tibble: 23 × 2 ## time_since_first_purchase has_churned ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.5 0.626 ## 2 -1.25 0.605 ## 3 -1 0.584 ## 4 -0.75 0.562 ## 5 -0.5 0.540 ## 6 -0.25 0.518 ## 7 0 0.496 ## 8 0.25 0.474 ## 9 0.5 0.452 ## 10 0.75 0.430 ## # ℹ 13 more rows Update the plt_churn_vs_relationship plot to add points from prediction_data. plt_churn_vs_relationship + # Add points from prediction_data, colored yellow, size 2 geom_point(data = prediction_data, color = &quot;yellow&quot;, size = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The probability of a positive response is a natural way of thinking about predictions. 14.4.2.2 Most likely outcome When explaining your results to a non-technical audience, you may wish to side-step talking about probabilities and simply explain the most likely outcome. That is, rather than saying there is a 60% chance of a customer churning, you say that the most likely outcome is that the customer will churn. The tradeoff here is easier interpretation at the cost of nuance. Cutoff probability = 0.5 # Update the data frame prediction_data &lt;- explanatory_data %&gt;% mutate( has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = &quot;response&quot;), # Add the most likely churn outcome, 四捨五入&amp;無小數點 most_likely_outcome = round(has_churned, digits = 0) ) # See the result prediction_data ## # A tibble: 23 × 3 ## time_since_first_purchase has_churned most_likely_outcome ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.5 0.626 1 ## 2 -1.25 0.605 1 ## 3 -1 0.584 1 ## 4 -0.75 0.562 1 ## 5 -0.5 0.540 1 ## 6 -0.25 0.518 1 ## 7 0 0.496 0 ## 8 0.25 0.474 0 ## 9 0.5 0.452 0 ## 10 0.75 0.430 0 ## # ℹ 13 more rows # Update the plot plt_churn_vs_relationship + # Add most likely outcome points from prediction_data, colored yellow, size 2 geom_point(data = prediction_data, aes(y = most_likely_outcome), color = &quot;yellow&quot;, size = 2) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 14.4.2.3 Odds ratios Odds ratios compare the probability of something happening with the probability of it not happening. This is sometimes easier to reason about than probabilities, particularly when you want to make decisions about choices. For example, if a customer has a 20% chance of churning, it maybe more intuitive to say “the chance of them not churning is four times higher than the chance of them churning”. # Update the data frame prediction_data &lt;- explanatory_data %&gt;% mutate( has_churned = predict( mdl_churn_vs_relationship, explanatory_data, type = &quot;response&quot; ), most_likely_outcome = round(has_churned, digits = 0), # Add the odds ratio odds_ratio = has_churned / (1 - has_churned) ) # See the result prediction_data ## # A tibble: 23 × 4 ## time_since_first_purchase has_churned most_likely_outcome odds_ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.5 0.626 1 1.68 ## 2 -1.25 0.605 1 1.53 ## 3 -1 0.584 1 1.40 ## 4 -0.75 0.562 1 1.29 ## 5 -0.5 0.540 1 1.18 ## 6 -0.25 0.518 1 1.08 ## 7 0 0.496 0 0.985 ## 8 0.25 0.474 0 0.901 ## 9 0.5 0.452 0 0.825 ## 10 0.75 0.430 0 0.755 ## # ℹ 13 more rows The dotted line where the odds ratio is one indicates where churning is just as likely as not churning. # Using prediction_data, plot odds_ratio vs. time_since_first_purchase ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) + # Make it a line plot geom_line() + # Add a dotted horizontal line at y = 1 geom_hline(yintercept = 1, linetype = &quot;dotted&quot;) The predictions are below one, so the chance of churning is less than the chance of not churning In the top-left, the chance of churning is about 2 times more than the chance of not churning. 14.4.2.4 Log odds ratio One downside to probabilities and odds ratios for logistic regression predictions is that the prediction lines for each are curved. This makes it harder to reason about what happens to the prediction when you make a change to the explanatory variable. The log odds ratio does have a linear relationship between predicted response and explanatory variable. That means that as the explanatory variable changes, you don’t see dramatic changes in the response metric - only linear changes. Since the actual values of log odds ratio are less intuitive than (linear) odds ratio, for visualization purposes it’s usually better to plot the odds ratio and apply a log transformation to the y-axis scale. # Update the data frame prediction_data &lt;- explanatory_data %&gt;% mutate( has_churned = predict(mdl_churn_vs_relationship, explanatory_data, type = &quot;response&quot;), most_likely_outcome = round(has_churned, digits = 0), odds_ratio = has_churned / (1 - has_churned), # Add the log odds ratio from odds_ratio log_odds_ratio = log(odds_ratio), # Add the log odds ratio using predict() log_odds_ratio2 = predict(mdl_churn_vs_relationship, explanatory_data) ) # See the result prediction_data ## # A tibble: 23 × 6 ## time_since_first_purchase has_churned most_likely_outcome odds_ratio ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.5 0.626 1 1.68 ## 2 -1.25 0.605 1 1.53 ## 3 -1 0.584 1 1.40 ## 4 -0.75 0.562 1 1.29 ## 5 -0.5 0.540 1 1.18 ## 6 -0.25 0.518 1 1.08 ## 7 0 0.496 0 0.985 ## 8 0.25 0.474 0 0.901 ## 9 0.5 0.452 0 0.825 ## 10 0.75 0.430 0 0.755 ## # ℹ 13 more rows ## # ℹ 2 more variables: log_odds_ratio &lt;dbl&gt;, log_odds_ratio2 &lt;dbl&gt; Update the plot to use a logarithmic y-scale. # Update the plot ggplot(prediction_data, aes(time_since_first_purchase, odds_ratio)) + geom_line() + geom_hline(yintercept = 1, linetype = &quot;dotted&quot;) + # Use a logarithmic y-scale scale_y_log10() The linear relationship between predicted log odds ratio and the explanatory variable makes changes easier to reason about. 14.4.3 Quantify logistic regression fit Confusion matrix actual false actual true predicted false correct (TN) false negative (FN) predicted true false positive (FP) correct (TP) Performance metrics summary(confusion, event_level = \"second\") Accuracy: the proportion of correct predictions. accuracy = (TN + TP) / (TN + FN + FP + TP) Sensitivity: the proportion of true positives. sensitivity = TP / (FN + TP) Specificity: the proportion of true negatives. specificity = TN / (TN + FP) Both of them the higher the better. But there’s a trade off between sensitivity and specificity. 14.4.3.1 Calculate confusion matrix predicted_response = round(fitted(model)) means predictions on the original dataset and get “most likely” responses. # Get the actual responses from the dataset actual_response &lt;- churn$has_churned # Get the &quot;most likely&quot; responses from the model predicted_response &lt;- round(fitted(mdl_churn_vs_relationship)) # Create a table of counts outcomes &lt;- table(predicted_response, actual_response) # See the result outcomes ## actual_response ## predicted_response 0 1 ## 0 112 76 ## 1 88 124 14.4.3.2 Measuring model performance By converting confusion matrix to a yardstick confusion matrix object, you get methods for plotting and extracting performance metrics. # Convert outcomes to a yardstick confusion matrix confusion &lt;- yardstick::conf_mat(outcomes) # Plot the confusion matrix autoplot(confusion) # Get performance metrics for the confusion matrix, the positive response is in the second column. summary(confusion, event_level = &quot;second&quot;) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.59 ## 2 kap binary 0.18 ## 3 sens binary 0.62 ## 4 spec binary 0.56 ## 5 ppv binary 0.585 ## 6 npv binary 0.596 ## 7 mcc binary 0.180 ## 8 j_index binary 0.180 ## 9 bal_accuracy binary 0.59 ## 10 detection_prevalence binary 0.53 ## 11 precision binary 0.585 ## 12 recall binary 0.62 ## 13 f_meas binary 0.602 "],["intermediate-regression.html", "Chapter 15 Intermediate Regression 15.1 Parallel Slopes 15.2 Interactions 15.3 Multiple Linear Regression 15.4 Multiple Logistic Regression", " Chapter 15 Intermediate Regression 15.1 Parallel Slopes The case when there is one numeric explanatory variable and one categorical explanatory variable is sometimes called a “parallel slopes” linear regression due to the shape of the predictions. 15.1.1 Parallel slopes linear regression Here, you’ll revisit the Taiwan real estate dataset. library(tidyverse) library(fst) taiwan_real_estate &lt;- read_fst(&quot;data/taiwan_real_estate2.fst&quot;) glimpse(taiwan_real_estate) ## Rows: 414 ## Columns: 4 ## $ dist_to_mrt_m &lt;dbl&gt; 84.9, 306.6, 562.0, 562.0, 390.6, 2175.0, 623.5, 287.6… ## $ n_convenience &lt;dbl&gt; 10, 9, 5, 5, 5, 3, 7, 6, 1, 3, 1, 9, 5, 4, 4, 2, 6, 1,… ## $ house_age_years &lt;fct&gt; 30 to 45, 15 to 30, 0 to 15, 0 to 15, 0 to 15, 0 to 15… ## $ price_twd_msq &lt;dbl&gt; 11.47, 12.77, 14.31, 16.58, 13.04, 9.71, 12.19, 14.13,… 15.1.1.1 Fitting &amp; Interpreting model To combine multiple explanatory variables in the regression formula, separate them with a +. # Fit a linear regr&#39;n of price_twd_msq vs. n_convenience mdl_price_vs_conv &lt;- lm(price_twd_msq ~ n_convenience, taiwan_real_estate) # See the result mdl_price_vs_conv ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 8.224 0.798 # Fit a linear regr&#39;n of price_twd_msq vs. house_age_years, no intercept mdl_price_vs_age &lt;- lm(price_twd_msq ~ house_age_years + 0, taiwan_real_estate) # See the result mdl_price_vs_age ## ## Call: ## lm(formula = price_twd_msq ~ house_age_years + 0, data = taiwan_real_estate) ## ## Coefficients: ## house_age_years0 to 15 house_age_years15 to 30 house_age_years30 to 45 ## 12.64 9.88 11.39 # Fit a linear regr&#39;n of price_twd_msq vs. n_convenience plus house_age_years, no intercept mdl_price_vs_both &lt;- lm(price_twd_msq ~ n_convenience + house_age_years + 0, taiwan_real_estate) # See the result mdl_price_vs_both ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience + house_age_years + ## 0, data = taiwan_real_estate) ## ## Coefficients: ## n_convenience house_age_years0 to 15 house_age_years15 to 30 ## 0.791 9.413 7.085 ## house_age_years30 to 45 ## 7.511 Interpreting parallel slopes coefficients The mdl_price_vs_both has one slope coefficient, and three intercept coefficients (one for each possible value of the categorical explanatory variable). What is the meaning of the n_convenience coefficient? For each additional nearby convenience store, the expected house price, in TWD per square meter, increases by 0.79. What is the meaning of the “0 to 15 years” coefficient? For a house aged 0 to 15 years with zero nearby convenience stores, the expected house price is 9.41 TWD per square meter. 15.1.1.2 Visualizing each IV To visualize the relationship between a numeric explanatory variable and the numeric response, you can draw a scatter plot with a linear trend line. With a single numeric explanatory variable, the predictions form a single straight line. # Using taiwan_real_estate, plot price_twd_msq vs. n_convenience ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq)) + # Add a point layer geom_point() + # Add a smooth trend line using linear regr&#39;n, no ribbon geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; To visualize the relationship between a categorical explanatory variable and the numeric response, you can draw a box plot. With a single categorical explanatory variable, the predictions are the means for each category. # Using taiwan_real_estate, plot price_twd_msq vs. house_age_years ggplot(taiwan_real_estate, aes(house_age_years, price_twd_msq)) + # Add a box plot layer geom_boxplot() 15.1.1.3 Visualizing parallel slopes The moderndive package includes an extra geom, geom_parallel_slopes() to show the predictions. # Using taiwan_real_estate, plot price_twd_msq vs. n_convenience colored by house_age_years ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) + # Add a point layer geom_point() + # Add parallel slopes, no ribbon moderndive::geom_parallel_slopes(se = FALSE) The “parallel slope” model name comes from the fact that the prediction for each category is a slope, and all those slopes are parallel. 15.1.2 Predicting parallel slopes 15.1.2.1 Predicting parallel slopes model Just as with the case of a single explanatory variable, the workflow has two steps: create a data frame of explanatory variables, then add a column of predictions. Make a grid (expand_grid()) of explanatory data, formed from combinations of the following variables. # Make a grid of explanatory data explanatory_data &lt;- expand_grid( # Set n_convenience to zero to ten n_convenience = 0:10, # Set house_age_years to the unique values of that variable house_age_years = unique(taiwan_real_estate$house_age_years) ) # See the result explanatory_data ## # A tibble: 33 × 2 ## n_convenience house_age_years ## &lt;int&gt; &lt;fct&gt; ## 1 0 30 to 45 ## 2 0 15 to 30 ## 3 0 0 to 15 ## 4 1 30 to 45 ## 5 1 15 to 30 ## 6 1 0 to 15 ## 7 2 30 to 45 ## 8 2 15 to 30 ## 9 2 0 to 15 ## 10 3 30 to 45 ## # ℹ 23 more rows # Add predictions to the data frame prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_both, explanatory_data)) # See the result prediction_data ## # A tibble: 33 × 3 ## n_convenience house_age_years price_twd_msq ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 30 to 45 7.51 ## 2 0 15 to 30 7.09 ## 3 0 0 to 15 9.41 ## 4 1 30 to 45 8.30 ## 5 1 15 to 30 7.88 ## 6 1 0 to 15 10.2 ## 7 2 30 to 45 9.09 ## 8 2 15 to 30 8.67 ## 9 2 0 to 15 11.0 ## 10 3 30 to 45 9.89 ## # ℹ 23 more rows To make sure you’ve got the right answer, you can add your predictions to the ggplot with the geom_parallel_slopes() lines. # Update the plot to add a point layer of predictions taiwan_real_estate %&gt;% ggplot(aes(n_convenience, price_twd_msq, color = house_age_years)) + geom_point() + moderndive::geom_parallel_slopes(se = FALSE) + # Add points using prediction_data, with size 5 and shape 15 geom_point(data = prediction_data, size = 5, shape = 15) 15.1.2.2 Manually calculating predictions As with simple linear regression, you can manually calculate the predictions from the model coefficients. The only change for the parallel slopes case is that the intercept is different for each category of the categorical explanatory variable. That means you need to consider the case when (case_when or if_else) each each category occurs separately. dataframe %&gt;% mutate( case_when( condition_1 ~ value_1, condition_2 ~ value_2, # ... condition_n ~ value_n ) ) Assign each of the elements of coeffs to the appropriate variable. # Get the coefficients from mdl_price_vs_both coeffs &lt;- coefficients(mdl_price_vs_both) # Extract the slope coefficient slope &lt;- coeffs[1] # Extract the intercept coefficient for 0 to 15 intercept_0_15 &lt;- coeffs[2] # Extract the intercept coefficient for 15 to 30 intercept_15_30 &lt;- coeffs[3] # Extract the intercept coefficient for 30 to 45 intercept_30_45 &lt;- coeffs[4] # See elements list(coeffs = coeffs, slope = slope, intercept_0_15 = intercept_0_15, intercept_15_30 = intercept_15_30, intercept_30_45 = intercept_30_45) ## $coeffs ## n_convenience house_age_years0 to 15 house_age_years15 to 30 ## 0.791 9.413 7.085 ## house_age_years30 to 45 ## 7.511 ## ## $slope ## n_convenience ## 0.791 ## ## $intercept_0_15 ## house_age_years0 to 15 ## 9.41 ## ## $intercept_15_30 ## house_age_years15 to 30 ## 7.09 ## ## $intercept_30_45 ## house_age_years30 to 45 ## 7.51 prediction_data &lt;- explanatory_data %&gt;% mutate( # Consider the 3 cases to choose the intercept intercept = case_when( house_age_years == &quot;0 to 15&quot; ~ intercept_0_15, house_age_years == &quot;15 to 30&quot; ~ intercept_15_30, house_age_years == &quot;30 to 45&quot; ~ intercept_30_45 ), # Manually calculate the predictions price_twd_msq = intercept + slope * n_convenience ) # See the results prediction_data ## # A tibble: 33 × 4 ## n_convenience house_age_years intercept price_twd_msq ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 30 to 45 7.51 7.51 ## 2 0 15 to 30 7.09 7.09 ## 3 0 0 to 15 9.41 9.41 ## 4 1 30 to 45 7.51 8.30 ## 5 1 15 to 30 7.09 7.88 ## 6 1 0 to 15 9.41 10.2 ## 7 2 30 to 45 7.51 9.09 ## 8 2 15 to 30 7.09 8.67 ## 9 2 0 to 15 9.41 11.0 ## 10 3 30 to 45 7.51 9.89 ## # ℹ 23 more rows 15.1.3 Assessing model performance Model performance metrics Coefficient of determination (R-squared) How well the linear regression line fits the observed values. Larger is better. Residual standard error (RSE) The typical size of the residuals. Smaller is better. Adjusted coefficient of determination (Adjusted R-squared) More explanatory variables increases R². Too many explanatory variables causes overfitting. Adjusted coefficient of determination penalizes more explanatory variables. 15.1.3.1 Compare adjusted R² Here you’ll compare the coefficient of determination for the three Taiwan house price models, to see which gives the best result. library(broom) mdl_price_vs_conv %&gt;% # Get the model-level coefficients glance() %&gt;% # Select the coeffs of determination select(r.squared, adj.r.squared) ## # A tibble: 1 × 2 ## r.squared adj.r.squared ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.326 0.324 # Get the coeffs of determination for mdl_price_vs_age mdl_price_vs_age %&gt;% glance() %&gt;% select(r.squared, adj.r.squared) ## # A tibble: 1 × 2 ## r.squared adj.r.squared ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.896 0.895 # Get the coeffs of determination for mdl_price_vs_both mdl_price_vs_both %&gt;% glance() %&gt;% select(r.squared, adj.r.squared) ## # A tibble: 1 × 2 ## r.squared adj.r.squared ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.931 0.931 When both explanatory variables are included in the model, the adjusted coefficient of determination is higher, resulting in a better fit. 15.1.3.2 Compare RSE mdl_price_vs_conv %&gt;% # Get the model-level coefficients glance() %&gt;% # Pull out the RSE pull(sigma) ## [1] 3.38 # Get the RSE for mdl_price_vs_age mdl_price_vs_age %&gt;% glance() %&gt;% pull(sigma) ## [1] 3.95 # Get the RSE for mdl_price_vs_both mdl_price_vs_both %&gt;% glance() %&gt;% pull(sigma) ## [1] 3.21 By including both explanatory variables in the model, a lower RSE was achieved, indicating a smaller difference between the predicted responses and the actual responses. 15.2 Interactions 15.2.1 Models for each category 15.2.1.1 One model per category The model you ran on the whole dataset fits some parts of the data better than others. It’s worth taking a look at what happens when you run a linear model on different parts of the dataset separately, to see if each model agrees or disagrees with the others. # Filter for rows where house age is 0 to 15 years taiwan_0_to_15 &lt;- taiwan_real_estate %&gt;% filter(house_age_years == &quot;0 to 15&quot;) # Filter for rows where house age is 15 to 30 years taiwan_15_to_30 &lt;- taiwan_real_estate %&gt;% filter(house_age_years == &quot;15 to 30&quot;) # Filter for rows where house age is 30 to 45 years taiwan_30_to_45 &lt;- taiwan_real_estate %&gt;% filter(house_age_years == &quot;30 to 45&quot;) # Model price vs. no. convenience stores using 0 to 15 data mdl_0_to_15 &lt;- lm(price_twd_msq ~ n_convenience, taiwan_0_to_15) # Model price vs. no. convenience stores using 15 to 30 data mdl_15_to_30 &lt;- lm(price_twd_msq ~ n_convenience, taiwan_15_to_30) # Model price vs. no. convenience stores using 30 to 45 data mdl_30_to_45 &lt;- lm(price_twd_msq ~ n_convenience, taiwan_30_to_45) # See the results mdl_0_to_15 ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_0_to_15) ## ## Coefficients: ## (Intercept) n_convenience ## 9.242 0.834 mdl_15_to_30 ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_15_to_30) ## ## Coefficients: ## (Intercept) n_convenience ## 6.872 0.852 mdl_30_to_45 ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_30_to_45) ## ## Coefficients: ## (Intercept) n_convenience ## 8.113 0.669 15.2.1.2 Predicting multiple models In order to see what each of the models for individual categories are doing, it’s helpful to make predictions from them. Remember that you only have a single explanatory variable in these models, so expand_grid() isn’t needed. # Add column of predictions using &quot;0 to 15&quot; model and explanatory data prediction_data_0_to_15 &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_0_to_15, explanatory_data)) # Same again, with &quot;15 to 30&quot; prediction_data_15_to_30 &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_15_to_30, explanatory_data)) # Same again, with &quot;30 to 45&quot; prediction_data_30_to_45 &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_30_to_45, explanatory_data)) 15.2.1.3 Visualizing multiple models # Extend the plot to include prediction points ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + # Add points using prediction_data_0_to_15, colored red, size 3, shape 15 geom_point(data = prediction_data_0_to_15, color = &quot;red&quot;, size = 3, shape = 15) + # Add points using prediction_data_15_to_30, colored green, size 3, shape 15 geom_point(data = prediction_data_15_to_30, color = &quot;green&quot;, size = 3, shape = 15) + # Add points using prediction_data_30_to_45, colored blue, size 3, shape 15 geom_point(data = prediction_data_30_to_45, color = &quot;blue&quot;, size = 3, shape = 15) ## `geom_smooth()` using formula = &#39;y ~ x&#39; It’s a good sign that our predictions match those of ggplot’s. Notice that the 30 to 45 year house age group has a much shallower slope compared to the other lines. 15.2.1.4 Assessing model performance To test which approach is best—the whole dataset model or the models for each house age category—you need to calculate some metrics. Here’s, you’ll compare the coefficient of determination and the residual standard error for each model. mdl_all_ages &lt;- lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate) # Get the coeff. of determination for mdl_all_ages mdl_all_ages %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.326 # Get the coeff. of determination for mdl_0_to_15 mdl_0_to_15 %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.312 # Get the coeff. of determination for mdl_15_to_30 mdl_15_to_30 %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.442 # Get the coeff. of determination for mdl_30_to_45 mdl_30_to_45 %&gt;% glance() %&gt;% pull(r.squared) ## [1] 0.313 # Get the RSE for mdl_all_age mdl_all_ages %&gt;% glance() %&gt;% pull(sigma) ## [1] 3.38 # Get the RSE for mdl_0_to_15 mdl_0_to_15 %&gt;% glance() %&gt;% pull(sigma) ## [1] 3.56 # Get the RSE for mdl_15_to_30 mdl_15_to_30 %&gt;% glance() %&gt;% pull(sigma) ## [1] 2.59 # Get the RSE for mdl_30_to_45 mdl_30_to_45 %&gt;% glance() %&gt;% pull(sigma) ## [1] 3.24 It seems that both metrics for the 15 to 30 age group model are much better than those for the whole dataset model, but the models for the other two age groups are similar to the whole dataset model. Thus using individual models will improve predictions for 15 to 30 age group. 15.2.2 One model with an interaction What is an interaction? The effect of one explanatory variable on the expected response changes depending on the value of another explanatory variable. Specifying interactions No interactions response ~ explntry1 + explntry2 With interactions (implicit) response_var ~ explntry1 * explntry2 With interactions (explicit) response ~ explntry1 + explntry2 + explntry1:explntry2 15.2.2.1 Specifying an interaction Ideally, you’d have a single model that had all the predictive power of the individual models. Defining this single model is achieved through adding interactions between explanatory variables. Concise code that is quick to type and to read. # Model price vs both with an interaction using &quot;times&quot; syntax lm(price_twd_msq ~ n_convenience * house_age_years, taiwan_real_estate) ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience * house_age_years, ## data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 9.2417 0.8336 ## house_age_years15 to 30 house_age_years30 to 45 ## -2.3698 -1.1286 ## n_convenience:house_age_years15 to 30 n_convenience:house_age_years30 to 45 ## 0.0183 -0.1649 Explicit code that describes what you are doing in detail. # Model price vs both with an interaction using &quot;colon&quot; syntax lm(price_twd_msq ~ n_convenience + house_age_years + n_convenience: house_age_years, taiwan_real_estate) ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience + house_age_years + ## n_convenience:house_age_years, data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 9.2417 0.8336 ## house_age_years15 to 30 house_age_years30 to 45 ## -2.3698 -1.1286 ## n_convenience:house_age_years15 to 30 n_convenience:house_age_years30 to 45 ## 0.0183 -0.1649 15.2.2.2 Understandable coeffs The previous model with the interaction term returned coefficients that were a little tricky to interpret. In order clarify what the model is predicting, you can reformulate the model in a way that returns understandable coefficients. For further clarity, you can compare the results to the models on the separate house age categories. Fit a linear regression of price_twd_msq versus house_age_years plus an interaction between n_convenience and house_age_years, and no global intercept. # Model price vs. house age plus an interaction, no intercept mdl_readable_inter &lt;- lm(price_twd_msq ~ house_age_years + house_age_years:n_convenience + 0, taiwan_real_estate) # See the result mdl_readable_inter ## ## Call: ## lm(formula = price_twd_msq ~ house_age_years + house_age_years:n_convenience + ## 0, data = taiwan_real_estate) ## ## Coefficients: ## house_age_years0 to 15 house_age_years15 to 30 ## 9.242 6.872 ## house_age_years30 to 45 house_age_years0 to 15:n_convenience ## 8.113 0.834 ## house_age_years15 to 30:n_convenience house_age_years30 to 45:n_convenience ## 0.852 0.669 The expected increase in house price for each nearby convenience store is lowest for the 30 to 45 year age group. For comparison, get the coefficients for the three models for each category. # Get coefficients for mdl_0_to_15 coefficients(mdl_0_to_15) ## (Intercept) n_convenience ## 9.242 0.834 # Get coefficients for mdl_15_to_30 coefficients(mdl_15_to_30) ## (Intercept) n_convenience ## 6.872 0.852 # Get coefficients for mdl_30_to_45 coefficients(mdl_15_to_30) ## (Intercept) n_convenience ## 6.872 0.852 Sometimes fiddling about with how the model formula is specified makes it easier to interpret the coefficients. In this version, you can see how each category has its own intercept and slope (just like the 3 separate models had). 15.2.3 Predict with interactions 15.2.3.1 Predicting with interactions mdl_price_vs_both_inter &lt;- lm(price_twd_msq ~ house_age_years + n_convenience:house_age_years + 0, taiwan_real_estate) # Make a grid of explanatory data explanatory_data &lt;- expand_grid( # Set n_convenience to zero to ten n_convenience = 0:10, # Set house_age_years to the unique values of that variable house_age_years = unique(taiwan_real_estate$house_age_years) ) # See the result explanatory_data ## # A tibble: 33 × 2 ## n_convenience house_age_years ## &lt;int&gt; &lt;fct&gt; ## 1 0 30 to 45 ## 2 0 15 to 30 ## 3 0 0 to 15 ## 4 1 30 to 45 ## 5 1 15 to 30 ## 6 1 0 to 15 ## 7 2 30 to 45 ## 8 2 15 to 30 ## 9 2 0 to 15 ## 10 3 30 to 45 ## # ℹ 23 more rows # Add predictions to the data frame prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_both_inter, explanatory_data)) # See the result prediction_data ## # A tibble: 33 × 3 ## n_convenience house_age_years price_twd_msq ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 30 to 45 8.11 ## 2 0 15 to 30 6.87 ## 3 0 0 to 15 9.24 ## 4 1 30 to 45 8.78 ## 5 1 15 to 30 7.72 ## 6 1 0 to 15 10.1 ## 7 2 30 to 45 9.45 ## 8 2 15 to 30 8.58 ## 9 2 0 to 15 10.9 ## 10 3 30 to 45 10.1 ## # ℹ 23 more rows # Using taiwan_real_estate, plot price vs. no. of convenience stores, colored by house age ggplot(taiwan_real_estate, aes(n_convenience, price_twd_msq, color = house_age_years)) + # Make it a scatter plot geom_point() + # Add linear regression trend lines, no ribbon geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Add points from prediction_data, size 5, shape 15 geom_point(data = prediction_data, size = 5, shape = 15) ## geom_point: na.rm = FALSE ## stat_identity: na.rm = FALSE ## position_identity 15.2.3.2 Manually calculating predictions For mdl_price_vs_both_inter model, there are three separate lines to calculate for, and in each one, the prediction is an intercept plus a slope times the numeric explanatory value. # Get the coefficients from mdl_price_vs_both_inter coeffs &lt;- coefficients(mdl_price_vs_both_inter) # Get the intercept for 0 to 15 year age group intercept_0_15 &lt;- coeffs[1] # Get the intercept for 15 to 30 year age group intercept_15_30 &lt;- coeffs[2] # Get the intercept for 30 to 45 year age group intercept_30_45 &lt;- coeffs[3] # Get the slope for 0 to 15 year age group slope_0_15 &lt;- coeffs[4] # Get the slope for 15 to 30 year age group slope_15_30 &lt;- coeffs[5] # Get the slope for 30 to 45 year age group slope_30_45 &lt;- coeffs[6] coeffs ## house_age_years0 to 15 house_age_years15 to 30 ## 9.242 6.872 ## house_age_years30 to 45 house_age_years0 to 15:n_convenience ## 8.113 0.834 ## house_age_years15 to 30:n_convenience house_age_years30 to 45:n_convenience ## 0.852 0.669 prediction_data &lt;- explanatory_data %&gt;% mutate( # Consider the 3 cases to choose the price price_twd_msq = case_when( house_age_years == &quot;0 to 15&quot; ~ intercept_0_15 + slope_0_15 * n_convenience, house_age_years == &quot;15 to 30&quot; ~ intercept_15_30 + slope_15_30 * n_convenience, house_age_years == &quot;30 to 45&quot; ~ intercept_30_45 + slope_30_45 * n_convenience ) ) # See the result prediction_data ## # A tibble: 33 × 3 ## n_convenience house_age_years price_twd_msq ## &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 30 to 45 8.11 ## 2 0 15 to 30 6.87 ## 3 0 0 to 15 9.24 ## 4 1 30 to 45 8.78 ## 5 1 15 to 30 7.72 ## 6 1 0 to 15 10.1 ## 7 2 30 to 45 9.45 ## 8 2 15 to 30 8.58 ## 9 2 0 to 15 10.9 ## 10 3 30 to 45 10.1 ## # ℹ 23 more rows 15.2.4 Simpson’s Paradox Simpson’s Paradox occurs when the trend (trend = slope coefficient) of a model on the whole dataset is very different from the trends shown by models on subsets of the dataset. You can’t choose the best model in general — it depends on the dataset and the question you are trying to answer. Usually (but not always) the grouped model contains more insight. 15.2.4.1 Modeling Sometimes modeling a whole dataset suggests trends that disagree with models on separate parts of that dataset. This is known as Simpson’s paradox. In the most extreme case, you may see a positive slope on the whole dataset, and negative slopes on every subset of that dataset (or the other way around). You’ll look at eBay auctions of Palm Pilot M515 PDA models. price : Final sale price, USD openbid : The opening bid, USD auction_type : How long did the auction last? auctions &lt;- read_tsv(&quot;data/auctions.txt&quot;) # Take a glimpse at the dataset glimpse(auctions) ## Rows: 343 ## Columns: 3 ## $ price &lt;dbl&gt; 260, 257, 260, 238, 232, 251, 248, 238, 232, 242, 250, 24… ## $ openbid &lt;dbl&gt; 0.01, 0.01, 0.01, 0.01, 1.00, 9.99, 215.00, 155.00, 50.00… ## $ auction_type &lt;chr&gt; &quot;7 day auction&quot;, &quot;3 day auction&quot;, &quot;5 day auction&quot;, &quot;7 day… Fit a linear regression model of price versus openbid. Look at the coefficients. # Model price vs. opening bid using auctions mdl_price_vs_openbid &lt;- lm(price ~ openbid, auctions) # See the result mdl_price_vs_openbid ## ## Call: ## lm(formula = price ~ openbid, data = auctions) ## ## Coefficients: ## (Intercept) openbid ## 229.2457 -0.0021 Plot price versus openbid as a scatter plot with linear regression trend lines (no ribbon). Look at the trend line. # Using auctions, plot price vs. opening bid as a scatter plot with linear regression trend lines ggplot(auctions, aes(openbid, price)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; The slope coefficient is small enough that it might as well be zero. That is, opening bid appears to have no effect on the final sale price for Palm Pilots. 15.2.4.2 Modeling each category type Now let’s look at what happens when you model the three auction types (3 day, 5 day, and 7 day) separately. # Fit linear regression of price vs. opening bid and auction type, with an interaction. mdl_price_vs_both &lt;- lm(price ~ openbid + auction_type + auction_type:openbid, auctions) # See the result mdl_price_vs_both ## ## Call: ## lm(formula = price ~ openbid + auction_type + auction_type:openbid, ## data = auctions) ## ## Coefficients: ## (Intercept) openbid ## 226.3690 -0.0290 ## auction_type5 day auction auction_type7 day auction ## -4.7697 5.2339 ## openbid:auction_type5 day auction openbid:auction_type7 day auction ## 0.1130 0.0327 # Using auctions, plot price vs. opening bid colored by auction type as a scatter plot with linear regr&#39;n trend lines ggplot(auctions, aes(openbid, price, color = auction_type)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = F) ## `geom_smooth()` using formula = &#39;y ~ x&#39; Interpreting models is a subtle art, and your conclusions need to be based on the question you are trying to answer. Here, the answer to ‘Does opening bid affect final sale price?’ is no overall. But the answer to ‘Does opening bid price affect final sale price for any type of auction?’ is yes, for 5 day auctions. 15.3 Multiple Linear Regression 15.3.1 Two numeric IVs Visualizing 3 numeric variables 3D scatter plot 2D scatter plot with response as color 15.3.1.1 3D visualizations For the case of three continuous variables, you can draw a 3D scatter plot, but perspective problems usually make it difficult to interpret. There are some “flat” alternatives that provide easier interpretation, though they require a little thinking about to make. # 3D scatter plot library(plot3D) scatter3D(fish$length_cm, fish$height_cm, fish$mass_g) # cleaner code library(plot3D) library(magrittr) fish %$% scatter3D(length_cm, height_cm, mass_g) With the taiwan_real_estate dataset, draw a 3D scatter plot of the number of nearby convenience stores on the x-axis, the square-root of the distance to the nearest MRT stop on the y-axis, and the house price on the z-axis. library(magrittr) library(plot3D) ## Warning: package &#39;plot3D&#39; was built under R version 4.3.2 # With taiwan_real_estate, draw a 3D scatter plot of no. of conv. stores, sqrt dist to MRT, and price taiwan_real_estate %$% scatter3D(n_convenience, sqrt(dist_to_mrt_m), price_twd_msq) Use the continuous viridis color scale with scatter plot, using the \"plasma\" option. # Using taiwan_real_estate, plot sqrt dist to MRT vs. no. of conv stores, colored by price ggplot(taiwan_real_estate, aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq)) + # Make it a scatter plot geom_point() + # Use the continuous viridis plasma color scale scale_color_viridis_c(option = &quot;plasma&quot;) 3D scatter plots are usually a pain to easily interpret due to problems with perspective. The best alternative for displaying a third variable involves using colors. 15.3.1.2 Modeling 2 numeric IVs The code for modeling and predicting with two numeric explanatory variables in the same, other than a slight difference in how to specify the explanatory variables to make predictions against. Here you’ll model and predict the house prices against the number of nearby convenience stores and the square-root of the distance to the nearest MRT station. # Fit a linear regression of price vs. no. of conv. stores and sqrt dist. to nearest MRT, no interaction mdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), taiwan_real_estate) # See the result mdl_price_vs_conv_dist ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m), ## data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience sqrt(dist_to_mrt_m) ## 15.104 0.214 -0.157 Create expanded grid of explanatory variables with number of convenience stores from 0 to 10 and the distance to the nearest MRT station as a sequence from 0 to 80 in steps of 10, all squared (0, 100, 400, …, 6400). # Create expanded grid of explanatory variables with no. of conv. stores and dist. to nearest MRT explanatory_data &lt;- expand_grid( n_convenience = 0:10, dist_to_mrt_m = seq(0, 80, 10)^2 ) # Add predictions using mdl_price_vs_conv_dist and explanatory_data prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)) # See the result prediction_data ## # A tibble: 99 × 3 ## n_convenience dist_to_mrt_m price_twd_msq ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 15.1 ## 2 0 100 13.5 ## 3 0 400 12.0 ## 4 0 900 10.4 ## 5 0 1600 8.81 ## 6 0 2500 7.24 ## 7 0 3600 5.67 ## 8 0 4900 4.09 ## 9 0 6400 2.52 ## 10 1 0 15.3 ## # ℹ 89 more rows Extend the plot to add a layer of points using the prediction data. # Add predictions to plot ggplot( taiwan_real_estate, aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq) ) + geom_point() + scale_color_viridis_c(option = &quot;plasma&quot;)+ # Add prediction points colored yellow, size 3 geom_point(data = prediction_data, color = &quot;yellow&quot;, size = 3) 15.3.1.3 Including an interaction Here you’ll run and predict the same model as in the previous exercise, but this time including an interaction between the explanatory variables. # Fit a linear regression of price vs. no. of conv. stores and sqrt dist. to nearest MRT, with interaction mdl_price_vs_conv_dist &lt;- lm(price_twd_msq ~ n_convenience * sqrt(dist_to_mrt_m), taiwan_real_estate) # See the result mdl_price_vs_conv_dist ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience * sqrt(dist_to_mrt_m), ## data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 14.7373 0.4243 ## sqrt(dist_to_mrt_m) n_convenience:sqrt(dist_to_mrt_m) ## -0.1412 -0.0112 # Create expanded grid of explanatory variables with no. of conv. stores and dist. to nearest MRT explanatory_data &lt;- expand_grid( n_convenience = 0:10, dist_to_mrt_m = seq(0, 80, 10)^2 ) # Add predictions using mdl_price_vs_conv_dist and explanatory_data prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_conv_dist, explanatory_data)) # See the result prediction_data ## # A tibble: 99 × 3 ## n_convenience dist_to_mrt_m price_twd_msq ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 14.7 ## 2 0 100 13.3 ## 3 0 400 11.9 ## 4 0 900 10.5 ## 5 0 1600 9.09 ## 6 0 2500 7.68 ## 7 0 3600 6.26 ## 8 0 4900 4.85 ## 9 0 6400 3.44 ## 10 1 0 15.2 ## # ℹ 89 more rows # Add predictions to plot ggplot( taiwan_real_estate, aes(n_convenience, sqrt(dist_to_mrt_m), color = price_twd_msq) ) + geom_point() + scale_color_viridis_c(option = &quot;plasma&quot;) + # Add prediction points colored yellow, size 3 geom_point(data = prediction_data, color = &quot;yellow&quot;, size = 3) 15.3.2 More than 2 IVs 15.3.2.1 Visualizing many variables In addition to using x and y scales for two numeric variables, you can use color for a third numeric variable, and you can use faceting for categorical variables. And that’s about your limit before the plots become to difficult to interpret. Here you’ll push the limits of the scatter plot by showing the house price, the distance to the MRT station, the number of nearby convenience stores, and the house age, all together in one plot. # Using taiwan_real_estate, no. of conv. stores vs. sqrt of dist. to MRT, colored by plot house price ggplot(taiwan_real_estate, aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)) + # Make it a scatter plot geom_point() + # Use the continuous viridis plasma color scale scale_color_viridis_c(option = &quot;plasma&quot;) + # Facet, wrapped by house age facet_wrap(~ house_age_years) 15.3.2.2 Different levels of interaction No interactions response ~ explntry1 + explntry2 + explntry3 2-way interactions between pairs of variables response ~ explntry1 + explntry2 + explntry3 + explntry1:explntry2 + explntry1:explntry3 + explntry2:explntry3 the same as response ~ (explntry1 + explntry2 + explntry3) ^ 2 3-way interaction between all three variables response ~ explntry1 + explntry2 + explntry3 + explntry1:explntry2 + explntry1:explntry3 + explntry2:explntry3 + explntry1:explntry2:explntry3 the same as response ~ explntry1 * explntry2 * explntry3 Don’t include a global intercept, and any interactions. # Model price vs. no. of conv. stores, sqrt dist. to MRT station &amp; house age, no global intercept, no interactions mdl_price_vs_all_no_inter &lt;- lm(price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m) + house_age_years + 0, taiwan_real_estate) # See the result mdl_price_vs_all_no_inter ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience + sqrt(dist_to_mrt_m) + ## house_age_years + 0, data = taiwan_real_estate) ## ## Coefficients: ## n_convenience sqrt(dist_to_mrt_m) house_age_years0 to 15 ## 0.258 -0.148 15.474 ## house_age_years15 to 30 house_age_years30 to 45 ## 14.130 13.765 Don’t include a global intercept, but do include 2-way and 3-way interactions between the explanatory variables. # Model price vs. sqrt dist. to MRT station, no. of conv. stores &amp; house age, no global intercept, 3-way interactions mdl_price_vs_all_3_way_inter &lt;- lm( price_twd_msq ~ sqrt(dist_to_mrt_m) * n_convenience * house_age_years + 0, taiwan_real_estate) # See the result mdl_price_vs_all_3_way_inter ## ## Call: ## lm(formula = price_twd_msq ~ sqrt(dist_to_mrt_m) * n_convenience * ## house_age_years + 0, data = taiwan_real_estate) ## ## Coefficients: ## sqrt(dist_to_mrt_m) ## -0.16294 ## n_convenience ## 0.37498 ## house_age_years0 to 15 ## 16.04685 ## house_age_years15 to 30 ## 13.76007 ## house_age_years30 to 45 ## 12.08877 ## sqrt(dist_to_mrt_m):n_convenience ## -0.00839 ## sqrt(dist_to_mrt_m):house_age_years15 to 30 ## 0.03662 ## sqrt(dist_to_mrt_m):house_age_years30 to 45 ## 0.06128 ## n_convenience:house_age_years15 to 30 ## 0.07837 ## n_convenience:house_age_years30 to 45 ## 0.06672 ## sqrt(dist_to_mrt_m):n_convenience:house_age_years15 to 30 ## -0.00382 ## sqrt(dist_to_mrt_m):n_convenience:house_age_years30 to 45 ## 0.00440 Don’t include a global intercept, but do include 2-way (not 3-way) interactions between the explanatory variables. # Model price vs. sqrt dist. to MRT station, no. of conv. stores &amp; house age, no global intercept, 2-way interactions mdl_price_vs_all_2_way_inter &lt;- lm( price_twd_msq ~ (sqrt(dist_to_mrt_m) + n_convenience + house_age_years)^2 + 0, taiwan_real_estate) # See the result mdl_price_vs_all_2_way_inter ## ## Call: ## lm(formula = price_twd_msq ~ (sqrt(dist_to_mrt_m) + n_convenience + ## house_age_years)^2 + 0, data = taiwan_real_estate) ## ## Coefficients: ## sqrt(dist_to_mrt_m) ## -0.16202 ## n_convenience ## 0.38491 ## house_age_years0 to 15 ## 16.02663 ## house_age_years15 to 30 ## 13.88079 ## house_age_years30 to 45 ## 11.92690 ## sqrt(dist_to_mrt_m):n_convenience ## -0.00896 ## sqrt(dist_to_mrt_m):house_age_years15 to 30 ## 0.03160 ## sqrt(dist_to_mrt_m):house_age_years30 to 45 ## 0.06820 ## n_convenience:house_age_years15 to 30 ## -0.00689 ## n_convenience:house_age_years30 to 45 ## 0.14342 15.3.2.3 Predicting # Make a grid of explanatory data explanatory_data &lt;- expand_grid( # Set dist_to_mrt_m a seq from 0 to 80 by 10s, squared dist_to_mrt_m = seq(0, 80, 10) ^ 2, # Set n_convenience to 0 to 10 n_convenience = 0:10, # Set house_age_years to the unique values of that variable house_age_years = unique(taiwan_real_estate$house_age_years) ) # See the result explanatory_data ## # A tibble: 297 × 3 ## dist_to_mrt_m n_convenience house_age_years ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; ## 1 0 0 30 to 45 ## 2 0 0 15 to 30 ## 3 0 0 0 to 15 ## 4 0 1 30 to 45 ## 5 0 1 15 to 30 ## 6 0 1 0 to 15 ## 7 0 2 30 to 45 ## 8 0 2 15 to 30 ## 9 0 2 0 to 15 ## 10 0 3 30 to 45 ## # ℹ 287 more rows # Add predictions to the data frame prediction_data &lt;- explanatory_data %&gt;% mutate(price_twd_msq = predict(mdl_price_vs_all_3_way_inter, explanatory_data)) # See the result prediction_data ## # A tibble: 297 × 4 ## dist_to_mrt_m n_convenience house_age_years price_twd_msq ## &lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 0 0 30 to 45 12.1 ## 2 0 0 15 to 30 13.8 ## 3 0 0 0 to 15 16.0 ## 4 0 1 30 to 45 12.5 ## 5 0 1 15 to 30 14.2 ## 6 0 1 0 to 15 16.4 ## 7 0 2 30 to 45 13.0 ## 8 0 2 15 to 30 14.7 ## 9 0 2 0 to 15 16.8 ## 10 0 3 30 to 45 13.4 ## # ℹ 287 more rows # Extend the plot ggplot( taiwan_real_estate, aes(sqrt(dist_to_mrt_m), n_convenience, color = price_twd_msq)) + geom_point() + scale_color_viridis_c(option = &quot;plasma&quot;) + facet_wrap(vars(house_age_years)) + # Add points from prediction data, size 3, shape 15 geom_point(data = prediction_data, size = 3, shape = 15) The plot nicely shows that the house price decreases as the square-root of the distance to the nearest MRT station increases, and increases as the number of nearby convenience stores increases, and is higher for houses under 15 years old. 15.3.3 Model performance 15.3.3.1 Sum of squares In order to choose the “best” line to fit the data, regression models need to optimize some metric. For linear regression, this metric is called the sum of squares. It takes the square of each residual, and add up those squares. So a smaller number is better. 15.3.3.2 Linear regression algorithm The workflow is Write a script to calculate the sum of squares. Turn this into a function. Use R’s general purpose optimization function find the coefficients that minimize this. optim(par = c(var1 = int1, var2 = int2...), fn = function) x_actual &lt;- taiwan_real_estate$n_convenience y_actual &lt;- taiwan_real_estate$price_twd_msq Calculate the sum of squares. # Set the intercept to 10 intercept &lt;- 10 # Set the slope to 1 slope &lt;- 1 # Calculate the predicted y values y_pred &lt;- intercept + slope * x_actual # Calculate the differences between actual and predicted y_diff &lt;- y_pred - y_actual # Calculate the sum of squares sum(y_diff ^ 2) ## [1] 7668 Complete the function body. calc_sum_of_squares &lt;- function(coeffs) { # Get the intercept coeff intercept &lt;- coeffs[1] # Get the slope coeff slope &lt;- coeffs[2] # Calculate the predicted y values y_pred &lt;- intercept + slope * x_actual # Calculate the differences between actual and predicted y_diff &lt;- y_pred - y_actual # Calculate the sum of squares sum(y_diff ^ 2) } Optimize the sum of squares metric. # Optimize the metric optim( # Initially guess 0 intercept and 0 slope par = c(intercept = 0, slope = 0), # Use calc_sum_of_squares as the optimization fn fn = calc_sum_of_squares ) ## $par ## intercept slope ## 8.226 0.798 ## ## $value ## [1] 4718 ## ## $counts ## function gradient ## 87 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL # Compare the coefficients to those calculated by lm() lm(price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Call: ## lm(formula = price_twd_msq ~ n_convenience, data = taiwan_real_estate) ## ## Coefficients: ## (Intercept) n_convenience ## 8.224 0.798 The sum of square is 4717.687 15.4 Multiple Logistic Regression 15.4.1 Multiple Logistic Regression 15.4.1.1 Visualizing Use faceting for categorical variables. For 2 numeric explanatory variables, use color for response. Give responses below 0.5 one color; responses above 0.5 another color. scale_color_gradient2(midpoint = 0.5) Here we’ll look at the case of two numeric explanatory variables, and the solution is basically the same as before: use color to denote the response. churn &lt;- read_fst(&quot;data/churn.fst&quot;) # Using churn, plot recency vs. length of relationship colored by churn status ggplot(churn, aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)) + # Make it a scatter plot, with transparency 0.5 geom_point(alpha = 0.5) + # Use a 2-color gradient split at 0.5 scale_color_gradient2(midpoint = 0.5) + # Use the black and white theme theme_bw() The 2-color gradient is excellent for distinguishing the two cases of a positive and negative response. 15.4.1.2 With 2 IVs With interaction: glm(response ~ explanatory1 * explanatory2, data = dataset, family = binomial) Here you’ll fit a model of churn status with both of the explanatory variables from the dataset: the length of customer relationship and the recency of purchase. # Fit a logistic regression of churn status vs. length of relationship, recency, and an interaction mdl_churn_vs_both_inter &lt;- glm( has_churned ~ time_since_first_purchase * time_since_last_purchase, churn, family = &quot;binomial&quot;) # See the result mdl_churn_vs_both_inter ## ## Call: glm(formula = has_churned ~ time_since_first_purchase * time_since_last_purchase, ## family = &quot;binomial&quot;, data = churn) ## ## Coefficients: ## (Intercept) ## -0.151 ## time_since_first_purchase ## -0.638 ## time_since_last_purchase ## 0.423 ## time_since_first_purchase:time_since_last_purchase ## 0.112 ## ## Degrees of Freedom: 399 Total (i.e. Null); 396 Residual ## Null Deviance: 555 ## Residual Deviance: 520 AIC: 528 15.4.1.3 Predicting The only thing to remember here is to set the prediction type to \"response\". # Make a grid of explanatory data explanatory_data &lt;- expand_grid( # Set len. relationship to seq from -2 to 4 in steps of 0.1 time_since_first_purchase = seq(-2, 4, 0.1), # Set recency to seq from -1 to 6 in steps of 0.1 time_since_last_purchase = seq(-1, 6, 0.1) ) # See the result explanatory_data ## # A tibble: 4,331 × 2 ## time_since_first_purchase time_since_last_purchase ## &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 -1 ## 2 -2 -0.9 ## 3 -2 -0.8 ## 4 -2 -0.7 ## 5 -2 -0.6 ## 6 -2 -0.5 ## 7 -2 -0.4 ## 8 -2 -0.3 ## 9 -2 -0.2 ## 10 -2 -0.1 ## # ℹ 4,321 more rows # Add a column of predictions using mdl_churn_vs_both_inter and explanatory_data with type response prediction_data &lt;- explanatory_data %&gt;% mutate(has_churned = predict(mdl_churn_vs_both_inter, explanatory_data, type = &quot;response&quot;)) # See the result prediction_data ## # A tibble: 4,331 × 3 ## time_since_first_purchase time_since_last_purchase has_churned ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -2 -1 0.716 ## 2 -2 -0.9 0.720 ## 3 -2 -0.8 0.724 ## 4 -2 -0.7 0.728 ## 5 -2 -0.6 0.732 ## 6 -2 -0.5 0.736 ## 7 -2 -0.4 0.740 ## 8 -2 -0.3 0.744 ## 9 -2 -0.2 0.747 ## 10 -2 -0.1 0.751 ## # ℹ 4,321 more rows # Extend the plot ggplot(churn, aes(time_since_first_purchase, time_since_last_purchase, color = has_churned)) + geom_point(alpha = 0.5) + scale_color_gradient2(midpoint = 0.5) + theme_bw() + # Add points from prediction_data with size 3 and shape 15 geom_point(data = prediction_data, size = 3, shape = 15) 15.4.1.4 Confusion matrix Generating a confusion matrix and calculating metrics like accuracy, sensitivity, and specificity is the standard way to measure how well a logistic model fits. Get the predicted responses from the rounded, fitted values of mdl_churn_vs_both_inter. library(yardstick) ## Warning: package &#39;yardstick&#39; was built under R version 4.3.2 # Get the actual responses from churn actual_response &lt;- churn$has_churned # Get the predicted responses from the model predicted_response &lt;- round(fitted(mdl_churn_vs_both_inter)) # Get a table of these values outcomes &lt;- table(predicted_response, actual_response) # Convert the table to a conf_mat object confusion &lt;- conf_mat(outcomes) # See the result confusion ## actual_response ## predicted_response 0 1 ## 0 102 53 ## 1 98 147 Remember that the churn event is in the second row/column of the matrix. # &quot;Automatically&quot; plot the confusion matrix autoplot(confusion) # Get summary metrics summary(confusion, event_level = &quot;second&quot;) ## # A tibble: 13 × 3 ## .metric .estimator .estimate ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 accuracy binary 0.622 ## 2 kap binary 0.245 ## 3 sens binary 0.735 ## 4 spec binary 0.51 ## 5 ppv binary 0.6 ## 6 npv binary 0.658 ## 7 mcc binary 0.251 ## 8 j_index binary 0.245 ## 9 bal_accuracy binary 0.622 ## 10 detection_prevalence binary 0.612 ## 11 precision binary 0.6 ## 12 recall binary 0.735 ## 13 f_meas binary 0.661 15.4.2 Logistic distribution Distribution function names curve prefix normal logistic nmemonic Probability density function (PDF) d dnorm() dlogis() “d” for differentiate - you differentiate the CDF to get the PDF Cumulative distribution function (CDF) p pnorm() plogis() “p” is backwards “q” so it’s the inverse of the inverse CDF Inverse CDF q qnorm() qlogis() “q” for quantile family = gaussian family = binomial 15.4.2.1 CDF &amp; Inverse CDF Logistic distribution CDF is also called the logistic function. \\(cdf(x) = 1 / (1 + exp(-x))\\) The plot of this has an S-shape, known as a sigmoid curve. This function takes an input that can be any number from minus infinity to infinity, and returns a value between zero and one. Each x input value is transformed to a unique value. That means that the transformation can be reversed. Logistic distribution inverse CDF is also called the logit function. \\(inverse\\_cdf(p) = log(p / (1 - p))\\) Cumulative distribution function Like the normal (Gaussian) distribution, it is a probability distribution of a single continuous variable. Here you’ll visualize the cumulative distribution function (CDF) for the logistic distribution. logistic_distn_cdf &lt;- tibble( # Make a seq from -10 to 10 in steps of 0.1 x = seq(-10, 10, 0.1), # Transform x with built-in logistic CDF logistic_x = plogis(x), # Transform x with manual logistic logistic_x_man = 1 / (1 + exp(-x)) ) # Check that each logistic function gives the same results all.equal( logistic_distn_cdf$logistic_x, logistic_distn_cdf$logistic_x_man ) ## [1] TRUE The logistic distribution’s cumulative distribution function is a sigmoid curve. # Using logistic_distn_cdf, plot logistic_x vs. x ggplot(logistic_distn_cdf, aes(x, logistic_x)) + # Make it a line plot geom_line() Inverse cumulative distribution function logistic_distn_inv_cdf &lt;- tibble( # Make a seq from 0.001 to 0.999 in steps of 0.001 p = seq(0.001, 0.999, 0.001), # Transform with built-in logistic inverse CDF logit_p = qlogis(p), # Transform with manual logit logit_p_man = log(p / (1 - p)) ) # Check that each logistic function gives the same results all.equal( logistic_distn_inv_cdf$logit_p, logistic_distn_inv_cdf$logit_p_man ) ## [1] TRUE # Using logistic_distn_inv_cdf, plot logit_p vs. p ggplot(logistic_distn_inv_cdf, aes(p, logit_p)) + # Make it a line plot geom_line() The inverse CDF is the “opposite” transformation to the CDF. If you flip the x and y axes on this plot, you get the same plot you saw in the previous exercise. 15.4.2.2 binomial family argument binomial() is a function that returns a list of other functions that tell glm() how to perform calculations in the regression. The two most interesting functions are linkinv and linkfun, which are used for transforming variables from the whole number line (minus infinity to infinity) to probabilities (zero to one) and back again. (Link function is a transformation of the response variable) x &lt;- seq(-10, 10, 0.2) p &lt;- seq(0.01, 0.99, 0.01) # Look at the structure of binomial() function str(binomial()) ## List of 13 ## $ family : chr &quot;binomial&quot; ## $ link : chr &quot;logit&quot; ## $ linkfun :function (mu) ## $ linkinv :function (eta) ## $ variance :function (mu) ## $ dev.resids:function (y, mu, wt) ## $ aic :function (y, n, mu, wt, dev) ## $ mu.eta :function (eta) ## $ initialize: language { if (NCOL(y) == 1) { ... ## $ validmu :function (mu) ## $ valideta :function (eta) ## $ simulate :function (object, nsim) ## $ dispersion: num 1 ## - attr(*, &quot;class&quot;)= chr &quot;family&quot; Notice that it contains two elements that are functions, binomial()$linkinv, and binomial()$linkfun. binomial()$linkinv = logistic distribution CDF, plogis() # Call the link inverse on x linkinv_x &lt;- binomial()$linkinv(x) # Check linkinv_x and plogis() of x give same results all.equal(linkinv_x, plogis(x)) ## [1] TRUE binomial()$linkfun = inverse CDF, qlogis() # Call the link fun on p linkfun_p &lt;- binomial()$linkfun(p) # Check linkfun_p and qlogis() of p give same results all.equal(linkfun_p, qlogis(p)) ## [1] TRUE These are used to translate between numbers and probabilities. 15.4.2.3 Logistic distribution parameters The normal distribution has mean and standard deviation parameters that affect the CDF curve. The logistic distribution has location and scale parameters. These two parameters allows logistic model prediction curves to have different positions or steepnesses. location increases — logistic CDF curve moves rightwards scale increases — the steepness of the slope deceases 15.4.3 Model performance Likelihood sum(y_pred * y_actual + (1 - y_pred) * (1 - y_actual)) When y_actual = 1 y_pred * 1 + (1 - y_pred) * (1 - 1) = y_pred When y_actual = 0 y_pred * 0 + (1 - y_pred) * (1 - 0) = 1 - y_pred Log-likelihood Computing likelihood involves adding many very small numbers, leading to numerical error. Log-likelihood is easier to compute. log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual) Negative log-likelihood Maximizing log-likelihood is the same as minimizing negative log-likelihood. -sum(log_likelihoods) 15.4.3.1 Likelihood &amp; log-likelihood Logistic regression chooses the prediction line that gives you the maximum likelihood value. It also gives maximum log-likelihood. 15.4.3.2 Logistic regression algorithm Rather than using sum of squares as the metric, we want to use likelihood. However, log-likelihood is more computationally stable, so we’ll use that instead. Actually, there is one more change: since we want to maximize log-likelihood, but optim() defaults to finding minimum values, it is easier to calculate the negative log-likelihood. Calculate the predicted y-values as the intercept plus the slope times the actual x-values, all transformed with the logistic distribution CDF. x_actual &lt;- churn$time_since_last_purchase y_actual &lt;- churn$has_churned # Set the intercept to 1 intercept &lt;- 1 # Set the slope to 0.5 slope &lt;- 0.5 # Calculate the predicted y values y_pred &lt;- plogis(intercept + slope * x_actual) # Calculate the log-likelihood for each term log_likelihoods &lt;- log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual) # Calculate minus the sum of the log-likelihoods for each term -sum(log_likelihoods) ## [1] 326 Complete the function body. calc_neg_log_likelihood &lt;- function(coeffs) { # Get the intercept coeff intercept &lt;- coeffs[1] # Get the slope coeff slope &lt;- coeffs[2] # Calculate the predicted y values y_pred &lt;- plogis(intercept + slope * x_actual) # Calculate the log-likelihood for each term log_likelihoods &lt;- log(y_pred) * y_actual + log(1 - y_pred) * (1 - y_actual) # Calculate minus the sum of the log-likelihoods for each term -sum(log_likelihoods) } Optimize the maximize log-likelihood metric. # Optimize the metric optim( # Initially guess 0 intercept and 1 slope par = c(intercept = 0, slope = 1), # Use calc_neg_log_likelihood as the optimization fn fn = calc_neg_log_likelihood ) ## $par ## intercept slope ## -0.0348 0.2689 ## ## $value ## [1] 273 ## ## $counts ## function gradient ## 51 NA ## ## $convergence ## [1] 0 ## ## $message ## NULL # Compare the coefficients to those calculated by glm() glm(has_churned ~ time_since_last_purchase, data = churn, family = binomial) ## ## Call: glm(formula = has_churned ~ time_since_last_purchase, family = binomial, ## data = churn) ## ## Coefficients: ## (Intercept) time_since_last_purchase ## -0.035 0.269 ## ## Degrees of Freedom: 399 Total (i.e. Null); 398 Residual ## Null Deviance: 555 ## Residual Deviance: 546 AIC: 550 The maximize log-likelihood is 273.2002 "],["sampling-in-r.html", "Chapter 16 Sampling in R 16.1 Introduction to Sampling 16.2 Sampling Methods 16.3 Sampling Distributions 16.4 Bootstrap Distributions", " Chapter 16 Sampling in R 16.1 Introduction to Sampling 16.1.1 Sampling &amp; point estimates Population vs. sample The population is the complete dataset. It doesn’t have to refer to people. You typically don’t know what the whole population is. The sample is the subset of data you calculate on. Base-R sampling slice_sample(df, n = num of sampling) : for data frames sample(vec, size = num of sampling) : for vectors Population parameters vs. point estimates A population parameter is a calculation made on the population dataset. A point estimate or sample statistic is a calculation made on the sample dataset. 16.1.1.1 Simple sampling with dplyr you’ll be exploring song data from Spotify. Each row of the dataset represents a song. Columns include the name of the song, the artists who performed it, the release year, and attributes of the song like its duration, tempo, and danceability. library(tidyverse) library(fst) spotify_population &lt;- read_fst(&quot;data/spotify_2000_2020.fst&quot;) # View the whole population dataset glimpse(spotify_population) ## Rows: 41,656 ## Columns: 20 ## $ acousticness &lt;dbl&gt; 0.9720000, 0.3210000, 0.0065900, 0.0039000, 0.1220000… ## $ artists &lt;chr&gt; &quot;[&#39;David Bauer&#39;]&quot;, &quot;[&#39;Etta James&#39;]&quot;, &quot;[&#39;Quasimoto&#39;]&quot;,… ## $ danceability &lt;dbl&gt; 0.567, 0.821, 0.706, 0.368, 0.501, 0.829, 0.352, 0.97… ## $ duration_ms &lt;dbl&gt; 313293, 360240, 202507, 173360, 344200, 195293, 17477… ## $ duration_minutes &lt;dbl&gt; 5.222, 6.004, 3.375, 2.889, 5.737, 3.255, 2.913, 0.94… ## $ energy &lt;dbl&gt; 0.2270, 0.4180, 0.6020, 0.9770, 0.5110, 0.6140, 0.985… ## $ explicit &lt;dbl&gt; 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ id &lt;chr&gt; &quot;0w0D8H1ubRerCXHWYJkinO&quot;, &quot;4JVeqfE2tpi7Pv63LJZtPh&quot;, &quot;… ## $ instrumentalness &lt;dbl&gt; 0.60100000, 0.00037200, 0.00013800, 0.00000000, 0.000… ## $ key &lt;dbl&gt; 10, 9, 11, 11, 7, 1, 2, 7, 8, 4, 9, 1, 10, 8, 2, 10, … ## $ liveness &lt;dbl&gt; 0.1100, 0.2220, 0.4000, 0.3500, 0.2790, 0.0975, 0.367… ## $ loudness &lt;dbl&gt; -13.44, -9.84, -8.31, -2.76, -9.84, -8.55, -2.56, -8.… ## $ mode &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0,… ## $ name &lt;chr&gt; &quot;Shout to the Lord&quot;, &quot;Miss You&quot;, &quot;Real Eyes&quot;, &quot;Pengui… ## $ popularity &lt;dbl&gt; 47, 51, 44, 52, 53, 46, 40, 39, 41, 47, 39, 42, 52, 3… ## $ release_date &lt;chr&gt; &quot;2000&quot;, &quot;2000-12-12&quot;, &quot;2000-06-13&quot;, &quot;2000-02-22&quot;, &quot;20… ## $ speechiness &lt;dbl&gt; 0.0290, 0.0407, 0.3420, 0.1270, 0.0291, 0.2670, 0.220… ## $ tempo &lt;dbl&gt; 136.1, 117.4, 89.7, 165.9, 78.0, 90.9, 130.0, 114.0, … ## $ valence &lt;dbl&gt; 0.0396, 0.8030, 0.4790, 0.5480, 0.1130, 0.4960, 0.241… ## $ year &lt;dbl&gt; 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000,… Here, sample the song dataset and compare a calculation on the whole population and on a sample. # Sample 1000 rows from spotify_population spotify_sample &lt;- slice_sample(spotify_population, n = 1000) # See the result glimpse(spotify_sample) ## Rows: 1,000 ## Columns: 20 ## $ acousticness &lt;dbl&gt; 0.033100, 0.073400, 0.775000, 0.641000, 0.206000, 0.0… ## $ artists &lt;chr&gt; &quot;[&#39;Aloe Blacc&#39;]&quot;, &quot;[&#39;One Direction&#39;]&quot;, &quot;[&#39;Bo Burnham&#39;… ## $ danceability &lt;dbl&gt; 0.308, 0.678, 0.338, 0.207, 0.884, 0.463, 0.292, 0.91… ## $ duration_ms &lt;dbl&gt; 254880, 214720, 261222, 147787, 236501, 103280, 22040… ## $ duration_minutes &lt;dbl&gt; 4.25, 3.58, 4.35, 2.46, 3.94, 1.72, 3.67, 4.47, 3.44,… ## $ energy &lt;dbl&gt; 0.769, 0.933, 0.478, 0.256, 0.936, 0.881, 0.678, 0.64… ## $ explicit &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1,… ## $ id &lt;chr&gt; &quot;2stPxcgjdSImK7Gizl8ZUN&quot;, &quot;6AzCBeiDuUXGXjznBufswB&quot;, &quot;… ## $ instrumentalness &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.95900000, 0.000… ## $ key &lt;dbl&gt; 11, 2, 0, 9, 1, 3, 10, 1, 1, 5, 7, 0, 8, 6, 0, 6, 11,… ## $ liveness &lt;dbl&gt; 0.2140, 0.0863, 0.7370, 0.0893, 0.2620, 0.0406, 0.146… ## $ loudness &lt;dbl&gt; -7.26, -4.96, -5.89, -13.38, -3.93, -7.13, -5.59, -7.… ## $ mode &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1,… ## $ name &lt;chr&gt; &quot;The Man&quot;, &quot;Stockholm Syndrome&quot;, &quot;From God&#39;s Perspect… ## $ popularity &lt;dbl&gt; 59, 69, 53, 49, 73, 46, 61, 42, 59, 64, 6, 57, 59, 61… ## $ release_date &lt;chr&gt; &quot;2014-01-01&quot;, &quot;2014-11-17&quot;, &quot;2013-12-17&quot;, &quot;2007-11-20… ## $ speechiness &lt;dbl&gt; 0.0650, 0.1400, 0.0404, 0.0401, 0.1670, 0.3270, 0.072… ## $ tempo &lt;dbl&gt; 81.9, 120.6, 86.9, 125.7, 120.0, 160.1, 87.4, 97.7, 1… ## $ valence &lt;dbl&gt; 0.4880, 0.3360, 0.3570, 0.0905, 0.7580, 0.3660, 0.365… ## $ year &lt;dbl&gt; 2014, 2014, 2013, 2007, 2020, 2001, 2015, 2001, 2006,… Notice that the mean duration of songs in the sample is similar, but not identical to the mean duration of songs in the whole population. # Calculate the mean duration in mins from spotify_population mean_dur_pop &lt;- spotify_population %&gt;% summarise(mean_dur_pop = mean(duration_minutes)) # Calculate the mean duration in mins from spotify_sample mean_dur_samp &lt;- spotify_sample %&gt;% summarise(mean_dur_samp = mean(duration_minutes)) # See the results cbind(mean_dur_pop, mean_dur_samp) ## mean_dur_pop mean_dur_samp ## 1 3.85 3.88 16.1.1.2 Simple sampling with base-R Let’s look at the loudness property of each song. # Get the loudness column of spotify_population loudness_pop &lt;- spotify_population$loudness # Sample 100 values of loudness_pop loudness_samp &lt;- sample(loudness_pop, size = 100) # See the results loudness_samp ## [1] -5.48 -4.80 -11.30 -5.04 -12.78 -9.04 -10.52 -4.82 -5.74 -6.16 ## [11] -6.99 -8.03 -4.49 -9.75 -7.42 -4.87 -13.49 -6.48 -3.42 -8.39 ## [21] -3.74 -16.54 -6.19 -5.29 -4.62 -5.42 -4.11 -6.46 -4.64 -7.46 ## [31] -6.76 -5.59 -4.37 -5.18 -3.31 -4.00 -4.77 -3.74 -3.81 -9.32 ## [41] -5.77 -11.45 -7.34 -8.83 -6.65 -8.74 -7.36 -9.98 -8.93 -5.86 ## [51] -6.08 -8.88 -4.83 -4.55 -5.35 -4.40 -8.02 -4.62 -5.86 -3.93 ## [61] -3.19 -6.97 -5.54 -9.31 -3.89 -4.61 -9.51 -5.44 -9.47 -13.16 ## [71] -4.74 -5.73 -2.24 -9.53 -10.45 -8.32 -3.71 -4.82 -7.90 -3.71 ## [81] -3.71 -14.57 -6.73 -6.97 -7.02 -9.46 -4.01 -7.51 -4.49 -7.73 ## [91] -6.59 -3.47 -5.60 -13.34 -7.35 -6.74 -4.31 -2.83 -3.40 -4.14 Again, notice that the calculated value (the standard deviation) is close but not identical in each case. # Calculate the standard deviation of loudness_pop sd_loudness_pop &lt;- sd(loudness_pop) # Calculate the standard deviation of loudness_samp sd_loudness_samp &lt;- sd(loudness_samp) # See the results c(sd_loudness_pop, sd_loudness_samp) ## [1] 4.52 2.82 16.1.2 Convenience sampling Sample not representative of population, causing sample bias. Collecting data by the easiest method is called convenience sampling. Visualizing selection bias with histogram. 16.1.2.1 Generalizable Visualizing the distributions of the population and the sample can help determine whether or not the sample is representative of the population. The Spotify dataset contains a column named acousticness, which is a confidence measure from zero to one of whether the track is acoustic, that is, it was made with instruments that aren’t plugged in. # Visualize the distribution of acousticness as a histogram with a binwidth of 0.01 ggplot(spotify_population, aes(x = acousticness)) + geom_histogram(binwidth = 0.01) # sampling spotify_mysterious_sample &lt;- slice_max(spotify_population, order_by = acousticness, n = 1107) # Update the histogram to use spotify_mysterious_sample with x-axis limits from 0 to 1 ggplot(spotify_mysterious_sample, aes(acousticness)) + geom_histogram(binwidth = 0.01) + xlim(0, 1) ## Warning: Removed 2 rows containing missing values ## (`geom_bar()`). The acousticness values in the sample are all greater than 0.95, whereas they range from 0 to 1 in the whole population. Let’s look at another sample to see if it is representative of the population. This time, you’ll look at the duration_minutes column of the Spotify datasets. # Visualize the distribution of duration_minutes as a histogram with a binwidth of 0.5 ggplot(spotify_population, aes(duration_minutes)) + geom_histogram(binwidth = 0.5) + xlim(0, 15) ## Warning: Removed 28 rows containing non-finite values ## (`stat_bin()`). ## Warning: Removed 2 rows containing missing values ## (`geom_bar()`). spotify_mysterious_sample2 &lt;- slice_sample(spotify_population, n = 50) # Update the histogram to use spotify_mysterious_sample2 with x-axis limits from 0 to 15 ggplot(spotify_mysterious_sample2, aes(duration_minutes)) + geom_histogram(binwidth = 0.01) + xlim(0, 15) ## Warning: Removed 2 rows containing missing values ## (`geom_bar()`). The duration values in the sample show a similar distribution to those in the whole population, so the results are generalizable. 16.1.3 Pseudo-random number generation Next “random” number calculated from previous “random” number. The first “random” number calculated from a seed. If you start from the same seed value, all future random numbers will be the same. Random number generating functions 16.1.3.1 Generating random numbers A related task is to generate random numbers that follow a statistical distribution, like the uniform distribution or the normal distribution. Each random number generation function has a name beginning with “r”. It’s first argument is the number of numbers to generate, but other arguments are distribution-specific. n_numbers &lt;- 5000 # see what arguments you need to pass to those functions args(runif) ## function (n, min = 0, max = 1) ## NULL args(rnorm) ## function (n, mean = 0, sd = 1) ## NULL Complete the data frame of random numbers. # Generate random numbers from ... randoms &lt;- data.frame( # a uniform distribution from -3 to 3 uniform = runif(n_numbers, min = -3, max = 3), # a normal distribution with mean 5 and sd 2 normal = rnorm(n_numbers, 5, 2) ) # Plot a histogram of uniform values, binwidth 0.25 ggplot(randoms, aes(x = uniform)) + geom_histogram(binwidth = 0.25) # Plot a histogram of normal values, binwidth 0.5 ggplot(randoms, aes(x = normal)) + geom_histogram(binwidth = 0.5) 16.1.3.2 Random seeds Setting the seed to R’s random number generator helps avoid different output every time by making the random number generation reproducible. # different value set.seed(123) list(r1 = rnorm(5), r2 = rnorm(5)) ## $r1 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 ## ## $r2 ## [1] 1.715 0.461 -1.265 -0.687 -0.446 # same value set.seed(123) r1 &lt;- rnorm(5) set.seed(123) r2 &lt;- rnorm(5) list(r1 = r1, r2 = r2) ## $r1 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 ## ## $r2 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 # different value set.seed(123) r1 &lt;- rnorm(5) set.seed(456) r2 &lt;- rnorm(5) list(r1 = r1, r2 = r2) ## $r1 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 ## ## $r2 ## [1] -1.344 0.622 0.801 -1.389 -0.714 # same value set.seed(123) r1 &lt;- c(rnorm(5), rnorm(5)) set.seed(123) r2 &lt;- rnorm(10) list(r1 = r1, r2 = r2) ## $r1 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 1.7151 0.4609 -1.2651 -0.6869 ## [10] -0.4457 ## ## $r2 ## [1] -0.5605 -0.2302 1.5587 0.0705 0.1293 1.7151 0.4609 -1.2651 -0.6869 ## [10] -0.4457 Setting the seed to a particular value means that subsequent random code that generates random numbers will have the same answer each time you run it. 16.2 Sampling Methods 16.2.1 Simple random &amp; systematic sampling 16.2.1.1 Simple random sampling Simple random sampling (sometimes abbreviated to “SRS”), involves picking rows at random, one at a time, where each row has the same chance of being picked as any other. To make it easier to see which rows end up in the sample, it’s helpful to include a row ID column (rowid_to_column()) in the dataset before you take the sample. We’ll look at a synthetic (fictional) employee attrition dataset from IBM, where “attrition” means leaving the company. attrition_pop &lt;- read_fst(&quot;data/attrition.fst&quot;) glimpse(attrition_pop) ## Rows: 1,470 ## Columns: 31 ## $ Age &lt;int&gt; 21, 19, 18, 18, 18, 27, 18, 18, 18, 18, 18, 3… ## $ Attrition &lt;fct&gt; No, Yes, Yes, No, Yes, No, No, Yes, No, Yes, … ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Rarely, Travel_Rarely, … ## $ DailyRate &lt;int&gt; 391, 528, 230, 812, 1306, 443, 287, 247, 1124… ## $ Department &lt;fct&gt; Research_Development, Sales, Research_Develop… ## $ DistanceFromHome &lt;int&gt; 15, 22, 3, 10, 5, 3, 5, 8, 1, 3, 14, 24, 16, … ## $ Education &lt;ord&gt; College, Below_College, Bachelor, Bachelor, B… ## $ EducationField &lt;fct&gt; Life_Sciences, Marketing, Life_Sciences, Medi… ## $ EnvironmentSatisfaction &lt;ord&gt; High, Very_High, High, Very_High, Medium, Ver… ## $ Gender &lt;fct&gt; Male, Male, Male, Female, Male, Male, Male, M… ## $ HourlyRate &lt;int&gt; 96, 50, 54, 69, 69, 50, 73, 80, 97, 70, 33, 6… ## $ JobInvolvement &lt;ord&gt; High, High, High, Medium, High, High, High, H… ## $ JobLevel &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ JobRole &lt;fct&gt; Research_Scientist, Sales_Representative, Lab… ## $ JobSatisfaction &lt;ord&gt; Very_High, High, High, High, Medium, Very_Hig… ## $ MaritalStatus &lt;fct&gt; Single, Single, Single, Single, Single, Marri… ## $ MonthlyIncome &lt;int&gt; 1232, 1675, 1420, 1200, 1878, 1706, 1051, 190… ## $ MonthlyRate &lt;int&gt; 19281, 26820, 25233, 9724, 8059, 16571, 13493… ## $ NumCompaniesWorked &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ OverTime &lt;fct&gt; No, Yes, No, No, Yes, No, No, No, No, Yes, No… ## $ PercentSalaryHike &lt;int&gt; 14, 19, 13, 12, 14, 11, 15, 12, 15, 12, 16, 2… ## $ PerformanceRating &lt;ord&gt; Excellent, Excellent, Excellent, Excellent, E… ## $ RelationshipSatisfaction &lt;ord&gt; Very_High, Very_High, High, Low, Very_High, H… ## $ StockOptionLevel &lt;int&gt; 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 2, 1, 1, … ## $ TotalWorkingYears &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, … ## $ TrainingTimesLastYear &lt;int&gt; 6, 2, 2, 2, 3, 6, 2, 0, 5, 2, 4, 2, 2, 3, 6, … ## $ WorkLifeBalance &lt;ord&gt; Better, Good, Better, Better, Better, Good, B… ## $ YearsAtCompany &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, … ## $ YearsInCurrentRole &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ YearsSinceLastPromotion &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ YearsWithCurrManager &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … Add a row ID column to the dataset, then use simple random sampling to get 200 rows. # Set the seed set.seed(5643) attrition_samp &lt;- attrition_pop %&gt;% # Add a row ID column rowid_to_column() %&gt;% # Get 200 rows using simple random sampling slice_sample(n = 200) # View the attrition_samp dataset glimpse(attrition_samp) ## Rows: 200 ## Columns: 32 ## $ rowid &lt;int&gt; 870, 1186, 151, 988, 631, 1351, 1279, 521, 13… ## $ Age &lt;int&gt; 45, 51, 31, 35, 46, 52, 41, 38, 52, 32, 48, 4… ## $ Attrition &lt;fct&gt; No, Yes, No, No, Yes, No, No, No, No, No, Yes… ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Travel_Freq… ## $ DailyRate &lt;int&gt; 1015, 1150, 793, 882, 669, 322, 840, 1444, 31… ## $ Department &lt;fct&gt; Research_Development, Research_Development, S… ## $ DistanceFromHome &lt;int&gt; 5, 8, 20, 3, 9, 28, 9, 1, 3, 1, 1, 10, 1, 4, … ## $ Education &lt;ord&gt; Doctor, Master, Bachelor, Master, College, Co… ## $ EducationField &lt;fct&gt; Medical, Life_Sciences, Life_Sciences, Life_S… ## $ EnvironmentSatisfaction &lt;ord&gt; High, Low, High, Very_High, High, Very_High, … ## $ Gender &lt;fct&gt; Female, Male, Male, Male, Male, Female, Male,… ## $ HourlyRate &lt;int&gt; 50, 53, 67, 92, 64, 59, 64, 88, 39, 68, 98, 6… ## $ JobInvolvement &lt;ord&gt; Low, Low, Very_High, High, Medium, Very_High,… ## $ JobLevel &lt;int&gt; 2, 3, 1, 3, 3, 4, 5, 1, 3, 1, 3, 3, 1, 3, 3, … ## $ JobRole &lt;fct&gt; Laboratory_Technician, Manufacturing_Director… ## $ JobSatisfaction &lt;ord&gt; Low, Very_High, Very_High, Very_High, Very_Hi… ## $ MaritalStatus &lt;fct&gt; Single, Single, Married, Divorced, Single, Ma… ## $ MonthlyIncome &lt;int&gt; 5769, 10650, 2791, 7823, 9619, 13247, 19419, … ## $ MonthlyRate &lt;int&gt; 23447, 25150, 21981, 6812, 13596, 9731, 3735,… ## $ NumCompaniesWorked &lt;int&gt; 1, 2, 0, 6, 1, 2, 2, 0, 2, 0, 9, 5, 5, 1, 3, … ## $ OverTime &lt;fct&gt; Yes, No, No, No, No, Yes, No, Yes, Yes, No, Y… ## $ PercentSalaryHike &lt;int&gt; 14, 15, 12, 13, 16, 11, 17, 11, 14, 14, 13, 2… ## $ PerformanceRating &lt;ord&gt; Excellent, Excellent, Excellent, Excellent, E… ## $ RelationshipSatisfaction &lt;ord&gt; Low, Very_High, Low, Medium, Very_High, Mediu… ## $ StockOptionLevel &lt;int&gt; 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 3, 1, 0, 1, … ## $ TotalWorkingYears &lt;int&gt; 10, 18, 3, 12, 9, 24, 21, 7, 28, 3, 23, 14, 7… ## $ TrainingTimesLastYear &lt;int&gt; 3, 2, 4, 2, 3, 3, 2, 2, 4, 2, 2, 2, 2, 2, 4, … ## $ WorkLifeBalance &lt;ord&gt; Better, Better, Better, Better, Better, Good,… ## $ YearsAtCompany &lt;int&gt; 10, 4, 2, 10, 9, 5, 18, 6, 5, 2, 1, 8, 5, 14,… ## $ YearsInCurrentRole &lt;int&gt; 7, 2, 2, 9, 8, 3, 16, 2, 4, 2, 0, 7, 4, 8, 6,… ## $ YearsSinceLastPromotion &lt;int&gt; 1, 0, 2, 0, 4, 0, 0, 1, 0, 2, 0, 0, 0, 9, 5, … ## $ YearsWithCurrManager &lt;int&gt; 4, 3, 2, 8, 7, 2, 11, 2, 4, 2, 0, 7, 2, 8, 17… Notice how the row IDs in the sampled dataset aren’t always increasing order. They are just random. 16.2.1.2 Systematic sampling Systematic sampling avoids randomness. Here, you pick rows from the population at regular intervals. For example, if the population dataset had 1000 rows and you wanted a sample size of 5, you’d pick rows 200, 400, 600, 800, and 1000. (1000 / 5 = 200 = interval) # Set the sample size to 200 sample_size &lt;- 200 # Get the population size from attrition_pop pop_size &lt;- nrow(attrition_pop) # Calculate the interval between rows to be sampled interval &lt;- pop_size %/% sample_size; interval ## [1] 7 Get the row indexes for the sample as a numeric sequence of interval. # Get row indexes for the sample row_indexes &lt;- seq_len(sample_size) * interval str(row_indexes) ## num [1:200] 7 14 21 28 35 42 49 56 63 70 ... # Systematically sample attrition_pop attrition_sys_samp &lt;- attrition_pop %&gt;% # Add a row ID column rowid_to_column() %&gt;% # Get 200 rows using systematic sampling # Get the rows of the population corresponding to row_indexes dplyr::slice(row_indexes) # See the result glimpse(attrition_sys_samp) ## Rows: 200 ## Columns: 32 ## $ rowid &lt;int&gt; 7, 14, 21, 28, 35, 42, 49, 56, 63, 70, 77, 84… ## $ Age &lt;int&gt; 18, 35, 22, 21, 20, 29, 19, 28, 19, 24, 32, 2… ## $ Attrition &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, Yes, No, No, Y… ## $ BusinessTravel &lt;fct&gt; Non-Travel, Travel_Rarely, Travel_Rarely, Tra… ## $ DailyRate &lt;int&gt; 287, 464, 534, 156, 959, 805, 419, 1009, 265,… ## $ Department &lt;fct&gt; Research_Development, Research_Development, R… ## $ DistanceFromHome &lt;int&gt; 5, 4, 15, 12, 1, 1, 21, 1, 25, 13, 2, 3, 1, 9… ## $ Education &lt;ord&gt; College, College, Bachelor, Bachelor, Bachelo… ## $ EducationField &lt;fct&gt; Life_Sciences, Other, Medical, Life_Sciences,… ## $ EnvironmentSatisfaction &lt;ord&gt; Medium, High, Medium, High, Very_High, Medium… ## $ Gender &lt;fct&gt; Male, Male, Female, Female, Female, Female, M… ## $ HourlyRate &lt;int&gt; 73, 75, 59, 90, 83, 36, 37, 45, 57, 78, 95, 7… ## $ JobInvolvement &lt;ord&gt; High, High, High, Very_High, Medium, Medium, … ## $ JobLevel &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ JobRole &lt;fct&gt; Research_Scientist, Laboratory_Technician, La… ## $ JobSatisfaction &lt;ord&gt; Very_High, Very_High, Very_High, Medium, Medi… ## $ MaritalStatus &lt;fct&gt; Single, Divorced, Single, Single, Single, Mar… ## $ MonthlyIncome &lt;int&gt; 1051, 1951, 2871, 2716, 2836, 2319, 2121, 259… ## $ MonthlyRate &lt;int&gt; 13493, 10910, 23785, 25422, 11757, 6689, 9947… ## $ NumCompaniesWorked &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ OverTime &lt;fct&gt; No, No, No, No, No, Yes, Yes, No, Yes, No, No… ## $ PercentSalaryHike &lt;int&gt; 15, 12, 15, 15, 13, 11, 13, 15, 12, 13, 12, 1… ## $ PerformanceRating &lt;ord&gt; Excellent, Excellent, Excellent, Excellent, E… ## $ RelationshipSatisfaction &lt;ord&gt; Very_High, High, High, Very_High, Very_High, … ## $ StockOptionLevel &lt;int&gt; 0, 1, 0, 0, 0, 1, 0, 2, 0, 1, 0, 1, 3, 0, 0, … ## $ TotalWorkingYears &lt;int&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, … ## $ TrainingTimesLastYear &lt;int&gt; 2, 3, 5, 0, 0, 1, 3, 2, 2, 2, 2, 3, 2, 3, 3, … ## $ WorkLifeBalance &lt;ord&gt; Better, Better, Better, Better, Best, Better,… ## $ YearsAtCompany &lt;int&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, … ## $ YearsInCurrentRole &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, … ## $ YearsSinceLastPromotion &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, … ## $ YearsWithCurrManager &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 2, … Systematic sampling avoids randomness by picking rows at regular intervals. The trouble with systematic sampling Systematic sampling has problems when the data are sorted or contain a pattern, then the resulting sample may not be representative of the whole population. The problem can be solved by shuffling the rows, but then systematic sampling is equivalent to simple random sampling. # Add a row ID column to attrition_pop attrition_pop_id &lt;- attrition_pop %&gt;% rowid_to_column() # Using attrition_pop_id, plot YearsAtCompany vs. rowid ggplot(attrition_pop_id, aes(rowid, YearsAtCompany)) + # Make it a scatter plot geom_point() + # Add a smooth trend line geom_smooth(se = F) ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Shuffle the rows of attrition_pop, by slice_sample(prop = 1) # Shuffle the rows of attrition_pop then add row IDs attrition_shuffled &lt;- attrition_pop %&gt;% slice_sample(prop = 1) %&gt;% rowid_to_column() # Using attrition_shuffled, plot YearsAtCompany vs. rowid # Add points and a smooth trend line ggplot(attrition_shuffled, aes(rowid, YearsAtCompany)) + geom_point() + geom_smooth(se = F) ## `geom_smooth()` using method = &#39;gam&#39; and ## formula = &#39;y ~ s(x, bs = &quot;cs&quot;)&#39; Shu+ing rows + systematic sampling is the same as simple random sampling. 16.2.2 Stratified &amp; weighted random sampling Stratified sampling is useful if you care about subgroups. Otherwise, simple random sampling is more appropriate. 16.2.2.1 Proportional stratified sampling If you are interested in subgroups within the population, then you may need to carefully control the counts of each subgroup within the population. Proportional stratified sampling results in subgroup sizes within the sample that are representative of the subgroup sizes within the population. education_counts_pop &lt;- attrition_pop %&gt;% # Count the employees by Education level, sorting by n count(Education, sort = T) %&gt;% # Add a percent column mutate(percent = (n / sum(n)) * 100) # See the results education_counts_pop ## Education n percent ## 1 Bachelor 572 38.91 ## 2 Master 398 27.07 ## 3 College 282 19.18 ## 4 Below_College 170 11.56 ## 5 Doctor 48 3.27 # Use proportional stratified sampling to get 40% of each Education group attrition_strat &lt;- attrition_pop %&gt;% group_by(Education) %&gt;% slice_sample(prop = 0.4) %&gt;% # Make sure to Ungroup the stratified sample ungroup() # See the result glimpse(attrition_strat) ## Rows: 586 ## Columns: 31 ## $ Age &lt;int&gt; 37, 29, 33, 39, 47, 27, 29, 26, 24, 46, 23, 2… ## $ Attrition &lt;fct&gt; No, Yes, No, No, No, Yes, No, No, No, No, Yes… ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Frequently, Travel_Rare… ## $ DailyRate &lt;int&gt; 1439, 337, 461, 116, 1309, 1420, 352, 683, 14… ## $ Department &lt;fct&gt; Research_Development, Research_Development, R… ## $ DistanceFromHome &lt;int&gt; 4, 14, 13, 24, 4, 2, 6, 2, 4, 18, 8, 26, 10, … ## $ Education &lt;ord&gt; Below_College, Below_College, Below_College, … ## $ EducationField &lt;fct&gt; Life_Sciences, Other, Life_Sciences, Life_Sci… ## $ EnvironmentSatisfaction &lt;ord&gt; High, High, Medium, Low, Medium, High, Very_H… ## $ Gender &lt;fct&gt; Male, Female, Female, Male, Male, Male, Male,… ## $ HourlyRate &lt;int&gt; 54, 84, 53, 52, 99, 85, 87, 36, 42, 86, 93, 8… ## $ JobInvolvement &lt;ord&gt; High, High, High, High, High, High, Medium, M… ## $ JobLevel &lt;int&gt; 1, 3, 1, 2, 2, 1, 1, 1, 2, 3, 1, 1, 2, 1, 1, … ## $ JobRole &lt;fct&gt; Research_Scientist, Healthcare_Representative… ## $ JobSatisfaction &lt;ord&gt; High, Very_High, Very_High, Very_High, High, … ## $ MaritalStatus &lt;fct&gt; Married, Single, Single, Single, Single, Divo… ## $ MonthlyIncome &lt;int&gt; 2996, 7553, 3452, 4108, 2976, 3041, 2804, 390… ## $ MonthlyRate &lt;int&gt; 5182, 22930, 17241, 5340, 25751, 16346, 15434… ## $ NumCompaniesWorked &lt;int&gt; 7, 0, 3, 7, 3, 0, 1, 0, 1, 5, 1, 1, 1, 0, 1, … ## $ OverTime &lt;fct&gt; Yes, Yes, No, No, No, No, No, No, Yes, No, Ye… ## $ PercentSalaryHike &lt;int&gt; 15, 12, 18, 13, 19, 11, 11, 12, 12, 11, 11, 1… ## $ PerformanceRating &lt;ord&gt; Excellent, Excellent, Excellent, Excellent, E… ## $ RelationshipSatisfaction &lt;ord&gt; Very_High, Low, Low, Low, Low, Medium, Very_H… ## $ StockOptionLevel &lt;int&gt; 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 2, 2, 0, 1, … ## $ TotalWorkingYears &lt;int&gt; 8, 9, 5, 18, 5, 5, 1, 5, 5, 28, 5, 4, 7, 4, 1… ## $ TrainingTimesLastYear &lt;int&gt; 2, 1, 4, 2, 3, 3, 3, 2, 3, 3, 2, 2, 2, 3, 5, … ## $ WorkLifeBalance &lt;ord&gt; Better, Better, Better, Better, Better, Bette… ## $ YearsAtCompany &lt;int&gt; 6, 8, 3, 7, 0, 4, 1, 4, 5, 2, 5, 4, 7, 3, 10,… ## $ YearsInCurrentRole &lt;int&gt; 4, 7, 2, 7, 0, 3, 0, 3, 4, 2, 4, 2, 7, 2, 8, … ## $ YearsSinceLastPromotion &lt;int&gt; 1, 7, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 2, 0, … ## $ YearsWithCurrManager &lt;int&gt; 3, 7, 2, 7, 0, 2, 0, 1, 3, 2, 2, 2, 7, 2, 8, … # Get the counts and percents from attrition_strat education_counts_strat &lt;- attrition_strat %&gt;% count(Education, sort = T) %&gt;% mutate(percent = n / sum(n) * 100) # See the results education_counts_strat ## # A tibble: 5 × 3 ## Education n percent ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Bachelor 228 38.9 ## 2 Master 159 27.1 ## 3 College 112 19.1 ## 4 Below_College 68 11.6 ## 5 Doctor 19 3.24 By grouping then sampling, the size of each group in the sample is representative of the size of the sample in the population. 16.2.2.2 Equal counts stratified sampling If one subgroup is larger than another subgroup in the population, but you don’t want to reflect that difference in your analysis, then you can use equal counts stratified sampling to generate samples where each subgroup has the same amount of data. # Use equal counts stratified sampling to get 30 employees from each Education group attrition_eq &lt;- attrition_pop %&gt;% group_by(Education) %&gt;% slice_sample(n = 30) %&gt;% ungroup() # See the results str(attrition_eq) ## tibble [150 × 31] (S3: tbl_df/tbl/data.frame) ## $ Age : int [1:150] 36 25 26 19 22 33 32 29 21 30 ... ## $ Attrition : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 1 1 2 2 1 1 1 1 1 ... ## $ BusinessTravel : Factor w/ 3 levels &quot;Non-Travel&quot;,&quot;Travel_Frequently&quot;,..: 3 3 3 3 3 1 3 3 3 1 ... ## $ DailyRate : int [1:150] 530 977 683 528 391 1038 499 441 501 829 ... ## $ Department : Factor w/ 3 levels &quot;Human_Resources&quot;,..: 3 2 2 3 2 3 3 2 3 2 ... ## $ DistanceFromHome : int [1:150] 3 2 2 22 7 8 2 8 5 1 ... ## $ Education : Ord.factor w/ 5 levels &quot;Below_College&quot;&lt;..: 1 1 1 1 1 1 1 1 1 1 ... ## $ EducationField : Factor w/ 6 levels &quot;Human_Resources&quot;,..: 2 5 4 3 2 2 3 5 4 2 ... ## $ EnvironmentSatisfaction : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 3 4 1 4 4 2 3 3 3 3 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 1 2 1 2 2 ... ## $ HourlyRate : int [1:150] 51 57 36 50 75 88 36 39 58 88 ... ## $ JobInvolvement : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 2 3 2 3 3 2 3 1 3 2 ... ## $ JobLevel : int [1:150] 3 1 1 1 1 1 2 2 1 3 ... ## $ JobRole : Factor w/ 9 levels &quot;Healthcare_Representative&quot;,..: 8 3 7 9 7 9 8 1 9 5 ... ## $ JobSatisfaction : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 4 3 4 3 2 4 2 1 1 3 ... ## $ MaritalStatus : Factor w/ 3 levels &quot;Divorced&quot;,&quot;Married&quot;,..: 2 1 3 3 3 3 2 2 3 3 ... ## $ MonthlyIncome : int [1:150] 10325 3977 3904 1675 2472 2342 4078 9715 2380 8474 ... ## $ MonthlyRate : int [1:150] 5518 7298 4050 26820 26092 21437 20497 7288 25479 20925 ... ## $ NumCompaniesWorked : int [1:150] 1 6 0 1 1 0 0 3 1 1 ... ## $ OverTime : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 2 2 1 2 1 2 1 ... ## $ PercentSalaryHike : int [1:150] 11 19 12 19 23 19 13 13 11 22 ... ## $ PerformanceRating : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Good&quot;&lt;&quot;Excellent&quot;&lt;..: 3 3 3 3 4 3 3 3 3 4 ... ## $ RelationshipSatisfaction: Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 1 3 4 4 1 4 1 3 4 3 ... ## $ StockOptionLevel : int [1:150] 1 1 0 0 0 0 3 1 0 0 ... ## $ TotalWorkingYears : int [1:150] 16 7 5 0 1 3 4 9 2 12 ... ## $ TrainingTimesLastYear : int [1:150] 6 2 2 2 2 2 3 3 6 2 ... ## $ WorkLifeBalance : Ord.factor w/ 4 levels &quot;Bad&quot;&lt;&quot;Good&quot;&lt;&quot;Better&quot;&lt;..: 3 2 3 2 3 2 2 3 3 3 ... ## $ YearsAtCompany : int [1:150] 16 2 4 0 1 2 3 7 2 11 ... ## $ YearsInCurrentRole : int [1:150] 7 2 3 0 0 2 2 7 2 8 ... ## $ YearsSinceLastPromotion : int [1:150] 3 0 1 0 0 2 1 0 1 5 ... ## $ YearsWithCurrManager : int [1:150] 7 2 1 0 0 2 2 7 2 8 ... # Get the counts and percents from attrition_eq education_counts_eq &lt;- attrition_eq %&gt;% count(Education, sort = T) %&gt;% mutate(percent = n / sum(n) * 100) # See the results education_counts_eq ## # A tibble: 5 × 3 ## Education n percent ## &lt;ord&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Below_College 30 20 ## 2 College 30 20 ## 3 Bachelor 30 20 ## 4 Master 30 20 ## 5 Doctor 30 20 If you want each subgroup to have equal weight in your analysis, then equal counts stratified sampling is the appropriate technique. 16.2.2.3 Weighted sampling Weighted sampling, which lets you specify rules about the probability of picking rows at the row level. The probability of picking any given row is proportional to the weight value for that row. # Using attrition_pop, plot YearsAtCompany as a histogram with binwidth 1 ggplot(attrition_pop, aes(x = YearsAtCompany)) + geom_histogram(binwidth = 1) + geom_vline(xintercept = mean(attrition_pop$YearsAtCompany), color = &quot;red&quot;, linetype = &quot;dotted&quot;) + annotate( &quot;text&quot;, x = 15, y = 175, label = paste(&quot;mean = &quot;, mean(attrition_pop$YearsAtCompany), sep = &quot;&quot;), vjust = 1, size = 4, color = &quot;grey40&quot; ) # Sample 400 employees weighted by YearsAtCompany attrition_weight &lt;- attrition_pop %&gt;% slice_sample(n = 400, weight_by = YearsAtCompany) # See the results glimpse(attrition_weight) ## Rows: 400 ## Columns: 31 ## $ Age &lt;int&gt; 30, 40, 31, 32, 40, 58, 26, 36, 28, 30, 42, 4… ## $ Attrition &lt;fct&gt; No, No, No, No, No, Yes, Yes, No, No, No, No,… ## $ BusinessTravel &lt;fct&gt; Travel_Rarely, Travel_Rarely, Travel_Rarely, … ## $ DailyRate &lt;int&gt; 438, 1398, 746, 117, 898, 147, 575, 301, 950,… ## $ Department &lt;fct&gt; Research_Development, Sales, Research_Develop… ## $ DistanceFromHome &lt;int&gt; 18, 2, 8, 13, 6, 23, 3, 15, 3, 9, 2, 26, 1, 2… ## $ Education &lt;ord&gt; Bachelor, Master, Master, Master, College, Ma… ## $ EducationField &lt;fct&gt; Life_Sciences, Life_Sciences, Life_Sciences, … ## $ EnvironmentSatisfaction &lt;ord&gt; Low, High, High, Medium, High, Very_High, Hig… ## $ Gender &lt;fct&gt; Female, Female, Female, Male, Male, Female, M… ## $ HourlyRate &lt;int&gt; 75, 79, 61, 73, 38, 94, 73, 88, 93, 48, 35, 5… ## $ JobInvolvement &lt;ord&gt; High, High, High, High, High, High, High, Low… ## $ JobLevel &lt;int&gt; 1, 5, 2, 2, 4, 3, 1, 2, 3, 2, 4, 2, 4, 2, 5, … ## $ JobRole &lt;fct&gt; Research_Scientist, Manager, Manufacturing_Di… ## $ JobSatisfaction &lt;ord&gt; High, High, Very_High, Very_High, Very_High, … ## $ MaritalStatus &lt;fct&gt; Single, Married, Single, Divorced, Single, Ma… ## $ MonthlyIncome &lt;int&gt; 2632, 18041, 4424, 4403, 16437, 10312, 3102, … ## $ MonthlyRate &lt;int&gt; 23910, 13022, 20682, 9250, 17381, 3465, 6582,… ## $ NumCompaniesWorked &lt;int&gt; 1, 0, 1, 2, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, … ## $ OverTime &lt;fct&gt; No, No, No, No, Yes, No, No, No, No, Yes, No,… ## $ PercentSalaryHike &lt;int&gt; 14, 14, 23, 11, 21, 12, 22, 24, 17, 19, 17, 1… ## $ PerformanceRating &lt;ord&gt; Excellent, Excellent, Outstanding, Excellent,… ## $ RelationshipSatisfaction &lt;ord&gt; High, Very_High, Very_High, High, Very_High, … ## $ StockOptionLevel &lt;int&gt; 0, 0, 0, 1, 0, 1, 0, 1, 3, 0, 1, 1, 1, 1, 1, … ## $ TotalWorkingYears &lt;int&gt; 5, 21, 11, 8, 21, 40, 7, 15, 10, 12, 23, 10, … ## $ TrainingTimesLastYear &lt;int&gt; 4, 2, 2, 3, 2, 3, 2, 4, 3, 2, 3, 2, 3, 3, 3, … ## $ WorkLifeBalance &lt;ord&gt; Good, Better, Better, Good, Better, Good, Bet… ## $ YearsAtCompany &lt;int&gt; 5, 20, 11, 5, 21, 40, 6, 15, 9, 11, 22, 10, 2… ## $ YearsInCurrentRole &lt;int&gt; 4, 15, 7, 2, 7, 10, 4, 12, 7, 9, 6, 7, 3, 9, … ## $ YearsSinceLastPromotion &lt;int&gt; 0, 1, 1, 0, 7, 15, 0, 11, 1, 4, 13, 4, 11, 8,… ## $ YearsWithCurrManager &lt;int&gt; 4, 12, 8, 3, 7, 6, 4, 11, 7, 7, 7, 5, 11, 8, … # Using attrition_weight, plot YearsAtCompany as a histogram with binwidth 1 ggplot(attrition_weight, aes(x = YearsAtCompany)) + geom_histogram(binwidth = 1) + geom_vline(xintercept = mean(attrition_weight$YearsAtCompany), color = &quot;red&quot;, linetype = &quot;dotted&quot;) + annotate( &quot;text&quot;, x = 16, y = 40, label = paste(&quot;mean = &quot;, mean(attrition_weight$YearsAtCompany), sep = &quot;&quot;), vjust = 1, size = 4, color = &quot;grey40&quot; ) The weighted sample mean is around 11, which is higher than the population mean of around 7. The fact that the two numbers are different means that the weighted simple random sample is biased. 16.2.3 Cluster sampling Stratified sampling vs. cluster sampling The main benefit of cluster sampling over stratified sampling is that you can save time or money by not including every subgroup in your sample. Stratified sampling Split the population into subgroups Use simple random sampling on every subgroup Cluster sampling Use simple random sampling to pick some subgroups Stage 1: sampling for subgroups Use simple random sampling on only those subgroups Stage 2: sampling each group Multistage sampling Cluster sampling is a type of multistage sampling. You can have &gt; 2 stages. e.g, Countrywide surveys may sample states, counties, cities, and neighborhoods. 16.2.3.1 Performing cluster sampling You’ll explore the JobRole column of the attrition dataset. You can think of each job role as a subgroup of the whole population of employees. # Get unique JobRole values job_roles_pop &lt;- unique(attrition_pop$JobRole) # Randomly sample four JobRole values job_roles_samp &lt;- sample(job_roles_pop, size = 4) # See the result job_roles_samp ## [1] Sales_Representative Human_Resources Manufacturing_Director ## [4] Manager ## 9 Levels: Healthcare_Representative Human_Resources ... Sales_Representative # Filter for rows where JobRole is in job_roles_samp attrition_filtered &lt;- attrition_pop %&gt;% filter(JobRole %in% job_roles_samp) # Randomly sample 10 employees from each sampled job role attrition_clus &lt;- attrition_filtered %&gt;% group_by(JobRole) %&gt;% slice_sample(n = 10) %&gt;% ungroup() # See the result str(attrition_clus) ## tibble [40 × 31] (S3: tbl_df/tbl/data.frame) ## $ Age : int [1:40] 31 38 59 34 54 29 35 44 34 37 ... ## $ Attrition : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ BusinessTravel : Factor w/ 3 levels &quot;Non-Travel&quot;,&quot;Travel_Frequently&quot;,..: 3 3 3 2 3 3 3 3 3 3 ... ## $ DailyRate : int [1:40] 1398 433 818 648 397 352 528 528 829 807 ... ## $ Department : Factor w/ 3 levels &quot;Human_Resources&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ DistanceFromHome : int [1:40] 8 1 6 11 19 6 8 1 3 6 ... ## $ Education : Ord.factor w/ 5 levels &quot;Below_College&quot;&lt;..: 2 3 2 3 4 1 4 3 2 4 ... ## $ EducationField : Factor w/ 6 levels &quot;Human_Resources&quot;,..: 4 1 4 2 4 4 6 2 1 1 ... ## $ EnvironmentSatisfaction : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 4 3 2 3 3 4 3 3 3 3 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 2 2 2 2 2 2 1 2 2 ... ## $ HourlyRate : int [1:40] 96 37 52 56 88 87 100 44 88 63 ... ## $ JobInvolvement : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 4 4 3 2 3 2 3 3 3 3 ... ## $ JobLevel : int [1:40] 1 1 1 2 3 1 1 1 1 1 ... ## $ JobRole : Factor w/ 9 levels &quot;Healthcare_Representative&quot;,..: 2 2 2 2 2 2 2 2 2 2 ... ## $ JobSatisfaction : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 2 3 3 2 2 2 3 4 4 1 ... ## $ MaritalStatus : Factor w/ 3 levels &quot;Divorced&quot;,&quot;Married&quot;,..: 3 2 2 2 2 2 3 1 2 1 ... ## $ MonthlyIncome : int [1:40] 2109 2844 2267 4490 10725 2804 4323 3195 3737 2073 ... ## $ MonthlyRate : int [1:40] 24609 6004 25657 21833 6729 15434 7108 4167 2243 23648 ... ## $ NumCompaniesWorked : int [1:40] 9 1 8 4 2 1 1 4 0 4 ... ## $ OverTime : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 2 1 2 ... ## $ PercentSalaryHike : int [1:40] 18 13 17 11 15 11 17 18 19 22 ... ## $ PerformanceRating : Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Good&quot;&lt;&quot;Excellent&quot;&lt;..: 3 3 3 3 3 3 3 3 3 4 ... ## $ RelationshipSatisfaction: Ord.factor w/ 4 levels &quot;Low&quot;&lt;&quot;Medium&quot;&lt;..: 4 4 4 4 3 4 2 1 3 4 ... ## $ StockOptionLevel : int [1:40] 0 1 0 2 1 0 0 3 1 0 ... ## $ TotalWorkingYears : int [1:40] 8 7 7 14 16 1 6 8 4 7 ... ## $ TrainingTimesLastYear : int [1:40] 3 2 2 5 1 3 2 2 1 3 ... ## $ WorkLifeBalance : Ord.factor w/ 4 levels &quot;Bad&quot;&lt;&quot;Good&quot;&lt;&quot;Better&quot;&lt;..: 3 4 2 4 4 3 1 3 1 3 ... ## $ YearsAtCompany : int [1:40] 3 7 2 10 9 1 5 2 3 3 ... ## $ YearsInCurrentRole : int [1:40] 2 6 2 9 7 0 4 2 2 2 ... ## $ YearsSinceLastPromotion : int [1:40] 0 5 2 1 7 0 1 2 0 0 ... ## $ YearsWithCurrManager : int [1:40] 2 0 2 8 1 0 4 2 2 2 ... The two-stage sampling technique gives you control over sampling both between subgroups and within subgroups. 16.2.4 Comparing sampling methods 16.2.4.1 3 kinds of sampling Let’s compare the performance of point estimates using simple, stratified, and cluster sampling. Before we do that, you’ll have to set up the samples. In these exercises, we’ll use the RelationshipSatisfaction column of the attrition dataset, which categorizes the employee’s relationship with the company. # simple random sampling # Perform simple random sampling to get 0.25 of the population attrition_srs &lt;- attrition_pop %&gt;% slice_sample(prop = 0.25) # stratified sampling # Perform stratified sampling to get 0.25 of each relationship group attrition_strat &lt;- attrition_pop %&gt;% group_by(RelationshipSatisfaction) %&gt;% slice_sample(prop = 0.25) %&gt;% ungroup() # cluster sampling # Get unique values of RelationshipSatisfaction satisfaction_unique &lt;- unique(attrition_pop$RelationshipSatisfaction) # Randomly sample for 2 of the unique satisfaction values satisfaction_samp &lt;- sample(satisfaction_unique, size = 2) # Perform cluster sampling on the selected group getting 0.25 (one quarter) of the population attrition_clust &lt;- attrition_pop %&gt;% filter(RelationshipSatisfaction %in% satisfaction_samp) %&gt;% group_by(RelationshipSatisfaction) %&gt;% slice_sample(n = round(nrow(attrition_pop) / 4)) %&gt;% ungroup() 16.2.4.2 Summary statistics on different sample Now you have three types of sample (simple, stratified, cluster), you can compare point estimates from each sample to the population parameter. That is, you can calculate the same summary statistic on each sample and see how it compares to the summary statistic for the population. Here, we’ll look at how satisfaction with the company affects whether or not the employee leaves the company. That is, you’ll calculate the proportion of employees who left the company (they have an Attrition value of \"Yes\"), for each value of RelationshipSatisfaction. # Use the whole population dataset mean_attrition_pop &lt;- attrition_pop %&gt;% # Group by relationship satisfaction level group_by(RelationshipSatisfaction) %&gt;% # Calculate the proportion of employee attrition summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) # See the result mean_attrition_pop ## # A tibble: 4 × 2 ## RelationshipSatisfaction mean_attrition ## &lt;ord&gt; &lt;dbl&gt; ## 1 Low 0.207 ## 2 Medium 0.149 ## 3 High 0.155 ## 4 Very_High 0.148 # Calculate the same thing for the simple random sample mean_attrition_srs &lt;- attrition_srs %&gt;% group_by(RelationshipSatisfaction) %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) # See the result mean_attrition_srs ## # A tibble: 4 × 2 ## RelationshipSatisfaction mean_attrition ## &lt;ord&gt; &lt;dbl&gt; ## 1 Low 0.167 ## 2 Medium 0.147 ## 3 High 0.202 ## 4 Very_High 0.159 # Calculate the same thing for the stratified sample mean_attrition_strat &lt;- attrition_strat %&gt;% group_by(RelationshipSatisfaction) %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) # See the result mean_attrition_strat ## # A tibble: 4 × 2 ## RelationshipSatisfaction mean_attrition ## &lt;ord&gt; &lt;dbl&gt; ## 1 Low 0.130 ## 2 Medium 0.12 ## 3 High 0.114 ## 4 Very_High 0.157 # Calculate the same thing for the cluster sample mean_attrition_clust &lt;- attrition_clust %&gt;% group_by(RelationshipSatisfaction) %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) # See the result mean_attrition_clust ## # A tibble: 2 × 2 ## RelationshipSatisfaction mean_attrition ## &lt;ord&gt; &lt;dbl&gt; ## 1 Low 0.207 ## 2 High 0.152 The numbers are all fairly similar, with the notable exception that cluster sampling only gives results for the clusters included in the sample. 16.3 Sampling Distributions 16.3.1 Relative error of point estimates 16.3.1.1 Relative errors The size of the sample you take affects how accurately the point estimates reflect the corresponding population parameter. For example, when you calculate a sample mean, you want it to be close to the population mean. However, if your sample is too small, this might not be the case. The most common metric for assessing accuracy is relative error. This is the absolute difference between the population parameter and the point estimate, all divided by the population parameter. It is sometimes expressed as a percentage. 100 * abs(population_mean - sample_mean) / population_mean # Population mean mean_attrition_pop &lt;- attrition_pop %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)); mean_attrition_pop ## mean_attrition ## 1 0.161 When sample size is 10. # Generate a simple random sample of 10 rows attrition_srs10 &lt;- slice_sample(attrition_pop, n = 10) # Calculate the proportion of employee attrition in the sample mean_attrition_srs10 &lt;- attrition_srs10 %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) # Calculate the relative error percentage rel_error_pct10 &lt;- abs(mean_attrition_pop - mean_attrition_srs10) / mean_attrition_pop * 100 # See the result rel_error_pct10 ## mean_attrition ## 1 38 When sample size is 100. # Calculate the relative error percentage again with a sample of 100 rows mean_attrition_srs100 &lt;- attrition_pop %&gt;% slice_sample(n = 100) %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) rel_error_pct100 &lt;- abs(mean_attrition_pop - mean_attrition_srs100) / mean_attrition_pop * 100 # See the result rel_error_pct100 ## mean_attrition ## 1 19.4 As you increase the sample size, on average the sample mean gets closer to the population mean and the relative error decreases. 16.3.1.2 Relative error vs. sample size Here’s a scatter plot of relative error versus sample size, with a smooth trend line calculated using the LOESS method. The relative error decreases as the sample size increases. As you increase sample size, the relative error decreases quickly at first, then more slowly as it drops to zero. 16.3.2 Creating a sampling distribution A sampling distribution is a distribution of several replicates of point estimates. Base-R’s replicate function let’s you run the same code multiple times. It’s especially useful for situations like this where the result contains some randomness. replicate( n = 1000, expr = data %&gt;% slice_sample(n = 30) %&gt;% summarize(mean_points = mean(total_points)) %&gt;% pull(mean_points) ) The first argument, n: is the number of times to run the code, The second argument, expr: is the code to run. Each time the code is run, we get one sample mean, so running the code a thousand times gives us a vector of a thousand sample means. 16.3.2.1 Replicating samples When you calculate a point estimate such as a sample mean, the value you calculate depends on the rows that were included in the sample. That means that there is some randomness in the answer. In order to quantify the variation caused by this randomness, you can create many samples and calculate the sample mean (or other statistic) for each sample. # Replicate this code 500 times mean_attritions &lt;- replicate( n = 500, expr = attrition_pop %&gt;% slice_sample(n = 20) %&gt;% summarize(mean_attrition = mean(Attrition == &quot;Yes&quot;)) %&gt;% pull(mean_attrition) ) # See the result str(mean_attritions) ## num [1:500] 0.25 0.15 0.15 0.15 0.15 0.15 0.2 0.25 0.2 0.2 ... # Store mean_attritions in a tibble in a column named sample_mean sample_means &lt;- tibble(sample_mean = mean_attritions) # Plot a histogram of the `sample_mean` column, binwidth 0.05 ggplot(sample_means, aes(x = sample_mean)) + geom_histogram(binwidth = 0.05) By generating the sample statistic many times with different samples, you can quantify the amount of variation in those statistics. As sample size increases, on average each sample mean has a lower relative error compared to the population mean, thus reducing the range of the distribution. 16.3.3 Approximate sampling distributions 16.3.3.1 Exact sampling distribution The distribution of a sample statistic is called the sampling distribution. When we can calculate this exactly, rather than using an approximation, it is known as the exact sampling distribution. Let’s take another look at the sampling distribution of dice rolls. This time, we’ll look at five eight-sided dice. # Expand a grid representing 5 8-sided dice dice &lt;- expand_grid( die1 = 1:8, die2 = 1:8, die3 = 1:8, die4 = 1:8, die5 = 1:8 ) # See the result dice ## # A tibble: 32,768 × 5 ## die1 die2 die3 die4 die5 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 1 1 1 ## 2 1 1 1 1 2 ## 3 1 1 1 1 3 ## 4 1 1 1 1 4 ## 5 1 1 1 1 5 ## 6 1 1 1 1 6 ## 7 1 1 1 1 7 ## 8 1 1 1 1 8 ## 9 1 1 1 2 1 ## 10 1 1 1 2 2 ## # ℹ 32,758 more rows Add a column, mean_roll, to dice, that contains the mean of the five rolls. dice &lt;- dice %&gt;% # Add a column of mean rolls mutate(mean_roll = rowSums(.)/ncol(.)) dice ## # A tibble: 32,768 × 6 ## die1 die2 die3 die4 die5 mean_roll ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 1 ## 2 1 1 1 1 2 1.2 ## 3 1 1 1 1 3 1.4 ## 4 1 1 1 1 4 1.6 ## 5 1 1 1 1 5 1.8 ## 6 1 1 1 1 6 2 ## 7 1 1 1 1 7 2.2 ## 8 1 1 1 1 8 2.4 ## 9 1 1 1 2 1 1.2 ## 10 1 1 1 2 2 1.4 ## # ℹ 32,758 more rows # Using dice, draw a bar plot of mean_roll as a factor ggplot(dice, aes(x = factor(mean_roll))) + geom_bar() + theme(axis.text.x = element_text(angle = 70, vjust = 0.8, hjust = 1)) The exact sampling distribution shows all possible variations of the point estimate that you are interested in. 16.3.3.2 Approximate sampling distribution Calculating the exact sampling distribution is only possible in very simple situations. With just five eight-sided dice, the number of possible rolls is 8 ^ 5, which is over thirty thousand. When the dataset is more complicated, for example where a variable has hundreds or thousands or categories, the number of possible outcomes becomes too difficult to compute exactly. In this situation, you can calculate an approximate sampling distribution by simulating the exact sampling distribution. That is, you can repeat a procedure over and over again to simulate both the sampling process and the sample statistic calculation process. # Sample one to eight, five times, with replacement five_rolls &lt;- sample(1:8, size = 5, replace = TRUE) # Calculate the mean of five_rolls mean(five_rolls) ## [1] 5.2 Replicate the sampling code 1000 times. The code to generate each sample mean was two lines, so we have to wrap the expr argument to replicate in {}, like in a for loop or a function body. # Replicate the sampling code 1000 times sample_means_1000 &lt;- replicate( n = 1000, expr = { five_rolls &lt;- sample(1:8, size = 5, replace = TRUE) mean(five_rolls) } ) # See the result str(sample_means_1000) ## num [1:1000] 4.6 4.4 5.4 5.8 2.2 4.8 5.6 4.6 6.8 3.6 ... # Wrap sample_means_1000 in the sample_mean column of a tibble sample_means &lt;- tibble(sample_mean = sample_means_1000) # See the result sample_means ## # A tibble: 1,000 × 1 ## sample_mean ## &lt;dbl&gt; ## 1 4.6 ## 2 4.4 ## 3 5.4 ## 4 5.8 ## 5 2.2 ## 6 4.8 ## 7 5.6 ## 8 4.6 ## 9 6.8 ## 10 3.6 ## # ℹ 990 more rows # Using sample_means, draw a bar plot of sample_mean as a factor ggplot(sample_means, aes(x = factor(sample_mean))) + geom_bar() + theme(axis.text.x = element_text(angle = 70, vjust = 0.8, hjust = 1)) Once your dataset gets sufficiently big, exact sampling distributions cannot be calculated, so an approximate sampling distribution has to be used. Notice that the histogram is close to but not exactly the same as the histogram from the previous exercise. 16.3.4 Standard errors &amp; CLT Consequences of the central limit theorem Averages of independent samples have approximately normal distributions. As the sample size increases, the distribution of the averages gets closer to being normally distributed the width of the sampling distribution gets narrower. 16.3.4.1 Population &amp; sampling distribution means Here, we’ll look at the relationship between the mean of the sampling distribution and the population parameter that the sampling is supposed to estimate. Three sampling distributions are provided. In each case, the employee attrition dataset was sampled using simple random sampling, then the mean attrition was calculated. This was done 1000 times to get a sampling distribution of mean attritions. One sampling distribution used a sample size of 5 for each replicate, one used 50, and one used 500. # function sampling_distribution_fun &lt;- function(sample_size) { replicate( n = 1000, expr = attrition_pop %&gt;% slice_sample(n = sample_size) %&gt;% summarise(mean_attrition = mean(Attrition == &quot;Yes&quot;)) %&gt;% pull(mean_attrition) ) %&gt;% tibble(mean_attrition = .) } # sample size = 5 sampling_distribution_5 &lt;- sampling_distribution_fun(5) sampling_distribution_5 ## # A tibble: 1,000 × 1 ## mean_attrition ## &lt;dbl&gt; ## 1 0 ## 2 0 ## 3 0 ## 4 0 ## 5 0 ## 6 0.4 ## 7 0.2 ## 8 0.2 ## 9 0 ## 10 0.4 ## # ℹ 990 more rows # sample size = 50 sampling_distribution_50 &lt;- sampling_distribution_fun(50) sampling_distribution_50 ## # A tibble: 1,000 × 1 ## mean_attrition ## &lt;dbl&gt; ## 1 0.24 ## 2 0.1 ## 3 0.1 ## 4 0.26 ## 5 0.1 ## 6 0.16 ## 7 0.2 ## 8 0.2 ## 9 0.16 ## 10 0.08 ## # ℹ 990 more rows sampling_distribution_500 &lt;- sampling_distribution_fun(500) sampling_distribution_500 ## # A tibble: 1,000 × 1 ## mean_attrition ## &lt;dbl&gt; ## 1 0.152 ## 2 0.2 ## 3 0.15 ## 4 0.15 ## 5 0.172 ## 6 0.182 ## 7 0.152 ## 8 0.152 ## 9 0.176 ## 10 0.166 ## # ℹ 990 more rows Calculate a mean of sample means. # Calculate the mean across replicates of the mean attritions in sampling_distribution_5 mean_of_means_5 &lt;- sampling_distribution_5 %&gt;% summarise(mean_mean_attrition_5 = mean(mean_attrition)) # Do the same for sampling_distribution_50 mean_of_means_50 &lt;- sampling_distribution_50 %&gt;% summarise(mean_mean_attrition_50 = mean(mean_attrition)) # ... and for sampling_distribution_500 mean_of_means_500 &lt;- sampling_distribution_500 %&gt;% summarise(mean_mean_attrition_500 = mean(mean_attrition)) # See the results cbind(mean_of_means_5, mean_of_means_50, mean_of_means_500) ## mean_mean_attrition_5 mean_mean_attrition_50 mean_mean_attrition_500 ## 1 0.16 0.165 0.162 Regardless of sample size, the mean of the sampling distribution is a close approximation to the population mean. # For comparison: the mean attrition in the population attrition_pop %&gt;% summarize(mean_attrition = mean(Attrition == &quot;Yes&quot;)) ## mean_attrition ## 1 0.161 16.3.4.2 Population and sampling distribution variation Similarly, as a result of the central limit theorem, the standard deviation of the sampling distribution has an interesting relationship with the population parameter’s standard deviation and the sample size. Calculate standard deviation of sample means. # Calculate the standard deviation across replicates of the mean attritions in sampling_distribution_5 sd_of_means_5 &lt;- sampling_distribution_5 %&gt;% summarise(sd_mean_attrition_5 = sd(mean_attrition)) # Do the same for sampling_distribution_50 sd_of_means_50 &lt;- sampling_distribution_50 %&gt;% summarise(sd_mean_attrition_50 = sd(mean_attrition)) # ... and for sampling_distribution_500 sd_of_means_500 &lt;- sampling_distribution_500 %&gt;% summarise(sd_mean_attrition_500 = sd(mean_attrition)) # See the results cbind(sd_of_means_5, sd_of_means_50, sd_of_means_500) ## sd_mean_attrition_5 sd_mean_attrition_50 sd_mean_attrition_500 ## 1 0.166 0.0512 0.0136 The standard deviation of the sampling distribution is approximately equal to the population standard deviation divided by the square root of the sample size. # For comparison: population standard deviation sd_attrition_pop &lt;- attrition_pop %&gt;% summarize(sd_attrition = sd(Attrition == &quot;Yes&quot;)) %&gt;% pull(sd_attrition) # The sample sizes of each sampling distribution sample_sizes &lt;- c(5, 50, 500) # create compare df Std_sample_mean &lt;- c(pull(sd_of_means_5), pull(sd_of_means_50), pull(sd_of_means_500)) data.frame(sample_sizes, Std_sample_mean) %&gt;% mutate(Pop_mean_over_sqrt_sample_size = sd_attrition_pop / sqrt(sample_sizes)) ## sample_sizes Std_sample_mean Pop_mean_over_sqrt_sample_size ## 1 5 0.1665 0.1645 ## 2 50 0.0512 0.0520 ## 3 500 0.0136 0.0165 16.4 Bootstrap Distributions 16.4.1 Introduction to bootstrapping Bootstrapping treats your dataset as a sample and uses it to build up a theoretical population. Bootstrapping process Make a resample of the same size as the original sample. Calculate the statistic of interest for this bootstrap sample. Repeat steps 1 and 2 many times. The resulting statistics are called bootstrap statistics and when viewed to see their variability a bootstrap distribution. With or without replacement The key to deciding whether to sample without or with replacement is whether or not your dataset is best thought of as being the whole population or not. If dataset = whole population ⟶ with replacement If dataset ≠ whole population ⟶ without replacement 16.4.1.1 Generating a bootstrap distribution To make a bootstrap distribution, you start with a sample and sample that with replacement. After that, the steps are the same: calculate the summary statistic that you are interested in on that sample/resample, then replicate the process many times. Here, spotify_sample is a subset of the spotify_population dataset. spotify_sample &lt;- spotify_population %&gt;% select(artists, name, danceability) %&gt;% slice_sample(n = 1000) glimpse(spotify_sample) ## Rows: 1,000 ## Columns: 3 ## $ artists &lt;chr&gt; &quot;[&#39;Kirk Franklin&#39;]&quot;, &quot;[&#39;Noah Kahan&#39;]&quot;, &quot;[&#39;Kenny Chesney&#39;]… ## $ name &lt;chr&gt; &quot;He Reigns / Awesome God&quot;, &quot;False Confidence&quot;, &quot;Key’s in … ## $ danceability &lt;dbl&gt; 0.772, 0.414, 0.834, 0.758, 0.514, 0.269, 0.714, 0.409, 0… Step 1: Generate a single bootstrap resample from spotify_sample. # Generate 1 bootstrap resample spotify_1_resample &lt;- spotify_sample %&gt;% slice_sample(prop = 1, replace = TRUE) # See the result glimpse(spotify_1_resample) ## Rows: 1,000 ## Columns: 3 ## $ artists &lt;chr&gt; &quot;[&#39;Sylvan Esso&#39;]&quot;, &quot;[&#39;Aminé&#39;]&quot;, &quot;[&#39;Zach Williams&#39;, &#39;Dolly… ## $ name &lt;chr&gt; &quot;Coffee&quot;, &quot;Spice Girl&quot;, &quot;There Was Jesus&quot;, &quot;Corrido De El… ## $ danceability &lt;dbl&gt; 0.457, 0.693, 0.496, 0.524, 0.645, 0.312, 0.593, 0.426, 0… Step 2: Summarize to calculate the mean danceability of spotify_1_resample. # Calculate mean danceability of resample mean_danceability_1 &lt;- spotify_1_resample %&gt;% summarise(mean_danceability = mean(danceability)) %&gt;% pull(mean_danceability) # See the result mean_danceability_1 ## [1] 0.588 Replicate the expression provided 1000 times. # Step 3 # Replicate this 1000 times mean_danceability_1000 &lt;- replicate( n = 1000, expr = { # Step 1 spotify_1_resample &lt;- spotify_sample %&gt;% slice_sample(prop = 1, replace = TRUE) # Step 2 spotify_1_resample %&gt;% summarize(mean_danceability = mean(danceability)) %&gt;% pull(mean_danceability) } ) # See the result str(mean_danceability_1000) ## num [1:1000] 0.586 0.589 0.58 0.588 0.59 ... Draw a histogram. # Store the resamples in a tibble bootstrap_distn &lt;- tibble( resample_mean = mean_danceability_1000 ) # Draw a histogram of the resample means with binwidth 0.002 ggplot(bootstrap_distn, aes(resample_mean)) + geom_histogram(binwidth = 0.002) 16.4.2 Comparing sampling &amp; bootstrap distributions Sample, bootstrap distribution, population means The bootstrap distribution mean is usually almost identical to the sample mean. It may not be a good estimate of the population mean. Bootstrapping cannot correct biases due to differences between your sample and the population. Sample, bootstrap distribution, population stds Standard error is the standard deviation of the statistic of interest. Standard error * square root of sample size, estimates the population standard deviation. Estimated standard error is the standard deviation of the bootstrap distribution for a sample statistic. The bootstrap distribution standard error times the square root of the sample size estimates the standard deviation in the population. 16.4.2.1 Sampling distribution vs. bootstrap distribution Here, the statistic you are interested in is the mean popularity score of the songs. spotify_sample &lt;- spotify_population %&gt;% slice_sample(n = 500) glimpse(spotify_sample) ## Rows: 500 ## Columns: 20 ## $ acousticness &lt;dbl&gt; 0.025000, 0.086800, 0.008350, 0.004530, 0.263000, 0.3… ## $ artists &lt;chr&gt; &quot;[&#39;MadeinTYO&#39;, &#39;Travis Scott&#39;]&quot;, &quot;[&#39;Randy Rogers Band… ## $ danceability &lt;dbl&gt; 0.782, 0.522, 0.872, 0.495, 0.697, 0.553, 0.838, 0.83… ## $ duration_ms &lt;dbl&gt; 181104, 290853, 280627, 239894, 296107, 210200, 29314… ## $ duration_minutes &lt;dbl&gt; 3.02, 4.85, 4.68, 4.00, 4.94, 3.50, 4.89, 3.71, 3.90,… ## $ energy &lt;dbl&gt; 0.662, 0.824, 0.651, 0.894, 0.445, 0.506, 0.635, 0.85… ## $ explicit &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1,… ## $ id &lt;chr&gt; &quot;79wTHFxVJXRrR5afZeBd16&quot;, &quot;50mclsKBXwBuOmMGZMtgc7&quot;, &quot;… ## $ instrumentalness &lt;dbl&gt; 0.00000000, 0.00005150, 0.00000000, 0.00059600, 0.002… ## $ key &lt;dbl&gt; 1, 7, 7, 2, 5, 0, 5, 1, 2, 2, 7, 7, 8, 10, 1, 10, 6, … ## $ liveness &lt;dbl&gt; 0.3000, 0.2920, 0.0694, 0.1030, 0.1330, 0.7630, 0.060… ## $ loudness &lt;dbl&gt; -7.24, -6.03, -5.68, -4.81, -7.08, -5.97, -6.55, -3.6… ## $ mode &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0,… ## $ name &lt;chr&gt; &quot;Uber Everywhere (feat. Travis Scott)&quot;, &quot;Tonight&#39;s No… ## $ popularity &lt;dbl&gt; 59, 41, 36, 66, 55, 60, 60, 41, 61, 49, 44, 41, 46, 5… ## $ release_date &lt;chr&gt; &quot;2016-08-19&quot;, &quot;2004-08-24&quot;, &quot;2004-01-01&quot;, &quot;2013-01-01… ## $ speechiness &lt;dbl&gt; 0.1050, 0.0340, 0.1000, 0.0441, 0.0638, 0.0411, 0.032… ## $ tempo &lt;dbl&gt; 140.1, 121.7, 162.0, 126.0, 119.9, 130.0, 130.1, 91.5… ## $ valence &lt;dbl&gt; 0.102, 0.562, 0.732, 0.213, 0.396, 0.306, 0.948, 0.66… ## $ year &lt;dbl&gt; 2016, 2004, 2004, 2013, 2001, 2018, 2005, 2003, 2013,… Generate a sampling distribution of 2000 replicates. Sample 500 rows of the population without replacement. # Generate a sampling distribution mean_popularity_2000_samp &lt;- replicate( # Use 2000 replicates n = 2000, expr = { # Start with the population spotify_population %&gt;% # Sample 500 rows without replacement slice_sample(n = 500, replace = FALSE) %&gt;% # Calculate the mean popularity as mean_popularity summarise(mean_popularity = mean(popularity)) %&gt;% # Pull out the mean popularity pull(mean_popularity) } ) # See the result str(mean_popularity_2000_samp) ## num [1:2000] 54.6 53.6 55 55.5 54 ... Generate a bootstrap distribution of 2000 replicates. Sample 500 rows of the sample with replacement. # Generate a bootstrap distribution mean_popularity_2000_boot &lt;- replicate( # Use 2000 replicates n = 2000, expr = { # Start with the sample spotify_sample %&gt;% # Sample same number of rows with replacement slice_sample(prop = 1, replace = TRUE) %&gt;% # Calculate the mean popularity summarise(mean_popularity = mean(popularity)) %&gt;% # Pull out the mean popularity pull(mean_popularity) } ) # See the result str(mean_popularity_2000_boot) ## num [1:2000] 54.9 55.2 55.2 55 55.3 ... 16.4.2.2 Compare sampling and bootstrap means # create tibble to compare sampling_distribution &lt;- tibble(sample_mean = mean_popularity_2000_samp) bootstrap_distribution &lt;- tibble(resample_mean = mean_popularity_2000_boot) sampling_distribution ## # A tibble: 2,000 × 1 ## sample_mean ## &lt;dbl&gt; ## 1 54.6 ## 2 53.6 ## 3 55.0 ## 4 55.5 ## 5 54.0 ## 6 55.4 ## 7 54.9 ## 8 55.0 ## 9 54.4 ## 10 54.9 ## # ℹ 1,990 more rows bootstrap_distribution ## # A tibble: 2,000 × 1 ## resample_mean ## &lt;dbl&gt; ## 1 54.9 ## 2 55.2 ## 3 55.2 ## 4 55.0 ## 5 55.3 ## 6 55.5 ## 7 54.6 ## 8 54.3 ## 9 55.1 ## 10 55.7 ## # ℹ 1,990 more rows Calculate the mean popularity with summarize() in 4 ways. Population: from spotify_population, take the mean of popularity. Sample: from spotify_sample, take the mean of popularity. Sampling distribution: from sampling_distribution, take the mean of sample_mean. Bootstrap distribution: from bootstrap_distribution, take the mean of resample_mean. # Calculate the true population mean popularity pop_mean &lt;- spotify_population %&gt;% summarise(mean = mean(popularity)) # Calculate the original sample mean popularity samp_mean &lt;- spotify_sample %&gt;% summarise(mean = mean(popularity)) # Calculate the sampling dist&#39;n estimate of mean popularity samp_distn_mean &lt;- sampling_distribution %&gt;% summarise(mean = mean(sample_mean)) # Calculate the bootstrap dist&#39;n estimate of mean popularity boot_distn_mean &lt;- bootstrap_distribution %&gt;% summarise(mean = mean(resample_mean)) # See the results c(pop = pop_mean, samp = samp_mean, sam_distn = samp_distn_mean, boot_distn = boot_distn_mean) ## $pop.mean ## [1] 54.8 ## ## $samp.mean ## [1] 55 ## ## $sam_distn.mean ## [1] 54.8 ## ## $boot_distn.mean ## [1] 55 The sampling distribution mean (54.82882) is the best estimate of the true population mean (54.83714); The bootstrap distribution mean (54.71672) is closest to the original sample mean (54.706). The sampling distribution mean can be used to estimate the population mean, but that is not the case with the boostrap distribution. 16.4.2.3 Compare sampling and bootstrap std # Calculate the true popluation std dev popularity pop_sd &lt;- spotify_population %&gt;% summarise(sd = sd(popularity)) # Calculate the true sample std dev popularity samp_sd &lt;- spotify_sample %&gt;% summarise(sd = sd(popularity)) # Calculate the sampling dist&#39;n estimate of std dev popularity samp_distn_sd &lt;- sampling_distribution %&gt;% summarise(sd = sd(sample_mean) * sqrt(500)) # Calculate the bootstrap dist&#39;n estimate of std dev popularity boot_distn_sd &lt;- bootstrap_distribution %&gt;% summarise(sd = sd(resample_mean) * sqrt(500)) # See the results c(pop = pop_sd, samp = samp_sd, sam_distn = samp_distn_sd, boot_distn = boot_distn_sd) ## $pop.sd ## [1] 10.9 ## ## $samp.sd ## [1] 10.5 ## ## $sam_distn.sd ## [1] 10.6 ## ## $boot_distn.sd ## [1] 10.4 When you don’t know have all the values from the true population, you can use bootstrapping to get a good estimate of the population standard deviation. (Although it isn’t the closest of the values given) 16.4.3 Confidence intervals Confidence intervals account for uncertainty in our estimate of a population parameter by providing a range of possible values. We are confident that the true value lies somewhere in the interval specified by that range. 16.4.3.1 Calculating confidence intervals Mean plus or minus one standard deviation Quantile method for confidence intervals Standard error method for confidence interval Inverse cumulative distribution function: qnorm() Generate a 95% confidence interval using the quantile method. Summarize to get the 0.025 quantile as lower, and the 0.975 quantile as upper. # Generate a 95% confidence interval using the quantile method conf_int_quantile &lt;- bootstrap_distribution %&gt;% summarise( lower = quantile(resample_mean, 0.025), upper = quantile(resample_mean, 0.975) ) # See the result conf_int_quantile ## # A tibble: 1 × 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 54.1 55.9 Generate a 95% confidence interval using the standard error method. Calculate point_estimate as the mean of resample_mean, and standard_error as the standard deviation of resample_mean. Calculate lower as the 0.025 quantile of an inv. CDF from a normal distribution with mean point_estimate and standard deviation standard_error. Calculate upper as the 0.975 quantile of that same inv. CDF. # Generate a 95% confidence interval using the std error method conf_int_std_error &lt;- bootstrap_distribution %&gt;% summarise( point_estimate = mean(resample_mean), standard_error = sd(resample_mean), lower = qnorm(p = 0.025, mean = point_estimate, sd = standard_error), upper = qnorm(p = 0.975, mean = point_estimate, sd = standard_error) ) # See the result conf_int_std_error ## # A tibble: 1 × 4 ## point_estimate standard_error lower upper ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 55.0 0.465 54.1 55.9 The standard error method for calculating the confidence interval assumes that the bootstrap distribution is normal. This assumption should hold if the sample size and number of replicates are sufficiently large. "],["hypothesis-testing.html", "Chapter 17 Hypothesis Testing 17.1 Introduction to Hypothesis Testing 17.2 Two-Sample &amp; ANOVA Tests 17.3 Proportion Tests 17.4 Non-Parametric Tests", " Chapter 17 Hypothesis Testing 17.1 Introduction to Hypothesis Testing 17.1.1 Hypothesis tests &amp; z-scores A/B testing Involves splitting participants into control and treatment groups. Let’s you compare scenarios to see which best achieves some goal. z-scores \\(standardized\\ value = value − mean / standard\\ deviation\\) \\( = sample\\ stat − hypoth. param. value / standard\\ error\\) Standard normal () distribution: the normal distribution with mean zero, standard deviation 1. 17.1.1.1 Calculating the sample mean The late_shipments dataset contains supply chain data on the delivery of medical supplies. Each row represents one delivery of a part. The late columns denotes whether or not the part was delivered late. \"Yes\" means that the part was delivered late, \"No\" means the part was delivered on time. Let’s calculating a point estimate (sample statistic), namely the proportion of late shipments. library(tidyverse) library(fst) late_shipments &lt;- read_fst(&quot;data/late_shipments.fst&quot;) glimpse(late_shipments) ## Rows: 1,000 ## Columns: 26 ## $ id &lt;dbl&gt; 73003, 41222, 52354, 28471, 16901, 27238, 910… ## $ country &lt;chr&gt; &quot;Vietnam&quot;, &quot;Kenya&quot;, &quot;Zambia&quot;, &quot;Nigeria&quot;, &quot;Vie… ## $ managed_by &lt;chr&gt; &quot;PMO - US&quot;, &quot;PMO - US&quot;, &quot;PMO - US&quot;, &quot;PMO - US… ## $ fulfill_via &lt;chr&gt; &quot;Direct Drop&quot;, &quot;Direct Drop&quot;, &quot;Direct Drop&quot;, … ## $ vendor_inco_term &lt;chr&gt; &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EX… ## $ shipment_mode &lt;chr&gt; &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Ai… ## $ late_delivery &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ late &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N… ## $ product_group &lt;chr&gt; &quot;ARV&quot;, &quot;HRDT&quot;, &quot;HRDT&quot;, &quot;HRDT&quot;, &quot;ARV&quot;, &quot;HRDT&quot;,… ## $ sub_classification &lt;chr&gt; &quot;Adult&quot;, &quot;HIV test&quot;, &quot;HIV test&quot;, &quot;HIV test&quot;, … ## $ vendor &lt;chr&gt; &quot;HETERO LABS LIMITED&quot;, &quot;Orgenics, Ltd&quot;, &quot;Orge… ## $ item_description &lt;chr&gt; &quot;Efavirenz/Lamivudine/Tenofovir Disoproxil Fu… ## $ molecule_test_type &lt;chr&gt; &quot;Efavirenz/Lamivudine/Tenofovir Disoproxil Fu… ## $ brand &lt;chr&gt; &quot;Generic&quot;, &quot;Determine&quot;, &quot;Determine&quot;, &quot;Determi… ## $ dosage &lt;chr&gt; &quot;600/300/300mg&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;300mg&quot;… ## $ dosage_form &lt;chr&gt; &quot;Tablet - FDC&quot;, &quot;Test kit&quot;, &quot;Test kit&quot;, &quot;Test… ## $ unit_of_measure_per_pack &lt;dbl&gt; 30, 100, 100, 100, 60, 20, 100, 30, 30, 25, 1… ## $ line_item_quantity &lt;dbl&gt; 19200, 6100, 1364, 2835, 112, 53, 1500, 6180,… ## $ line_item_value &lt;dbl&gt; 201600, 542900, 109120, 252315, 1618, 1696, 1… ## $ pack_price &lt;dbl&gt; 10.50, 89.00, 80.00, 89.00, 14.45, 32.00, 80.… ## $ unit_price &lt;dbl&gt; 0.35, 0.89, 0.80, 0.89, 0.24, 1.60, 0.80, 0.5… ## $ manufacturing_site &lt;chr&gt; &quot;Hetero Unit III Hyderabad IN&quot;, &quot;Alere Medica… ## $ first_line_designation &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Ye… ## $ weight_kilograms &lt;dbl&gt; 2719, 3497, 553, 1352, 1701, 18, 445, 2130, 2… ## $ freight_cost_usd &lt;dbl&gt; 4085, 40917, 7845, 31284, 4289, 569, 7120, 17… ## $ line_item_insurance_usd &lt;dbl&gt; 207.24, 895.78, 112.18, 353.75, 2.67, 2.80, 2… # Calculate the proportion of late shipments late_prop_samp &lt;- late_shipments %&gt;% summarise(sample_mean = mean(late == &quot;Yes&quot;)) %&gt;% pull(sample_mean) # See the results late_prop_samp ## [1] 0.067 The proportion of late shipments is 0.067. 17.1.1.2 Calculating z-score Since variables have arbitrary ranges and units, we need to standardize them. One standardized value of interest in a hypothesis test is called a z-score. To calculate it, we need three numbers: the sample statistic (point estimate) = late_prop_samp the hypothesized statistic the standard error of the statistic (which we estimate from the bootstrap distribution). # A bootstrap distribution of the proportion of late shipments late_shipments_boot_distn &lt;- replicate( n = 5000, expr = { late_shipments %&gt;% slice_sample(prop = 1, replace = TRUE) %&gt;% summarise(late_prop = mean(late == &quot;Yes&quot;)) %&gt;% pull(late_prop) } ) %&gt;% tibble(late_prop = .) late_shipments_boot_distn ## # A tibble: 5,000 × 1 ## late_prop ## &lt;dbl&gt; ## 1 0.072 ## 2 0.072 ## 3 0.062 ## 4 0.075 ## 5 0.046 ## 6 0.068 ## 7 0.062 ## 8 0.058 ## 9 0.071 ## 10 0.061 ## # ℹ 4,990 more rows # Hypothesize that the proportion is 6% late_prop_hyp &lt;- 0.06 # Calculate the standard error = the standard deviation of the bootstrap distribution std_error &lt;- late_shipments_boot_distn %&gt;% summarise(sd(late_prop)) %&gt;% pull() # Find z-score of late_prop_samp z_score &lt;- (late_prop_samp - late_prop_hyp) / std_error # See the results z_score ## [1] 0.88 The z-score is a standardized measure of the difference between the sample statistic and the hypothesized statistic. 17.1.2 p-values Hypothesis testing A hypothesis is a statement about an unknown population parameter. A hypothesis test is a test of two competing hypotheses. null hypothesis (\\(H_0\\)) is the existing “champion” idea. alternative hypothesis (\\(H_A\\)) is the new “challenger” idea of the researcher. Initially the null hypothesis, \\(H_0\\) , is assumed to be true. The test ends in either “reject \\(H_0\\)” or “fail to reject \\(H_0\\)”. If the evidence from the sample is “significant” that \\(H_A\\) is true, choose that hypothesis, else choose \\(H_0\\). One-tailed and two-tailed tests Test Tails pnorm() = normal CDF alternative different from null two-tailed alternative greater than null right-tailed lower.tail = FALSE alternative less than null left-tailed lower.tail = TRUE p-values The “p” in p-value stands for probability. The smaller the p-value, the stronger the evidence against \\(H_0\\). 17.1.2.1 Calculating p-values The p-value is calculated by transforming the z-score with the standard normal cumulative distribution function. # standard normal cumulative distribution args(pnorm) ## function (q, mean = 0, sd = 1, lower.tail = TRUE, log.p = FALSE) ## NULL Let’s return to the late shipments dataset and the proportion of late shipments. The null hypothesis (\\(H_0\\)): the proportion of late shipments is six percent. The alternative hypothesis (\\(H_A\\)): the proportion of late shipments is greater than six percent. # Calculate the z-score of late_prop_samp z_score &lt;- (late_prop_samp - late_prop_hyp) / std_error # Calculate the p-value, assuming a right-tailed test p_value &lt;- pnorm(z_score, lower.tail = FALSE) # See the result p_value ## [1] 0.189 17.1.3 Statistical significance Significance level The significance level of a hypothesis test (α) is the threshold point for “beyond a reasonable doubt”. Common values of α are 0.1 , 0.05 , and 0.01 . If \\(p ≤ α\\), reject \\(H_0\\), else fail to reject \\(H_0\\). Confidence intervals For a significance level of 0.05, it’s common to choose a confidence interval of 1 - 0.05 = 0.95. Types of errors actual \\(H_0\\) actual \\(H_A\\) chosen \\(H_0\\) correct 1 -  false negative Type II errors =  chosen \\(H_A\\) false positive Type I errors =  correct 1 -  17.1.3.1 Calculating confidence intervals For example, the hypothesized proportion of late shipments was 6%. Even if evidence suggests the null hypothesis that the proportion of late shipments is equal to this, for any new sample of shipments, the proportion is likely to be a little different. Consequently, it’s a good idea to state a confidence interval. That is, you say “we are 95% ‘confident’ the proportion of late shipments is between A and B” (for some value of A and B). # Calculate 95% confidence interval using quantile method conf_int_quantile &lt;- late_shipments_boot_distn %&gt;% summarise( lower = quantile(late_prop, 0.025), upper = quantile(late_prop, 0.975) ) # See the result conf_int_quantile ## # A tibble: 1 × 2 ## lower upper ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.052 0.083 If the hypothesized population parameter is within the confidence interval, you should fail to reject the null hypothesis. Due to the large p-value, the results are similar. 17.1.3.2 Type I and type II errors 17.2 Two-Sample &amp; ANOVA Tests 17.2.1 Performing t-tests 17.2.1.1 Two sample mean test statistic Test statistics Sample mean estimates the population mean. \\(\\overline{x}\\) denotes a sample mean. While trying to determine why some shipments are late, you may wonder if the weight of the shipments that were late is different from the weight of the shipments that were on time. The late_shipments dataset has been split into a “yes” group, where late == \"Yes\" and a “no” group where late == \"No\". The weight of the shipment is given in the weight_kilograms variable. For convenience, the sample means for the two groups are available as xbar_no and xbar_yes. The sample standard deviations are s_no and s_yes. The sample sizes are n_no and n_yes. xbar_no &lt;- 2082.171 xbar_yes &lt;- 2377.821 s_no &lt;- 6094.609 s_yes &lt;- 2242.502 n_no &lt;- 933 n_yes &lt;- 67 # Calculate the numerator of the test statistic numerator &lt;- xbar_no - xbar_yes # Calculate the denominator of the test statistic denominator &lt;- sqrt((s_yes^2 / n_yes) + (s_no^2 / n_no)) # Calculate the test statistic t_stat &lt;- numerator / denominator # See the result t_stat ## [1] -0.872 When testing for differences between means, the test statistic is called ‘t’ rather than ‘z’. 17.2.2 Calculating p-values from t-statistics t-distributions The test statistic, t, follows a t-distribution. t-distributions have a parameter named degrees of freedom (df). t-distributions look like normal distributions, with fatter tails. Degrees of freedom As you increase the degrees of freedom, the t-distribution gets closer to the normal distribution. A normal distribution is a t-distribution with infinite degrees of freedom. Degrees of freedom are the maximum number of logically independent values in the data sample. \\(df = (n_{group1} - 1) + (n_{group2} − 1)\\) 17.2.2.1 From t to p When the standard error is estimated from the sample standard deviation and sample size, the test statistic is transformed into a p-value using the t-distribution. Previously, you calculated the test statistic for the two-sample problem of whether the mean weight of shipments is lower for shipments that weren’t late (late == \"No\") compared to shipments that were late (late == \"Yes\"). In order to make decisions about it, you need to transform the test statistic with a cumulative distribution function to get a p-value. t-distribution CDF: pt(t_stat, df, lower.tail) \\(H_0\\): The mean weight of shipments that weren’t late is the same as the mean weight of shipments that were late. \\(H_A\\): The mean weight of shipments that weren’t late is less than the mean weight of shipments that were late. # Calculate the degrees of freedom degrees_of_freedom &lt;- n_no - 1 + n_yes - 1 # Calculate the p-value from the test stat p_value &lt;- pt(t_stat, degrees_of_freedom, lower.tail = TRUE) # See the result p_value ## [1] 0.192 Fall to reject \\(H_0\\) 17.2.3 Paired t-tests If you have repeated observations of something, then those observations form pairs. 17.2.3.1 Visualizing the difference Here, you’ll look at the proportion of county-level votes for the Democratic candidate in 2012 and 2016, dem_votes_potus_12_16. Since the counties are the same in both years, these samples are paired. dem_votes_potus_12_16 &lt;- read_fst(&quot;data/dem_county_pres_joined.fst&quot;) %&gt;% select(state, county, dem_percent_12, dem_percent_16) glimpse(dem_votes_potus_12_16) ## Rows: 500 ## Columns: 4 ## $ state &lt;chr&gt; &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, &quot;Alabama&quot;, … ## $ county &lt;chr&gt; &quot;Bullock&quot;, &quot;Chilton&quot;, &quot;Clay&quot;, &quot;Cullman&quot;, &quot;Escambia&quot;, &quot;F… ## $ dem_percent_12 &lt;dbl&gt; 76.3, 19.5, 26.7, 14.7, 36.9, 22.9, 29.2, 62.6, 23.0, 3… ## $ dem_percent_16 &lt;dbl&gt; 74.9, 15.8, 18.7, 10.0, 31.0, 16.5, 18.2, 59.4, 14.9, 2… Before you start running hypothesis tests, it’s a great idea to perform some exploratory data analysis. # Calculate the differences from 2012 to 2016 sample_dem_data &lt;- dem_votes_potus_12_16 %&gt;% mutate(diff = dem_percent_12 - dem_percent_16) # See the result sample_dem_data ## state county dem_percent_12 dem_percent_16 diff ## 1 Alabama Bullock 76.31 74.95 1.3590 ## 2 Alabama Chilton 19.45 15.85 3.6063 ## 3 Alabama Clay 26.67 18.67 7.9992 ## 4 Alabama Cullman 14.66 10.03 4.6335 ## 5 Alabama Escambia 36.92 31.02 5.8952 ## 6 Alabama Fayette 22.87 16.51 6.3558 ## 7 Alabama Franklin 29.20 18.25 10.9541 ## 8 Alabama Hale 62.61 59.39 3.2150 ## 9 Alabama Lamar 22.97 14.87 8.0973 ## 10 Alabama Lauderdale 33.88 25.18 8.7041 ## 11 Alabama Monroe 45.89 42.18 3.7139 ## 12 Alabama Pike 42.78 38.40 4.3841 ## 13 Alabama Shelby 21.68 22.69 -1.0173 ## 14 Alabama Walker 22.99 15.26 7.7315 ## 15 Arizona Graham 30.44 16.26 14.1866 ## 16 Arkansas Baxter 26.74 21.09 5.6521 ## 17 Arkansas Benton 28.56 28.92 -0.3634 ## 18 Arkansas Bradley 39.68 36.05 3.6243 ## 19 Arkansas Craighead 33.21 29.62 3.5846 ## 20 Arkansas Dallas 43.35 42.04 1.3102 ## 21 Arkansas Desha 55.27 52.34 2.9342 ## 22 Arkansas Faulkner 32.85 30.78 2.0730 ## 23 Arkansas Howard 32.97 28.90 4.0627 ## 24 Arkansas Logan 27.40 21.65 5.7529 ## 25 Arkansas Marion 28.89 20.18 8.7122 ## 26 Arkansas Newton 27.12 18.63 8.4986 ## 27 California Alameda 78.85 78.69 0.1634 ## 28 California Calaveras 39.80 34.34 5.4601 ## 29 California Humboldt 59.96 56.04 3.9190 ## 30 California Mariposa 38.87 35.17 3.6971 ## 31 California Merced 53.18 52.72 0.4692 ## 32 California Modoc 27.87 23.15 4.7205 ## 33 California Mono 52.75 52.56 0.1917 ## 34 California Napa 62.97 63.87 -0.9060 ## 35 California Placer 39.00 40.20 -1.2028 ## 36 California Riverside 49.71 49.73 -0.0194 ## 37 California Santa Barbara 57.63 60.61 -2.9708 ## 38 California Santa Clara 70.10 72.71 -2.6107 ## 39 California Stanislaus 50.03 47.43 2.5965 ## 40 California Tulare 41.30 42.36 -1.0626 ## 41 Colorado Alamosa 56.75 45.96 10.7959 ## 42 Colorado Arapahoe 53.90 52.76 1.1448 ## 43 Colorado Cheyenne 15.74 11.98 3.7583 ## 44 Colorado Elbert 25.41 19.61 5.7981 ## 45 Colorado Las Animas 50.20 39.01 11.1933 ## 46 Colorado Moffat 21.56 13.39 8.1683 ## 47 Colorado Morgan 36.30 26.35 9.9534 ## 48 Colorado Park 41.23 32.84 8.3937 ## 49 Colorado Phillips 25.96 18.70 7.2639 ## 50 Colorado Rio Blanco 16.86 12.64 4.2219 ## 51 Connecticut Hartford 62.37 59.09 3.2829 ## 52 Connecticut New Haven 60.65 54.25 6.4032 ## 53 Florida Broward 67.20 66.51 0.6908 ## 54 Florida Calhoun 27.05 20.41 6.6403 ## 55 Florida Desoto 42.31 34.95 7.3602 ## 56 Florida Flagler 45.87 38.30 7.5641 ## 57 Florida Franklin 33.70 28.99 4.7106 ## 58 Florida Hardee 34.09 28.34 5.7455 ## 59 Florida Highlands 38.05 32.69 5.3505 ## 60 Florida Lake 40.98 36.86 4.1144 ## 61 Florida Madison 47.92 41.46 6.4594 ## 62 Florida Orange 58.68 60.39 -1.7085 ## 63 Florida St. Johns 30.65 31.57 -0.9165 ## 64 Florida Walton 23.41 20.44 2.9702 ## 65 Georgia Bacon 20.12 15.12 4.9977 ## 66 Georgia Clinch 34.47 27.94 6.5230 ## 67 Georgia Columbia 27.92 29.17 -1.2528 ## 68 Georgia Coweta 27.30 26.86 0.4454 ## 69 Georgia Dawson 12.11 12.31 -0.1933 ## 70 Georgia Dougherty 69.35 68.39 0.9584 ## 71 Georgia Fannin 20.25 16.28 3.9655 ## 72 Georgia Franklin 19.31 14.62 4.6919 ## 73 Georgia Houston 39.10 37.54 1.5647 ## 74 Georgia Jenkins 43.89 36.75 7.1464 ## 75 Georgia Lumpkin 18.84 17.87 0.9774 ## 76 Georgia McDuffie 42.16 39.91 2.2486 ## 77 Georgia Mitchell 49.32 44.33 4.9898 ## 78 Georgia Paulding 27.57 27.78 -0.2076 ## 79 Georgia Pierce 16.42 12.35 4.0671 ## 80 Georgia Talbot 64.88 61.68 3.2052 ## 81 Georgia Telfair 41.63 34.59 7.0392 ## 82 Georgia Toombs 29.39 25.59 3.8024 ## 83 Georgia Twiggs 54.02 48.56 5.4631 ## 84 Georgia Ware 32.67 28.23 4.4323 ## 85 Georgia Webster 49.03 42.38 6.6477 ## 86 Georgia Wheeler 35.74 30.73 5.0081 ## 87 Hawaii Honolulu 68.86 61.48 7.3869 ## 88 Hawaii Maui 74.10 64.45 9.6475 ## 89 Idaho Bingham 21.71 17.59 4.1242 ## 90 Idaho Bonneville 22.92 20.19 2.7227 ## 91 Idaho Camas 27.51 18.64 8.8646 ## 92 Idaho Canyon 30.06 23.20 6.8633 ## 93 Idaho Custer 22.53 17.69 4.8455 ## 94 Idaho Franklin 5.80 7.00 -1.1977 ## 95 Idaho Fremont 13.96 11.39 2.5696 ## 96 Idaho Gooding 25.15 17.92 7.2309 ## 97 Idaho Lemhi 23.36 17.90 5.4622 ## 98 Illinois DeKalb 51.61 46.94 4.6682 ## 99 Illinois Franklin 40.49 25.26 15.2303 ## 100 Illinois Grundy 44.49 34.71 9.7803 ## 101 Illinois Kankakee 47.45 40.10 7.3486 ## 102 Illinois McHenry 44.68 42.89 1.7821 ## 103 Illinois Marshall 41.97 29.90 12.0694 ## 104 Illinois Menard 34.14 27.89 6.2513 ## 105 Illinois Moultrie 35.40 23.57 11.8255 ## 106 Illinois Ogle 40.72 33.27 7.4415 ## 107 Illinois Randolph 40.02 24.23 15.7867 ## 108 Illinois Rock Island 60.23 50.47 9.7564 ## 109 Illinois Wayne 19.71 12.69 7.0274 ## 110 Illinois Whiteside 57.82 43.14 14.6836 ## 111 Illinois Woodford 29.52 25.63 3.8878 ## 112 Indiana Clark 44.13 36.77 7.3607 ## 113 Indiana Hamilton 32.00 37.12 -5.1222 ## 114 Indiana Huntington 29.15 21.85 7.2994 ## 115 Indiana LaGrange 31.11 21.74 9.3641 ## 116 Indiana Newton 39.12 24.23 14.8950 ## 117 Indiana Ohio 35.26 23.49 11.7756 ## 118 Indiana Putnam 32.59 22.81 9.7843 ## 119 Indiana Randolph 36.99 23.39 13.6021 ## 120 Indiana Shelby 31.97 23.60 8.3726 ## 121 Indiana Starke 43.51 26.98 16.5334 ## 122 Indiana Vermillion 45.27 29.99 15.2768 ## 123 Iowa Adair 44.79 29.73 15.0651 ## 124 Iowa Appanoose 47.25 29.56 17.6906 ## 125 Iowa Benton 48.93 33.79 15.1431 ## 126 Iowa Butler 44.12 28.71 15.4117 ## 127 Iowa Chickasaw 54.81 35.21 19.5982 ## 128 Iowa Clinton 60.56 43.76 16.8002 ## 129 Iowa Crawford 45.41 28.71 16.6993 ## 130 Iowa Guthrie 43.63 30.12 13.5094 ## 131 Iowa Harrison 42.83 28.55 14.2833 ## 132 Iowa Howard 59.59 36.78 22.8066 ## 133 Iowa Iowa 46.74 34.64 12.1042 ## 134 Iowa Jackson 57.67 37.22 20.4580 ## 135 Iowa Lee 56.65 38.48 18.1743 ## 136 Iowa Muscatine 57.00 43.06 13.9412 ## 137 Iowa Page 36.91 25.65 11.2590 ## 138 Iowa Pocahontas 37.77 24.92 12.8569 ## 139 Iowa Story 55.55 50.74 4.8093 ## 140 Kansas Anderson 28.48 20.06 8.4169 ## 141 Kansas Atchison 38.46 30.25 8.2117 ## 142 Kansas Brown 26.95 21.38 5.5646 ## 143 Kansas Clay 22.63 17.72 4.9036 ## 144 Kansas Decatur 17.36 12.26 5.1040 ## 145 Kansas Edwards 21.52 16.07 5.4435 ## 146 Kansas Ellis 26.10 22.95 3.1456 ## 147 Kansas Ford 31.09 27.84 3.2490 ## 148 Kansas Franklin 33.61 26.37 7.2375 ## 149 Kansas Hamilton 18.59 13.81 4.7733 ## 150 Kansas Lincoln 19.49 14.69 4.8017 ## 151 Kansas Miami 31.72 26.57 5.1501 ## 152 Kansas Neosho 31.64 23.77 7.8632 ## 153 Kansas Phillips 14.89 11.26 3.6315 ## 154 Kansas Rawlins 13.16 11.07 2.0845 ## 155 Kansas Reno 33.11 28.24 4.8650 ## 156 Kansas Stanton 18.84 18.05 0.7872 ## 157 Kansas Wyandotte 67.61 61.80 5.8075 ## 158 Kentucky Anderson 32.12 23.06 9.0613 ## 159 Kentucky Barren 32.59 23.12 9.4698 ## 160 Kentucky Bell 23.45 17.70 5.7559 ## 161 Kentucky Boyle 36.14 33.07 3.0670 ## 162 Kentucky Bracken 35.49 19.99 15.5002 ## 163 Kentucky Caldwell 31.60 21.09 10.5162 ## 164 Kentucky Cumberland 20.99 14.97 6.0174 ## 165 Kentucky Fayette 49.32 51.19 -1.8665 ## 166 Kentucky Fleming 33.05 21.53 11.5173 ## 167 Kentucky Greenup 39.73 25.47 14.2634 ## 168 Kentucky Knott 24.94 21.60 3.3396 ## 169 Kentucky Madison 34.55 31.56 2.9912 ## 170 Kentucky Mercer 29.84 22.63 7.2157 ## 171 Kentucky Muhlenberg 37.45 25.05 12.3983 ## 172 Kentucky Oldham 30.92 31.25 -0.3348 ## 173 Kentucky Powell 36.15 25.66 10.4920 ## 174 Kentucky Rowan 44.85 37.24 7.6132 ## 175 Kentucky Shelby 35.55 30.50 5.0506 ## 176 Kentucky Todd 29.74 21.80 7.9335 ## 177 Kentucky Trimble 38.24 23.14 15.1007 ## 178 Louisiana Bossier 26.68 25.36 1.3202 ## 179 Louisiana Caldwell 21.54 16.79 4.7491 ## 180 Louisiana East Baton Rouge 51.80 52.33 -0.5304 ## 181 Louisiana Franklin 31.29 27.35 3.9354 ## 182 Louisiana Sabine 21.82 17.40 4.4237 ## 183 Louisiana St. Martin 37.05 32.10 4.9454 ## 184 Maine Kennebec 55.23 44.26 10.9703 ## 185 Maine Knox 59.92 53.76 6.1656 ## 186 Maine Lincoln 54.51 47.63 6.8858 ## 187 Maine Sagadahoc 56.85 49.33 7.5143 ## 188 Maryland Garrett 23.74 18.32 5.4227 ## 189 Maryland Montgomery 70.92 74.72 -3.8025 ## 190 Maryland Worcester 40.17 34.50 5.6720 ## 191 Massachusetts Barnstable 53.20 54.11 -0.9043 ## 192 Massachusetts Bristol 59.20 52.03 7.1626 ## 193 Massachusetts Essex 57.40 58.52 -1.1194 ## 194 Massachusetts Franklin 71.70 64.12 7.5816 ## 195 Michigan Arenac 46.79 30.98 15.8154 ## 196 Michigan Calhoun 50.18 41.01 9.1653 ## 197 Michigan Kalamazoo 56.06 53.17 2.8920 ## 198 Michigan Macomb 51.48 42.05 9.4290 ## 199 Michigan Mecosta 44.43 33.94 10.4922 ## 200 Michigan Midland 41.79 36.78 5.0094 ## 201 Michigan Missaukee 32.36 21.39 10.9723 ## 202 Michigan Newaygo 40.64 27.44 13.1988 ## 203 Michigan Ottawa 32.30 31.51 0.7919 ## 204 Michigan St. Joseph 43.32 31.68 11.6438 ## 205 Michigan Schoolcraft 46.07 32.96 13.1159 ## 206 Michigan Van Buren 49.61 39.84 9.7637 ## 207 Michigan Washtenaw 67.31 68.13 -0.8189 ## 208 Minnesota Dakota 50.37 47.74 2.6287 ## 209 Minnesota Douglas 41.30 28.58 12.7133 ## 210 Minnesota Kanabec 44.09 28.46 15.6280 ## 211 Minnesota Lake 59.28 47.19 12.0955 ## 212 Minnesota Lyon 44.12 31.31 12.8013 ## 213 Minnesota Nobles 44.52 31.66 12.8575 ## 214 Minnesota Norman 53.94 38.76 15.1834 ## 215 Minnesota Pennington 46.40 31.97 14.4285 ## 216 Minnesota Pipestone 37.13 23.44 13.6886 ## 217 Minnesota Pope 47.73 33.33 14.3985 ## 218 Minnesota Ramsey 66.33 65.07 1.2569 ## 219 Minnesota Renville 44.02 27.83 16.1875 ## 220 Minnesota Roseau 37.70 23.78 13.9214 ## 221 Minnesota Sherburne 37.84 27.53 10.3043 ## 222 Minnesota Waseca 44.83 29.40 15.4341 ## 223 Mississippi Bolivar 68.59 65.44 3.1527 ## 224 Mississippi Calhoun 36.59 29.86 6.7250 ## 225 Mississippi Claiborne 88.22 86.80 1.4225 ## 226 Mississippi Copiah 54.87 51.93 2.9422 ## 227 Mississippi Covington 41.48 37.18 4.3040 ## 228 Mississippi Grenada 46.65 42.08 4.5707 ## 229 Mississippi Jefferson Davis 62.16 59.64 2.5209 ## 230 Mississippi Leflore 71.39 69.90 1.4915 ## 231 Mississippi Lowndes 49.30 46.21 3.0836 ## 232 Mississippi Pearl River 19.65 16.47 3.1760 ## 233 Mississippi Tate 39.76 33.77 5.9968 ## 234 Mississippi Tunica 79.30 74.77 4.5330 ## 235 Mississippi Yalobusha 47.69 42.49 5.1984 ## 236 Missouri Adair 41.67 34.23 7.4385 ## 237 Missouri Barton 21.41 13.39 8.0201 ## 238 Missouri Chariton 35.04 22.37 12.6698 ## 239 Missouri Clark 43.35 22.52 20.8294 ## 240 Missouri Henry 35.62 23.89 11.7317 ## 241 Missouri Holt 23.85 14.73 9.1182 ## 242 Missouri Howell 26.89 16.51 10.3820 ## 243 Missouri Lafayette 35.80 25.53 10.2734 ## 244 Missouri Lincoln 34.14 22.33 11.8087 ## 245 Missouri Madison 32.21 18.89 13.3235 ## 246 Missouri Miller 24.06 15.33 8.7291 ## 247 Missouri Platte 42.09 41.03 1.0658 ## 248 Missouri Putnam 25.42 15.05 10.3754 ## 249 Missouri St. Clair 31.56 20.22 11.3448 ## 250 Missouri St. Louis County 56.20 55.69 0.5104 ## 251 Missouri Saline 41.61 30.07 11.5384 ## 252 Missouri Shelby 29.89 18.62 11.2711 ## 253 Missouri Webster 28.58 18.98 9.6010 ## 254 Missouri St. Louis City 82.71 79.45 3.2590 ## 255 Montana Fergus 27.01 20.58 6.4315 ## 256 Montana Flathead 31.95 28.54 3.4097 ## 257 Montana Glacier 65.68 61.50 4.1808 ## 258 Montana McCone 22.46 14.53 7.9289 ## 259 Montana Meagher 27.67 19.75 7.9205 ## 260 Montana Missoula 57.39 53.28 4.1132 ## 261 Montana Powell 31.99 19.72 12.2676 ## 262 Montana Rosebud 40.36 28.58 11.7795 ## 263 Montana Wheatland 27.42 18.92 8.4976 ## 264 Nebraska Antelope 17.67 11.67 5.9938 ## 265 Nebraska Banner 13.38 4.73 8.6556 ## 266 Nebraska Blaine 9.57 9.46 0.1072 ## 267 Nebraska Buffalo 27.58 22.54 5.0368 ## 268 Nebraska Burt 38.25 26.14 12.1136 ## 269 Nebraska Cass 35.85 26.94 8.9150 ## 270 Nebraska Frontier 20.77 12.12 8.6428 ## 271 Nebraska Hamilton 23.53 17.55 5.9810 ## 272 Nebraska Hooker 14.94 9.59 5.3444 ## 273 Nebraska Keya Paha 16.53 8.35 8.1782 ## 274 Nebraska Nance 29.67 17.33 12.3381 ## 275 Nebraska Perkins 17.07 11.07 6.0079 ## 276 Nebraska Sherman 36.08 21.61 14.4637 ## 277 Nebraska Sioux 13.67 11.01 2.6617 ## 278 Nebraska Thomas 10.29 7.63 2.6605 ## 279 Nebraska Valley 22.69 15.37 7.3138 ## 280 Nebraska Wayne 29.17 22.11 7.0556 ## 281 Nevada Esmeralda 21.15 15.37 5.7830 ## 282 Nevada Eureka 13.24 8.67 4.5775 ## 283 Nevada Nye 36.07 26.00 10.0725 ## 284 New Hampshire Grafton 60.86 55.69 5.1698 ## 285 New Jersey Cape May 45.18 38.48 6.7001 ## 286 New Jersey Morris 44.04 46.02 -1.9819 ## 287 New Jersey Passaic 63.70 59.94 3.7625 ## 288 New Jersey Sussex 38.43 32.66 5.7628 ## 289 New Mexico Chaves 32.54 27.30 5.2448 ## 290 New Mexico De Baca 31.82 21.21 10.6094 ## 291 New Mexico Sierra 38.49 31.11 7.3810 ## 292 New York Cattaraugus 40.45 30.48 9.9703 ## 293 New York Chautauqua 42.50 35.20 7.3005 ## 294 New York Clinton 58.95 46.91 12.0455 ## 295 New York Erie 54.88 50.86 4.0235 ## 296 New York Essex 56.04 45.08 10.9580 ## 297 New York Franklin 59.82 43.05 16.7767 ## 298 New York Monroe 55.69 54.23 1.4688 ## 299 New York New York 81.40 86.56 -5.1520 ## 300 New York Oneida 45.02 37.08 7.9419 ## 301 New York Otsego 47.77 40.72 7.0490 ## 302 New York Richmond 49.01 40.97 8.0372 ## 303 New York St. Lawrence 55.09 42.11 12.9823 ## 304 New York Saratoga 48.33 44.62 3.7110 ## 305 New York Warren 48.27 41.68 6.5851 ## 306 North Carolina Ashe 32.64 26.07 6.5681 ## 307 North Carolina Bertie 66.14 61.82 4.3199 ## 308 North Carolina Catawba 34.58 29.32 5.2577 ## 309 North Carolina Currituck 31.51 22.99 8.5178 ## 310 North Carolina Dare 41.13 36.83 4.3016 ## 311 North Carolina Edgecombe 67.89 65.19 2.6919 ## 312 North Carolina Iredell 34.15 29.96 4.1903 ## 313 North Carolina Jackson 48.47 41.22 7.2528 ## 314 North Carolina Lincoln 29.98 24.73 5.2486 ## 315 North Carolina Moore 35.56 33.54 2.0243 ## 316 North Carolina Perquimans 41.48 34.57 6.9056 ## 317 North Carolina Randolph 24.33 20.43 3.8997 ## 318 North Carolina Rockingham 38.91 33.65 5.2590 ## 319 North Carolina Rowan 36.35 30.14 6.2106 ## 320 North Carolina Sampson 44.19 40.68 3.5109 ## 321 North Carolina Stokes 27.84 20.69 7.1493 ## 322 North Carolina Vance 63.89 61.22 2.6716 ## 323 North Carolina Wake 54.94 57.38 -2.4333 ## 324 North Carolina Wilson 53.38 51.56 1.8149 ## 325 North Carolina Yancey 42.12 32.09 10.0342 ## 326 North Dakota Cavalier 39.54 23.73 15.8072 ## 327 North Dakota Foster 36.09 20.19 15.9018 ## 328 North Dakota Hettinger 22.98 12.96 10.0179 ## 329 North Dakota LaMoure 33.73 23.34 10.3904 ## 330 North Dakota McLean 33.82 20.34 13.4807 ## 331 North Dakota Nelson 45.71 31.22 14.4919 ## 332 North Dakota Pembina 38.49 21.60 16.8961 ## 333 North Dakota Richland 42.01 27.19 14.8206 ## 334 Ohio Athens 66.24 55.60 10.6396 ## 335 Ohio Carroll 41.80 24.12 17.6719 ## 336 Ohio Delaware 37.76 39.35 -1.5931 ## 337 Ohio Erie 55.31 42.73 12.5754 ## 338 Ohio Greene 38.63 35.36 3.2770 ## 339 Ohio Harrison 41.31 23.85 17.4656 ## 340 Ohio Hocking 48.37 29.40 18.9688 ## 341 Ohio Knox 36.88 28.48 8.3985 ## 342 Ohio Licking 42.02 33.08 8.9428 ## 343 Ohio Marion 45.72 30.12 15.5958 ## 344 Ohio Ottawa 51.22 37.29 13.9350 ## 345 Ohio Paulding 38.76 23.13 15.6251 ## 346 Ohio Vinton 44.56 24.55 20.0036 ## 347 Ohio Washington 39.50 26.90 12.5995 ## 348 Oklahoma Alfalfa 15.46 9.57 5.8925 ## 349 Oklahoma Carter 28.66 21.59 7.0721 ## 350 Oklahoma Cherokee 42.95 33.09 9.8603 ## 351 Oklahoma Cleveland 37.03 35.48 1.5585 ## 352 Oklahoma Delaware 29.39 21.07 8.3243 ## 353 Oklahoma Johnston 30.03 19.56 10.4697 ## 354 Oklahoma Love 29.80 19.40 10.4000 ## 355 Oklahoma Noble 24.68 18.47 6.2108 ## 356 Oklahoma Pushmataha 25.25 16.69 8.5690 ## 357 Oklahoma Rogers 24.93 19.34 5.5875 ## 358 Oklahoma Stephens 23.38 17.24 6.1446 ## 359 Oregon Benton 62.00 59.88 2.1178 ## 360 Oregon Clackamas 50.44 47.70 2.7375 ## 361 Oregon Columbia 50.28 38.20 12.0826 ## 362 Oregon Curry 39.60 34.10 5.5072 ## 363 Oregon Josephine 37.16 30.19 6.9706 ## 364 Oregon Klamath 29.49 23.63 5.8560 ## 365 Oregon Malheur 27.71 21.52 6.1882 ## 366 Oregon Multnomah 75.37 73.30 2.0643 ## 367 Oregon Union 32.92 25.05 7.8623 ## 368 Pennsylvania Butler 31.92 29.45 2.4726 ## 369 Pennsylvania Cameron 34.30 24.29 10.0056 ## 370 Pennsylvania Fayette 45.30 33.38 11.9190 ## 371 Pennsylvania Lawrence 44.86 34.38 10.4883 ## 372 Pennsylvania Lebanon 35.17 30.65 4.5255 ## 373 Pennsylvania Schuylkill 42.52 26.67 15.8463 ## 374 Pennsylvania Tioga 31.50 21.29 10.2165 ## 375 Pennsylvania Washington 42.58 35.80 6.7776 ## 376 Rhode Island Newport 59.47 55.67 3.7992 ## 377 South Carolina Anderson 31.03 26.21 4.8137 ## 378 South Carolina Beaufort 40.72 40.93 -0.2056 ## 379 South Carolina Darlington 51.27 46.80 4.4651 ## 380 South Carolina Dillon 57.71 49.87 7.8421 ## 381 South Carolina Lexington 30.32 28.86 1.4656 ## 382 South Carolina Marion 64.65 60.02 4.6232 ## 383 South Dakota Brookings 46.99 38.48 8.5140 ## 384 South Dakota Buffalo 73.63 60.41 13.2268 ## 385 South Dakota Custer 29.54 23.74 5.7904 ## 386 South Dakota Day 52.11 35.46 16.6490 ## 387 South Dakota Fall River 32.42 23.04 9.3808 ## 388 South Dakota Haakon 12.64 7.38 5.2619 ## 389 South Dakota Hughes 34.16 29.97 4.1937 ## 390 South Dakota Jackson 38.52 29.50 9.0195 ## 391 South Dakota McPherson 22.39 16.89 5.5003 ## 392 South Dakota Potter 24.55 16.08 8.4667 ## 393 South Dakota Sully 22.74 15.91 6.8267 ## 394 South Dakota Todd 79.13 70.82 8.3114 ## 395 South Dakota Turner 33.52 23.16 10.3588 ## 396 South Dakota Walworth 27.25 18.45 8.8045 ## 397 Tennessee Anderson 34.13 30.16 3.9649 ## 398 Tennessee Bedford 29.15 22.11 7.0419 ## 399 Tennessee Carroll 32.02 22.41 9.6152 ## 400 Tennessee Cheatham 30.69 24.35 6.3345 ## 401 Tennessee Fayette 34.17 30.29 3.8801 ## 402 Tennessee Grundy 38.78 20.97 17.8033 ## 403 Tennessee Hamilton 41.67 38.84 2.8308 ## 404 Tennessee Obion 27.01 19.81 7.2009 ## 405 Tennessee Scott 21.78 13.11 8.6632 ## 406 Tennessee Union 25.36 16.20 9.1606 ## 407 Tennessee Van Buren 38.04 22.28 15.7615 ## 408 Tennessee Washington 29.82 26.18 3.6450 ## 409 Texas Atascosa 40.35 34.02 6.3347 ## 410 Texas Blanco 24.53 21.88 2.6503 ## 411 Texas Briscoe 16.67 12.36 4.3025 ## 412 Texas Cameron 64.99 64.51 0.4792 ## 413 Texas Ector 24.97 28.06 -3.0879 ## 414 Texas El Paso 65.51 69.08 -3.5701 ## 415 Texas Erath 15.75 15.55 0.2066 ## 416 Texas Galveston 35.89 35.52 0.3713 ## 417 Texas Gonzales 29.34 24.80 4.5353 ## 418 Texas Hudspeth 43.92 37.20 6.7179 ## 419 Texas Jeff Davis 36.91 35.43 1.4803 ## 420 Texas Kaufman 27.32 24.91 2.4131 ## 421 Texas Kendall 17.11 18.11 -0.9945 ## 422 Texas Lamb 24.40 19.30 5.0959 ## 423 Texas Lampasas 20.53 18.07 2.4559 ## 424 Texas La Salle 58.63 54.83 3.7945 ## 425 Texas Leon 15.29 12.22 3.0722 ## 426 Texas Lipscomb 10.19 10.14 0.0532 ## 427 Texas McLennan 34.47 34.22 0.2452 ## 428 Texas Madison 24.01 20.54 3.4660 ## 429 Texas Menard 20.14 17.82 2.3173 ## 430 Texas Mills 12.68 10.82 1.8520 ## 431 Texas Mitchell 23.23 16.12 7.1095 ## 432 Texas Navarro 28.31 24.35 3.9595 ## 433 Texas Palo Pinto 19.40 16.63 2.7650 ## 434 Texas Panola 21.61 17.62 3.9958 ## 435 Texas Parker 16.47 14.69 1.7796 ## 436 Texas Pecos 38.34 37.13 1.2047 ## 437 Texas Refugio 37.28 35.07 2.2059 ## 438 Texas Roberts 6.50 3.61 2.8860 ## 439 Texas San Saba 14.30 12.43 1.8673 ## 440 Texas Scurry 16.67 13.86 2.8110 ## 441 Texas Shelby 25.01 19.35 5.6623 ## 442 Texas Smith 26.95 26.31 0.6473 ## 443 Texas Sterling 6.28 11.06 -4.7831 ## 444 Texas Sutton 24.78 22.10 2.6772 ## 445 Texas Titus 29.91 27.57 2.3355 ## 446 Texas Tom Green 25.31 23.84 1.4715 ## 447 Texas Trinity 25.89 19.28 6.6046 ## 448 Texas Wilson 28.02 24.65 3.3709 ## 449 Utah Grand 43.72 43.24 0.4830 ## 450 Utah Summit 46.16 50.88 -4.7268 ## 451 Vermont Rutland 59.73 46.03 13.6948 ## 452 Virginia Amelia 36.01 30.23 5.7814 ## 453 Virginia Amherst 39.41 32.85 6.5660 ## 454 Virginia Essex 53.15 47.00 6.1409 ## 455 Virginia Greensville 63.64 58.63 5.0125 ## 456 Virginia Henry 41.33 34.01 7.3210 ## 457 Virginia Highland 32.48 26.67 5.8126 ## 458 Virginia James City 43.35 44.25 -0.8987 ## 459 Virginia Mecklenburg 45.90 42.05 3.8502 ## 460 Virginia Middlesex 38.98 35.03 3.9462 ## 461 Virginia Stafford 44.87 42.33 2.5402 ## 462 Virginia Surry 59.80 53.74 6.0587 ## 463 Virginia Warren 38.64 28.78 9.8587 ## 464 Virginia Washington 27.61 21.48 6.1220 ## 465 Virginia Lexington 55.30 61.42 -6.1166 ## 466 Virginia Lynchburg 43.76 41.47 2.2876 ## 467 Washington Douglas 34.57 31.64 2.9324 ## 468 Washington Ferry 37.62 30.49 7.1247 ## 469 Washington Mason 52.29 42.72 9.5689 ## 470 Washington San Juan 67.26 66.57 0.6874 ## 471 West Virginia Barbour 30.69 20.19 10.5041 ## 472 West Virginia Braxton 41.59 25.95 15.6375 ## 473 West Virginia Calhoun 37.06 17.42 19.6394 ## 474 West Virginia Harrison 37.22 27.52 9.7024 ## 475 West Virginia Nicholas 30.32 19.39 10.9376 ## 476 West Virginia Ohio 37.89 30.62 7.2652 ## 477 West Virginia Raleigh 26.84 21.75 5.0908 ## 478 West Virginia Tucker 28.04 21.61 6.4318 ## 479 West Virginia Wayne 35.34 22.11 13.2233 ## 480 Wisconsin Ashland 64.49 52.66 11.8297 ## 481 Wisconsin Barron 47.99 34.80 13.1939 ## 482 Wisconsin Calumet 43.49 36.20 7.2909 ## 483 Wisconsin Chippewa 49.26 37.66 11.5993 ## 484 Wisconsin Columbia 56.23 45.43 10.7932 ## 485 Wisconsin Florence 36.30 25.07 11.2389 ## 486 Wisconsin Jefferson 45.52 38.38 7.1393 ## 487 Wisconsin Juneau 52.78 34.72 18.0605 ## 488 Wisconsin Kewaunee 46.69 33.68 13.0111 ## 489 Wisconsin Lafayette 57.04 42.91 14.1292 ## 490 Wisconsin Lincoln 49.70 36.52 13.1886 ## 491 Wisconsin Ozaukee 34.32 36.99 -2.6652 ## 492 Wisconsin Portage 56.12 48.11 8.0062 ## 493 Wyoming Albany 45.75 40.39 5.3677 ## 494 Wyoming Laramie 36.19 28.25 7.9371 ## 495 Wyoming Lincoln 14.93 12.45 2.4832 ## 496 Wyoming Uinta 19.07 14.19 4.8742 ## 497 Wyoming Washakie 20.13 13.95 6.1832 ## 498 Alaska District 3 33.51 16.30 17.2135 ## 499 Alaska District 18 61.28 52.81 8.4742 ## 500 Alaska District 24 42.91 39.41 3.5087 # Find mean and standard deviation of differences diff_stats &lt;- sample_dem_data %&gt;% summarise( xbar_diff = mean(diff), s_diff = sd(diff) ) # See the result diff_stats ## xbar_diff s_diff ## 1 6.83 5.04 # Using sample_dem_data, plot diff as a histogram ggplot(sample_dem_data, aes(x = diff)) + geom_histogram(binwidth = 1) Notice that the majority of the histogram lies to the right of zero. 17.2.3.2 Using t.test() The comparison of two sample means is called a t-test, and R has a t.test() function to accomplish it. This function provides some flexibility in how you perform the test. t.test( # Vector of data sample_data$time1, sample_data$time2, # Choose between &quot;two.sided&quot;, &quot;less&quot;, &quot;greater&quot; alternative = &quot;less&quot;, # Null hypothesis population parameter mu = 0 # Set pair paired = TRUE ) Conduct a t-test on the sample differences (the diff column of sample_dem_data). Use an appropriate alternative hypothesis chosen from \"two.sided\", \"less\", and \"greater\". # Conduct a t-test on diff test_results &lt;- t.test( sample_dem_data$diff, alternative = &quot;greater&quot;, mu = 0 ) # See the results test_results ## ## One Sample t-test ## ## data: sample_dem_data$diff ## t = 30, df = 499, p-value &lt;0.0000000000000002 ## alternative hypothesis: true mean is greater than 0 ## 95 percent confidence interval: ## 6.46 Inf ## sample estimates: ## mean of x ## 6.83 Conduct a paired test on the democratic votes in 2012 and 2016. # Conduct a paired t-test on dem_percent_12 and dem_percent_16 test_results &lt;- t.test( sample_dem_data$dem_percent_12, sample_dem_data$dem_percent_16, alternative = &quot;greater&quot;, mu = 0, paired = TRUE ) # See the results, reject H0 test_results ## ## Paired t-test ## ## data: sample_dem_data$dem_percent_12 and sample_dem_data$dem_percent_16 ## t = 30, df = 499, p-value &lt;0.0000000000000002 ## alternative hypothesis: true mean difference is greater than 0 ## 95 percent confidence interval: ## 6.46 Inf ## sample estimates: ## mean difference ## 6.83 When you have paired data, a paired t-test is preferable to the unpaired version because it gives lower p-values (higher statistical power), which reduces the chance of a false negative error. 17.2.4 ANOVA tests ANOVA tests determine whether there are differences between the groups. # First fit a linear regression model &lt;- lm(y ~ x, data) # Then perform an analysis of variance test anova(model) The problem is that this method doesn’t tell you which two categories they are. For this reason, we then conduct a pairwise t-test. pairwise.t.test(numeric variable, categorical variable, p.adjust.method = &quot;&quot;) p.adjust.method: apply an adjustment to increase the p-values, reducing the chance of getting a false positive. 17.2.4.1 Visualizing many categories Here, we’ll return to the late shipments data, and how the price of each package (pack_price) varies between the three shipment modes (shipment_mode): \"Air\", \"Air Charter\", and \"Ocean\". # Using late_shipments, group by shipment mode, and calculate the mean and std dev of pack price late_shipments %&gt;% filter(shipment_mode != &quot;N/A&quot;) %&gt;% group_by(shipment_mode) %&gt;% summarise( xbar_pack_price = mean(pack_price), s_pack_price = sd(pack_price) ) ## # A tibble: 3 × 3 ## shipment_mode xbar_pack_price s_pack_price ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Air 43.1 65.8 ## 2 Air Charter 3.39 1.34 ## 3 Ocean 7.82 9.86 # Using late_shipments, plot pack_price vs. shipment_mode # as a box plot with flipped x and y coordinates late_shipments %&gt;% filter(shipment_mode != &quot;N/A&quot;) %&gt;% ggplot(aes(x = shipment_mode, y = pack_price)) + geom_boxplot() + coord_flip() 17.2.4.2 Conducting an ANOVA test The box plots made it look like the distribution of pack price was different for each of the three shipment modes. However, it didn’t tell us whether the mean pack price was different in each category. To determine that, we can use an ANOVA test. \\(H_0\\): Pack prices for every category of shipment mode are the same. \\(H_A\\): Pack prices for some categories of shipment mode are different. We’ll set a significance level of 0.1. # Run a linear regression of pack price vs. shipment mode mdl_pack_price_vs_shipment_mode &lt;- lm(pack_price ~ shipment_mode, late_shipments) # See the results summary(mdl_pack_price_vs_shipment_mode) ## ## Call: ## lm(formula = pack_price ~ shipment_mode, data = late_shipments) ## ## Residuals: ## Min 1Q Median 3Q Max ## -43.1 -34.5 -12.3 26.9 1199.4 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 43.15 2.09 20.68 &lt; 0.0000000000000002 *** ## shipment_modeAir Charter -39.75 25.77 -1.54 0.12 ## shipment_modeN/A -5.65 62.94 -0.09 0.93 ## shipment_modeOcean -35.33 7.17 -4.92 0.00000099 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 62.9 on 996 degrees of freedom ## Multiple R-squared: 0.0257, Adjusted R-squared: 0.0228 ## F-statistic: 8.76 on 3 and 996 DF, p-value: 0.0000097 Perform ANOVA. # Perform ANOVA on the regression model anova(mdl_pack_price_vs_shipment_mode) ## Analysis of Variance Table ## ## Response: pack_price ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## shipment_mode 3 104048 34683 8.76 0.0000097 *** ## Residuals 996 3941466 3957 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There is a significant difference in pack prices between the shipment modes. However, we don’t know which shipment modes this applies to. 17.2.4.3 Pairwise t-tests To pinpoint which categories had differences, we could instead use pairwise t-tests. late_shipments &lt;- late_shipments %&gt;% filter(shipment_mode != &quot;N/A&quot;) %&gt;% mutate(shipment_mode = as.factor(shipment_mode)) # Perform pairwise t-tests on pack price, grouped by shipment mode, no p-value adjustment test_results &lt;- pairwise.t.test( late_shipments$pack_price, late_shipments$shipment_mode, p.adjust.method = &quot;none&quot; ) # See the results test_results ## ## Pairwise comparisons using t tests with pooled SD ## ## data: late_shipments$pack_price and late_shipments$shipment_mode ## ## Air Air Charter ## Air Charter 0.1 - ## Ocean 0.000001 0.9 ## ## P value adjustment method: none Modify the pairwise t-tests to use Bonferroni p-value adjustment. p.adjust.methods ## [1] &quot;holm&quot; &quot;hochberg&quot; &quot;hommel&quot; &quot;bonferroni&quot; &quot;BH&quot; ## [6] &quot;BY&quot; &quot;fdr&quot; &quot;none&quot; # Modify the pairwise t-tests to use Bonferroni p-value adjustment test_results &lt;- pairwise.t.test( late_shipments$pack_price, late_shipments$shipment_mode, p.adjust.method = &quot;bonferroni&quot; ) # See the results, &quot;Air&quot; &amp; &quot;Ocean&quot; only test_results ## ## Pairwise comparisons using t tests with pooled SD ## ## data: late_shipments$pack_price and late_shipments$shipment_mode ## ## Air Air Charter ## Air Charter 0.4 - ## Ocean 0.000003 1.0 ## ## P value adjustment method: bonferroni Pairwise t-tests give you more information than ANOVA about where the differences between categories lie, but since you are conducting more tests, the p-values need to be adjusted, making it more difficult to see a significanct difference. 17.3 Proportion Tests 17.3.1 One-sample proportion tests 17.3.1.1 Test for single proportions In Chapter 1, you calculated a p-value for a test hypothesizing that the proportion of late shipments was greater than 6%. In that chapter, you used a bootstrap distribution to estimate the standard error of the statistic. A simpler alternative is to use an equation for the standard error based on the sample proportion, hypothesized proportion, and sample size. \\(p\\): population proportion (unknown population parameter) \\(\\hat{p}\\): sample proportion (sample statistic) \\(p_0\\): hypothesized population proportion # Hypothesize that the proportion of late shipments is 6% p_0 &lt;- 0.06 # Calculate the sample proportion of late shipments p_hat &lt;- late_shipments %&gt;% summarise(prop_late = mean(late == &quot;Yes&quot;)) %&gt;% pull(prop_late) # Calculate the sample size n &lt;- nrow(late_shipments) # Calculate the numerator of the test statistic numerator &lt;- p_hat - p_0 # Calculate the denominator of the test statistic denominator &lt;- sqrt(p_0 * (1 - p_0) / n) # Calculate the test statistic z_score &lt;- numerator / denominator # See the result z_score ## [1] 0.941 Transform the z-score into a p-value, remembering that this is a “greater than” alternative hypothesis. # Calculate the p-value from the z-score p_value &lt;- pnorm(z_score, lower.tail = FALSE) # See the result p_value ## [1] 0.173 While bootstrapping can be used to estimate the standard error of any statistic, it is computationally intensive. For proportions, using a simple equation of the hypothesized proportion and sample size is easier to compute, and the resulting p-value is almost identical (0.19 rather than 0.17). 17.3.2 Two-sample proportion tests 17.3.2.1 Test of two proportions You may wonder if the amount paid for freight affects whether or not the shipment was late. Recall that in late_shipments dataset, whether or not the shipment was late is stored in the late column. Freight costs are stored in the freight_cost_group column, and the categories are \"expensive\" and \"reasonable\". \\(H_0: late_{expensive} - late_{reasonable} = 0\\) \\(H_A: late_{expensive} - late_{reasonable} &gt; 0\\) freight_cost_group &lt;- read_delim(&quot;data/late_shipments_add_freight_cost_group.txt&quot;, delim = &quot;,&quot;) late_shipments &lt;- read_fst(&quot;data/late_shipments.fst&quot;) %&gt;% add_column(freight_cost_group) glimpse(late_shipments) ## Rows: 1,000 ## Columns: 27 ## $ id &lt;dbl&gt; 73003, 41222, 52354, 28471, 16901, 27238, 910… ## $ country &lt;chr&gt; &quot;Vietnam&quot;, &quot;Kenya&quot;, &quot;Zambia&quot;, &quot;Nigeria&quot;, &quot;Vie… ## $ managed_by &lt;chr&gt; &quot;PMO - US&quot;, &quot;PMO - US&quot;, &quot;PMO - US&quot;, &quot;PMO - US… ## $ fulfill_via &lt;chr&gt; &quot;Direct Drop&quot;, &quot;Direct Drop&quot;, &quot;Direct Drop&quot;, … ## $ vendor_inco_term &lt;chr&gt; &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EXW&quot;, &quot;EX… ## $ shipment_mode &lt;chr&gt; &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Air&quot;, &quot;Ai… ## $ late_delivery &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ late &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;N… ## $ product_group &lt;chr&gt; &quot;ARV&quot;, &quot;HRDT&quot;, &quot;HRDT&quot;, &quot;HRDT&quot;, &quot;ARV&quot;, &quot;HRDT&quot;,… ## $ sub_classification &lt;chr&gt; &quot;Adult&quot;, &quot;HIV test&quot;, &quot;HIV test&quot;, &quot;HIV test&quot;, … ## $ vendor &lt;chr&gt; &quot;HETERO LABS LIMITED&quot;, &quot;Orgenics, Ltd&quot;, &quot;Orge… ## $ item_description &lt;chr&gt; &quot;Efavirenz/Lamivudine/Tenofovir Disoproxil Fu… ## $ molecule_test_type &lt;chr&gt; &quot;Efavirenz/Lamivudine/Tenofovir Disoproxil Fu… ## $ brand &lt;chr&gt; &quot;Generic&quot;, &quot;Determine&quot;, &quot;Determine&quot;, &quot;Determi… ## $ dosage &lt;chr&gt; &quot;600/300/300mg&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;N/A&quot;, &quot;300mg&quot;… ## $ dosage_form &lt;chr&gt; &quot;Tablet - FDC&quot;, &quot;Test kit&quot;, &quot;Test kit&quot;, &quot;Test… ## $ unit_of_measure_per_pack &lt;dbl&gt; 30, 100, 100, 100, 60, 20, 100, 30, 30, 25, 1… ## $ line_item_quantity &lt;dbl&gt; 19200, 6100, 1364, 2835, 112, 53, 1500, 6180,… ## $ line_item_value &lt;dbl&gt; 201600, 542900, 109120, 252315, 1618, 1696, 1… ## $ pack_price &lt;dbl&gt; 10.50, 89.00, 80.00, 89.00, 14.45, 32.00, 80.… ## $ unit_price &lt;dbl&gt; 0.35, 0.89, 0.80, 0.89, 0.24, 1.60, 0.80, 0.5… ## $ manufacturing_site &lt;chr&gt; &quot;Hetero Unit III Hyderabad IN&quot;, &quot;Alere Medica… ## $ first_line_designation &lt;chr&gt; &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Ye… ## $ weight_kilograms &lt;dbl&gt; 2719, 3497, 553, 1352, 1701, 18, 445, 2130, 2… ## $ freight_cost_usd &lt;dbl&gt; 4085, 40917, 7845, 31284, 4289, 569, 7120, 17… ## $ line_item_insurance_usd &lt;dbl&gt; 207.24, 895.78, 112.18, 353.75, 2.67, 2.80, 2… ## $ freight_cost_group &lt;chr&gt; &quot;reasonable&quot;, &quot;expensive&quot;, &quot;expensive&quot;, &quot;expe… # p_hats contains the estimates of population proportions (sample proportions) for the &quot;expensive&quot; and &quot;reasonable&quot; groups. p_hats &lt;- c(expensive = 0.09611830, reasonable = 0.03267974); p_hats ## expensive reasonable ## 0.0961 0.0327 # ns contains the sample sizes for these groups. ns &lt;- c(expensive = 541, reasonable = 459); ns ## expensive reasonable ## 541 459 Calculate the pooled sample proportion, \\(\\hat{p}\\) as the mean of p_hats weighted by ns. Use weighted.mean() or arithmetic with this equation. \\[\\hat{p} = \\frac{n_{expensive} * \\hat{p}_{expensive} + n_{reasonable} * \\hat{p}_{reasonable}}{n_{expensive} + n_{reasonable}}\\] # Calculate the pooled estimate of the population proportion p_hat &lt;- (ns[1] * p_hats[1] + ns[2] * p_hats[2]) / sum(ns) # See the result p_hat ## expensive ## 0.067 # using weighted.mean function weighted.mean(p_hats, ns) ## [1] 0.067 Calculate the standard error of the sample. Use this equation. # Calculate sample prop&#39;n times one minus sample prop&#39;n p_hat_times_not_p_hat &lt;- p_hat * (1 - p_hat) # Divide this by the sample sizes p_hat_times_not_p_hat_over_ns &lt;- p_hat_times_not_p_hat / ns # Calculate std. error std_error &lt;- sqrt(sum(p_hat_times_not_p_hat_over_ns)) # See the result std_error ## [1] 0.0159 Calculate the z-score. Use the following equation. You’ll need square bracket indexing to access elements of p_hats. # Calculate the z-score z_score &lt;- (p_hats[&quot;expensive&quot;] - p_hats[&quot;reasonable&quot;]) / std_error # See the result z_score ## expensive ## 4 Calculate the p-value from the z-score. # Calculate the p-value from the z-score p_value &lt;- pnorm(z_score, lower.tail = FALSE) # See the result p_value ## expensive ## 0.0000319 17.3.2.2 prop_test() for two samples For daily usage, it’s better to use the infer package. library(infer) stack_overflow %&gt;% prop_test( # proportions ~ categories hobbyist ~ age_cat, # which p-hat to subtract order = c(&quot;At least 30&quot;, &quot;Under 30&quot;), # which response value to count proportions of success = &quot;Yes&quot;, # type of alternative hypothesis alternative = &quot;two-sided&quot;, # should Yates&#39; continuity correction be applied? correct = FALSE ) Using the late_shipments dataset, use prop_test() to perform a proportion test appropriate to the hypotheses. Specify a hypothesis of late versus freight_cost_group. Set the order of the freight cost groups. Specify the success value for late and the type of alternative hypothesis. Don’t use Yates’ continuity correction. library(infer) ## Warning: package &#39;infer&#39; was built under R version 4.3.2 # Perform a proportion test appropriate to the hypotheses test_results &lt;- late_shipments %&gt;% prop_test( late ~ freight_cost_group, order = c(&quot;expensive&quot;, &quot;reasonable&quot;), success = &quot;Yes&quot;, alternative = &quot;greater&quot;, correct = FALSE ) # See the results test_results ## # A tibble: 1 × 6 ## statistic chisq_df p_value alternative lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16.0 1 0.0000319 greater 0.0385 1 17.3.3 Chi-square test of independence Declaring the hypotheses The chi-square independence test compares proportions of successes of a categorical variable across categories of another categorical variable. Q: Are the variables X and Y independent? Test statistic denoted \\(\\chi^2\\). Direction and tails Observed and expected counts are square numbers, so it is always non-negative. chi-square tests are almost always right-tailed. The chi-square distribution Chi-square hypothesis tests rely on the chi-square distribution. Like the t-distribution, the chi-square distribution has degrees of freedom and non-centrality parameters. When these parameters are large, the chi-square distribution can be approximated by a normal distribution. df = (No. of response categories − 1) × (No. of explanatory categories − 1) 17.3.3.1 Performing a chi-square test Flow: Exploratory visualization: proportional stacked bar plot. Chi-square independence test using chisq_test(y ~ x) The late_shipments dataset includes a vendor_inco_term that describes the incoterms that applied to a given shipment. The choices are: EXW: “Ex works”. The buyer pays for transportation of the goods. CIP: “Carriage and insurance paid to”. The seller pays for freight and insurance until the goods board a ship. DDP: “Delivered duty paid”. The seller pays for transportation of the goods until they reach a destination port. FCA: “Free carrier”. The seller pays for transportation of the goods. Perhaps the incoterms affect whether or not the freight costs are expensive. Test these hypotheses with a significance level of 0.01. \\(H_0\\): vendor_inco_term and freight_cost_group are independent. \\(H_A\\): vendor_inco_term and freight_cost_group are associated. Draw a proportional stacked bar plot. # Plot vendor_inco_term filled by freight_cost_group. # Make it a proportional stacked bar plot. late_shipments %&gt;% filter(vendor_inco_term %in% c(&quot;CIP&quot;, &quot;DDP&quot;, &quot;EXW&quot;, &quot;FCA&quot;)) %&gt;% ggplot(aes(vendor_inco_term, fill = freight_cost_group)) + geom_bar(position = &quot;fill&quot;) + ylab(&quot;proportion&quot;) Perform a chi-square test of independence. late_shipments &lt;- late_shipments %&gt;% filter(vendor_inco_term %in% c(&quot;CIP&quot;, &quot;DDP&quot;, &quot;EXW&quot;, &quot;FCA&quot;)) # Perform a chi-square test of independence on freight_cost_group and vendor_inco_term # infer package test_results &lt;- late_shipments %&gt;% chisq_test(freight_cost_group ~ vendor_inco_term) # See the results test_results ## # A tibble: 1 × 3 ## statistic chisq_df p_value ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 44.1 3 0.00000000142 Reject the null hypothesis and conclude that vendor_inco_term and freight_cost_group are associated. 17.3.4 Chi-square goodness of fit tests The chi-square goodness of fit test compares proportions of each level of a categorical variable to hypothesized values. Declaring the hypotheses The test statistic \\(\\chi^2\\) measures how far observed results are from expectations in each group. \\(H_0\\): The sample matches with the hypothesized distribution. \\(H_A\\): The sample does not match with the hypothesized distribution. Flow: Hypothesized counts by category: tribble() Visualizing counts: geom_col() + geom_point() chi-square goodness of fit test using chisq_test(response, p = hypothesized_props) 17.3.4.1 Visualizing goodness of fit Before running such a test, it can be helpful to visually compare the distribution in the sample to the hypothesized distribution. Recall the vendor incoterms in the late_shipments dataset. Let’s hypothesize that the four values occur with these frequencies in the population of shipments. EXW: 0.75 CIP: 0.05 DDP: 0.1 FCA: 0.1 # Using late_shipments, count the vendor incoterms vendor_inco_term_counts &lt;- late_shipments %&gt;% count(vendor_inco_term) # Get the number of rows in the whole sample n_total &lt;- nrow(late_shipments) hypothesized &lt;- tribble( ~ vendor_inco_term, ~ prop, &quot;EXW&quot;, 0.75, &quot;CIP&quot;, 0.05, &quot;DDP&quot;, 0.1, &quot;FCA&quot;, 0.1 ) %&gt;% # Add a column of hypothesized counts for the incoterms mutate(n = prop * n_total) # See the results hypothesized ## # A tibble: 4 × 3 ## vendor_inco_term prop n ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 EXW 0.75 748. ## 2 CIP 0.05 49.8 ## 3 DDP 0.1 99.7 ## 4 FCA 0.1 99.7 # Using vendor_inco_term_counts, plot n vs. vendor_inco_term ggplot(vendor_inco_term_counts, aes(vendor_inco_term, n)) + # Make it a (precalculated) bar plot geom_col() + # Add points from hypothesized geom_point(data = hypothesized, color = &quot;red&quot;) Two of the bars in the sample are very close to the hypothesized values, one is a little high, and one is a little low. We’ll need a test to see if the differences are statistically significant. 17.3.4.2 Performing a goodness of fit test To decide which hypothesis to choose, we’ll set a significance level of 0.1. hypothesized_props &lt;- c( EXW = 0.75, CIP = 0.05, DDP = 0.1, FCA = 0.1 ) # Run chi-square goodness of fit test on vendor_inco_term test_results &lt;- late_shipments %&gt;% chisq_test( response = vendor_inco_term, p = hypothesized_props ) # See the results test_results ## # A tibble: 1 × 3 ## statistic chisq_df p_value ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4861. 3 0 Fall to reject \\(H_0\\) 17.4 Non-Parametric Tests 17.4.1 Assumptions in hypothesis testing All hypothesis tests assume that the data are Randomness: samples are collected at random from the population. Independence of observations: each row is independent of the others. Large sample size: the sample size is big enough to mitigate uncertainty, and so that the Central Limit Theorem applies. 17.4.1.1 Testing sample size The minimum sample size depends on the type of hypothesis tests you want to perform. t-test one sample, two samples, paired samples, ANOVA ⟶ \\(n\\) ≥ 30 proportion tests One sample, two samples ⟶ \\(n\\) ≥ 10 chi-square tests \\(n\\) ≥ 5 Let’s test some scenarios on the late_shipments dataset. late_shipments &lt;- read_fst(&quot;data/late_shipments.fst&quot;) %&gt;% add_column(freight_cost_group) Whether the counts are “big enough” for a two sample t-test. # Get counts by freight_cost_group counts &lt;- late_shipments %&gt;% count(freight_cost_group) # See the result counts ## freight_cost_group n ## 1 expensive 541 ## 2 reasonable 459 # Inspect whether the counts are big enough all(counts$n &gt;= 30) ## [1] TRUE Whether the counts are “big enough” for a one sample proportion test. # Get counts by late counts &lt;- late_shipments %&gt;% count(late) # See the result counts ## late n ## 1 No 933 ## 2 Yes 67 # Inspect whether the counts are big enough all(counts$n &gt;= 10) ## [1] TRUE Whether the counts are “big enough” for a chi-square independence test. # Count the values of vendor_inco_term and freight_cost_group counts &lt;- late_shipments %&gt;% count(vendor_inco_term, freight_cost_group) # See the result counts ## vendor_inco_term freight_cost_group n ## 1 CIF reasonable 1 ## 2 CIP expensive 14 ## 3 CIP reasonable 40 ## 4 DAP reasonable 1 ## 5 DDP expensive 59 ## 6 DDP reasonable 27 ## 7 DDU reasonable 1 ## 8 EXW expensive 429 ## 9 EXW reasonable 317 ## 10 FCA expensive 39 ## 11 FCA reasonable 72 # Inspect whether the counts are big enough all(counts$n &gt;= 5) ## [1] FALSE Whether the counts are “big enough” for an ANOVA test. # Count the values of shipment_mode counts &lt;- late_shipments %&gt;% count(shipment_mode) # See the result counts ## shipment_mode n ## 1 Air 909 ## 2 Air Charter 6 ## 3 N/A 1 ## 4 Ocean 84 # Inspect whether the counts are big enough all(counts$n &gt;= 30) ## [1] FALSE While randomness and independence of observations can’t easily be tested programmatically, you can test that your sample sizes are big enough to make a hypothesis test appropriate. 17.4.2 Simulation-based infer pipeline Simulation-based hypothesis tests allow more flexibility, and are not bound by the assumptions of traditional hypothesis tests. A grammar of hypothesis tests The infer pipeline for hypothesis testing requires four steps to calculate the null distribution: specify, hypothesize, generate, and calculate. null_distn &lt;- dataset %&gt;% specify() %&gt;% hypothesize() %&gt;% generate() %&gt;% calculate() obs_stat &lt;- dataset %&gt;% specify() %&gt;% calculate() get_p_value(null_distn, obs_stat) specify() selects the variable(s) you want to test. For 2 sample tests: response ~ explanatory. For 1 sample tests: response ~ NULL. hypothesize() declares the type of null hypothesis. For 2 sample tests: \"independence\" or \"point\". For 1 sample tests: \"point\". generate() generates simulated data reflecting the null hypothesis. For “independence” null hypotheses: type = \"permute\". For “point” null hypotheses: set type to \"bootstrap\" or \"simulate\". calculate() calculates a distribution of test statistics known as the null distribution. 17.4.2.1 Specifying &amp; hypothesizing In Chapter 3, you ran a two sample proportion test on the proportion of late shipments across freight cost groups. Recall the hypotheses. \\(H_0: late_{expensive} - late_{reasonable} = 0\\) \\(H_A: late_{expensive} - late_{reasonable} &gt; 0\\) Let’s compare that traditional approach using prop_test() with a simulation-based infer pipeline. # Perform a proportion test appropriate to the hypotheses test_results &lt;- late_shipments %&gt;% prop_test( late ~ freight_cost_group, order = c(&quot;expensive&quot;, &quot;reasonable&quot;), success = &quot;Yes&quot;, alternative = &quot;greater&quot;, correct = FALSE ) # See the results, Reject null hypothesis test_results ## # A tibble: 1 × 6 ## statistic chisq_df p_value alternative lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 16.0 1 0.0000319 greater 0.0385 1 Simulation-based infer pipeline. # Specify that we are interested in late proportions across freight_cost_groups, where &quot;Yes&quot; denotes success specified &lt;- late_shipments %&gt;% specify(late ~ freight_cost_group, success = &quot;Yes&quot;) # See the result specified ## Response: late (factor) ## Explanatory: freight_cost_group (factor) ## # A tibble: 1,000 × 2 ## late freight_cost_group ## &lt;fct&gt; &lt;fct&gt; ## 1 No reasonable ## 2 No expensive ## 3 No expensive ## 4 Yes expensive ## 5 No reasonable ## 6 No reasonable ## 7 No expensive ## 8 No expensive ## 9 No expensive ## 10 No reasonable ## # ℹ 990 more rows # Extend the pipeline to declare a null hypothesis that the variables are independent hypothesized &lt;- late_shipments %&gt;% specify( late ~ freight_cost_group, success = &quot;Yes&quot; ) %&gt;% hypothesize(null = &quot;independence&quot;) # See the result hypothesized ## Response: late (factor) ## Explanatory: freight_cost_group (factor) ## Null Hypothesis: independence ## # A tibble: 1,000 × 2 ## late freight_cost_group ## &lt;fct&gt; &lt;fct&gt; ## 1 No reasonable ## 2 No expensive ## 3 No expensive ## 4 Yes expensive ## 5 No reasonable ## 6 No reasonable ## 7 No expensive ## 8 No expensive ## 9 No expensive ## 10 No reasonable ## # ℹ 990 more rows The first two steps in the infer pipeline add attributes to the dataset in order to set up the simulation. 17.4.2.2 Generating &amp; calculating Extend the infer pipeline to generate two thousand permutation replicates. # Extend the pipeline to generate 2000 permutations generated &lt;- late_shipments %&gt;% specify( late ~ freight_cost_group, success = &quot;Yes&quot; ) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 2000, type = &quot;permute&quot;) # See the result generated ## Response: late (factor) ## Explanatory: freight_cost_group (factor) ## Null Hypothesis: independence ## # A tibble: 2,000,000 × 3 ## # Groups: replicate [2,000] ## late freight_cost_group replicate ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; ## 1 No reasonable 1 ## 2 No expensive 1 ## 3 No expensive 1 ## 4 No expensive 1 ## 5 No reasonable 1 ## 6 No reasonable 1 ## 7 No expensive 1 ## 8 No expensive 1 ## 9 No expensive 1 ## 10 No reasonable 1 ## # ℹ 1,999,990 more rows Complete the infer pipeline for the null distribution by calculating the difference in proportions, setting the order to expensive proportion minus reasonable proportion. # Extend the pipeline to calculate the difference in proportions (expensive minus reasonable) null_distn &lt;- late_shipments %&gt;% specify( late ~ freight_cost_group, success = &quot;Yes&quot; ) %&gt;% hypothesize(null = &quot;independence&quot;) %&gt;% generate(reps = 2000, type = &quot;permute&quot;) %&gt;% calculate( stat = &quot;diff in props&quot;, order = c(&quot;expensive&quot;, &quot;reasonable&quot;) ) # See the result null_distn ## Response: late (factor) ## Explanatory: freight_cost_group (factor) ## Null Hypothesis: independence ## # A tibble: 2,000 × 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -0.000995 ## 2 2 0.0111 ## 3 3 -0.0211 ## 4 4 -0.0211 ## 5 5 0.0111 ## 6 6 -0.000995 ## 7 7 0.0111 ## 8 8 -0.0211 ## 9 9 -0.0131 ## 10 10 -0.0614 ## # ℹ 1,990 more rows Visualize the null distribution. # Visualize the null distribution visualize(null_distn) To determine a result from the test, we need to compare this null distribution to the observed statistic. 17.4.2.3 Observed statistic &amp; p-value In order to get a p-value and weigh up the evidence against the null hypothesis, you need to calculate the difference in proportions that is observed in the late_shipments sample. Copy, paste, and modify the null distribution pipeline to get the observed statistic. # Copy, paste, and modify the pipeline to get the observed statistic obs_stat &lt;- late_shipments %&gt;% specify( late ~ freight_cost_group, success = &quot;Yes&quot; ) %&gt;% calculate( stat = &quot;diff in props&quot;, order = c(&quot;expensive&quot;, &quot;reasonable&quot;) ) # See the result obs_stat ## Response: late (factor) ## Explanatory: freight_cost_group (factor) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 0.0634 Visualize the null distribution, adding a vertical line at the observed statistic. # Visualize the null dist&#39;n, adding a vertical line at the observed statistic visualize(null_distn) + geom_vline(data = obs_stat, aes(xintercept = stat), color = &quot;red&quot;) Get the p-value from the null distribution and observed statistic, assuming an appropriate direction for the alternative hypothesis. # Get the p-value p_value &lt;- get_p_value( null_distn, obs_stat, direction = &quot;greater&quot; ) # See the result p_value ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0 The p-value is calculated from the null distribution and the observed statistic. Here, the observed difference in proportions appears way outside of the null distribution histogram, which resulted in a p-value of zero. This can be interpreted as “p is very small”, rather than actually zero. 17.4.3 Non-parametric ANOVA &amp; unpaired t-tests Non-parametric tests A non-parametric test is a hypothesis test that doesn’t assume a probability distribution for the test statistic. There are two types of non-parametric hypothesis test: Simulation-based. infer pipeline Rank-based. Wilcoxon-Mann-Whitney test: is a t-test on the ranks of the numeric input. wilcox.test(y ~ x, data, alternative, correct) Kruskal-Wallis test: Kruskal-Wallis test is to Wilcoxon-Mann-Whitney test as ANOVA is to t-test. kruskal.test(y ~ x, data) 17.4.3.1 Simulation-based t-test In Chapter 2 you manually performed the steps for a t-test to explore these hypotheses. \\(H_0\\): The mean weight of shipments that weren’t late is the same as the mean weight of shipments that were late. \\(H_A\\): The mean weight of shipments that weren’t late is less than the mean weight of shipments that were late. You can run the test more concisely using infer’s t_test(). late_shipments %&gt;% t_test( weight_kilograms ~ late, order = c(&quot;No&quot;, &quot;Yes&quot;), alternative = &quot;less&quot; ) ## # A tibble: 1 × 7 ## statistic t_df p_value alternative estimate lower_ci upper_ci ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -0.872 152. 0.192 less -296. -Inf 265. t_test() assumes that the null distribution is normal. We can avoid assumptions by using a simulation-based non-parametric equivalent. # Fill out the null distribution pipeline null_distn &lt;- late_shipments %&gt;% # Specify weight_kilograms vs. late specify(weight_kilograms ~ late) %&gt;% # Declare a null hypothesis of independence hypothesize(null = &quot;independence&quot;) %&gt;% # Generate 1000 permutation replicates generate(reps = 1000, type = &quot;permute&quot;) %&gt;% # Calculate the difference in means (&quot;No&quot; minus &quot;Yes&quot;) calculate( stat = &quot;diff in means&quot;, order = c(&quot;No&quot;, &quot;Yes&quot;) ) # See the results null_distn ## Response: weight_kilograms (numeric) ## Explanatory: late (factor) ## Null Hypothesis: independence ## # A tibble: 1,000 × 2 ## replicate stat ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -1727. ## 2 2 552. ## 3 3 -2694. ## 4 4 -228. ## 5 5 -713. ## 6 6 -1881. ## 7 7 -46.5 ## 8 8 -770. ## 9 9 253. ## 10 10 539. ## # ℹ 990 more rows Calculate the difference in means observed in the late_shipments dataset. # Calculate the observed difference in means obs_stat &lt;- late_shipments %&gt;% specify(weight_kilograms ~ late) %&gt;% calculate(stat = &quot;diff in means&quot;, order = c(&quot;No&quot;, &quot;Yes&quot;)) # See the result obs_stat ## Response: weight_kilograms (numeric) ## Explanatory: late (factor) ## # A tibble: 1 × 1 ## stat ## &lt;dbl&gt; ## 1 -296. Get the p-value from the null distribution and the observed difference in means, setting an appropriate direction. # Get the p-value p_value &lt;- get_p_value( null_distn, obs_stat, direction = &quot;less&quot; ) # See the result p_value ## # A tibble: 1 × 1 ## p_value ## &lt;dbl&gt; ## 1 0.211 The p-value with the traditional t-test was 0.19, and the p-value from the simulation was close to 0.21. This difference in p-values could have important consequences for whether or not to reject the null hypothesis. 17.4.3.2 Rank sum tests Ranks are the positions of numeric values from smallest to largest. By calculating on the ranks of data instead of the actual values, you can avoid making assumptions about the distribution of the test statistic. It’s most robust in the same way that a median is more robust than a mean. Two commonly used rank-based tests are the Wilcoxon-Mann-Whitney test, which is like a non-parametric t-test, and the Kruskal-Wallis test, which is like a non-parametric ANOVA. Run a Wilcoxon-Mann-Whitney test on the weight in kilograms versus whether or not the shipment was late. # Run a Wilcoxon-Mann-Whitney test on weight_kilograms vs. late test_results &lt;- wilcox.test( weight_kilograms ~ late, data = late_shipments ) # See the result test_results ## ## Wilcoxon rank sum test with continuity correction ## ## data: weight_kilograms by late ## W = 21480, p-value = 0.00002 ## alternative hypothesis: true location shift is not equal to 0 Run a Kruskal-Wallace test on the weight in kilograms versus the shipment mode. # Run a Kruskal-Wallace test on weight_kilograms vs. shipment_mode test_results &lt;- kruskal.test( weight_kilograms ~ shipment_mode, data = late_shipments ) # See the result test_results ## ## Kruskal-Wallis rank sum test ## ## data: weight_kilograms by shipment_mode ## Kruskal-Wallis chi-squared = 160, df = 3, p-value &lt;0.0000000000000002 "],["experimental-design.html", "Chapter 18 Experimental Design 18.1 Introduction to experimental design 18.2 Basic Experiments 18.3 Block Designs 18.4 Squares &amp; Factorial Experiments", " Chapter 18 Experimental Design 18.1 Introduction to experimental design 18.1.1 Introduction Key components of an experiment Randomization Replication Blocking 18.1.1.1 A basic experiment ToothGrowth is a built-in R dataset from a study that examined the effect of three different doses of Vitamin C on the length of the odontoplasts, the cells responsible for teeth growth in 60 guinea pigs, where tooth length was the measured outcome variable. # Load the ToothGrowth dataset data(ToothGrowth) # View the first 6 rows of ToothGrowth head(ToothGrowth) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10.0 VC 0.5 library(tidyverse) # Find mean len, median len, and standard deviation len with summarize() ToothGrowth %&gt;% summarize(mean(len), median(len), sd(len)) ## mean(len) median(len) sd(len) ## 1 18.8 19.2 7.65 Conduct a two-sided t-test to check if average length of a guinea pig’s odontoplasts differs from 18 micrometers. # Perform a two-sided t-test t.test(x = ToothGrowth$len, alternative = &quot;two.sided&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.8, df = 59, p-value = 0.4 ## alternative hypothesis: true mean is not equal to 18 ## 95 percent confidence interval: ## 16.8 20.8 ## sample estimates: ## mean of x ## 18.8 Given the high p-value, we fail to reject the null hypothesis that the mean of len is equal to 18. That is, we don’t have evidence that it is different from 18 micrometers. 18.1.1.2 Randomization Randomization of subjects in an experiment helps spread any variability that exists naturally between subjects evenly across groups. In the experiment that yielded the ToothGrowth dataset, guinea pigs were randomized to receive Vitamin C either through orange juice(OJ) or ascorbic acid(VC), indicated in the dataset by the supp variable. It’s natural to wonder if there is a difference in tooth length by supplement type? # Perform a t-test ToothGrowth_ttest &lt;- t.test(len ~ supp, data = ToothGrowth) # Load broom library(broom) # Tidy ToothGrowth_ttest # method = Welch Two Sample t-test, alternative = two.sided tidy(ToothGrowth_ttest) ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.7 20.7 17.0 1.92 0.0606 55.3 -0.171 7.57 ## # ℹ 2 more variables: method &lt;chr&gt;, alternative &lt;chr&gt; Given the p-value of around 0.06, there seems to be no evidence to support the hypothesis that there’s a difference in mean tooth length by supplement type, or, more simply, that there is no difference in mean tooth length by supplement type. Generally in most experiments, any p-value above 0.05 will offer no evidence to support the given hypothesis. 18.1.2 Replication &amp; blocking Replication Must repeat an experiment to fully assess variability. Blocking Helps control variability by making treatment groups more alike. Inside of groups, differences will be minimal. Across groups, differences will be larger. Visualize Boxplots Functions for modeling Linear models lm(formula, data, na.action,…) One-way ANOVA model (單因子獨立樣本) aov(formula, data = NULL, …) Nested ANOVA model (相依樣本) anova(object_model,…) 18.1.2.1 Replication Recall that replication means you need to conduct an experiment with an adequate number of subjects to achieve an acceptable statistical power. Let’s examine the ToothGrowth dataset to make sure they followed the principle of replication. # Count number of observations for each combination of supp and dose ToothGrowth %&gt;% count(supp, dose) ## supp dose n ## 1 OJ 0.5 10 ## 2 OJ 1.0 10 ## 3 OJ 2.0 10 ## 4 VC 0.5 10 ## 5 VC 1.0 10 ## 6 VC 2.0 10 The researchers seem to have tested each combination of supp and dose on 10 subjects each, which is low, but was deemed adequate for this experiment. 18.1.2.2 Blocking Make a boxplot to visually examine if the tooth length is different by dose. # Create a boxplot with geom_boxplot() ggplot(ToothGrowth, aes(x = factor(dose), y = len)) + geom_boxplot() Use aov() to detect the effect of dose and supp on len. Save as a model object called ToothGrowth_aov. Examine ToothGrowth_aov with summary() to determine if dose has a significant effect on tooth length. ToothGrowth$dose &lt;- as.factor(ToothGrowth$dose) # Create ToothGrowth_aov ToothGrowth_aov &lt;- aov(len ~ supp + dose, data = ToothGrowth) # Examine ToothGrowth_aov with summary() summary(ToothGrowth_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## supp 1 205 205 14.0 0.00043 *** ## dose 2 2426 1213 82.8 &lt; 0.0000000000000002 *** ## Residuals 56 820 15 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This is called - Randomized Complete Block Design (RCBD) experiment. Given the very small observed p-value for dose, it appears we have evidence to support the hypothesis that mean len is different by dose amount. 18.1.3 Hypothesis testing Power and sample size Power: probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true. One “golden rule” in statistics is to aim to have 80% power in experiments. Effect size: standardized measure of the difference you’re trying to detect. Calculated as the difference between group means divided by the pooled standard deviation of the data. It’s easier to detect a larger difference in means. Sample size: How many experimental units you need to survey to detect the desired difference at the desired power. library(pwr) pwr.anova.test(k = 3, # number of groups in the comparison n = 20, # number of observations per group f = 0.2, # effect size sig.level = 0.05, power = NULL) 18.1.3.1 One sided vs. Two-sided tests alternative = two.sided, less, greater Test to see if the mean of the length variable of ToothGrowth is less than 18. # Less than t.test(x = ToothGrowth$len, alternative = &quot;less&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.8, df = 59, p-value = 0.8 ## alternative hypothesis: true mean is less than 18 ## 95 percent confidence interval: ## -Inf 20.5 ## sample estimates: ## mean of x ## 18.8 Test to see if the mean of the length variable of ToothGrowth is greater than 18. # Greater than t.test(x = ToothGrowth$len, alternative = &quot;greater&quot;, mu = 18) ## ## One Sample t-test ## ## data: ToothGrowth$len ## t = 0.8, df = 59, p-value = 0.2 ## alternative hypothesis: true mean is greater than 18 ## 95 percent confidence interval: ## 17.2 Inf ## sample estimates: ## mean of x ## 18.8 18.1.3.2 Power &amp; Sample Size Calculations One key part of designing an experiment is knowing the required sample size you’ll need to be able to test your hypothesis. The pwr package provides a handy function, pwr.t.test(), which will calculate that for you. owever, you do need to know desired significance level test is one- or two-sided data is from one sample, two samples, or paired effect size power A power or sample size calculation is usually different each time you conduct one, and the details of the calculation strongly depend on what kind of experiment you’re designing and what your end goals are. Calculate power using an effect size of 0.35, a sample size of 100 in each group, and a significance level of 0.10. # Load the pwr package library(pwr) ## Warning: package &#39;pwr&#39; was built under R version 4.3.2 # Calculate power pwr.t.test(n = 100, d = 0.35, sig.level = 0.10, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;, power = NULL) ## ## Two-sample t test power calculation ## ## n = 100 ## d = 0.35 ## sig.level = 0.1 ## power = 0.794 ## alternative = two.sided ## ## NOTE: n is number in *each* group Calculate the sample size needed with an effect size of 0.25, a significance level of 0.05, and a power of 0.8. # Calculate sample size pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;, power = 0.8) ## ## One-sample t test power calculation ## ## n = 100 ## d = 0.25 ## sig.level = 0.05 ## power = 0.8 ## alternative = greater # Inspect output class sample_est &lt;- pwr.t.test(n = NULL, d = 0.25, sig.level = 0.05, type = &quot;one.sample&quot;, alternative = &quot;greater&quot;, power = 0.8) class(sample_est) ## [1] &quot;power.htest&quot; The pwr package includes functions for calculating power and sample size for a variety of different tests 18.2 Basic Experiments 18.2.1 ANOVA &amp; factor experiments ANOVA Used to compare 3+ groups Won’t know which groups’ means are different without additional post hoc testing Two ways to implement in R: #one model_1 &lt;- lm(y ~ x, data = dataset) anova(model_1) #two aov(y ~ x, data = dataset) 18.2.1.1 Exploratory Data Analysis (EDA) A sample of 1500 observations from the Lending Club dataset has been loaded for you and is called lendingclub. Let’s do some EDA on the data, in hopes that we’ll learn what the dataset contains. lendingclub &lt;- read_csv(&quot;data/lendclub.csv&quot;) # Examine the variables with glimpse() glimpse(lendingclub) ## Rows: 1,500 ## Columns: 12 ## $ member_id &lt;dbl&gt; 55096114, 1555332, 1009151, 69524202, 72128084, 53… ## $ loan_amnt &lt;dbl&gt; 11000, 10000, 13000, 5000, 18000, 14000, 8000, 500… ## $ funded_amnt &lt;dbl&gt; 11000, 10000, 13000, 5000, 18000, 14000, 8000, 500… ## $ term &lt;chr&gt; &quot;36 months&quot;, &quot;36 months&quot;, &quot;60 months&quot;, &quot;36 months&quot;… ## $ int_rate &lt;dbl&gt; 12.69, 6.62, 10.99, 12.05, 5.32, 16.99, 13.11, 7.8… ## $ emp_length &lt;chr&gt; &quot;10+ years&quot;, &quot;10+ years&quot;, &quot;3 years&quot;, &quot;10+ years&quot;, … ## $ home_ownership &lt;chr&gt; &quot;RENT&quot;, &quot;MORTGAGE&quot;, &quot;MORTGAGE&quot;, &quot;MORTGAGE&quot;, &quot;MORTG… ## $ annual_inc &lt;dbl&gt; 51000, 40000, 78204, 51000, 96000, 47000, 40000, 3… ## $ verification_status &lt;chr&gt; &quot;Not Verified&quot;, &quot;Verified&quot;, &quot;Not Verified&quot;, &quot;Not V… ## $ loan_status &lt;chr&gt; &quot;Current&quot;, &quot;Fully Paid&quot;, &quot;Fully Paid&quot;, &quot;Current&quot;, … ## $ purpose &lt;chr&gt; &quot;debt_consolidation&quot;, &quot;debt_consolidation&quot;, &quot;home_… ## $ grade &lt;chr&gt; &quot;C&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;A&quot;, &quot;D&quot;, &quot;C&quot;, &quot;A&quot;, &quot;D&quot;, &quot;B&quot;, … # Find median loan_amnt and mean int_rate, annual_inc lendingclub %&gt;% summarise(median(loan_amnt), mean(int_rate), mean(annual_inc)) ## # A tibble: 1 × 3 ## `median(loan_amnt)` `mean(int_rate)` `mean(annual_inc)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 13000 13.3 75736. The axes have been flipped for you using coord_flip() so the labels are easier to read. # Use ggplot2 to build a bar chart of purpose ggplot(data = lendingclub, aes(x = purpose)) + geom_bar() + coord_flip() You can see that the original purpose variable were very detailed. By using recode() here, you created purpose_recode, which has a much more manageable 4 general levels (debt_related, big_purchase, home_related, life_change) that describe the purpose for people’s loans. # Use recode() to create the new purpose_recode variable lendingclub$purpose_recode &lt;- lendingclub$purpose %&gt;% recode( &quot;credit_card&quot; = &quot;debt_related&quot;, &quot;debt_consolidation&quot; = &quot;debt_related&quot;, &quot;medical&quot; = &quot;debt_related&quot;, &quot;car&quot; = &quot;big_purchase&quot;, &quot;major_purchase&quot; = &quot;big_purchase&quot;, &quot;vacation&quot; = &quot;big_purchase&quot;, &quot;moving&quot; = &quot;life_change&quot;, &quot;small_business&quot; = &quot;life_change&quot;, &quot;wedding&quot; = &quot;life_change&quot;, &quot;house&quot; = &quot;home_related&quot;, &quot;home_improvement&quot; = &quot;home_related&quot;) unique(lendingclub$purpose_recode) ## [1] &quot;debt_related&quot; &quot;home_related&quot; &quot;big_purchase&quot; &quot;renewable_energy&quot; ## [5] &quot;other&quot; &quot;life_change&quot; 18.2.1.2 Single factor experiments How does loan purpose affect amount funded? Design an experiment where we examine how the loan purpose influences the amount funded, which is the money actually issued to the applicant. \\(H_0\\): all of the mean funded amounts are equal across the levels of purpose_recode. \\(H_A\\): at least one level of purpose_recode has a different mean. These are the results of the linear regression. # Build a linear regression model, purpose_recode_model purpose_recode_model &lt;- lm(funded_amnt ~ purpose_recode, data = lendingclub) # Examine results of purpose_recode_model summary(purpose_recode_model) ## ## Call: ## lm(formula = funded_amnt ~ purpose_recode, data = lendingclub) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14472 -6251 -1322 4678 25761 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9888 1249 7.92 0.0000000000000047 ## purpose_recodedebt_related 5434 1270 4.28 0.0000201591378533 ## purpose_recodehome_related 4845 1501 3.23 0.0013 ## purpose_recodelife_change 4095 2197 1.86 0.0625 ## purpose_recodeother -649 1598 -0.41 0.6846 ## purpose_recoderenewable_energy -1796 4943 -0.36 0.7164 ## ## (Intercept) *** ## purpose_recodedebt_related *** ## purpose_recodehome_related ** ## purpose_recodelife_change . ## purpose_recodeother ## purpose_recoderenewable_energy ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8280 on 1494 degrees of freedom ## Multiple R-squared: 0.0347, Adjusted R-squared: 0.0315 ## F-statistic: 10.7 on 5 and 1494 DF, p-value: 0.00000000036 Call anova() on model object. # Get anova results and save as purpose_recode_anova purpose_recode_anova &lt;- anova(purpose_recode_model) # Print purpose_recode_anova purpose_recode_anova ## Analysis of Variance Table ## ## Response: funded_amnt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## purpose_recode 5 3688783338 737756668 10.8 0.00000000036 *** ## Residuals 1494 102533145566 68629950 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Examine class of purpose_recode_anova class(purpose_recode_anova) ## [1] &quot;anova&quot; &quot;data.frame&quot; Results indicate that there is evidence to support the hypothesis that the mean loan amounts are different for at least one combination of purpose_recode’s levels. 18.2.1.3 Post-hoc test The result of that ANOVA test was statistically significant with a very low p-value. This means we can reject the null hypothesis and accept the alternative hypothesis that at least one mean was different. But which one? Here comes the post-hoc test. We should use Tukey’s HSD test, which stands for Honest Significant Difference. TukeyHSD(aov_model, \"independent_variable_name\", conf.level = 0.9) # Use aov() to build purpose_aov purpose_aov &lt;- aov(funded_amnt ~ purpose_recode, data = lendingclub) # Conduct Tukey&#39;s HSD test to create tukey_output tukey_output &lt;- TukeyHSD(purpose_aov, &quot;purpose_recode&quot;, conf.level = 0.95) # Tidy tukey_output to make sense of the results tidy(tukey_output) ## # A tibble: 15 × 7 ## term contrast null.value estimate conf.low conf.high adj.p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 purpose_recode debt_relat… 0 5434. 1808. 9059. 2.91e-4 ## 2 purpose_recode home_relat… 0 4845. 562. 9128. 1.61e-2 ## 3 purpose_recode life_chang… 0 4095. -2174. 10365. 4.25e-1 ## 4 purpose_recode other-big_… 0 -649. -5210. 3911. 9.99e-1 ## 5 purpose_recode renewable_… 0 -1796. -15902. 12309. 9.99e-1 ## 6 purpose_recode home_relat… 0 -589. -3056. 1879. 9.84e-1 ## 7 purpose_recode life_chang… 0 -1338. -6539. 3863. 9.78e-1 ## 8 purpose_recode other-debt… 0 -6083. -9005. -3160. 5.32e-8 ## 9 purpose_recode renewable_… 0 -7230. -20894. 6434. 6.58e-1 ## 10 purpose_recode life_chang… 0 -750. -6429. 4929. 9.99e-1 ## 11 purpose_recode other-home… 0 -5494. -9201. -1787. 3.58e-4 ## 12 purpose_recode renewable_… 0 -6641. -20494. 7212. 7.46e-1 ## 13 purpose_recode other-life… 0 -4745. -10636. 1147. 1.95e-1 ## 14 purpose_recode renewable_… 0 -5892. -20482. 8698. 8.59e-1 ## 15 purpose_recode renewable_… 0 -1147. -15088. 12794. 1.00e+0 we can see that only a few of the mean differences are statistically significant, for example the differences in the means for the debt_related and big_purchase loan amounts. In this case, these tiny p-values are most likely to be due to large sample size, and further tests would be required to determine what’s actually significant in the case of loans (known as the practical significance.) 18.2.1.4 Multiple Factor Experiments We can examine more than one explanatory factor in a multiple factor experiment. Use aov() to build a linear model and ANOVA in one step, examining how purpose_recode and employment length (emp_length) affect the funded amount. # Use aov() to build purpose_emp_aov purpose_emp_aov &lt;- aov(funded_amnt ~ purpose_recode + emp_length, data = lendingclub) # Print purpose_emp_aov to the console purpose_emp_aov ## Call: ## aov(formula = funded_amnt ~ purpose_recode + emp_length, data = lendingclub) ## ## Terms: ## purpose_recode emp_length Residuals ## Sum of Squares 3688783338 2044273211 100488872355 ## Deg. of Freedom 5 11 1483 ## ## Residual standard error: 8232 ## Estimated effects may be unbalanced The printed purpose_emp_aov does not show p-values, which we might be interested in. Display those by calling summary() on the aov object. # Call summary() to see the p-values summary(purpose_emp_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## purpose_recode 5 3688783338 737756668 10.89 0.00000000026 *** ## emp_length 11 2044273211 185843019 2.74 0.0016 ** ## Residuals 1483 100488872355 67760534 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 18.2.2 Model validation Post-modeling model validation Residual plot QQ-plot for normality Test ANOVA assumptions Homogeneity of variances Try non-parametric alternatives to ANOVA 18.2.2.1 Pre-modeling EDA Examine what effect their Lending Club-assigned loan grade variable has on the interest rate, int_rate. # Examine the summary of int_rate, range and interquartile range summary(lendingclub$int_rate) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 5.32 9.99 12.99 13.31 16.29 26.77 # Examine int_rate by grade lendingclub %&gt;% group_by(grade) %&gt;% summarize(mean = mean(int_rate), var = var(int_rate), median = median(int_rate)) ## # A tibble: 7 × 4 ## grade mean var median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 7.27 0.961 7.26 ## 2 B 10.9 2.08 11.0 ## 3 C 14.0 1.42 14.0 ## 4 D 17.4 1.62 17.6 ## 5 E 20.1 2.71 20.0 ## 6 F 23.6 2.87 23.5 ## 7 G 26.1 0.198 25.9 # Make a boxplot of int_rate by grade ggplot(lendingclub, aes(x = grade, y = int_rate)) + geom_boxplot() # Use aov() to create grade_aov and call summary() to print results grade_aov &lt;- aov(int_rate ~ grade, data = lendingclub) summary(grade_aov) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## grade 6 27013 4502 2637 &lt;0.0000000000000002 *** ## Residuals 1493 2549 2 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 You can see from the numeric summary and the boxplot that grade seems to heavily influence interest rate. Therefore, the linear model results indicating that int_rate is significantly different by grade are unsurprising. 18.2.2.2 Post-modeling validation In the last exercise, we found that int_rate does differ by grade. Now we should validate this model, which for linear regression means examining the Residuals vs. Fitted and Normal Q-Q plots. plot(model) Another assumption of ANOVA and linear modeling is homogeneity of variance. Homogeneity means “same”. e.g, the variance of int_rate is the same for each level of grade. Using bartlett.test(formula, data) to test. Produce the model diagnostic plots. # For a 2x2 grid of plots: par(mfrow=c(2, 2)) # Plot grade_aov plot(grade_aov) The residuals on this model are okay, though the residuals on G have a much smaller range than any other level of grade (the dots are far less spread out.) The Q-Q plot, however, shows that the residuals are fairly normal. Test for homogeneity of variances using bartlett.test(). non.sig means data is homogeneity. # Bartlett&#39;s test for homogeneity of variance bartlett.test(int_rate ~ grade, lendingclub) ## ## Bartlett test of homogeneity of variances ## ## data: int_rate by grade ## Bartlett&#39;s K-squared = 79, df = 6, p-value = 0.000000000000007 However, given the highly significant p-value from Bartlett’s test, the assumption of homogeneity of variances is violated. 18.2.2.3 Kruskal-Wallis rank sum test Given that we found in the last exercise that the homogeneity of variance assumption of linear modeling was violated, we may want to try an alternative. One non-parametric alternative to ANOVA is the Kruskal-Wallis rank sum test. It is an extension of the Mann-Whitney U test for when there are more than two groups. kruskal.test(formula, data) Use kruskal.test() to examine whether int_rate varies by grade when a non-parametric model is employed. # Conduct the Kruskal-Wallis rank sum test kruskal.test(int_rate ~ grade, data = lendingclub) ## ## Kruskal-Wallis rank sum test ## ## data: int_rate by grade ## Kruskal-Wallis chi-squared = 1366, df = 6, p-value &lt;0.0000000000000002 The low p-value indicates that int_rate varies by grade. 18.2.3 A/B testing A type of controlled experiment with only two variants of something. (只有一個獨變項，且僅一操弄) e.g, How many consumers click through to create an account based on two different website headers? Calculate sample size, given power, significance level, and effect size 18.2.3.1 Sample size for A/B test We’ll be testing the mean loan_amnt, which is the requested amount of money the loan applicants ask for, based on which color header (green or blue) that they saw on the Lending Club website. calculate the required sample size for each group with d = 0.2, a power of 0.8, and a 0.05 significance level. # Use the correct function from pwr to find the sample size pwr.t.test( d = 0.2, n = NULL, power = 0.8, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 393 ## d = 0.2 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group We need about 400 people per group to reach our desired power in this A/B test. 18.2.3.2 Basic A/B test The A/B test was run until there were 500 applicants in each group. Each applicant has been labeled as group A or B. Where A was shown a mint green website header and B was shown a light blue website header. lendingclub_ab &lt;- read_delim(&quot;data/lendingclub_ab.txt&quot;, delim = &quot;,&quot;) glimpse(lendingclub_ab) ## Rows: 1,000 ## Columns: 75 ## $ id &lt;dbl&gt; 11976148, 1203719, 54998739, 5801830, 3158… ## $ member_id &lt;dbl&gt; 13968311, 1444848, 58569477, 7233534, 3418… ## $ loan_amnt &lt;dbl&gt; 8000, 1200, 15000, 9000, 16000, 15600, 240… ## $ funded_amnt &lt;dbl&gt; 8000, 1200, 15000, 9000, 16000, 15600, 240… ## $ funded_amnt_inv &lt;dbl&gt; 8000, 1200, 15000, 9000, 16000, 15600, 240… ## $ term &lt;chr&gt; &quot;36 months&quot;, &quot;36 months&quot;, &quot;36 months&quot;, &quot;60… ## $ int_rate &lt;dbl&gt; 9.67, 12.12, 12.69, 12.12, 11.67, 15.10, 1… ## $ installment &lt;dbl&gt; 256.9, 39.9, 503.2, 200.8, 528.9, 371.9, 8… ## $ grade &lt;chr&gt; &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;C&quot;, &quot;C… ## $ sub_grade &lt;chr&gt; &quot;B1&quot;, &quot;B3&quot;, &quot;C2&quot;, &quot;B3&quot;, &quot;B4&quot;, &quot;C2&quot;, &quot;D1&quot;, … ## $ emp_title &lt;chr&gt; &quot;Escalation Manager&quot;, &quot;new vanderbilt reha… ## $ emp_length &lt;chr&gt; &quot;9 years&quot;, &quot;4 years&quot;, &quot;10+ years&quot;, &quot;10+ ye… ## $ home_ownership &lt;chr&gt; &quot;MORTGAGE&quot;, &quot;RENT&quot;, &quot;MORTGAGE&quot;, &quot;RENT&quot;, &quot;M… ## $ annual_inc &lt;dbl&gt; 74000, 58000, 109400, 85000, 250000, 43000… ## $ verification_status &lt;chr&gt; &quot;Verified&quot;, &quot;Not Verified&quot;, &quot;Not Verified&quot;… ## $ issue_d &lt;chr&gt; &quot;Feb-2014&quot;, &quot;Apr-2012&quot;, &quot;Jul-2015&quot;, &quot;Jul-2… ## $ loan_status &lt;chr&gt; &quot;Current&quot;, &quot;Fully Paid&quot;, &quot;Current&quot;, &quot;Fully… ## $ pymnt_plan &lt;chr&gt; &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n&quot;, &quot;n… ## $ url &lt;chr&gt; &quot;https://www.lendingclub.com/browse/loanDe… ## $ desc &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ purpose &lt;chr&gt; &quot;major_purchase&quot;, &quot;credit_card&quot;, &quot;debt_con… ## $ title &lt;chr&gt; &quot;Major purchase&quot;, &quot;Credit Card Loan&quot;, &quot;Deb… ## $ zip_code &lt;chr&gt; &quot;492xx&quot;, &quot;103xx&quot;, &quot;935xx&quot;, &quot;891xx&quot;, &quot;300xx… ## $ addr_state &lt;chr&gt; &quot;MI&quot;, &quot;NY&quot;, &quot;CA&quot;, &quot;NV&quot;, &quot;GA&quot;, &quot;NJ&quot;, &quot;IN&quot;, … ## $ dti &lt;dbl&gt; 7.49, 13.50, 26.18, 7.02, 19.65, 13.71, 22… ## $ delinq_2yrs &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, … ## $ earliest_cr_line &lt;chr&gt; &quot;Aug-2001&quot;, &quot;Jul-2003&quot;, &quot;May-1996&quot;, &quot;Mar-1… ## $ inq_last_6mths &lt;dbl&gt; 0, 0, 2, 0, 0, 1, 1, 1, 1, 3, 0, 1, 3, 3, … ## $ mths_since_last_delinq &lt;dbl&gt; 24, NA, NA, 10, NA, NA, NA, 13, 57, 17, 67… ## $ mths_since_last_record &lt;dbl&gt; NA, NA, 117, NA, NA, NA, NA, NA, NA, NA, N… ## $ open_acc &lt;dbl&gt; 6, 22, 16, 15, 18, 8, 18, 33, 16, 10, 6, 1… ## $ pub_rec &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16,… ## $ revol_bal &lt;dbl&gt; 3750, 21826, 15250, 28024, 46894, 10292, 8… ## $ revol_util &lt;dbl&gt; 61.5, 66.9, 71.0, 19.6, 68.9, 63.9, 63.1, … ## $ total_acc &lt;dbl&gt; 18, 35, 24, 38, 21, 20, 32, 52, 32, 22, 25… ## $ initial_list_status &lt;chr&gt; &quot;w&quot;, &quot;f&quot;, &quot;w&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f… ## $ out_prncp &lt;dbl&gt; 3388, 0, 12877, 0, 10002, 0, 2073, 22198, … ## $ out_prncp_inv &lt;dbl&gt; 3388, 0, 12877, 0, 10002, 0, 2073, 21846, … ## $ total_pymnt &lt;dbl&gt; 5652, 1437, 3008, 10638, 7913, 15990, 511,… ## $ total_pymnt_inv &lt;dbl&gt; 5652, 1437, 3008, 10638, 7913, 15990, 511,… ## $ total_rec_prncp &lt;dbl&gt; 4612, 1200, 2123, 9000, 5998, 15600, 327, … ## $ total_rec_int &lt;dbl&gt; 1040, 237, 886, 1638, 1916, 390, 185, 1325… ## $ total_rec_late_fee &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ recoveries &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ collection_recovery_fee &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ last_pymnt_d &lt;chr&gt; &quot;Dec-2015&quot;, &quot;Apr-2015&quot;, &quot;Jan-2016&quot;, &quot;Mar-2… ## $ last_pymnt_amnt &lt;dbl&gt; 256.9, 41.1, 503.2, 6829.8, 528.9, 15618.6… ## $ next_pymnt_d &lt;chr&gt; &quot;Feb-2016&quot;, NA, &quot;Feb-2016&quot;, NA, &quot;Feb-2016&quot;… ## $ last_credit_pull_d &lt;chr&gt; &quot;Jan-2016&quot;, &quot;Apr-2015&quot;, &quot;Jan-2016&quot;, &quot;Jan-2… ## $ collections_12_mths_ex_med &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, … ## $ mths_since_last_major_derog &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 13, 57, 35, 67… ## $ policy_code &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ application_type &lt;chr&gt; &quot;INDIVIDUAL&quot;, &quot;INDIVIDUAL&quot;, &quot;INDIVIDUAL&quot;, … ## $ annual_inc_joint &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ dti_joint &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ verification_status_joint &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ acc_now_delinq &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ tot_coll_amt &lt;dbl&gt; 313, NA, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ tot_cur_bal &lt;dbl&gt; 291589, NA, 367506, 28024, 134267, 17546, … ## $ open_acc_6m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ open_il_6m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ open_il_12m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ open_il_24m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mths_since_rcnt_il &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ total_bal_il &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ il_util &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ open_rv_12m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ open_rv_24m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ max_bal_bc &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ all_util &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ total_rev_hi_lim &lt;dbl&gt; 6100, NA, 21400, 143100, 68100, 16100, 139… ## $ inq_fi &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ total_cu_tl &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ inq_last_12m &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ Group &lt;chr&gt; &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A&quot;, &quot;A… Conduct the proper test to see if the mean of loan_amnt is different between the two groups. # Plot the A/B test results ggplot(lendingclub_ab, aes(x = Group, y = loan_amnt)) + geom_boxplot() # Conduct a two-sided t-test t.test(loan_amnt ~ Group, data = lendingclub_ab) ## ## Welch Two Sample t-test ## ## data: loan_amnt by Group ## t = -0.6, df = 997, p-value = 0.6 ## alternative hypothesis: true difference in means between group A and group B is not equal to 0 ## 95 percent confidence interval: ## -1377 748 ## sample estimates: ## mean in group A mean in group B ## 14723 15038 By looking at both the boxplot and the results of the t-test, it seems that there is no compelling evidence to support the hypothesis that there is a difference between the two A/B test groups’ mean loan_amnt, a result which you would use to help make data-driven decisions at Lending Club. 18.2.3.3 Multivariable experiments The point of an A/B test is that only one thing is changed and the effect of that change is measured. On the other hand, a multivariate experiment, is where a few things are changed (similar to a multiple factor experiment.) Let’s examine how Group, grade, and verification_status affect loan_amnt in the lendingclub_ab dataset. # Build lendingclub_multi lendingclub_multi &lt;- lm(loan_amnt ~ Group + grade + verification_status, lendingclub_ab) # Examine lendingclub_multi results tidy(lendingclub_multi) ## # A tibble: 10 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 11244. 792. 14.2 8.44e-42 ## 2 GroupB 205. 515. 0.398 6.91e- 1 ## 3 gradeB -975. 817. -1.19 2.33e- 1 ## 4 gradeC -631. 806. -0.783 4.34e- 1 ## 5 gradeD 718. 917. 0.783 4.34e- 1 ## 6 gradeE 1477. 1208. 1.22 2.22e- 1 ## 7 gradeF 5453. 1942. 2.81 5.09e- 3 ## 8 gradeG 3490. 3396. 1.03 3.04e- 1 ## 9 verification_statusSource Verified 4528. 637. 7.10 2.30e-12 ## 10 verification_statusVerified 5900. 668. 8.84 4.41e-18 From the results, verification status and having an F grade are the factors in this model that have a significant effect on loan amount. 18.3 Block Designs 18.3.1 Intro to sampling Probability Sampling: probability is used to select the sample (in various ways) Simple Random Sampling (SRS) Every unit in a population has an equal probability of being sampled. sample() Stratified Sampling Splitting your population by some strata variable. Taking a simple random sample inside of each stratified group. dataset %&gt;% group_by(strata_variable) %&gt;% slice_sample() Cluster Sampling Divide the population into groups called clusters cluster(dataset, cluster_var_name, number_to_select, method = &quot;option&quot;) Systematic Sampling Choosing a sample in a systematic way. Best implemented in R with a custom function. Multi-stage Sampling Combines one or more sampling methods. Non-probability Sampling: probability is not used to select the sample Voluntary response: Whoever agrees to respond is the sample. Convenience sampling: Subjects convenient to the researcher are chosen. 18.3.1.1 NHANES dataset construction NHANES = National Health and Nutrition Examination Survey Conducted by the National Center for Health Statistics (NCHS), a division of the Centers for Disease Control (CDC). Data collected a variety of ways, including interviews &amp; a physical exam. Questions cover medical, dental, socioeconomic, dietary, and general health-related conditions. # Import the three datasets using read_xpt() nhanes_demo &lt;- read_csv(&quot;data/nhanes_demo.csv&quot;) ## Rows: 9971 Columns: 47 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## dbl (47): seqn, sddsrvyr, ridstatr, riagendr, ridageyr, ridagemn, ridreth1, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. nhanes_medical &lt;- read_csv(&quot;data/nhanes_medicalconditions.csv&quot;) ## Warning: One or more parsing issues, call `problems()` ## on your data frame for details, e.g.: ## dat &lt;- vroom(...) ## problems(dat) ## Rows: 9575 Columns: 90 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## dbl (74): seqn, mcq010, mcq025, mcq035, mcq040, mcq050, agq030, mcq053, mcq0... ## lgl (16): mcq230c, mcq230d, mcq240aa, mcq240bb, mcq240d, mcq240dk, mcq240h, ... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. nhanes_bodymeasures &lt;- read_csv(&quot;data/nhanes_bodymeasures.csv&quot;) ## Rows: 9544 Columns: 26 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## dbl (25): seqn, bmdstats, bmxwt, bmiwt, bmxrecum, bmirecum, bmxhead, bmxht, ... ## lgl (1): bmihead ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Merge the 3 datasets you just created to create nhanes_combined nhanes_combined &lt;- list(nhanes_demo, nhanes_medical, nhanes_bodymeasures) %&gt;% Reduce(function(df1, df2) inner_join(df1, df2, by = &quot;seqn&quot;), .) glimpse(nhanes_combined) ## Rows: 9,165 ## Columns: 161 ## $ seqn &lt;dbl&gt; 83732, 83733, 83734, 83735, 83736, 83737, 83738, 83739, 83740… ## $ sddsrvyr &lt;dbl&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9… ## $ ridstatr &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ riagendr &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 1, 2, 1, 2, 2… ## $ ridageyr &lt;dbl&gt; 62, 53, 78, 56, 42, 72, 11, 4, 1, 22, 32, 18, 56, 15, 4, 46, … ## $ ridagemn &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 13, NA, NA, NA, NA, NA, NA, N… ## $ ridreth1 &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 1, 3, 2, 4, 1, 5, 4, 3, 5, 3, 4, 3, 5, 1, 2… ## $ ridreth3 &lt;dbl&gt; 3, 3, 3, 3, 4, 1, 1, 3, 2, 4, 1, 6, 4, 3, 6, 3, 4, 3, 7, 1, 2… ## $ ridexmon &lt;dbl&gt; 1, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1… ## $ ridexagm &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 141, 54, 14, NA, NA, 217, NA, 185, 52… ## $ dmqmiliz &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, 2, 2, NA, NA, 2, NA, 2, 2… ## $ dmqadfc &lt;dbl&gt; NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ dmdborn4 &lt;dbl&gt; 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1… ## $ dmdcitzn &lt;dbl&gt; 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1… ## $ dmdyrsus &lt;dbl&gt; NA, 7, NA, NA, NA, 2, NA, NA, NA, NA, 6, NA, NA, NA, 2, 8, NA… ## $ dmdeduc3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 6, NA, NA, NA, NA, 11, NA, 9, NA, NA,… ## $ dmdeduc2 &lt;dbl&gt; 5, 3, 3, 5, 4, 2, NA, NA, NA, 4, 4, NA, 3, NA, NA, 5, NA, NA,… ## $ dmdmartl &lt;dbl&gt; 1, 3, 1, 6, 3, 4, NA, NA, NA, 5, 1, NA, 3, NA, NA, 6, NA, NA,… ## $ ridexprg &lt;dbl&gt; NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA,… ## $ sialang &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ siaproxy &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 1, 1, 2, 1, 2, 2, 2, 2… ## $ siaintrp &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2… ## $ fialang &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ fiaproxy &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ fiaintrp &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2… ## $ mialang &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, NA, NA, 1, 1, NA, 1, 1, NA, 1, NA, 1, 1,… ## $ miaproxy &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, 2, NA, 2, 2, NA, 2, NA, 2, 2,… ## $ miaintrp &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, 2, NA, 2, 2, NA, 2, NA, 2, 2,… ## $ aialanga &lt;dbl&gt; 1, 1, NA, 1, 1, NA, 1, NA, NA, 1, 1, 1, 1, 1, NA, 1, NA, 1, 1… ## $ dmdhhsiz &lt;dbl&gt; 2, 1, 2, 1, 5, 5, 5, 5, 7, 3, 4, 3, 1, 3, 4, 2, 6, 5, 5, 6, 2… ## $ dmdfmsiz &lt;dbl&gt; 2, 1, 2, 1, 5, 5, 5, 5, 7, 3, 4, 3, 1, 3, 4, 2, 6, 5, 1, 6, 2… ## $ dmdhhsza &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 2, 2, 0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 1, 0… ## $ dmdhhszb &lt;dbl&gt; 0, 0, 0, 0, 2, 1, 2, 1, 2, 0, 1, 0, 0, 2, 1, 0, 0, 1, 2, 2, 0… ## $ dmdhhsze &lt;dbl&gt; 1, 0, 2, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0… ## $ dmdhrgnd &lt;dbl&gt; 1, 1, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 2… ## $ dmdhrage &lt;dbl&gt; 62, 53, 79, 56, 42, 34, 68, 35, 48, 47, 31, 55, 56, 42, 45, 4… ## $ dmdhrbr4 &lt;dbl&gt; 1, 2, 1, 1, 1, 2, 2, 1, 2, 1, 1, 2, 1, 1, NA, 2, 1, 1, NA, 1,… ## $ dmdhredu &lt;dbl&gt; 5, 3, 3, 5, 4, 5, 1, 5, 9, 4, 4, 5, 3, 4, NA, 5, 4, 4, NA, 2,… ## $ dmdhrmar &lt;dbl&gt; 1, 3, 1, 6, 3, 1, 2, 1, 5, 1, 1, 1, 3, 5, 1, 6, 5, 1, 1, 1, 6… ## $ dmdhsedu &lt;dbl&gt; 3, NA, 3, NA, NA, 5, NA, 5, NA, 5, 4, 4, NA, NA, 4, NA, NA, 4… ## $ wtint2yr &lt;dbl&gt; 134671, 24329, 12400, 102718, 17628, 11252, 9965, 44750, 9892… ## $ wtmec2yr &lt;dbl&gt; 135630, 25282, 12576, 102079, 18235, 10879, 9861, 46173, 1096… ## $ sdmvpsu &lt;dbl&gt; 1, 1, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 1, 1, 1… ## $ sdmvstra &lt;dbl&gt; 125, 125, 131, 131, 126, 128, 120, 124, 119, 128, 125, 122, 1… ## $ indhhin2 &lt;dbl&gt; 10, 4, 5, 10, 7, 14, 6, 15, 77, 7, 6, 15, 3, 4, 12, 3, 6, 14,… ## $ indfmin2 &lt;dbl&gt; 10, 4, 5, 10, 7, 14, 6, 15, 77, 7, 6, 15, 3, 4, 12, 3, 6, 14,… ## $ indfmpir &lt;dbl&gt; 4.39, 1.32, 1.51, 5.00, 1.23, 2.82, 1.18, 4.22, NA, 2.08, 1.0… ## $ mcq010 &lt;dbl&gt; 2, 2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 1… ## $ mcq025 &lt;dbl&gt; NA, NA, 60, NA, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1… ## $ mcq035 &lt;dbl&gt; NA, NA, 1, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, … ## $ mcq040 &lt;dbl&gt; NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mcq050 &lt;dbl&gt; NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ agq030 &lt;dbl&gt; NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mcq053 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ mcq080 &lt;dbl&gt; 1, 2, 1, 1, 2, 1, NA, NA, NA, 1, 2, 1, 1, NA, NA, 1, NA, 2, 2… ## $ mcq092 &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, NA, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, … ## $ mcd093 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq149 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mcq151 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160a &lt;dbl&gt; 1, 2, 1, 2, 1, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 1, NA, NA,… ## $ mcq180a &lt;dbl&gt; 40, NA, 55, NA, 10, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 4… ## $ mcq195 &lt;dbl&gt; 1, NA, 4, NA, 9, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 2, N… ## $ mcq160n &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 1, NA, NA, 2, NA, NA,… ## $ mcq180n &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 20, NA, NA, N… ## $ mcq160b &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 1, NA, NA, 2, NA, NA,… ## $ mcq180b &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 53, NA, NA, N… ## $ mcq160c &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq180c &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160d &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq180d &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160e &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 1, NA, NA, 2, NA, NA,… ## $ mcq180e &lt;dbl&gt; NA, NA, 58, NA, NA, NA, NA, NA, NA, NA, NA, NA, 53, NA, NA, N… ## $ mcq160f &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq180f &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160g &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq180g &lt;dbl&gt; NA, NA, 59, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160m &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq170m &lt;dbl&gt; NA, NA, 2, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mcq180m &lt;dbl&gt; NA, NA, 39, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160k &lt;dbl&gt; 2, 2, 2, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq170k &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq180k &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160l &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq170l &lt;dbl&gt; NA, NA, 1, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ mcq180l &lt;dbl&gt; NA, NA, 11, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq160o &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq203 &lt;dbl&gt; 2, 2, 1, 2, 2, 2, 2, NA, NA, 2, 2, 2, 2, 2, NA, 2, NA, 2, 2, … ## $ mcq206 &lt;dbl&gt; NA, NA, 11, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq220 &lt;dbl&gt; 1, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, NA, 2, NA, NA, 2, NA, NA,… ## $ mcq230a &lt;dbl&gt; 25, NA, 33, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq230b &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq230c &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq230d &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240a &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240aa &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240b &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240bb &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240c &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240cc &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240d &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240dd &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240dk &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240e &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240f &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240g &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240h &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240i &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240j &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240k &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240l &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240m &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240n &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240o &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240p &lt;dbl&gt; 58, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240q &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240r &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240s &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240t &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240u &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240v &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240w &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240x &lt;dbl&gt; NA, NA, 57, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240y &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq240z &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ mcq300a &lt;dbl&gt; 2, 2, 2, 9, 2, 2, NA, NA, NA, 1, 2, NA, 2, NA, NA, 9, NA, NA,… ## $ mcq300b &lt;dbl&gt; 2, 2, 1, 9, 2, 2, 1, NA, NA, 2, 2, 2, 2, 2, NA, 9, NA, 2, 2, … ## $ mcq300c &lt;dbl&gt; 1, 1, 2, 9, 9, 1, NA, NA, NA, 1, 1, NA, 1, NA, NA, 9, NA, NA,… ## $ mcq365a &lt;dbl&gt; 2, 2, 2, 1, 2, 2, NA, NA, NA, 2, 2, 1, 1, NA, NA, 1, NA, 2, 2… ## $ mcq365b &lt;dbl&gt; 1, 2, 1, 1, 2, 2, NA, NA, NA, 2, 2, 1, 1, NA, NA, 2, NA, 1, 2… ## $ mcq365c &lt;dbl&gt; 2, 2, 1, 2, 2, 2, NA, NA, NA, 2, 2, 1, 1, NA, NA, 1, NA, 2, 2… ## $ mcq365d &lt;dbl&gt; 2, 2, 1, 1, 2, 1, NA, NA, NA, 2, 2, 1, 1, NA, NA, 1, NA, 2, 1… ## $ mcq370a &lt;dbl&gt; 1, 2, 1, 1, 2, 2, NA, NA, NA, 1, 1, 2, 1, NA, NA, 2, NA, 2, 1… ## $ mcq370b &lt;dbl&gt; 1, 2, 2, 2, 2, 2, NA, NA, NA, 1, 1, 2, 1, NA, NA, 1, NA, 2, 1… ## $ mcq370c &lt;dbl&gt; 2, 2, 2, 1, 2, 1, NA, NA, NA, 1, 2, 1, 1, NA, NA, 1, NA, 2, 2… ## $ mcq370d &lt;dbl&gt; 2, 2, 1, 1, 2, 1, NA, NA, NA, 1, 1, 1, 1, NA, NA, 1, NA, 2, 2… ## $ osq230 &lt;dbl&gt; 1, 2, 2, 2, 2, 2, NA, NA, NA, NA, NA, NA, 2, NA, NA, 2, NA, N… ## $ bmdstats &lt;dbl&gt; 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ bmxwt &lt;dbl&gt; 94.8, 90.4, 83.4, 109.8, 55.2, 64.4, 37.2, 16.4, 10.1, 76.6, … ## $ bmiwt &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 3, NA, NA, NA, NA… ## $ bmxrecum &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmirecum &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA, NA, NA, NA… ## $ bmxhead &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmihead &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmxht &lt;dbl&gt; 184, 171, 170, 161, 165, 150, 144, 102, NA, 165, 151, 166, 17… ## $ bmiht &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmxbmi &lt;dbl&gt; 27.8, 30.8, 28.8, 42.4, 20.3, 28.6, 18.1, 15.7, NA, 28.0, 28.… ## $ bmdbmic &lt;dbl&gt; NA, NA, NA, NA, NA, NA, 2, 2, NA, NA, NA, 3, NA, 3, 2, NA, 2,… ## $ bmxleg &lt;dbl&gt; 43.3, 38.0, 35.6, 38.5, 37.4, 34.4, 32.2, NA, NA, 38.8, 34.1,… ## $ bmileg &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA… ## $ bmxarml &lt;dbl&gt; 43.6, 40.0, 37.0, 37.7, 36.0, 33.5, 30.5, 22.0, NA, 38.0, 33.… ## $ bmiarml &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, NA, NA,… ## $ bmxarmc &lt;dbl&gt; 35.9, 33.2, 31.0, 38.3, 27.2, 31.4, 21.7, 16.4, NA, 34.0, 31.… ## $ bmiarmc &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, 1, NA, NA, NA, NA,… ## $ bmxwaist &lt;dbl&gt; 101.1, 107.9, 116.5, 110.1, 80.4, 92.9, 67.5, 48.5, NA, 86.6,… ## $ bmiwaist &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA… ## $ bmxsad1 &lt;dbl&gt; 22.9, 27.5, 26.7, 25.2, NA, 23.2, 15.3, NA, NA, 19.1, 22.5, N… ## $ bmxsad2 &lt;dbl&gt; 22.7, 27.1, 26.5, 25.0, NA, 23.0, 15.3, NA, NA, 19.3, 22.1, N… ## $ bmxsad3 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmxsad4 &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ bmdavsad &lt;dbl&gt; 22.8, 27.3, 26.6, 25.1, NA, 23.1, 15.3, NA, NA, 19.2, 22.3, N… ## $ bmdsadcm &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 1, NA, NA, NA, NA… 18.3.1.2 EDA Say we have access to NHANES patients and want to conduct a study on the effect of being told by a physician to reduce calories/fat in their diet on weight. However, we suspect that there may be a difference in weight based on the gender of the patient - a blocking factor! Is there anything interesting about the NA treated patients? # find mean weight (bmxwt) in kg by our treatment (mcq365d) nhanes_combined %&gt;% group_by(mcq365d) %&gt;% summarize(mean = mean(bmxwt, na.rm = TRUE)) ## # A tibble: 4 × 2 ## mcq365d mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 90.7 ## 2 2 76.5 ## 3 9 90.8 ## 4 NA 33.5 Look at a boxplot of the IQR of patients’ weights by the treatment variable. # Fill in the ggplot2 code nhanes_combined %&gt;% ggplot(aes(as.factor(mcq365d), bmxwt)) + geom_boxplot() + labs(x = &quot;Treatment&quot;, y = &quot;Weight&quot;) ## Warning: Removed 99 rows containing non-finite values ## (`stat_boxplot()`). Children weren’t given the treatment - that’s why we see an NA age category. We also have some patients have weights missing, thus the warning that the boxplot throws. 18.3.1.3 Data Cleaning During data cleaning, we discovered that no one under the age of 16 was given the treatment. Let’s only keep patients who are greater than 16 years old in the dataset. # Filter to keep only those 16+ years old nhanes_filter &lt;- nhanes_combined %&gt;% filter(ridageyr &gt; 16) One option for dealing with the missing weights, imputation, can be implemented using the simputation package. Imputation is a technique for dealing with missing values where you replace them either with a summary statistic, like mean or median, or use a model to predict a value to use. We’ll use impute_median(), which takes a dataset and the variable to impute or formula to impute by as arguments. # Load simputation &amp; impute bmxwt by riagendr library(simputation) nhanes_final &lt;- simputation::impute_median(nhanes_filter, bmxwt ~ riagendr) Recode the nhanes_final$mcq365d variable by setting any observations with a value of 9 to 2 instead. Verify the recoding worked with count(). # Recode mcq365d with recode() &amp; examine with count() nhanes_final$mcq365d &lt;- recode(nhanes_final$mcq365d, `1` = 1, `2` = 2, `9` = 2) nhanes_final %&gt;% count(mcq365d) ## # A tibble: 2 × 2 ## mcq365d n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 1802 ## 2 2 4085 18.3.1.4 Resampling The NHANES data is collected on sampled units (people) specifically selected to represent the U.S. population. However, let’s resample the nhanes_final dataset in different ways so we get a feel for the different sampling methods. # Use slice_sample() to create nhanes_srs nhanes_srs &lt;- nhanes_final %&gt;% slice_sample(n = 2500) Stratify by riagendr and select 2000 of each gender. Confirm that it worked by using count() to examine nhanes_stratified’s gender variable. # Create nhanes_stratified with group_by() and slice_sample() nhanes_stratified &lt;- nhanes_final %&gt;% group_by(riagendr) %&gt;% slice_sample(n = 2000) nhanes_stratified %&gt;% count(riagendr) ## # A tibble: 2 × 2 ## # Groups: riagendr [2] ## riagendr n ## &lt;dbl&gt; &lt;int&gt; ## 1 1 2000 ## 2 2 2000 Use cluster() to divide nhanes_final by \"indhhin2\" into 6 clusters using the \"srswor\" method. # Load sampling package and create nhanes_cluster with cluster() library(sampling) nhanes_cluster &lt;- cluster(nhanes_final, &quot;indhhin2&quot;, 6, method = &quot;srswor&quot;) 18.3.2 Randomized Complete Block Designs (RCBD) RCBDs Randomized: the treatment is assigned randomly inside each block Complete: each treatment is used the same number of times in every block Block: experimental groups are blocked to be similar (e.g. by sex) The purpose of blocking an experiment is to make the experimental groups more like one another. Groups are blocked by a variable that is known to introduce variability that will affect the outcome of the experiment but is not of interest to study in the experiment itself. A rule of thumb in experimental design is often “block what you can, randomize what you cannot”, which means you should aim to block the effects you can control for (e.g. sex) and randomize on those you cannot (e.g. smoking status). Variability inside a block is expected to be fairly small, but variability between blocks will be larger. 18.3.2.1 Drawing RCBDs with Agricolae agricolae package enables you to “draw” some of the different experimental designs. library(agricolae) # Create designs using ls() # see all possible designs that agricolae can draw designs &lt;- ls(&quot;package:agricolae&quot;, pattern = &quot;design&quot;) designs ## [1] &quot;design.ab&quot; &quot;design.alpha&quot; &quot;design.bib&quot; &quot;design.crd&quot; ## [5] &quot;design.cyclic&quot; &quot;design.dau&quot; &quot;design.graeco&quot; &quot;design.lattice&quot; ## [9] &quot;design.lsd&quot; &quot;design.mat&quot; &quot;design.rcbd&quot; &quot;design.split&quot; ## [13] &quot;design.strip&quot; &quot;design.youden&quot; Let’s draw an RCBD design with 5 treatments and 4 blocks, which go in the r argument. # Use str() to view design.rcbd&#39;s criteria str(design.rcbd) ## function (trt, r, serie = 2, seed = 0, kinds = &quot;Super-Duper&quot;, first = TRUE, ## continue = FALSE, randomization = TRUE) # Build treats and rep treats &lt;- LETTERS[1:5] blocks &lt;- 4 # row # Build my_design_rcbd and view the sketch my_design_rcbd &lt;- design.rcbd(treats, r = blocks, seed = 42) my_design_rcbd$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;D&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;E&quot; ## [2,] &quot;E&quot; &quot;A&quot; &quot;C&quot; &quot;D&quot; &quot;B&quot; ## [3,] &quot;D&quot; &quot;B&quot; &quot;E&quot; &quot;A&quot; &quot;C&quot; ## [4,] &quot;B&quot; &quot;D&quot; &quot;E&quot; &quot;C&quot; &quot;A&quot; 18.3.2.2 NHANES RCBD Recall that our blocked experiment involved a treatment wherein the doctor asks the patient to reduce their fat or calories in their diet, and we’re testing the effect this has on weight (bmxwt). Blocking this experiment by gender means that if we observe an effect of the treatment on bmxwt, it’s more likely that the effect was actually due to the treatment versus the individual’s gender. In your R code, you denote a blocked experiment by using a formula that looks like: outcome ~ treatment + blocking_factor # Use aov() to create nhanes_rcbd nhanes_rcbd &lt;- aov(bmxwt ~ mcq365d + riagendr, nhanes_final) # Check results of nhanes_rcbd with summary() summary(nhanes_rcbd) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## mcq365d 1 229164 229164 571 &lt;0.0000000000000002 *** ## riagendr 1 163069 163069 406 &lt;0.0000000000000002 *** ## Residuals 5884 2360774 401 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Print mean weights by mcq365d and riagendr nhanes_final %&gt;% group_by(mcq365d, riagendr) %&gt;% summarize(mean_wt = mean(bmxwt)) ## `summarise()` has grouped output by ## &#39;mcq365d&#39;. You can override using the ## `.groups` argument. ## # A tibble: 4 × 3 ## # Groups: mcq365d [2] ## mcq365d riagendr mean_wt ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 95.2 ## 2 1 2 86.6 ## 3 2 1 82.7 ## 4 2 2 71.3 There truly is a mean difference in weight by gender, so blocking was a good call for this experiment. We also observed a statistically significant effect of the treatment on bmxwt. 18.3.2.3 RCBD Model Validation We can also look at Interaction plots. We hope to see parallel lines, no matter which of the block or the treatment is on the x-axis. If they are, they satisfy a key assumption of the RCBD model called Additivity. The initial diganostic plots show that this model is pretty good but not great - especially at the larger end of the data, the Q-Q plot shows the data might not be normal. # Set up the 2x2 plotting grid and plot nhanes_rcbd par(mfrow = c(2, 2)) plot(nhanes_rcbd) Interaction plot: with(dataset, interaction.plot(x.factor, trace.factor, response)) # Run the code to view the interaction plots with(nhanes_final, interaction.plot(mcq365d, riagendr, bmxwt)) # Run the code to view the interaction plots with(nhanes_final, interaction.plot(riagendr, mcq365d, bmxwt)) The interaction plots show nearly parallel lines. 18.3.3 Balanced Incomplete Block Designs (BIBD) Balanced Incomplete Block Designs Balanced: each pair of treatments occur together in a block an equal number of times Incomplete: not every treatment will appear in every block Block: experimental groups are blocked to be similar (e.g. by sex) Is there a BIBD? t = # of treatments k = # of treatments per block r = # replications \\(λ = r × \\frac{(k - 1)}{t - 1}\\) If λ is whole number, there is a BIBD. (整除才有辦法設計BIBD) # It takes as input t = number of treatments, k = number of treatments per block, and r = number of repetitions. lambda &lt;- function(t, k, r){ return((r*(k-1)) / (t-1)) } # there is BIBD lambda(2,3,4) ## [1] 8 # no BIBD lambda(3,4,11) ## [1] 16.5 18.3.3.1 Drawing BIBDs with agricolae We can also use agricolae to draw BIBDs. design.bib() takes, at minimum, the treatments (treats), an integer k corresponding to the number of levels of the blocks, and a seed as inputs. The main thing you should notice about a BIBD is that not every treatment will be used in each block (column) of the output. design.bib() will return an error message letting you know if a design is not valid. # using A, B, and C for the treatments, 4 blocks # Create my_design_bibd_1 my_design_bibd_1 &lt;- design.bib(LETTERS[1:3], k = 4, seed = 42) Error in AlgDesign::optBlock(~., withinData = factor(1:v), blocksizes = rep(k, : The number of trials must be at least as large as the minimum blocksize. # using LETTERS[1:8] for treatments, 3 blocks # Create my_design_bibd_2 my_design_bibd_2 &lt;- design.bib(LETTERS[1:8], k = 3, seed = 42) Error in rep(k, b) : invalid &#39;times&#39; argument # using A, B, C, and D as treatments, 4 blocks, and the same seed. Examine the sketch of the object. # Create my_design_bibd_3 my_design_bibd_3 &lt;- design.bib(LETTERS[1:4], k = 4, seed = 42) ## ## Parameters BIB ## ============== ## Lambda : 2 ## treatmeans : 4 ## Block size : 4 ## Blocks : 2 ## Replication: 2 ## ## Efficiency factor 1 ## ## &lt;&lt;&lt; Book &gt;&gt;&gt; my_design_bibd_3$sketch ## [,1] [,2] [,3] [,4] ## [1,] &quot;C&quot; &quot;A&quot; &quot;D&quot; &quot;B&quot; ## [2,] &quot;C&quot; &quot;D&quot; &quot;B&quot; &quot;A&quot; The blocks are now the columns. 18.3.3.2 BIBD - cat’s kidney function Say we want to test the difference between four different wet foods in cats’ diets on their kidney function. Cat food, however, is expensive, so we’ll only test 3 foods per block to save some money. The blocking factor is the color of cat, as we aren’t interested in that as part of our experiment. The outcome will be measured blood creatinine level. # make sure a BIBD is possible # Calculate lambda lambda(t = 4, k = 3, r = 3) ## [1] 2 You can see the order in which the food treatments are used in each block. # Build the data.frame creatinine &lt;- c(1.98, 1.97, 2.35, 2.09, 1.87, 1.95, 2.08, 2.01, 1.84, 2.06, 1.97, 2.22) food &lt;- as.factor(c(&quot;A&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;A&quot;, &quot;B&quot;, &quot;D&quot;)) color &lt;- as.factor(rep(c(&quot;Black&quot;, &quot;White&quot;, &quot;Orange&quot;, &quot;Spotted&quot;), each = 3)) cat_experiment &lt;- as.data.frame(cbind(creatinine, food, color)) cat_experiment ## creatinine food color ## 1 1.98 1 1 ## 2 1.97 3 1 ## 3 2.35 4 1 ## 4 2.09 1 4 ## 5 1.87 2 4 ## 6 1.95 3 4 ## 7 2.08 2 2 ## 8 2.01 3 2 ## 9 1.84 4 2 ## 10 2.06 1 3 ## 11 1.97 2 3 ## 12 2.22 4 3 Does type of wet food make a difference on creatinine levels? # Create cat_model and examine with summary() cat_model &lt;- aov(creatinine ~ food + color, data = cat_experiment) summary(cat_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## food 1 0.012 0.01204 0.53 0.49 ## color 1 0.007 0.00697 0.31 0.59 ## Residuals 9 0.205 0.02273 It seems there are no differences by type of wet food in kidney function. 18.3.3.3 NHANES BIBD Let’s jump back into the NHANES data and pretend we have access to NHANES patients ages 18-45. Blocking: by race, stored in NHANES as ridreth1. Groups, weightlift_treat: either no particular upper body weightlifting regimen, a weightlifting regimen, a weightlifting regimen plus a prescribed daily vitamin supplement. Outcome: arm circumference, bmxarmc. Those funding the study decide they want it to be a BIBD where only 2 treatments appear in each block. # Does a BIBD exist? # Calculate lambda lambda(3, 2, 2) ## [1] 1 nhanes_final &lt;- read_delim(&quot;data/nhanes_final_add_weightlift_treat.txt&quot;, delim = &quot;,&quot;) ## Warning: One or more parsing issues, call `problems()` ## on your data frame for details, e.g.: ## dat &lt;- vroom(...) ## problems(dat) ## Rows: 2452 Columns: 162 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## dbl (129): seqn, sddsrvyr, ridstatr, riagendr, ridageyr, ridreth1, ridreth3,... ## lgl (33): ridagemn, mcq149, mcq151, mcq230b, mcq230c, mcq230d, mcq240a, mcq... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Create weightlift_model &amp; examine results weightlift_model &lt;- aov(bmxarmc ~ weightlift_treat + ridreth1, nhanes_final) summary(weightlift_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## weightlift_treat 1 4 4 0.13 0.72 ## ridreth1 1 529 529 17.13 0.000036 *** ## Residuals 2334 72059 31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## 115 observations deleted due to missingness The weight lifting regimen doesn’t seem to have a significant effect on arm circumference when the patient population is blocked by race. 18.4 Squares &amp; Factorial Experiments 18.4.1 Latin squares Two blocking factors All factors must have the same number of levels Key assumption: the treatment and two blocking factors do not interact 18.4.1.1 NYC SAT Scores EDA nyc_scores dataset includes: All accredited NYC high schools SAT scores (Reading, Writing, and Math) 2014-2015 school year we’ll do experiments where we block by Borough and Teacher_Education_Level, so let’s examine math scores by those variables. nyc_scores &lt;- read_delim(&quot;data/nyc_scores_Teacher_Education.txt&quot;, delim = &quot;,&quot;) glimpse(nyc_scores) ## Rows: 435 ## Columns: 23 ## $ School_ID &lt;chr&gt; &quot;02M260&quot;, &quot;06M211&quot;, &quot;01M539&quot;, &quot;02M294&quot;, &quot;02M… ## $ School_Name &lt;chr&gt; &quot;Clinton School Writers and Artists&quot;, &quot;Inwoo… ## $ Borough &lt;chr&gt; &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manh… ## $ Building_Code &lt;chr&gt; &quot;M933&quot;, &quot;M052&quot;, &quot;M022&quot;, &quot;M445&quot;, &quot;M445&quot;, &quot;M44… ## $ Street_Address &lt;chr&gt; &quot;425 West 33rd Street&quot;, &quot;650 Academy Street&quot;… ## $ City &lt;chr&gt; &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manh… ## $ State &lt;chr&gt; &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;N… ## $ Zip_Code &lt;dbl&gt; 10001, 10002, 10002, 10002, 10002, 10002, 10… ## $ Latitude &lt;dbl&gt; 40.8, 40.9, 40.7, 40.7, 40.7, 40.7, 40.7, 40… ## $ Longitude &lt;dbl&gt; -74.0, -73.9, -74.0, -74.0, -74.0, -74.0, -7… ## $ Phone_Number &lt;chr&gt; &quot;212-695-9114&quot;, &quot;718-935-3660 &quot;, &quot;212-677-5… ## $ Start_Time &lt;time&gt; NA, 08:30:00, 08:15:00, 08:00:00, 08:… ## $ End_Time &lt;time&gt; NA, 15:00:00, 16:00:00, 14:45:00, 15:… ## $ Student_Enrollment &lt;dbl&gt; NA, 87, 1735, 358, 383, 416, 255, 545, 329, … ## $ Percent_White &lt;dbl&gt; NA, 0.03, 0.29, 0.12, 0.03, 0.02, 0.04, 0.45… ## $ Percent_Black &lt;dbl&gt; NA, 0.22, 0.13, 0.39, 0.28, 0.03, 0.24, 0.17… ## $ Percent_Hispanic &lt;dbl&gt; NA, 0.68, 0.18, 0.41, 0.57, 0.06, 0.57, 0.19… ## $ Percent_Asian &lt;dbl&gt; NA, 0.05, 0.39, 0.06, 0.09, 0.89, 0.13, 0.17… ## $ Average_Score_SAT_Math &lt;dbl&gt; NA, NA, 657, 395, 418, 613, 410, 634, 389, 4… ## $ Average_Score_SAT_Reading &lt;dbl&gt; NA, NA, 601, 411, 428, 453, 406, 641, 395, 4… ## $ Average_Score_SAT_Writing &lt;dbl&gt; NA, NA, 601, 387, 415, 463, 381, 639, 381, 3… ## $ Percent_Tested &lt;dbl&gt; NA, NA, 0.91, 0.79, 0.65, 0.96, 0.60, 0.71, … ## $ Teacher_Education_Level &lt;chr&gt; &quot;BA&quot;, &quot;BA&quot;, &quot;MA&quot;, &quot;BA&quot;, &quot;Grad Student&quot;, &quot;MA&quot;… # Mean, var, and median of Math score nyc_scores %&gt;% group_by(Borough) %&gt;% summarise(mean = mean(Average_Score_SAT_Math, na.rm = TRUE), var = var(Average_Score_SAT_Math, na.rm = TRUE), median = median(Average_Score_SAT_Math, na.rm = TRUE)) ## # A tibble: 5 × 4 ## Borough mean var median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bronx 404. 2726. 396. ## 2 Brooklyn 416. 3658. 395 ## 3 Manhattan 456. 7026. 433 ## 4 Queens 462. 5168. 448 ## 5 Staten Island 486. 6911. 466. # Mean, var, and median of Math score by Teacher Education Level nyc_scores %&gt;% group_by(Teacher_Education_Level) %&gt;% summarise(mean = mean(Average_Score_SAT_Math, na.rm = TRUE), var = var(Average_Score_SAT_Math, na.rm = TRUE), median = median(Average_Score_SAT_Math, na.rm = TRUE)) ## # A tibble: 5 × 4 ## Teacher_Education_Level mean var median ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 BA 425. 4061. 410. ## 2 College Student 430. 5361. 403 ## 3 Grad Student 428. 3567. 415 ## 4 MA 440. 6476. 418 ## 5 PhD 449. 6648. 418 # Mean, var, and median of Math score by both nyc_scores %&gt;% group_by(Borough, Teacher_Education_Level) %&gt;% summarise(mean = mean(Average_Score_SAT_Math, na.rm = TRUE), var = var(Average_Score_SAT_Math, na.rm = TRUE), median = median(Average_Score_SAT_Math, na.rm = TRUE)) ## `summarise()` has grouped output by ## &#39;Borough&#39;. You can override using the ## `.groups` argument. ## # A tibble: 25 × 5 ## # Groups: Borough [5] ## Borough Teacher_Education_Level mean var median ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bronx BA 413. 1742. 407 ## 2 Bronx College Student 400. 4984. 390 ## 3 Bronx Grad Student 390. 684. 388. ## 4 Bronx MA 411. 3591. 400 ## 5 Bronx PhD 398. 297. 398 ## 6 Brooklyn BA 406. 3589. 394. ## 7 Brooklyn College Student 430. 5370. 398. ## 8 Brooklyn Grad Student 422. 2754. 391 ## 9 Brooklyn MA 402. 2470. 394. ## 10 Brooklyn PhD 439. 3365. 450. ## # ℹ 15 more rows 18.4.1.2 Dealing with Missing If we want to use SAT scores as our outcome, we should examine missingness. Examine the pattern of missingness across all the variables in nyc_scores using miss_var_summary() from the naniar package. # Load naniar library(naniar) ## Warning: package &#39;naniar&#39; was built under R version 4.3.2 ## ## Attaching package: &#39;naniar&#39; ## The following object is masked from &#39;package:simputation&#39;: ## ## impute_median ## The following object is masked from &#39;package:assertive.base&#39;: ## ## is_na # Examine missingness with miss_var_summary() nyc_scores %&gt;% miss_var_summary() ## # A tibble: 23 × 3 ## variable n_miss pct_miss ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Average_Score_SAT_Math 60 13.8 ## 2 Average_Score_SAT_Reading 60 13.8 ## 3 Average_Score_SAT_Writing 60 13.8 ## 4 Percent_Tested 49 11.3 ## 5 Student_Enrollment 7 1.61 ## 6 Percent_White 7 1.61 ## 7 Percent_Black 7 1.61 ## 8 Percent_Hispanic 7 1.61 ## 9 Percent_Asian 7 1.61 ## 10 Start_Time 4 0.920 ## # ℹ 13 more rows There are 60 missing scores in each subject. # Impute the Math score by Borough nyc_scores_2 &lt;- nyc_scores %&gt;% simputation::impute_median(Average_Score_SAT_Math ~ Borough) # note that impute_median() returns the imputed variable as type &quot;impute&quot;. # Convert Math score to numeric nyc_scores_2$Average_Score_SAT_Math &lt;- as.numeric(nyc_scores_2$Average_Score_SAT_Math) # Examine scores by Borough in both datasets, before and after imputation nyc_scores %&gt;% group_by(Borough) %&gt;% summarize(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE)) ## # A tibble: 5 × 3 ## Borough median mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bronx 396. 404. ## 2 Brooklyn 395 416. ## 3 Manhattan 433 456. ## 4 Queens 448 462. ## 5 Staten Island 466. 486. nyc_scores_2 %&gt;% group_by(Borough) %&gt;% summarize(median = median(Average_Score_SAT_Math, na.rm = TRUE), mean = mean(Average_Score_SAT_Math, na.rm = TRUE)) ## # A tibble: 5 × 3 ## Borough median mean ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bronx 396. 403. ## 2 Brooklyn 395 414. ## 3 Manhattan 433 452. ## 4 Queens 448 460. ## 5 Staten Island 466. 486. 18.4.1.3 Drawing Latin Squares Since a Latin Square experiment has two blocking factors, you can see that in this design, each treatment appears once in both each row (blocking factor 1) and each column (blocking factor 2). [,1] [,2] [,3] [,4] [1,] &quot;B&quot; &quot;D&quot; &quot;A&quot; &quot;C&quot; [2,] &quot;A&quot; &quot;C&quot; &quot;D&quot; &quot;B&quot; [3,] &quot;D&quot; &quot;B&quot; &quot;C&quot; &quot;A&quot; [4,] &quot;C&quot; &quot;A&quot; &quot;B&quot; &quot;D&quot; Create and view the sketch of a Latin Square design, using treatments A, B, C, D, &amp; E, and a seed of 42. # Design a LS with 5 treatments A:E then look at the sketch my_design_lsd &lt;- design.lsd(trt = LETTERS[1:5], seed = 42) my_design_lsd$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;E&quot; &quot;D&quot; &quot;A&quot; &quot;C&quot; &quot;B&quot; ## [2,] &quot;D&quot; &quot;C&quot; &quot;E&quot; &quot;B&quot; &quot;A&quot; ## [3,] &quot;A&quot; &quot;E&quot; &quot;B&quot; &quot;D&quot; &quot;C&quot; ## [4,] &quot;C&quot; &quot;B&quot; &quot;D&quot; &quot;A&quot; &quot;E&quot; ## [5,] &quot;B&quot; &quot;A&quot; &quot;C&quot; &quot;E&quot; &quot;D&quot; 18.4.1.4 Latin Square with NYC SAT Scores To execute a Latin Square design on this data, suppose we want to know the effect of our tutoring program, which includes one-on-one tutoring, two small groups, and an in and after-school SAT prep class. A new dataset nyc_scores_ls is available that represents this experiment. nyc_scores_ls &lt;- read_delim(&quot;data/nyc_scores_ls.txt&quot;, delim = &quot;,&quot;) glimpse(nyc_scores_ls) ## Rows: 25 ## Columns: 24 ## $ School_ID &lt;chr&gt; &quot;12X682&quot;, &quot;09X505&quot;, &quot;12X251&quot;, &quot;08X519&quot;, &quot;11X… ## $ School_Name &lt;chr&gt; &quot;Fannie Lou Hamer Freedom High School&quot;, &quot;Bro… ## $ Borough &lt;chr&gt; &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;,… ## $ Building_Code &lt;chr&gt; &quot;X878&quot;, &quot;X460&quot;, &quot;X098&quot;, &quot;X972&quot;, &quot;X415&quot;, &quot;K26… ## $ Street_Address &lt;chr&gt; &quot;1021 Jennings Street&quot;, &quot;244 East 163rd Stre… ## $ City &lt;chr&gt; &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;,… ## $ State &lt;chr&gt; &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;N… ## $ Zip_Code &lt;dbl&gt; 10460, 10451, 10460, 10473, 10469, 11205, 11… ## $ Latitude &lt;dbl&gt; 40.8, 40.8, 40.8, 40.8, 40.9, 40.7, 40.7, 40… ## $ Longitude &lt;dbl&gt; -73.9, -73.9, -73.9, -73.9, -73.9, -74.0, -7… ## $ Phone_Number &lt;chr&gt; &quot;718-861-0521&quot;, &quot;718-410-3430&quot;, &quot;718-893-617… ## $ Start_Time &lt;time&gt; 08:45:00, 08:15:00, 08:00:00, 08:00:00, 08:… ## $ End_Time &lt;time&gt; 14:45:00, 15:00:00, 15:30:00, 14:19:00, 15:… ## $ Student_Enrollment &lt;dbl&gt; 470, 778, 358, 307, 427, 421, 174, 496, 245,… ## $ Percent_White &lt;dbl&gt; 0.00, 0.01, 0.02, 0.03, 0.09, 0.01, 0.28, 0.… ## $ Percent_Black &lt;dbl&gt; 0.29, 0.23, 0.27, 0.25, 0.29, 0.73, 0.48, 0.… ## $ Percent_Hispanic &lt;dbl&gt; 0.70, 0.72, 0.70, 0.68, 0.57, 0.23, 0.06, 0.… ## $ Percent_Asian &lt;dbl&gt; 0.00, 0.02, 0.01, 0.02, 0.03, 0.01, 0.16, 0.… ## $ Average_Score_SAT_Math &lt;dbl&gt; 345, 403, 377, 389, 418, 332, 395, 428, 344,… ## $ Average_Score_SAT_Reading &lt;dbl&gt; 347, 409, 372, 408, 432, 346, NA, 413, 380, … ## $ Average_Score_SAT_Writing &lt;dbl&gt; 339, 415, 365, 413, 436, 350, NA, 417, 379, … ## $ Percent_Tested &lt;dbl&gt; 0.75, 0.64, 0.43, 0.31, 0.73, 0.55, NA, 0.84… ## $ Teacher_Education_Level &lt;chr&gt; &quot;College Student&quot;, &quot;BA&quot;, &quot;Grad Student&quot;, &quot;MA… ## $ Tutoring_Program &lt;chr&gt; &quot;One-on-One&quot;, &quot;Small Groups (2-3)&quot;, &quot;Small G… We’ll block by Borough and Teacher_Education_Level to reduce their known variance on the score outcome. # Build nyc_scores_ls_lm nyc_scores_ls_lm &lt;- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level, data = nyc_scores_ls) # Tidy the results with broom tidy(nyc_scores_ls_lm) ## # A tibble: 13 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 373. 35.5 10.5 2.07e-7 ## 2 Tutoring_ProgramSAT Prep Class (after s… 16.2 31.1 0.520 6.12e-1 ## 3 Tutoring_ProgramSAT Prep Class (school … 6.8 31.1 0.218 8.31e-1 ## 4 Tutoring_ProgramSmall Groups (2-3) 12.4 31.1 0.398 6.97e-1 ## 5 Tutoring_ProgramSmall Groups (4-6) 15.6 31.1 0.501 6.25e-1 ## 6 BoroughBrooklyn -5.00 31.1 -0.161 8.75e-1 ## 7 BoroughManhattan 19.8 31.1 0.636 5.37e-1 ## 8 BoroughQueens 88.6 31.1 2.85 1.47e-2 ## 9 BoroughStaten Island 64.6 31.1 2.08 6.01e-2 ## 10 Teacher_Education_LevelCollege Student -18.8 31.1 -0.604 5.57e-1 ## 11 Teacher_Education_LevelGrad Student 10.8 31.1 0.347 7.35e-1 ## 12 Teacher_Education_LevelMA 12.6 31.1 0.405 6.93e-1 ## 13 Teacher_Education_LevelPhD 10.4 31.1 0.334 7.44e-1 # Examine the results with anova anova(nyc_scores_ls_lm) ## Analysis of Variance Table ## ## Response: Average_Score_SAT_Math ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tutoring_Program 4 928 232 0.10 0.982 ## Borough 4 33977 8494 3.51 0.041 * ## Teacher_Education_Level 4 3460 865 0.36 0.834 ## Residuals 12 29063 2422 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It seems that when we block for Borough of the school and Teacher_Education_Level, our Tutoring_Program isn’t having a statistically significant effect on the Math SAT score. 18.4.2 Graeco-Latin squares Graeco-Latin squares Three blocking factors All factors must have the same number of levels Key assumption: the treatment and three blocking factors do not interact 18.4.2.1 NYC SAT Scores Data Viz Create and examine the requested boxplot. How do the medians differ by Borough? How many outliers are present, and where are they mostly present? # Create a boxplot of Math scores by Borough, with a title and x/y axis labels ggplot(nyc_scores, aes(x = Borough, y = Average_Score_SAT_Math)) + geom_boxplot() + labs(title = &quot;Average SAT Math Scores by Borough, NYC&quot;, xlab = &quot;Borough (NYC)&quot;, ylab = &quot;Average SAT Math Scores (2014-15)&quot;) ## Warning: Removed 60 rows containing non-finite values ## (`stat_boxplot()`). 18.4.2.2 Drawing Graeco-Latin Squares One difference in the input to design.graeco() that we haven’t seen before is that we’ll need to input 2 vectors, trt1 and trt2, which must be of equal length. You can think of trt1 as your actual treatment; trt2 as one of your blocking factors. # Create trt1 and trt2 trt1 &lt;- LETTERS[1:5] trt2 &lt;- 1:5 # Create my_graeco_design my_graeco_design &lt;- design.graeco(trt1, trt2, seed = 42) # Examine the parameters and sketch my_graeco_design$sketch ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;D 5&quot; &quot;A 1&quot; &quot;C 3&quot; &quot;B 4&quot; &quot;E 2&quot; ## [2,] &quot;A 3&quot; &quot;C 4&quot; &quot;B 2&quot; &quot;E 5&quot; &quot;D 1&quot; ## [3,] &quot;C 2&quot; &quot;B 5&quot; &quot;E 1&quot; &quot;D 3&quot; &quot;A 4&quot; ## [4,] &quot;B 1&quot; &quot;E 3&quot; &quot;D 4&quot; &quot;A 2&quot; &quot;C 5&quot; ## [5,] &quot;E 4&quot; &quot;D 2&quot; &quot;A 5&quot; &quot;C 1&quot; &quot;B 3&quot; my_graeco_design$parameters ## $design ## [1] &quot;graeco&quot; ## ## $trt1 ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; ## ## $trt2 ## [1] 1 2 3 4 5 ## ## $r ## [1] 5 ## ## $serie ## [1] 2 ## ## $seed ## [1] 42 ## ## $kinds ## [1] &quot;Super-Duper&quot; ## ## [[8]] ## [1] TRUE You can see that this time the sketch object includes your treatment (the capital letter) and a blocking factor (the number.) 18.4.2.3 Graeco-Latin Square with NYC SAT Scores Recall that our Latin Square exercise in this chapter tested the effect of our tutoring program, blocked by Borough and Teacher_Education_Level. For our Graeco-Latin Square, say we also want to block out the known effect of Homework_Type, which indicates what kind of homework the student was given: individual only, small or large group homework, or some combination. We can add this as another blocking factor to create a Graeco-Latin Square experiment. nyc_scores_gls &lt;- read_delim(&quot;data/nyc_scores_gls.txt&quot;, delim = &quot;,&quot;) glimpse(nyc_scores_gls) ## Rows: 25 ## Columns: 25 ## $ School_ID &lt;chr&gt; &quot;11X290&quot;, &quot;12X692&quot;, &quot;07X334&quot;, &quot;08X561&quot;, &quot;10X… ## $ School_Name &lt;chr&gt; &quot;Bronx Academy of Health Careers&quot;, &quot;Monroe A… ## $ Borough &lt;chr&gt; &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;,… ## $ Building_Code &lt;chr&gt; &quot;X425&quot;, &quot;X420&quot;, &quot;X139&quot;, &quot;X450&quot;, &quot;X430&quot;, &quot;K42… ## $ Street_Address &lt;chr&gt; &quot;800 East Gun Hill Road&quot;, &quot;1300 Boynton Aven… ## $ City &lt;chr&gt; &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;, &quot;Bronx&quot;,… ## $ State &lt;chr&gt; &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;N… ## $ Zip_Code &lt;dbl&gt; 10467, 10472, 10454, 10473, 10468, 11208, 11… ## $ Latitude &lt;dbl&gt; 40.9, 40.8, 40.8, 40.8, 40.9, 40.7, 40.7, 40… ## $ Longitude &lt;dbl&gt; -73.9, -73.9, -73.9, -73.9, -73.9, -73.9, -7… ## $ Phone_Number &lt;chr&gt; &quot;718-696-3340&quot;, &quot;718-860-8160&quot;, &quot;718-665-412… ## $ Start_Time &lt;time&gt; 08:25:00, 08:30:00, 09:00:00, 08:30:00, 08:… ## $ End_Time &lt;time&gt; 15:15:00, 15:00:00, 16:00:00, 15:15:00, 15:… ## $ Student_Enrollment &lt;dbl&gt; 465, 445, 463, 332, 530, 413, 284, 648, 514,… ## $ Percent_White &lt;dbl&gt; 0.02, 0.01, 0.01, 0.01, 0.02, 0.01, 0.01, 0.… ## $ Percent_Black &lt;dbl&gt; 0.45, 0.18, 0.14, 0.35, 0.17, 0.46, 0.76, 0.… ## $ Percent_Hispanic &lt;dbl&gt; 0.47, 0.78, 0.77, 0.60, 0.77, 0.46, 0.19, 0.… ## $ Percent_Asian &lt;dbl&gt; 0.05, 0.01, 0.07, 0.02, 0.03, 0.05, 0.03, 0.… ## $ Average_Score_SAT_Math &lt;dbl&gt; 386, 361, 345, 396, 432, 395, 380, 399, 389,… ## $ Average_Score_SAT_Reading &lt;dbl&gt; 380, 354, 338, NA, 396, 376, 389, 392, 374, … ## $ Average_Score_SAT_Writing &lt;dbl&gt; 391, 351, 312, NA, 395, 359, 384, 394, 378, … ## $ Percent_Tested &lt;dbl&gt; 0.59, 0.47, 0.58, NA, 0.40, 0.57, 0.41, 0.36… ## $ Teacher_Education_Level &lt;chr&gt; &quot;College Student&quot;, &quot;BA&quot;, &quot;Grad Student&quot;, &quot;MA… ## $ Tutoring_Program &lt;chr&gt; &quot;SAT Prep Class (school hours)&quot;, &quot;SAT Prep C… ## $ Homework_Type &lt;chr&gt; &quot;Small Group&quot;, &quot;Large Group&quot;, &quot;Individual&quot;, … # Build nyc_scores_gls_lm nyc_scores_gls_lm &lt;- lm(Average_Score_SAT_Math ~ Tutoring_Program + Borough + Teacher_Education_Level + Homework_Type, data = nyc_scores_gls) # Tidy the results with broom tidy(nyc_scores_gls_lm) ## # A tibble: 17 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 351. 44.9 7.81 5.19e-5 ## 2 Tutoring_ProgramSAT Prep Class (after s… 42.0 34.5 1.22 2.58e-1 ## 3 Tutoring_ProgramSAT Prep Class (school … 45.8 34.5 1.33 2.20e-1 ## 4 Tutoring_ProgramSmall Groups (2-3) 62.4 34.5 1.81 1.08e-1 ## 5 Tutoring_ProgramSmall Groups (4-6) 51.1 34.5 1.48 1.76e-1 ## 6 BoroughBrooklyn 2.70 34.5 0.0783 9.39e-1 ## 7 BoroughManhattan 110. 34.5 3.19 1.28e-2 ## 8 BoroughQueens 56.9 34.5 1.65 1.37e-1 ## 9 BoroughStaten Island 88.3 34.5 2.56 3.35e-2 ## 10 Teacher_Education_LevelCollege Student -35.4 34.5 -1.03 3.34e-1 ## 11 Teacher_Education_LevelGrad Student 8.60 34.5 0.250 8.09e-1 ## 12 Teacher_Education_LevelMA 19.1 34.5 0.554 5.95e-1 ## 13 Teacher_Education_LevelPhD -3.00 34.5 -0.0871 9.33e-1 ## 14 Homework_TypeLarge Group -2.00 34.5 -0.0580 9.55e-1 ## 15 Homework_TypeMix of Large Group/Individ… -23.5 34.5 -0.682 5.15e-1 ## 16 Homework_TypeMix of Small Group/Individ… -9.80 34.5 -0.284 7.83e-1 ## 17 Homework_TypeSmall Group 9.60 34.5 0.279 7.88e-1 # Examine the results with anova anova(nyc_scores_gls_lm) ## Analysis of Variance Table ## ## Response: Average_Score_SAT_Math ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tutoring_Program 4 11311 2828 0.95 0.482 ## Borough 4 49138 12285 4.14 0.042 * ## Teacher_Education_Level 4 8390 2098 0.71 0.610 ## Homework_Type 4 3062 765 0.26 0.897 ## Residuals 8 23753 2969 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 When blocked out by all the other factors, our Tutoring program has no effect on the Math score. 18.4.3 Factorial experiments Factorial designs 2 or more factor variables are combined and crossed. All of the possible interactions between levels of factors are considered as effects on the outcome. 2^k factorial experiments 2^k factorial experiments involve k factor variables with 2 levels It results in 2^k number of combinations of effects to test Analyzed with a linear model and ANOVA Also use TukeyHSD() to determine which combinations are significantly different 18.4.3.1 Factorial EDA Let’s test the effect of Percent_Black_HL, Percent_Tested_HL, and Tutoring_Program on the outcome, Average_Score_SAT_Math. The HL stands for high-low, where a 1 indicates respectively that less than 50% of Black students or that less than 50% of all students in an entire school were tested, and a 2 indicates that greater than 50% of either were tested. nyc_scores &lt;- read_delim(&quot;data/nyc_scores_factorial.txt&quot;, delim = &quot;,&quot;) nyc_scores$Percent_Tested_HL &lt;- as.factor(nyc_scores$Percent_Tested_HL) nyc_scores$Percent_Black_HL &lt;- as.factor(nyc_scores$Percent_Black_HL) glimpse(nyc_scores) ## Rows: 435 ## Columns: 25 ## $ School_ID &lt;chr&gt; &quot;02M260&quot;, &quot;06M211&quot;, &quot;01M539&quot;, &quot;02M294&quot;, &quot;02M… ## $ School_Name &lt;chr&gt; &quot;Clinton School Writers and Artists&quot;, &quot;Inwoo… ## $ Borough &lt;chr&gt; &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manh… ## $ Building_Code &lt;chr&gt; &quot;M933&quot;, &quot;M052&quot;, &quot;M022&quot;, &quot;M445&quot;, &quot;M445&quot;, &quot;M44… ## $ Street_Address &lt;chr&gt; &quot;425 West 33rd Street&quot;, &quot;650 Academy Street&quot;… ## $ City &lt;chr&gt; &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manhattan&quot;, &quot;Manh… ## $ State &lt;chr&gt; &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;NY&quot;, &quot;N… ## $ Zip_Code &lt;dbl&gt; 10001, 10002, 10002, 10002, 10002, 10002, 10… ## $ Latitude &lt;dbl&gt; 40.8, 40.9, 40.7, 40.7, 40.7, 40.7, 40.7, 40… ## $ Longitude &lt;dbl&gt; -74.0, -73.9, -74.0, -74.0, -74.0, -74.0, -7… ## $ Phone_Number &lt;chr&gt; &quot;212-695-9114&quot;, &quot;718-935-3660 &quot;, &quot;212-677-5… ## $ Start_Time &lt;time&gt; NA, 08:30:00, 08:15:00, 08:00:00, 08:… ## $ End_Time &lt;time&gt; NA, 15:00:00, 16:00:00, 14:45:00, 15:… ## $ Student_Enrollment &lt;dbl&gt; NA, 87, 1735, 358, 383, 416, 255, 545, 329, … ## $ Percent_White &lt;dbl&gt; NA, 0.03, 0.29, 0.12, 0.03, 0.02, 0.04, 0.45… ## $ Percent_Black &lt;dbl&gt; 0.50, 0.22, 0.13, 0.39, 0.28, 0.03, 0.24, 0.… ## $ Percent_Hispanic &lt;dbl&gt; NA, 0.68, 0.18, 0.41, 0.57, 0.06, 0.57, 0.19… ## $ Percent_Asian &lt;dbl&gt; NA, 0.05, 0.39, 0.06, 0.09, 0.89, 0.13, 0.17… ## $ Average_Score_SAT_Math &lt;dbl&gt; 433, 433, 657, 395, 418, 613, 410, 634, 389,… ## $ Average_Score_SAT_Reading &lt;dbl&gt; NA, NA, 601, 411, 428, 453, 406, 641, 395, 4… ## $ Average_Score_SAT_Writing &lt;dbl&gt; NA, NA, 601, 387, 415, 463, 381, 639, 381, 3… ## $ Percent_Tested &lt;dbl&gt; 0.50, 0.50, 0.91, 0.79, 0.65, 0.96, 0.60, 0.… ## $ Percent_Tested_HL &lt;fct&gt; 1, 1, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 2, 2,… ## $ Percent_Black_HL &lt;fct&gt; 2, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 2, 1, 1, 1,… ## $ Tutoring_Program &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;… Build a boxplot of each factor vs. the outcome to have an idea of which have a difference in median by factor level. # Build the boxplot for the tutoring program vs. Math SAT score ggplot(nyc_scores, aes(Tutoring_Program, Average_Score_SAT_Math)) + geom_boxplot() # Build the boxplot for the percent black vs. Math SAT score ggplot(nyc_scores, aes(Percent_Black_HL, Average_Score_SAT_Math)) + geom_boxplot() # Build the boxplot for percent tested vs. Math SAT score ggplot(nyc_scores, aes(Percent_Tested_HL, Average_Score_SAT_Math)) + geom_boxplot() 18.4.3.2 Factorial Experiment with NYC SAT Scores Now we want to examine the effect of tutoring programs on the NYC schools’ SAT Math score. As noted in the last exercise: the variable Tutoring_Program is simply yes or no, depending on if a school got a tutoring program implemented. For Percent_Black_HL and Percent_Tested_HL, HL stands for high/low. A 1 indicates less than 50% Black students or overall students tested, and a 2 indicates greater than 50% of both. Remember that because we intend to test all of the possible combinations of factor levels, we need to write the formula like: outcome ~ factor1 * factor2 * factor3. # Create nyc_scores_factorial and examine the results nyc_scores_factorial &lt;- aov(Average_Score_SAT_Math ~ Percent_Tested_HL * Percent_Black_HL * Tutoring_Program, data = nyc_scores) tidy(nyc_scores_factorial) ## # A tibble: 8 × 6 ## term df sumsq meansq statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Percent_Tested_HL 1 3.89e5 3.89e5 129. 2.91e-26 ## 2 Percent_Black_HL 1 1.78e5 1.78e5 58.8 1.20e-13 ## 3 Tutoring_Program 1 5.20e3 5.20e3 1.72 1.90e- 1 ## 4 Percent_Tested_HL:Percent_Black_HL 1 1.02e5 1.02e5 33.7 1.23e- 8 ## 5 Percent_Tested_HL:Tutoring_Program 1 4.33e3 4.33e3 1.43 2.32e- 1 ## 6 Percent_Black_HL:Tutoring_Program 1 6.30e3 6.30e3 2.08 1.50e- 1 ## 7 Percent_Tested_HL:Percent_Black_HL:Tu… 1 6.20e3 6.20e3 2.05 1.53e- 1 ## 8 Residuals 427 1.29e6 3.02e3 NA NA We can see from the results that we can not reject the null hypothesis that there is no difference in score based on tutoring program availability. We can also see from the low p-values that there are some interaction effects between the Percent Black and Percent Tested and the tutoring program. Next we need to check the model. 18.4.3.3 Evaluating the Factorial Model We need to examine both if our outcome and our model residuals are normally distributed. We’ll check the normality assumption using shapiro.test(). A low p-value means we can reject the null hypothesis that the sample came from a normally distributed population. Test the outcome Average_Score_SAT_Math from nyc_scores for normality using shapiro.test(). # Use shapiro.test() to test the outcome shapiro.test(nyc_scores$Average_Score_SAT_Math) ## ## Shapiro-Wilk normality test ## ## data: nyc_scores$Average_Score_SAT_Math ## W = 0.8, p-value &lt;0.0000000000000002 # Plot nyc_scores_factorial to examine residuals par(mfrow = c(2,2)) plot(nyc_scores_factorial) The model appears to be fairly well fit, though our evidence indicates the score may not be from a normally distributed population. Looking at the Q-Q plot, we can see that towards the higher end, the points are not on the line. 18.4.4 Other experimental designs Other factorial designs (besides 2^k) including fractional factorial designs Experiments with random factors Nested designs Split plot designs Lattice designs "],["supervised-learning-classification.html", "Chapter 19 Supervised Learning: Classification 19.1 k-Nearest Neighbors (kNN) 19.2 Naive Bayes 19.3 Logistic Regression 19.4 Classification Trees", " Chapter 19 Supervised Learning: Classification 19.1 k-Nearest Neighbors (kNN) 19.1.1 Classification with Nearest Neighbors Measuring similarity with distance Many nearest neighbor learners use the Euclidean distance formula here, which measures the straight-line distance between two points. \\(dist(p, q) = \\sqrt{(p_1 - q_1)^2 + (p_2 - q_2)^2 + ... + (p_n - q_n)^2}\\) Applying nearest neighbors in R library(class) pred &lt;- knn(training_data, testing_data, training_labels) 19.1.1.1 Recognizing a road sign with kNN After several trips with a human behind the wheel, it is time for the self-driving car to attempt the test course alone. As it begins to drive away, its camera captures the following image: Stop Sign Apply a kNN classifier to help the car recognize this sign. The dataset signs is loaded in your workspace along with the data frame next_sign, which holds the observation you want to classify. library(tidyverse) signs &lt;- read_csv(&quot;data/knn_traffic_signs.csv&quot;) signs ## # A tibble: 206 × 51 ## id sample sign_type r1 g1 b1 r2 g2 b2 r3 g3 b3 ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 train pedestrian 155 228 251 135 188 101 156 227 245 ## 2 2 train pedestrian 142 217 242 166 204 44 142 217 242 ## 3 3 train pedestrian 57 54 50 187 201 68 51 51 45 ## 4 4 train pedestrian 22 35 41 171 178 26 19 27 29 ## 5 5 train pedestrian 169 179 170 231 254 27 97 107 99 ## 6 6 train pedestrian 75 67 60 131 89 53 214 144 75 ## 7 7 train pedestrian 136 149 157 200 203 107 150 167 134 ## 8 8 test pedestrian 118 105 69 244 245 67 132 123 12 ## 9 9 train pedestrian 149 225 241 34 45 1 155 226 238 ## 10 10 train pedestrian 13 34 28 5 21 11 123 154 140 ## # ℹ 196 more rows ## # ℹ 39 more variables: r4 &lt;dbl&gt;, g4 &lt;dbl&gt;, b4 &lt;dbl&gt;, r5 &lt;dbl&gt;, g5 &lt;dbl&gt;, ## # b5 &lt;dbl&gt;, r6 &lt;dbl&gt;, g6 &lt;dbl&gt;, b6 &lt;dbl&gt;, r7 &lt;dbl&gt;, g7 &lt;dbl&gt;, b7 &lt;dbl&gt;, ## # r8 &lt;dbl&gt;, g8 &lt;dbl&gt;, b8 &lt;dbl&gt;, r9 &lt;dbl&gt;, g9 &lt;dbl&gt;, b9 &lt;dbl&gt;, r10 &lt;dbl&gt;, ## # g10 &lt;dbl&gt;, b10 &lt;dbl&gt;, r11 &lt;dbl&gt;, g11 &lt;dbl&gt;, b11 &lt;dbl&gt;, r12 &lt;dbl&gt;, ## # g12 &lt;dbl&gt;, b12 &lt;dbl&gt;, r13 &lt;dbl&gt;, g13 &lt;dbl&gt;, b13 &lt;dbl&gt;, r14 &lt;dbl&gt;, ## # g14 &lt;dbl&gt;, b14 &lt;dbl&gt;, r15 &lt;dbl&gt;, g15 &lt;dbl&gt;, b15 &lt;dbl&gt;, r16 &lt;dbl&gt;, … # Testing data next_sign &lt;- signs %&gt;% filter(sample == &quot;example&quot;) %&gt;% select(-c(1:3)) str(next_sign) ## tibble [1 × 48] (S3: tbl_df/tbl/data.frame) ## $ r1 : num 204 ## $ g1 : num 227 ## $ b1 : num 220 ## $ r2 : num 196 ## $ g2 : num 59 ## $ b2 : num 51 ## $ r3 : num 202 ## $ g3 : num 67 ## $ b3 : num 59 ## $ r4 : num 204 ## $ g4 : num 227 ## $ b4 : num 220 ## $ r5 : num 236 ## $ g5 : num 250 ## $ b5 : num 234 ## $ r6 : num 242 ## $ g6 : num 252 ## $ b6 : num 235 ## $ r7 : num 205 ## $ g7 : num 148 ## $ b7 : num 131 ## $ r8 : num 190 ## $ g8 : num 50 ## $ b8 : num 43 ## $ r9 : num 179 ## $ g9 : num 70 ## $ b9 : num 57 ## $ r10: num 242 ## $ g10: num 229 ## $ b10: num 212 ## $ r11: num 190 ## $ g11: num 50 ## $ b11: num 43 ## $ r12: num 193 ## $ g12: num 51 ## $ b12: num 44 ## $ r13: num 170 ## $ g13: num 197 ## $ b13: num 196 ## $ r14: num 190 ## $ g14: num 50 ## $ b14: num 43 ## $ r15: num 190 ## $ g15: num 47 ## $ b15: num 41 ## $ r16: num 165 ## $ g16: num 195 ## $ b16: num 196 # Training data signs &lt;- signs %&gt;% filter(sample == &quot;train&quot;) %&gt;% select(-c(1, 2)) signs ## # A tibble: 146 × 49 ## sign_type r1 g1 b1 r2 g2 b2 r3 g3 b3 r4 g4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pedestrian 155 228 251 135 188 101 156 227 245 145 211 ## 2 pedestrian 142 217 242 166 204 44 142 217 242 147 219 ## 3 pedestrian 57 54 50 187 201 68 51 51 45 59 62 ## 4 pedestrian 22 35 41 171 178 26 19 27 29 19 27 ## 5 pedestrian 169 179 170 231 254 27 97 107 99 123 147 ## 6 pedestrian 75 67 60 131 89 53 214 144 75 156 169 ## 7 pedestrian 136 149 157 200 203 107 150 167 134 171 218 ## 8 pedestrian 149 225 241 34 45 1 155 226 238 147 222 ## 9 pedestrian 13 34 28 5 21 11 123 154 140 21 46 ## 10 pedestrian 123 124 107 83 61 26 116 124 115 67 67 ## # ℹ 136 more rows ## # ℹ 37 more variables: b4 &lt;dbl&gt;, r5 &lt;dbl&gt;, g5 &lt;dbl&gt;, b5 &lt;dbl&gt;, r6 &lt;dbl&gt;, ## # g6 &lt;dbl&gt;, b6 &lt;dbl&gt;, r7 &lt;dbl&gt;, g7 &lt;dbl&gt;, b7 &lt;dbl&gt;, r8 &lt;dbl&gt;, g8 &lt;dbl&gt;, ## # b8 &lt;dbl&gt;, r9 &lt;dbl&gt;, g9 &lt;dbl&gt;, b9 &lt;dbl&gt;, r10 &lt;dbl&gt;, g10 &lt;dbl&gt;, b10 &lt;dbl&gt;, ## # r11 &lt;dbl&gt;, g11 &lt;dbl&gt;, b11 &lt;dbl&gt;, r12 &lt;dbl&gt;, g12 &lt;dbl&gt;, b12 &lt;dbl&gt;, ## # r13 &lt;dbl&gt;, g13 &lt;dbl&gt;, b13 &lt;dbl&gt;, r14 &lt;dbl&gt;, g14 &lt;dbl&gt;, b14 &lt;dbl&gt;, ## # r15 &lt;dbl&gt;, g15 &lt;dbl&gt;, b15 &lt;dbl&gt;, r16 &lt;dbl&gt;, g16 &lt;dbl&gt;, b16 &lt;dbl&gt; Create a vector of sign labels to use with kNN by extracting the column sign_type from signs. Identify the next_sign using the knn() function. Set the train argument equal to the signs data frame without the first column. Set the test argument equal to the data frame next_sign. Use the vector of labels you created as the cl argument. library(class) # Create a vector of labels sign_types &lt;- signs$sign_type # Classify the next sign observed knn(train = signs[-1], test = next_sign, cl = sign_types) ## [1] stop ## Levels: pedestrian speed stop knn simply looks for the most similar example. 19.1.1.2 Exploring the traffic sign dataset To better understand how the knn() function was able to classify the stop sign, it may help to examine the training dataset it used. Each previously observed street sign was divided into a 4x4 grid, and the red, green, and blue level for each of the 16 center pixels is recorded as illustrated here. Stop Sign Data Encoding The result is a dataset that records the sign_type as well as 16 x 3 = 48 color properties of each sign. # Examine the structure of the signs dataset str(signs) ## tibble [146 × 49] (S3: tbl_df/tbl/data.frame) ## $ sign_type: chr [1:146] &quot;pedestrian&quot; &quot;pedestrian&quot; &quot;pedestrian&quot; &quot;pedestrian&quot; ... ## $ r1 : num [1:146] 155 142 57 22 169 75 136 149 13 123 ... ## $ g1 : num [1:146] 228 217 54 35 179 67 149 225 34 124 ... ## $ b1 : num [1:146] 251 242 50 41 170 60 157 241 28 107 ... ## $ r2 : num [1:146] 135 166 187 171 231 131 200 34 5 83 ... ## $ g2 : num [1:146] 188 204 201 178 254 89 203 45 21 61 ... ## $ b2 : num [1:146] 101 44 68 26 27 53 107 1 11 26 ... ## $ r3 : num [1:146] 156 142 51 19 97 214 150 155 123 116 ... ## $ g3 : num [1:146] 227 217 51 27 107 144 167 226 154 124 ... ## $ b3 : num [1:146] 245 242 45 29 99 75 134 238 140 115 ... ## $ r4 : num [1:146] 145 147 59 19 123 156 171 147 21 67 ... ## $ g4 : num [1:146] 211 219 62 27 147 169 218 222 46 67 ... ## $ b4 : num [1:146] 228 242 65 29 152 190 252 242 41 52 ... ## $ r5 : num [1:146] 166 164 156 42 221 67 171 170 36 70 ... ## $ g5 : num [1:146] 233 228 171 37 236 50 158 191 60 53 ... ## $ b5 : num [1:146] 245 229 50 3 117 36 108 113 26 26 ... ## $ r6 : num [1:146] 212 84 254 217 205 37 157 26 75 26 ... ## $ g6 : num [1:146] 254 116 255 228 225 36 186 37 108 26 ... ## $ b6 : num [1:146] 52 17 36 19 80 42 11 12 44 21 ... ## $ r7 : num [1:146] 212 217 211 221 235 44 26 34 13 52 ... ## $ g7 : num [1:146] 254 254 226 235 254 42 35 45 27 45 ... ## $ b7 : num [1:146] 11 26 70 20 60 44 10 19 25 27 ... ## $ r8 : num [1:146] 188 155 78 181 90 192 180 221 133 117 ... ## $ g8 : num [1:146] 229 203 73 183 110 131 211 249 163 109 ... ## $ b8 : num [1:146] 117 128 64 73 9 73 236 184 126 83 ... ## $ r9 : num [1:146] 170 213 220 237 216 123 129 226 83 110 ... ## $ g9 : num [1:146] 216 253 234 234 236 74 109 246 125 74 ... ## $ b9 : num [1:146] 120 51 59 44 66 22 73 59 19 12 ... ## $ r10 : num [1:146] 211 217 254 251 229 36 161 30 13 98 ... ## $ g10 : num [1:146] 254 255 255 254 255 34 190 40 27 70 ... ## $ b10 : num [1:146] 3 21 51 2 12 37 10 34 25 26 ... ## $ r11 : num [1:146] 212 217 253 235 235 44 161 34 9 20 ... ## $ g11 : num [1:146] 254 255 255 243 254 42 190 44 23 21 ... ## $ b11 : num [1:146] 19 21 44 12 60 44 6 35 18 20 ... ## $ r12 : num [1:146] 172 158 66 19 163 197 187 241 85 113 ... ## $ g12 : num [1:146] 235 225 68 27 168 114 215 255 128 76 ... ## $ b12 : num [1:146] 244 237 68 29 152 21 236 54 21 14 ... ## $ r13 : num [1:146] 172 164 69 20 124 171 141 205 83 106 ... ## $ g13 : num [1:146] 235 227 65 29 117 102 142 229 125 69 ... ## $ b13 : num [1:146] 244 237 59 34 91 26 140 46 19 9 ... ## $ r14 : num [1:146] 172 182 76 64 188 197 189 226 85 102 ... ## $ g14 : num [1:146] 228 228 84 61 205 114 171 246 128 67 ... ## $ b14 : num [1:146] 235 143 22 4 78 21 140 59 21 6 ... ## $ r15 : num [1:146] 177 171 82 211 125 123 214 235 85 106 ... ## $ g15 : num [1:146] 235 228 93 222 147 74 221 252 128 69 ... ## $ b15 : num [1:146] 244 196 17 78 20 22 201 67 21 9 ... ## $ r16 : num [1:146] 22 164 58 19 160 180 188 237 83 43 ... ## $ g16 : num [1:146] 52 227 60 27 183 107 211 254 125 29 ... ## $ b16 : num [1:146] 53 237 60 29 187 26 227 53 19 11 ... Use table() to count the number of observations of each sign type by passing it the column containing the labels. # Count the number of signs of each type table(signs$sign_type) ## ## pedestrian speed stop ## 46 49 51 Run the provided aggregate() command to see whether the average red level might vary by sign type. # Check r10&#39;s average red level by sign type aggregate(r10 ~ sign_type, data = signs, mean) ## sign_type r10 ## 1 pedestrian 113.7 ## 2 speed 80.6 ## 3 stop 132.4 As you might have expected, stop signs tend to have a higher average red value. This is how kNN identifies similar signs. 19.1.1.3 Classifying a collection of road signs Now that the autonomous vehicle has successfully stopped on its own, your team feels confident allowing the car to continue the test course. The test course includes 59 additional road signs divided into three types: At the conclusion of the trial, you are asked to measure the car’s overall performance at recognizing these signs. So is the data frame test_signs, which holds a set of observations you’ll test your model on. # whole testing data test_signs &lt;- read_csv(&quot;data/knn_traffic_signs.csv&quot;) %&gt;% filter(sample == &quot;test&quot;) %&gt;% select(-c(1, 2)) test_signs ## # A tibble: 59 × 49 ## sign_type r1 g1 b1 r2 g2 b2 r3 g3 b3 r4 g4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pedestrian 118 105 69 244 245 67 132 123 12 138 123 ## 2 pedestrian 221 244 237 52 45 26 205 233 229 203 230 ## 3 pedestrian 44 50 43 98 69 25 170 182 172 170 182 ## 4 pedestrian 78 106 102 98 125 82 65 91 75 100 122 ## 5 pedestrian 163 181 172 53 51 36 170 181 171 44 51 ## 6 pedestrian 117 137 132 116 105 67 58 53 27 37 49 ## 7 pedestrian 204 227 224 27 44 11 140 163 140 69 83 ## 8 pedestrian 69 74 60 35 52 4 62 68 58 148 156 ## 9 pedestrian 186 213 210 30 51 18 181 212 187 146 170 ## 10 pedestrian 77 94 89 147 180 100 125 156 84 51 59 ## # ℹ 49 more rows ## # ℹ 37 more variables: b4 &lt;dbl&gt;, r5 &lt;dbl&gt;, g5 &lt;dbl&gt;, b5 &lt;dbl&gt;, r6 &lt;dbl&gt;, ## # g6 &lt;dbl&gt;, b6 &lt;dbl&gt;, r7 &lt;dbl&gt;, g7 &lt;dbl&gt;, b7 &lt;dbl&gt;, r8 &lt;dbl&gt;, g8 &lt;dbl&gt;, ## # b8 &lt;dbl&gt;, r9 &lt;dbl&gt;, g9 &lt;dbl&gt;, b9 &lt;dbl&gt;, r10 &lt;dbl&gt;, g10 &lt;dbl&gt;, b10 &lt;dbl&gt;, ## # r11 &lt;dbl&gt;, g11 &lt;dbl&gt;, b11 &lt;dbl&gt;, r12 &lt;dbl&gt;, g12 &lt;dbl&gt;, b12 &lt;dbl&gt;, ## # r13 &lt;dbl&gt;, g13 &lt;dbl&gt;, b13 &lt;dbl&gt;, r14 &lt;dbl&gt;, g14 &lt;dbl&gt;, b14 &lt;dbl&gt;, ## # r15 &lt;dbl&gt;, g15 &lt;dbl&gt;, b15 &lt;dbl&gt;, r16 &lt;dbl&gt;, g16 &lt;dbl&gt;, b16 &lt;dbl&gt; Classify the test_signs data using knn(). Set train equal to the observations in signs without labels. Use test_signs for the test argument, again without labels. For the cl argument, use the vector of labels provided for you. # Use kNN to identify the test road signs sign_types &lt;- signs$sign_type signs_pred &lt;- knn(train = signs[-1], test = test_signs[-1], cl = sign_types) Use table() to explore the classifier’s performance at identifying the three sign types (the confusion matrix). Create the vector signs_actual by extracting the labels from test_signs. Pass the vector of predictions and the vector of actual signs to table() to cross tabulate them. # Create a confusion matrix of the predicted versus actual values signs_actual &lt;- test_signs$sign_type table(signs_pred, signs_actual) ## signs_actual ## signs_pred pedestrian speed stop ## pedestrian 19 2 0 ## speed 0 17 0 ## stop 0 2 19 Compute the overall accuracy of the kNN learner using the mean() function. # Compute the accuracy mean(signs_pred == signs_actual) ## [1] 0.932 The confusion matrix lets you look for patterns in the classifier’s errors. 19.1.2 ‘k’ in kNN The letter k is a variable that specifies the number of neighbors to consider when making the classification. You can imagine it as determining the size of the neighborhoods. Bigger ‘k’ is not always better A small k creates very small neighborhoods; the classifier is able to discover very subtle patterns. As this image illustrates, you might imagine it as being able to distinguish between groups even when their boundary is somewhat “fuzzy.” On the other hand, sometimes a “fuzzy” boundary is not a true pattern, but rather due to some other factor that adds randomness into the data. This is called noise. Setting k larger, as this image shows, ignores some potentially-noisy points in an effort to discover a broader, more general pattern. Choosing ‘k’ In practice, the optimal value depends on the complexity of the pattern to be learned, as well as the impact of noisy data. Some suggest a rule of thumb starting with k equal to the square root of the number of observations in the training data. 19.1.2.1 Testing other ‘k’ values By default, the knn() function in the class package uses only the single nearest neighbor. Setting a k parameter allows the algorithm to consider additional nearby neighbors. This enlarges the collection of neighbors which will vote on the predicted class. Compare k values of 1, 7, and 15 to examine the impact on traffic sign classification accuracy. Modify the knn() function call by setting k = number and again find accuracy value. # Compute the accuracy of the baseline model (default k = 1) k_1 &lt;- knn(train = signs[-1], test = test_signs[-1], cl = sign_types) mean(k_1 == signs_actual) ## [1] 0.932 # Modify the above to set k = 7 k_7 &lt;- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 7) mean(k_7 == signs_actual) ## [1] 0.966 # Set k = 15 and compare to the above k_15 &lt;- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 15) mean(k_15 == signs_actual) ## [1] 0.915 k = 7 has the higest accurancy. 19.1.2.2 Seeing how the neighbors voted When multiple nearest neighbors hold a vote, it can sometimes be useful to examine whether the voters were unanimous or widely separated. For example, knowing more about the voters’ confidence in the classification could allow an autonomous vehicle to use caution in the case there is any chance at all that a stop sign is ahead. Build a kNN model with the prob = TRUE parameter to compute the vote proportions. # Use the prob parameter to get the proportion of votes for the winning class sign_pred &lt;- knn(train = signs[-1], test = test_signs[-1], cl = sign_types, k = 7, prob = TRUE) Use the attr() function to obtain the vote proportions for the predicted class. These are stored in the attribute \"prob\". # Get the &quot;prob&quot; attribute from the predicted classes sign_prob &lt;- attr(sign_pred, &quot;prob&quot;) # Examine the first several predictions head(sign_pred) ## [1] pedestrian pedestrian pedestrian stop pedestrian pedestrian ## Levels: pedestrian speed stop # Examine the proportion of votes for the winning class # see how the confidence varies from sign to sign head(sign_prob) ## [1] 0.571 0.571 0.857 0.571 0.857 0.571 Now you can get an idea of how certain your kNN learner is about its classifications. 19.1.3 1-3.Data preparation for kNN 1/0 dummy variables Variables with different scale: allows the features with a wider range to have more influence over the distance calculation Normalizing data: Rescaling reduces the influence of extreme values on kNN’s distance function. # define a min-max normalize() function normalize &lt;- function(x) { return((x - min(x)) / (max(x) - min(x))) } 19.2 Naive Bayes 19.2.1 Understanding Bayesian methods Joint probability and independent events The joint probability of events A and B is denoted P(A and B) One event is independent of another if knowing one doesn’t give you information about how likely the other is. Conditional probability and dependent events The conditional probability of events A and B is denoted P(A | B) Knowing that one occurred tells you much about the status of the other. \\(P(A|B) = \\frac{P(A\\ and\\ B)}{P(B)}\\) Making predictions with Naive Bayes # building a Naive Bayes model library(naivebayes) model &lt;- naive_bayes(y ~ x, data) # making predictions with Naive Bayes future_predict &lt;- predict(model, future_conditions) 19.2.1.1 Computing probabilities Calculations like these are the basis of the Naive Bayes destination prediction model you’ll develop in later exercises. location &lt;- read_csv(&quot;data/locations.csv&quot;) location ## # A tibble: 2,184 × 7 ## month day weekday daytype hour hourtype location ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 4 wednesday weekday 0 night home ## 2 1 4 wednesday weekday 1 night home ## 3 1 4 wednesday weekday 2 night home ## 4 1 4 wednesday weekday 3 night home ## 5 1 4 wednesday weekday 4 night home ## 6 1 4 wednesday weekday 5 night home ## 7 1 4 wednesday weekday 6 morning home ## 8 1 4 wednesday weekday 7 morning home ## 9 1 4 wednesday weekday 8 morning home ## 10 1 4 wednesday weekday 9 morning office ## # ℹ 2,174 more rows The where9am data frame contains 91 days (thirteen weeks) worth of data in which Brett recorded his location at 9am each day as well as whether the daytype was a weekend or weekday. where9am &lt;- location %&gt;% filter(hour == 9) %&gt;% select(daytype, location) %&gt;% mutate(location = factor(location, levels = c(&quot;appointment&quot;, &quot;campus&quot;, &quot;home&quot;, &quot;office&quot;))) where9am ## # A tibble: 91 × 2 ## daytype location ## &lt;chr&gt; &lt;fct&gt; ## 1 weekday office ## 2 weekday office ## 3 weekday office ## 4 weekend home ## 5 weekend home ## 6 weekday campus ## 7 weekday home ## 8 weekday appointment ## 9 weekday office ## 10 weekday office ## # ℹ 81 more rows Using the conditional probability formula, you can compute the probability that Brett is working in the office, given that it is a weekday. \\(P(A|B) = \\frac{P(A\\ and\\ B)}{P(B)}\\) # Compute P(A) #Find P(office) using nrow() and subset() to count rows in the dataset and save the result as p_A. p_A &lt;- nrow(subset(where9am, location == &quot;office&quot;)) / nrow(where9am) # Compute P(B) # Find P(weekday), using nrow() and subset() again, and save the result as p_B. p_B &lt;- nrow(subset(where9am, daytype == &quot;weekday&quot;)) / nrow(where9am) # Compute the observed P(A and B) # Find P(office and weekday) p_AB &lt;- nrow(subset(where9am, daytype == &quot;weekday&quot; &amp; location == &quot;office&quot;)) / nrow(where9am) # Compute P(A | B) and print its value p_A_given_B &lt;- p_AB / p_B p_A_given_B ## [1] 0.6 There is a 60% chance Brett is in the office at 9am given that it is a weekday. 19.2.1.2 A simple Naive Bayes model The previous exercises showed that the probability that Brett is at work or at home at 9am is highly dependent on whether it is the weekend or a weekday. To see this finding in action, use the where9am data frame to build a Naive Bayes model on the same data. You can then use this model to predict the future: where does the model think that Brett will be at 9am on Thursday and at 9am on Saturday? # Load the naivebayes package library(naivebayes) ## naivebayes 0.9.7 loaded ## ## Attaching package: &#39;naivebayes&#39; ## The following object is masked from &#39;package:data.table&#39;: ## ## tables # Build the location prediction model locmodel &lt;- naive_bayes(location ~ daytype, data = where9am) ## Warning: naive_bayes(): Feature daytype - zero probabilities are present. ## Consider Laplace smoothing. Forecast the Thursday 9am location using predict() with the thursday9am object as the newdata argument. thursday9am &lt;- data.frame(daytype = &quot;weekday&quot;) # Predict Thursday&#39;s 9am location predict(locmodel, thursday9am) ## [1] office ## Levels: appointment campus home office Do the same for predicting the saturday9am location. saturday9am &lt;- data.frame(daytype = &quot;weekend&quot;) # Predict Saturdays&#39;s 9am location predict(locmodel, saturday9am) ## [1] home ## Levels: appointment campus home office Not surprisingly, Brett is most likely at the office at 9am on a Thursday, but at home at the same time on a Saturday! 19.2.1.3 Examining “raw” probabilities Typing the name of the model object provides the a priori (overall) and conditional probabilities of each of the model’s predictors. Alternatively, R will compute the posterior probabilities for you if the type = \"prob\" parameter is supplied to the predict() function. Using these methods, examine how the model’s predicted 9am location probability varies from day-to-day. View the computed a priori and conditional probabilities. # Examine the location prediction model locmodel ## ## ================================== Naive Bayes ================================== ## ## Call: ## naive_bayes.formula(formula = location ~ daytype, data = where9am) ## ## --------------------------------------------------------------------------------- ## ## Laplace smoothing: 0 ## ## --------------------------------------------------------------------------------- ## ## A priori probabilities: ## ## appointment campus home office ## 0.011 0.110 0.451 0.429 ## ## --------------------------------------------------------------------------------- ## ## Tables: ## ## --------------------------------------------------------------------------------- ## ::: daytype (Bernoulli) ## --------------------------------------------------------------------------------- ## ## daytype appointment campus home office ## weekday 1.000 1.000 0.366 1.000 ## weekend 0.000 0.000 0.634 0.000 ## ## --------------------------------------------------------------------------------- See the predicted probabilities for Thursday at 9am. # Obtain the predicted probabilities for Thursday at 9am predict(locmodel, thursday9am, type = &quot;prob&quot;) ## appointment campus home office ## [1,] 0.0154 0.154 0.231 0.6 Compare these to the predicted probabilities for Saturday at 9am. # Obtain the predicted probabilities for Saturday at 9am predict(locmodel, saturday9am, type = &quot;prob&quot;) ## appointment campus home office ## [1,] 0.0000384 0.000384 0.998 0.0015 Notice the predicted probability of Brett being at the office on a Saturday is essentially zero. 19.2.2 Understanding NB’s “naivety” The challenge of multiple predictors When adding more predictor, it becomes more inefficient to calculate the overlap. A “naive” simplification Naive Bayes algorithm uses a shortcut to approximate the conditional probability we hope to compute. Rather than treating the problem as the intersection of all of the related events, the algorithm makes a so-called “naive” assumption about the data. Specifically, it assumes that the events are independent. When events are independent, the joint probability can be computed by multiplying the individual probabilities. An “infrequent” problem Suppose further that one of those events has never been observed previously in combination with the outcome. And whenever zero is multiplied in a chain, the entire sequence becomes zero. The Laplace correction The solution to this problem involves adding a small number, usually ‘1’, to each event and outcome combination to eliminate this veto power. As a result, there will be at least some predicted probability for every future outcome even if it has never been seen before. 19.2.2.1 A more sophisticated location model The locations dataset records Brett’s location every hour for 13 weeks. Each hour, the tracking information includes the daytype (weekend or weekday) as well as the hourtype (morning, afternoon, evening, or night). Using this data, build a more sophisticated model to see how Brett’s predicted location not only varies by the day of week but also by the time of day. # Build a NB model of location locmodel &lt;- naive_bayes(location ~ daytype + hourtype, location) # Predict Brett&#39;s location on a weekday afternoon weekday_afternoon &lt;- data.frame(daytype = &quot;weekday&quot;, hourtype = &quot;afternoon&quot;) predict(locmodel, weekday_afternoon) ## [1] office ## Levels: appointment campus home office restaurant store theater # Predict Brett&#39;s location on a weekday evening weekday_evening &lt;- data.frame(daytype = &quot;weekday&quot;, hourtype = &quot;evening&quot;) predict(locmodel, weekday_evening) ## [1] home ## Levels: appointment campus home office restaurant store theater Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening. 19.2.2.2 Preparing for unforeseen circumstances While Brett was tracking his location over 13 weeks, he never went into the office during the weekend. Consequently, the joint probability of P(office and weekend) = 0. Explore how this impacts the predicted probability that Brett may go to work on the weekend in the future. Additionally, you can see how using the Laplace correction will allow a small chance for these types of unforeseen circumstances. naive_bayes(..., laplace = 1) Output predicted probabilities for a weekend afternoon. weekend_afternoon &lt;- data.frame(daytype = &quot;weekend&quot;, hourtype = &quot;afternoon&quot;) # Observe the predicted probabilities for a weekend afternoon predict(locmodel, weekend_afternoon, type = &quot;prob&quot;) ## appointment campus home office restaurant store theater ## [1,] 0.0246 0.00048 0.844 0.00335 0.111 0.0164 0.0000739 Create a new naive Bayes model with the Laplace smoothing parameter set to 1. # Build a new model using the Laplace correction locmodel2 &lt;- naive_bayes(location ~ daytype + hourtype, location, laplace = 1) # Observe the new predicted probabilities for a weekend afternoon predict(locmodel2, weekend_afternoon, type = &quot;prob&quot;) ## appointment campus home office restaurant store theater ## [1,] 0.0201 0.00619 0.831 0.00793 0.11 0.0187 0.00634 Adding the Laplace correction allows for the small chance that Brett might go to the office on the weekend in the future. The small probability added to every outcome ensures that they are all possible even if never previously observed. 19.3 Logistic Regression 19.3.1 Making binary predictions with regression Making predictions with logistic regression Sometimes you may need to set this threshold (0.5) higher or lower to make the model more or less aggressive. 19.3.1.1 Building simple logistic regression models The donors dataset contains 93,462 examples of people mailed in a fundraising solicitation for paralyzed military veterans. The donated column is 1 if the person made a donation in response to the mailing and 0 otherwise. This binary outcome will be the dependent variable for the logistic regression model. The remaining columns are features of the prospective donors that may influence their donation behavior. These are the model’s independent variables. donors &lt;- read_csv(&quot;data/donors.csv&quot;) # Examine the dataset to identify potential independent variables glimpse(donors) ## Rows: 93,462 ## Columns: 13 ## $ donated &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ veteran &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ bad_address &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ age &lt;dbl&gt; 60, 46, NA, 70, 78, NA, 38, NA, NA, 65, NA, 75, 72, … ## $ has_children &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1… ## $ wealth_rating &lt;dbl&gt; 0, 3, 1, 2, 1, 0, 2, 3, 1, 0, 1, 2, 1, 0, 2, 3, 2, 0… ## $ interest_veterans &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ interest_religion &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0… ## $ pet_owner &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ catalog_shopper &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ recency &lt;chr&gt; &quot;CURRENT&quot;, &quot;CURRENT&quot;, &quot;CURRENT&quot;, &quot;CURRENT&quot;, &quot;CURRENT… ## $ frequency &lt;chr&gt; &quot;FREQUENT&quot;, &quot;FREQUENT&quot;, &quot;FREQUENT&quot;, &quot;FREQUENT&quot;, &quot;FRE… ## $ money &lt;chr&gt; &quot;MEDIUM&quot;, &quot;HIGH&quot;, &quot;MEDIUM&quot;, &quot;MEDIUM&quot;, &quot;MEDIUM&quot;, &quot;MED… Count the number of occurrences of each level of the donated variable. # Explore the dependent variable table(donors$donated) ## ## 0 1 ## 88751 4711 Hypothesis about which independent variables will be predictive of the dependent variable. The bad_address column, which is set to 1 for an invalid mailing address and 0 otherwise, seems like it might reduce the chances of a donation. One might suspect that religious interest (interest_religion) Interest in veterans affairs (interest_veterans) would be associated with greater charitable giving. # Build the donation model donation_model &lt;- glm( donated ~ bad_address + interest_religion + interest_veterans, data = donors, family = &quot;binomial&quot;) # Summarize the model results summary(donation_model) ## ## Call: ## glm(formula = donated ~ bad_address + interest_religion + interest_veterans, ## family = &quot;binomial&quot;, data = donors) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.9514 0.0165 -178.66 &lt;0.0000000000000002 *** ## bad_address -0.3078 0.1435 -2.15 0.032 * ## interest_religion 0.0672 0.0507 1.33 0.185 ## interest_veterans 0.1101 0.0468 2.35 0.019 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 37316 on 93458 degrees of freedom ## AIC: 37324 ## ## Number of Fisher Scoring iterations: 5 19.3.1.2 Making a binary prediction In the previous exercise, you used the glm() function to build a logistic regression model of donor behavior. By default, predict() outputs predictions in terms of log odds unless type = \"response\" is specified. This converts the log odds to probabilities. Because a logistic regression model estimates the probability of the outcome, it is up to you to determine the threshold at which the probability implies action. One must balance the extremes of being too cautious versus being too aggressive. # Estimate the donation probability # Estimate each person&#39;s donation probability donors$donation_prob &lt;- predict(donation_model, type = &quot;response&quot;) # Find the donation probability of the average prospect # Find the actual probability that an average person would donate mean(donors$donated) ## [1] 0.0504 Use ifelse() to predict a donation if their predicted donation probability is greater than average. Then calculate the model’s accuracy. # Predict a donation if probability of donation is greater than average (0.0504) donors$donation_pred &lt;- ifelse(donors$donation_prob &gt; 0.0504, 1, 0) # Calculate the model&#39;s accuracy mean(donors$donation_pred == donors$donated) ## [1] 0.795 With an accuracy of nearly 80%, the model seems to be doing its job. But is it too good to be true? There are the limitations of accuracy. 19.3.2 Model performance tradeoffs As the previous exercise illustrated, rare events create challenges for classification models. When one outcome is very rare, predicting the opposite can result in a very high accuracy. ROC curves Provides a way to better understand a model’s ability to distinguish between positive and negative predictions the outcome of interest versus all others. The ROC curve depicts the relationship between the percentage of positive examples as it relates to the percentage of the other outcomes. The diagonal line is the baseline performance for a very poor model. The further another curve is away from this, the better it is performing. Area under the ROC curve (AUC) The baseline model that is no better than random chance has an AUC = 0.5. A perfect model has an AUC = 1. The closer the AUC is to 1, the better. Using AUC and ROC appropriately Curves of varying shapes can have the same AUC value. For this reason, it is important to look not only at the AUC but also how the shape of each curve indicates how a model is performing across the range of predictions. For example, one model may do extremely well at identifying a few easy cases at first but perform poorly on more difficult cases. Another model may do just the opposite. When AUC values are very close, it’s important to know more about how the model will be used. 19.3.2.1 Calculating ROC Curves and AUC Graphing the model’s performance better illustrates the tradeoff between a model that is overly aggressive and one that is overly passive. Create a ROC curve with roc(actual, predicted) and the columns of actual and predicted donations. # Load the pROC package library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var # Create a ROC curve ROC &lt;- roc(donors$donated, donors$donation_prob) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases Use plot() to draw the ROC object. # Plot the ROC curve plot(ROC, col = &quot;blue&quot;) Compute the area under the curve with auc(). # Calculate the area under the curve (AUC) auc(ROC) ## Area under the curve: 0.51 Based on this visualization, the model isn’t doing much better than baseline—a model doing nothing but making predictions at random. 19.3.3 Dummy variables, missing data, and interactions 19.3.3.1 Dummy coding Sometimes a dataset contains numeric values that represent a categorical feature. In the donors dataset, wealth_rating uses numbers to indicate the donor’s wealth level: 0 = Unknown 1 = Low 2 = Medium 3 = High This exercise illustrates how to prepare this type of categorical feature and examines its impact on a logistic regression model. Create a factor wealth_levels from the numeric wealth_rating with labels as shown above. Then, use relevel() to change the reference category to Medium. (參照組) Build a logistic regression model using the column wealth_levels to predict donated and display the result with summary(). # Convert the wealth rating to a factor donors$wealth_levels &lt;- factor(donors$wealth_rating, levels = c(0, 1, 2, 3), labels = c(&quot;Unknown&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;)) # Use relevel() to change reference category donors$wealth_levels &lt;- relevel(donors$wealth_levels, ref = &quot;Medium&quot;) # See how our factor coding impacts the model summary(glm(donated ~ wealth_levels, donors, family = &quot;binomial&quot;)) ## ## Call: ## glm(formula = donated ~ wealth_levels, family = &quot;binomial&quot;, data = donors) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.9189 0.0361 -80.77 &lt;0.0000000000000002 *** ## wealth_levelsUnknown -0.0437 0.0424 -1.03 0.30 ## wealth_levelsLow -0.0524 0.0533 -0.98 0.33 ## wealth_levelsHigh 0.0480 0.0477 1.01 0.31 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 37323 on 93458 degrees of freedom ## AIC: 37331 ## ## Number of Fisher Scoring iterations: 5 19.3.3.2 Handling missing data Some of the prospective donors have missing age data. Unfortunately, R will exclude any cases with NA values when building a regression model. One workaround is to replace, or impute, the missing values with an estimated value. After doing so, you may also create a missing data indicator to model the possibility that cases with missing data are different in some way from those without. # Find the average age among non-missing values summary(donors$age) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1 48 62 62 75 98 22546 Use ifelse() and the test is.na(donors$age) to impute the average (rounded to 2 decimal places) for cases with missing age. Be sure to also ignore NAs. Create a binary dummy variable named missing_age indicating the presence of missing data using another ifelse() call and the same test. # Impute missing age values with the mean age donors$imputed_age &lt;- ifelse(is.na(donors$age), 61.65, donors$age) # Create missing value indicator for age donors$missing_age &lt;- ifelse(is.na(donors$age), 1, 0) head(donors %&gt;% select(age, imputed_age, missing_age)) ## # A tibble: 6 × 3 ## age imputed_age missing_age ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 60 60 0 ## 2 46 46 0 ## 3 NA 61.6 1 ## 4 70 70 0 ## 5 78 78 0 ## 6 NA 61.6 1 Sometimes missing data has to be dealt with using more complicated methods. 19.3.3.3 Building a interaction model One of the best predictors of future giving is a history of recent, frequent, and large gifts. In marketing terms, this is known as R/F/M: Recency Frequency Money Donors that haven’t given both recently and frequently may be especially likely to give again; in other words, the combined impact of recency and frequency may be greater than the sum of the separate effects. Because these predictors together have a greater impact on the dependent variable, their joint effect must be modeled as an interaction. Create a logistic regression model of donated as a function of money plus the interaction of recency and frequency. # Build a recency, frequency, and money (RFM) model rfm_model &lt;- glm(donated ~ money + recency * frequency, donors, family = &quot;binomial&quot;) # Summarize the RFM model to see how the parameters were coded summary(rfm_model) ## ## Call: ## glm(formula = donated ~ money + recency * frequency, family = &quot;binomial&quot;, ## data = donors) ## ## Coefficients: ## Estimate Std. Error z value ## (Intercept) -3.0114 0.0428 -70.38 ## moneyMEDIUM 0.3619 0.0430 8.41 ## recencyLAPSED -0.8668 0.4143 -2.09 ## frequencyINFREQUENT -0.5015 0.0311 -16.14 ## recencyLAPSED:frequencyINFREQUENT 1.0179 0.5171 1.97 ## Pr(&gt;|z|) ## (Intercept) &lt;0.0000000000000002 *** ## moneyMEDIUM &lt;0.0000000000000002 *** ## recencyLAPSED 0.036 * ## frequencyINFREQUENT &lt;0.0000000000000002 *** ## recencyLAPSED:frequencyINFREQUENT 0.049 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 37330 on 93461 degrees of freedom ## Residual deviance: 36938 on 93457 degrees of freedom ## AIC: 36948 ## ## Number of Fisher Scoring iterations: 6 Compute the AUC for the new model with the function auc() and compare performance to the simpler model. # Compute predicted probabilities for the RFM model rfm_prob &lt;- predict(rfm_model, type = &quot;response&quot;) # Plot the ROC curve and find AUC for the new model ROC &lt;- roc(donors$donated, rfm_prob) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(ROC, col = &quot;red&quot;) auc(ROC) ## Area under the curve: 0.578 Based on the ROC curve, you’ve confirmed that past giving patterns are certainly predictive of future giving. 19.3.4 Automatic feature selection Stepwise regression Backward stepwise Begins with a model containing all of the predictors. It then checks to see what happens when each one of the predictors is removed from the model. If removing a predictor does not substantially impact the model’s ability to predict the outcome, then it can be safely deleted. At each step, the predictor that impacts the model the least is removed. This continues step-by-step until only important predictors remain. Forward stepwise Beginning with a model containing no predictors. It examines each potential predictor to see which one, if any, offers the greatest improvement to the model’s predictive power. Predictors are added step-by-step until no new predictors add substantial value to the model. It is possible that the two could come to completely different conclusions about the most important predictors. 19.3.4.1 Building a stepwise regression model In the absence of subject-matter expertise, stepwise regression can assist with the search for the most important predictors of the outcome of interest. In this exercise, you will use a forward stepwise approach to add predictors to the model one-by-one until no additional benefit is seen. Specify the base model with no predictors. Specify the model with all predictors. # Specify a null model with no predictors null_model &lt;- glm(donated ~ 1, data = donors, family = &quot;binomial&quot;) # Specify the full model using all of the potential predictors full_model &lt;- glm(donated ~ ., data = donors, family = &quot;binomial&quot;) Apply step() to these models to perform forward stepwise regression. step(null, scope = list(lower = null, upper = full), direction = \"forward\") # Use a forward stepwise algorithm to build a parsimonious model step_model &lt;- step(null_model, scope = list(lower = null_model, upper = full_model), direction = &quot;forward&quot;) ## Start: AIC=37332 ## donated ~ 1 ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + frequency 1 28502 37122 ## + money 1 28621 37241 ## + wealth_rating 1 28705 37326 ## + has_children 1 28705 37326 ## + age 1 28707 37328 ## + imputed_age 1 28707 37328 ## + wealth_levels 3 28704 37328 ## + interest_veterans 1 28709 37330 ## + donation_prob 1 28710 37330 ## + donation_pred 1 28710 37330 ## + catalog_shopper 1 28710 37330 ## + pet_owner 1 28711 37331 ## &lt;none&gt; 28714 37332 ## + interest_religion 1 28712 37333 ## + recency 1 28713 37333 ## + bad_address 1 28714 37334 ## + veteran 1 28714 37334 ## ## Step: AIC=37025 ## donated ~ frequency ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + money 1 28441 36966 ## + wealth_rating 1 28493 37018 ## + wealth_levels 3 28490 37019 ## + has_children 1 28494 37019 ## + donation_prob 1 28498 37023 ## + interest_veterans 1 28498 37023 ## + catalog_shopper 1 28499 37024 ## + donation_pred 1 28499 37024 ## + age 1 28499 37024 ## + imputed_age 1 28499 37024 ## + pet_owner 1 28499 37024 ## &lt;none&gt; 28502 37025 ## + interest_religion 1 28501 37026 ## + recency 1 28501 37026 ## + bad_address 1 28502 37026 ## + veteran 1 28502 37027 ## ## Step: AIC=36950 ## donated ~ frequency + money ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + wealth_levels 3 28427 36942 ## + wealth_rating 1 28431 36942 ## + has_children 1 28432 36943 ## + interest_veterans 1 28438 36948 ## + donation_prob 1 28438 36949 ## + catalog_shopper 1 28438 36949 ## + donation_pred 1 28439 36949 ## + age 1 28439 36949 ## + imputed_age 1 28439 36949 ## + pet_owner 1 28439 36949 ## &lt;none&gt; 28441 36950 ## + interest_religion 1 28440 36951 ## + recency 1 28440 36951 ## + bad_address 1 28441 36951 ## + veteran 1 28441 36952 ## ## Step: AIC=36945 ## donated ~ frequency + money + wealth_levels ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + has_children 1 28416 36937 ## + age 1 28424 36944 ## + imputed_age 1 28424 36944 ## + interest_veterans 1 28424 36945 ## + donation_prob 1 28424 36945 ## + catalog_shopper 1 28424 36945 ## + donation_pred 1 28425 36945 ## &lt;none&gt; 28427 36945 ## + pet_owner 1 28425 36946 ## + interest_religion 1 28426 36947 ## + recency 1 28426 36947 ## + bad_address 1 28427 36947 ## + veteran 1 28427 36947 ## ## Step: AIC=36938 ## donated ~ frequency + money + wealth_levels + has_children ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## + pet_owner 1 28413 36937 ## + donation_prob 1 28413 36937 ## + catalog_shopper 1 28413 36937 ## + interest_veterans 1 28413 36937 ## + donation_pred 1 28414 36938 ## &lt;none&gt; 28416 36938 ## + interest_religion 1 28415 36939 ## + age 1 28416 36940 ## + imputed_age 1 28416 36940 ## + recency 1 28416 36940 ## + bad_address 1 28416 36940 ## + veteran 1 28416 36940 ## ## Step: AIC=36932 ## donated ~ frequency + money + wealth_levels + has_children + ## pet_owner ## Warning in add1.glm(fit, scope$add, scale = scale, trace = trace, k = k, : ## using the 70916/93462 rows from a combined fit ## Df Deviance AIC ## &lt;none&gt; 28413 36932 ## + donation_prob 1 28411 36932 ## + interest_veterans 1 28411 36932 ## + catalog_shopper 1 28412 36933 ## + donation_pred 1 28412 36933 ## + age 1 28412 36933 ## + imputed_age 1 28412 36933 ## + recency 1 28413 36934 ## + interest_religion 1 28413 36934 ## + bad_address 1 28413 36934 ## + veteran 1 28413 36934 # Estimate the stepwise donation probability step_prob &lt;- predict(step_model, type = &quot;response&quot;) # Plot the ROC of the stepwise model ROC &lt;- roc(donors$donated, step_prob) ## Setting levels: control = 0, case = 1 ## Setting direction: controls &lt; cases plot(ROC, col = &quot;red&quot;) auc(ROC) ## Area under the curve: 0.585 Despite the caveats of stepwise regression, it seems to have resulted in a relatively strong model! 19.4 Classification Trees 19.4.1 Making decisions with trees A decision tree model Beginning at the root node, data flows through if/else decision nodes that split the data according to its attributes. The branches indicate the potential choices. The leaf nodes denote the final decisions. These are also known as terminal nodes because they terminate the decision making process. Building trees in R # building a simple rpart classification tree library(rpart) model &lt;- rpart(y ~ x, data, method = &quot;class&quot;) # making predictions from an rpart tree p &lt;- predict(model, test_data, type = &quot;class&quot;) 19.4.1.1 Building a simple decision tree The loans dataset contains 11,312 randomly-selected people who applied for and later received loans from Lending Club, a US-based peer-to-peer lending company. You will use a decision tree to try to learn patterns in the outcome of these loans (either repaid or default) based on the requested loan amount and credit score at the time of application. Then, see how the tree’s predictions differ for an applicant with good credit versus one with bad credit. loans &lt;- read_csv(&quot;data/loans.csv&quot;) glimpse(loans) ## Rows: 39,732 ## Columns: 16 ## $ keep &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, … ## $ rand &lt;dbl&gt; 0.1305, 0.9982, 0.6283, 0.2524, 0.4745, 0.7499, 0.8… ## $ default &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, … ## $ loan_amount &lt;chr&gt; &quot;LOW&quot;, &quot;LOW&quot;, &quot;LOW&quot;, &quot;MEDIUM&quot;, &quot;LOW&quot;, &quot;LOW&quot;, &quot;MEDIU… ## $ emp_length &lt;chr&gt; &quot;10+ years&quot;, &quot;&lt; 2 years&quot;, &quot;10+ years&quot;, &quot;10+ years&quot;,… ## $ home_ownership &lt;chr&gt; &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RENT&quot;, &quot;RE… ## $ income &lt;chr&gt; &quot;LOW&quot;, &quot;LOW&quot;, &quot;LOW&quot;, &quot;MEDIUM&quot;, &quot;HIGH&quot;, &quot;LOW&quot;, &quot;MEDI… ## $ loan_purpose &lt;chr&gt; &quot;credit_card&quot;, &quot;car&quot;, &quot;small_business&quot;, &quot;other&quot;, &quot;o… ## $ debt_to_income &lt;chr&gt; &quot;HIGH&quot;, &quot;LOW&quot;, &quot;AVERAGE&quot;, &quot;HIGH&quot;, &quot;AVERAGE&quot;, &quot;AVERA… ## $ credit_score &lt;chr&gt; &quot;AVERAGE&quot;, &quot;AVERAGE&quot;, &quot;AVERAGE&quot;, &quot;AVERAGE&quot;, &quot;AVERAG… ## $ recent_inquiry &lt;chr&gt; &quot;YES&quot;, &quot;YES&quot;, &quot;YES&quot;, &quot;YES&quot;, &quot;NO&quot;, &quot;YES&quot;, &quot;YES&quot;, &quot;YE… ## $ delinquent &lt;chr&gt; &quot;NEVER&quot;, &quot;NEVER&quot;, &quot;NEVER&quot;, &quot;MORE THAN 2 YEARS AGO&quot;,… ## $ credit_accounts &lt;chr&gt; &quot;FEW&quot;, &quot;FEW&quot;, &quot;FEW&quot;, &quot;AVERAGE&quot;, &quot;MANY&quot;, &quot;AVERAGE&quot;, … ## $ bad_public_record &lt;chr&gt; &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO… ## $ credit_utilization &lt;chr&gt; &quot;HIGH&quot;, &quot;LOW&quot;, &quot;HIGH&quot;, &quot;LOW&quot;, &quot;MEDIUM&quot;, &quot;MEDIUM&quot;, &quot;… ## $ past_bankrupt &lt;chr&gt; &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO… # Setup dataset loans &lt;- loans %&gt;% filter(keep == 1) %&gt;% mutate(outcome = factor(default, levels = c(0, 1), labels = c(&quot;repaid&quot;, &quot;default&quot;))) %&gt;% select(-c(keep, rand, default)) loans$outcome &lt;- relevel(loans$outcome, ref = &quot;default&quot;) # view structure str(loans) ## tibble [11,312 × 14] (S3: tbl_df/tbl/data.frame) ## $ loan_amount : chr [1:11312] &quot;LOW&quot; &quot;LOW&quot; &quot;LOW&quot; &quot;MEDIUM&quot; ... ## $ emp_length : chr [1:11312] &quot;10+ years&quot; &quot;&lt; 2 years&quot; &quot;6 - 9 years&quot; &quot;2 - 5 years&quot; ... ## $ home_ownership : chr [1:11312] &quot;RENT&quot; &quot;RENT&quot; &quot;RENT&quot; &quot;OWN&quot; ... ## $ income : chr [1:11312] &quot;LOW&quot; &quot;LOW&quot; &quot;MEDIUM&quot; &quot;MEDIUM&quot; ... ## $ loan_purpose : chr [1:11312] &quot;credit_card&quot; &quot;car&quot; &quot;car&quot; &quot;small_business&quot; ... ## $ debt_to_income : chr [1:11312] &quot;HIGH&quot; &quot;LOW&quot; &quot;LOW&quot; &quot;LOW&quot; ... ## $ credit_score : chr [1:11312] &quot;AVERAGE&quot; &quot;AVERAGE&quot; &quot;LOW&quot; &quot;AVERAGE&quot; ... ## $ recent_inquiry : chr [1:11312] &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; &quot;YES&quot; ... ## $ delinquent : chr [1:11312] &quot;NEVER&quot; &quot;NEVER&quot; &quot;NEVER&quot; &quot;NEVER&quot; ... ## $ credit_accounts : chr [1:11312] &quot;FEW&quot; &quot;FEW&quot; &quot;FEW&quot; &quot;AVERAGE&quot; ... ## $ bad_public_record : chr [1:11312] &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; ... ## $ credit_utilization: chr [1:11312] &quot;HIGH&quot; &quot;LOW&quot; &quot;HIGH&quot; &quot;MEDIUM&quot; ... ## $ past_bankrupt : chr [1:11312] &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; &quot;NO&quot; ... ## $ outcome : Factor w/ 2 levels &quot;default&quot;,&quot;repaid&quot;: 2 1 2 1 1 1 1 2 1 1 ... # view outcome size table(loans$outcome) ## ## default repaid ## 5654 5658 Fit a decision tree model. # Load the rpart package library(rpart) # Build a lending model predicting loan outcome versus loan amount and credit score loan_model &lt;- rpart(outcome ~ loan_amount + credit_score, data = loans, method = &quot;class&quot;, control = rpart.control(cp = 0)) Use predict() with the resulting loan model to predict the outcome for the good_credit applicant. good_credit &lt;- head(loans %&gt;% filter(home_ownership == &quot;MORTGAGE&quot;), 1) # Make a prediction for someone with good credit predict(loan_model, good_credit, type = &quot;class&quot;) ## 1 ## repaid ## Levels: default repaid Do the same for the bad_credit applicant. bad_credit &lt;- head(loans %&gt;% filter(home_ownership == &quot;RENT&quot;, debt_to_income == &quot;LOW&quot;, emp_length == &quot;6 - 9 years&quot;), 1) # Make a prediction for someone with bad credit predict(loan_model, bad_credit, type = &quot;class&quot;) ## 1 ## default ## Levels: default repaid 19.4.1.2 Visualizing classification trees Due to government rules to prevent illegal discrimination, lenders are required to explain why a loan application was rejected. The structure of classification trees can be depicted visually, rpart.plot(), which helps to understand how the tree makes its decisions. Type loan_model to see a text representation of the classification tree. # Examine the loan_model object loan_model ## n= 11312 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 11312 5650 repaid (0.500 0.500) ## 2) credit_score=AVERAGE,LOW 9490 4440 default (0.532 0.468) ## 4) credit_score=LOW 1667 631 default (0.621 0.379) * ## 5) credit_score=AVERAGE 7823 3810 default (0.513 0.487) ## 10) loan_amount=HIGH 2472 1080 default (0.564 0.436) * ## 11) loan_amount=LOW,MEDIUM 5351 2620 repaid (0.490 0.510) ## 22) loan_amount=LOW 1810 874 default (0.517 0.483) * ## 23) loan_amount=MEDIUM 3541 1690 repaid (0.477 0.523) * ## 3) credit_score=HIGH 1822 601 repaid (0.330 0.670) * Apply the rpart.plot() to the loan model to visualize the tree. # Load the rpart.plot package library(rpart.plot) # Plot the loan_model with default settings rpart.plot(loan_model) Changing other plotting parameters. # Plot the loan_model with customized settings rpart.plot(loan_model, type = 3, box.palette = c(&quot;red&quot;, &quot;green&quot;), fallen.leaves = TRUE) 19.4.2 Growing larger classification trees Axis-parallel splits Decision tree always creates axis-parallel splits (divide-and-conquer process). The split that produces the purest partitions will be used first. As the tree continues to grow, it creates smaller and more homogeneous partitions. However, decision tree can be overly complex. The problem of overfitting When a tree has grown overly large and overly complex, it may experience the problem of overfitting. Rather than modeling the most important trends in the data, a tree that has been over-fitted tends to model the noise. Because it perfectly classifies every training example correctly does not mean it will do so on unseen data Evaluating model performance train / test split dataset If the tree performs much more poorly on the test set than the training set, it suggests the model may have been over-fitted. 19.4.2.1 Creating random test datasets Before building a more sophisticated lending model, it is important to hold out a portion of the loan data to simulate how well it will predict the outcomes of future loan applicants. The sample() function can be used to generate a random sample of rows to include in the training set. Simply supply it the total number of observations and the number needed for training. Use the resulting vector of row IDs to subset the loans into training and testing datasets. # Determine the number of rows for training nrow(loans) * 0.75 ## [1] 8484 Use the sample() function to create an integer vector of row IDs for the 75% sample. The first argument of sample() should be the number of rows in the data set, and the second is the number of rows you need in your training set. Subset the loans data using the row IDs to create the training dataset. Save this as loans_train. Subset loans again, but this time select all the rows that are not in sample_rows. Save this as loans_test . # Create a random sample of row IDs sample_rows &lt;- sample(nrow(loans), 8484) # Create the training dataset loans_train &lt;- loans[sample_rows, ] # Create the test dataset loans_test &lt;- loans[-sample_rows, ] list(train = dim(loans_train), test = dim(loans_test)) ## $train ## [1] 8484 14 ## ## $test ## [1] 2828 14 19.4.2.2 Building and evaluating a larger tree Lending Club has additional information about the applicants, such as home ownership status, length of employment, loan purpose, and past bankruptcies, that may be useful for making more accurate predictions. Using all of the available applicant data, build a more sophisticated lending model using the random training dataset created previously. Then, use this model to make predictions on the testing dataset to estimate the performance of the model on future loan applications. # Grow a tree using all of the available applicant data loan_model &lt;- rpart(outcome ~ ., data = loans_train, method = &quot;class&quot;, control = rpart.control(cp = 0)) # Make predictions on the test dataset. Don&#39;t forget the type argument. loans_test$pred &lt;- predict(loan_model, loans_test, type = &quot;class&quot;) # Examine the confusion matrix table(loans_test$pred, loans_test$outcome) ## ## default repaid ## default 789 621 ## repaid 621 797 Compute the accuracy of the predictions. # Compute the accuracy on the test dataset mean(loans_test$pred == loans_test$outcome) ## [1] 0.561 19.4.3 Tending to classification trees Pre-pruning Post-pruning 19.4.3.1 Preventing overgrown trees Using the pre-pruning methods for early stopping, you can prevent a tree from growing too large and complex. See how the rpart control options for maximum tree depth and minimum split count impact the resulting tree. rpart(..., control = rpart.control(cp, maxdepth, minsplit)) See how the test set accuracy of the simpler model compares to the original accuracy of 57%. # Grow a tree with maxdepth of 6 loan_model &lt;- rpart(outcome ~ ., loans_train, method = &quot;class&quot;, control = rpart.control(cp = 0, maxdepth = 6)) # Make a class prediction on the test set loans_test$pred &lt;- predict(loan_model, loans_test, type = &quot;class&quot;) # Compute the accuracy of the simpler tree mean(loans_test$pred == loans_test$outcome) ## [1] 0.588 # Swap maxdepth for a minimum split of 500 loan_model &lt;- rpart(outcome ~ ., data = loans_train, method = &quot;class&quot;, control = rpart.control(cp = 0, minsplit = 500)) # Run this. How does the accuracy change? loans_test$pred &lt;- predict(loan_model, loans_test, type = &quot;class&quot;) mean(loans_test$pred == loans_test$outcome) ## [1] 0.606 Creating a simpler decision tree may actually result in greater performance on the test dataset. 19.4.3.2 Creating a pruned tree Stopping a tree from growing all the way can lead it to ignore some aspects of the data or miss important trends it may have discovered later. By using post-pruning, you can intentionally grow a large and complex tree then prune it to be smaller and more efficient later on. In this exercise, you will have the opportunity to construct a visualization of the tree’s performance versus complexity, and use this information to prune the tree to an appropriate level. Use all of the applicant variables and no pre-pruning to create an overly complex tree. Make sure to set cp = 0 in rpart.control() to prevent pre-pruning. Create a complexity plot by using plotcp() on the model. # Grow an overly complex tree loan_model &lt;- rpart(outcome ~ ., loans_train, method = &quot;class&quot;, control = rpart.control(cp = 0)) # Examine the complexity plot plotcp(loan_model) Based on the complexity plot, prune the tree to a complexity of 0.0014 by using the prune(model, cp) . Compare the accuracy of the pruned tree to the original accuracy. # Prune the tree loan_model_pruned &lt;- prune(loan_model, cp = 0.0014) # Compute the accuracy of the pruned tree loans_test$pred &lt;- predict(loan_model_pruned, loans_test, type = &quot;class&quot;) mean(loans_test$pred == loans_test$outcome) ## [1] 0.609 As with pre-pruning, creating a simpler tree actually improved the performance of the tree on the test dataset. 19.4.4 Seeing the forest from the trees Making decisions as an ensemble Allocating each tree a random subset of data. So, both the features and examples may differ from tree to tree. Though each tree may reflect only a narrow portion of the data, the overall consensus is strengthened by these diverse perspectives. All ensemble methods are based on the principle that weaker learners become stronger with teamwork. In a random forest, each tree is asked to make a prediction, and the group’s overall prediction is determined by a majority vote. The teamwork-based approach of the random forest may help it find important trends a single tree may miss. Random forests in R # building a simple random forest library(randomForest) m &lt;- randomForest(y ~ x, data, ntree = num, # number of trees in the forest mtry = sqrt(p)) # number of predictors (p) per tree # making predictions from a random forest p &lt;- predict(m, test_data) 19.4.4.1 Building a random forest model Using the randomForest package, build a random forest and see how it compares to the single trees you built previously. Keep in mind that due to the random nature of the forest, the results may vary slightly each time you create the forest. Build a random forest model using all of the loan application variables. Compute the accuracy of the random forest model to compare to the original tree’s accuracy. # Load the randomForest package library(randomForest) # Build a random forest model loan_model &lt;- randomForest(outcome ~ ., data = loans_train) # Compute the accuracy of the random forest loans_test$pred &lt;- predict(loan_model, loans_test) mean(loans_test$pred == loans_test$outcome) ## [1] 0.599 # See the model loan_model ## ## Call: ## randomForest(formula = outcome ~ ., data = loans_train) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 3 ## ## OOB estimate of error rate: 41.4% ## Confusion matrix: ## default repaid class.error ## default 2644 1600 0.377 ## repaid 1913 2327 0.451 "],["supervised-learning-regression.html", "Chapter 20 Supervised Learning: Regression 20.1 What is Regression? 20.2 Training and Evaluating Models 20.3 Issues to Consider 20.4 Dealing with Non-Linear Responses 20.5 Tree-Based Methods", " Chapter 20 Supervised Learning: Regression 20.1 What is Regression? 20.1.1 Introduction Regression Predict a numerical outcome (“dependent variable”) from a set of inputs (“independent variables”). Statistical Sense: Predicting the expected value of the outcome. Casual Sense: Predicting a numerical outcome. Regression from a Machine Learning Perspective Scientific mindset: Modeling to understand the data generation process Collinearity Collinearity: when independent variables are partially correlated. High collinearity: Coefficients (or standard errors) look too large Model may be unstable 20.1.2 Linear regression \\(y = β_0 + β_1 x_1 + β_2 x_2 + ...\\) \\(y\\) is linearly related to each \\(x_i\\) Each \\(x_i\\) contributes additively to \\(y\\) Linear Regression in R # lm() function model &lt;- lm(y ~ x1 + x2..., data) # look at the model model # more information od the model summary(model) broom::glance(cmodel) sigr::wrapFTest(cmodel) 20.1.2.1 Simple regression unemployment given the rates of male and female unemployment in the United States over several years. The task is to predict the rate of female unemployment from the observed rate of male unemployment. The outcome is female_unemployment, and the input is male_unemployment. The sign of the variable coefficient tells you whether the outcome increases (+) or decreases (-) as the variable increases. library(tidyverse) unemployment &lt;- read_rds(&quot;data/unemployment.rds&quot;) str(unemployment) ## &#39;data.frame&#39;: 13 obs. of 2 variables: ## $ male_unemployment : num 2.9 6.7 4.9 7.9 9.8 ... ## $ female_unemployment: num 4 7.4 5 7.2 7.9 ... # Use the formula to fit a model: unemployment_model unemployment_model &lt;- lm(female_unemployment ~ male_unemployment, unemployment) # Print it unemployment_model ## ## Call: ## lm(formula = female_unemployment ~ male_unemployment, data = unemployment) ## ## Coefficients: ## (Intercept) male_unemployment ## 1.434 0.695 The coefficient for male unemployment is positive, so female unemployment increases as male unemployment does. 20.1.2.2 Examining a model There are a variety of different ways to examine a model; each way provides different information. # Call summary() on unemployment_model to get more details summary(unemployment_model) ## ## Call: ## lm(formula = female_unemployment ~ male_unemployment, data = unemployment) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.776 -0.341 -0.090 0.279 1.312 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.4341 0.6034 2.38 0.037 * ## male_unemployment 0.6945 0.0977 7.11 0.00002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.58 on 11 degrees of freedom ## Multiple R-squared: 0.821, Adjusted R-squared: 0.805 ## F-statistic: 50.6 on 1 and 11 DF, p-value: 0.0000197 # Call glance() on unemployment_model to see the details in a tidier form library(broom) glance(unemployment_model) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.821 0.805 0.580 50.6 0.0000197 1 -10.3 26.6 28.3 ## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; # Call wrapFTest() on unemployment_model to see the most relevant details sigr::wrapFTest(unemployment_model) ## [1] &quot;F Test summary: (R2=0.8213, F(1,11)=50.56, p=1.966e-05).&quot; 20.1.3 Predicting model You will also use your model to predict on the new data in newrates, which consists of only one observation, where male unemployment is 5%. predict(model, newdata) summary(unemployment) ## male_unemployment female_unemployment ## Min. :2.90 Min. :4.00 ## 1st Qu.:4.90 1st Qu.:4.40 ## Median :6.00 Median :5.20 ## Mean :5.95 Mean :5.57 ## 3rd Qu.:6.70 3rd Qu.:6.10 ## Max. :9.80 Max. :7.90 Plot a scatterplot of dframe$outcome versus dframe$pred (pred on the x axis, outcome on the y axis), along with a abine where outcome == pred. # Predict female unemployment in the unemployment dataset unemployment$prediction &lt;- predict(unemployment_model, unemployment) # Make a plot to compare predictions to actual (prediction on x axis). ggplot(unemployment, aes(x = prediction, y = female_unemployment)) + geom_point() + geom_abline(color = &quot;blue&quot;) newrates &lt;- data.frame(male_unemployment = 5) # Predict female unemployment rate when male unemployment is 5% predict(unemployment_model, newrates) ## 1 ## 4.91 20.1.3.1 Multivariate linear regression you will work with the blood pressure dataset, and model blood_pressure as a function of weight and age. bloodpressure &lt;- read_rds(&quot;data/bloodpressure.rds&quot;) str(bloodpressure) ## &#39;data.frame&#39;: 11 obs. of 3 variables: ## $ blood_pressure: int 132 143 153 162 154 168 137 149 159 128 ... ## $ age : int 52 59 67 73 64 74 54 61 65 46 ... ## $ weight : int 173 184 194 211 196 220 188 188 207 167 ... summary(bloodpressure) ## blood_pressure age weight ## Min. :128 Min. :46.0 Min. :167 ## 1st Qu.:140 1st Qu.:56.5 1st Qu.:186 ## Median :153 Median :64.0 Median :194 ## Mean :150 Mean :62.5 Mean :195 ## 3rd Qu.:160 3rd Qu.:69.5 3rd Qu.:209 ## Max. :168 Max. :74.0 Max. :220 # Fit the model: bloodpressure_model bloodpressure_model &lt;- lm(blood_pressure ~ age + weight, bloodpressure) # Print bloodpressure_model and call summary() summary(bloodpressure_model) ## ## Call: ## lm(formula = blood_pressure ~ age + weight, data = bloodpressure) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.464 -1.195 -0.408 1.851 2.698 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.994 11.944 2.59 0.0319 * ## age 0.861 0.248 3.47 0.0084 ** ## weight 0.335 0.131 2.56 0.0335 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.32 on 8 degrees of freedom ## Multiple R-squared: 0.977, Adjusted R-squared: 0.971 ## F-statistic: 169 on 2 and 8 DF, p-value: 0.000000287 In this case the coefficients for both age and weight are positive, which indicates that bloodpressure tends to increase as both age and weight increase. You will also compare the predictions to outcomes graphically. # Predict blood pressure using bloodpressure_model: prediction bloodpressure$prediction &lt;- predict(bloodpressure_model, bloodpressure) # Plot the results ggplot(bloodpressure, aes(prediction, blood_pressure)) + geom_point() + geom_abline(color = &quot;blue&quot;) The results stay fairly close to the line of perfect prediction, indicating that the model fits the training data well. From a prediction perspective, multivariate linear regression behaves much as simple (one-variable) linear regression does. 20.2 Training and Evaluating Models 20.2.1 Graphically 20.2.1.1 The Residual Plot Plot the model’s predictions against the actual value. Are the predictions near the \\(x = y\\) line? # Make predictions from the model unemployment$predictions &lt;- predict(unemployment_model, unemployment) # Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates ggplot(unemployment, aes(x = predictions, y = female_unemployment)) + geom_point() + geom_abline() Plot predictions (on the x-axis) versus residuals (on the y-axis). This gives you a different view of the model’s predictions as compared to ground truth. # Calculate residuals unemployment$residuals &lt;- unemployment$female_unemployment - unemployment$predictions # Fill in the blanks to plot predictions (on x-axis) versus the residuals ggplot(unemployment, aes(x = predictions, y = residuals)) + geom_pointrange(aes(ymin = 0, ymax = residuals)) + geom_hline(yintercept = 0, linetype = 3) + ggtitle(&quot;residuals vs. linear model prediction&quot;) 20.2.1.2 The Gain Curve Now, you will also plot the gain curve of the unemployment_model’s predictions against actual female_unemployment using the WVPlots::GainCurvePlot() function. GainCurvePlot(frame, xvar, truthvar, title) xvar = “prediction” truthvar = “actual outcome” title = title of the plot Relative gini coefficient When the predictions sort in exactly the same order, the relative Gini coefficient is 1. When the model sorts poorly, the relative Gini coefficient is close to zero, or even negative. Wizard curve: perfect model, 藍線越接近綠線越好 # Load the package WVPlots library(WVPlots) # Plot the Gain Curve GainCurvePlot(unemployment, &quot;predictions&quot;, &quot;female_unemployment&quot;, &quot;Unemployment model&quot;) A relative gini coefficient close to one shows that the model correctly sorts high unemployment situations from lower ones. 20.2.2 Root Mean Squared Error RMSE \\(RMSE = \\sqrt{\\overline{(pred − y)^2}}\\) \\(pred − y\\) : the error, or residuals vector = \\(res\\) \\(\\overline{(pred − y)^2}\\) : mean value of \\((pred − y)^2\\) One way to evaluate the RMSE is to compare it to the standard deviation of the outcome. With a good model, the RMSE should be smaller. # For convenience put the residuals in the variable res res &lt;- unemployment$residuals # Calculate RMSE, assign it to the variable rmse and print it rmse &lt;- sqrt(mean(res^2)) # Calculate the standard deviation of female_unemployment and print it sd_unemployment &lt;- sd(unemployment$female_unemployment) c(rmse = rmse, sd = sd_unemployment) ## rmse sd ## 0.534 1.314 An RMSE much smaller than the outcome’s standard deviation suggests a model that predicts well. 20.2.3 R-squared R-squared (\\(R^2\\)) A measure of how well the model fits or explains the data. A value between 0-1 near 1: model fits well near 0: no better than guessing the average value Calculating: the variance explained by the model \\(R^2 = 1 - \\frac{RSS}{SS_{Tot}}\\) \\(RSS = \\sum{(y - prediction)^2}\\) Residual sum of squares (variance from model) \\(SS_{Tot} = \\sum{(y - \\overline{y})^2}\\) Total sum of squares (variance of data) Correlation and \\(R^2\\) \\(ρ\\) = cor(predict, actual) \\(ρ^2\\)= \\(R^2\\) 20.2.3.1 Calculate R-squared You will examine how well the model fits the data: that is, how much variance does it explain. # Calculate and print the mean female_unemployment: fe_mean fe_mean &lt;- mean(unemployment$female_unemployment) # Calculate and print the total sum of squares: tss tss &lt;- sum((unemployment$female_unemployment - fe_mean)^2) # Calculate and print residual sum of squares: rss rss &lt;- sum(unemployment$residuals^2) # Calculate and print the R-squared: rsq rsq &lt;- 1 - rss / tss # Get R-squared from glance and print it rsq_glance &lt;- glance(unemployment_model)$r.squared # Is it the same as what you calculated? list(rsq_calculate = rsq, rsq_glance = rsq_glance) ## $rsq_calculate ## [1] 0.821 ## ## $rsq_glance ## [1] 0.821 An R-squared close to one suggests a model that predicts well. 20.2.3.2 Correlation and R-squared The linear correlation of two variables, x and y, measures the strength of the linear relationship between them. When x and y are respectively: the outcomes of a regression model that minimizes squared-error (like linear regression) and the true outcomes of the training data, then the square of the correlation is the same as \\(R^2\\). # Get the correlation between the prediction and true outcome: rho and print it (rho &lt;- cor(unemployment$predictions, unemployment$female_unemployment)) ## [1] 0.906 # Square rho: rho2 and print it (rho2 &lt;- rho^2) ## [1] 0.821 # Get R-squared from glance and print it (rsq_glance &lt;- glance(unemployment_model)$r.squared) ## [1] 0.821 Remember this equivalence is only true for the training data, and only for models that minimize squared error. 20.2.4 Training a Model Test/Train Split Recommended method when data is plentiful Cross-Validation Preferred when data is not large enough to split off a test set library(vtreat) splitPlan &lt;- kWayCrossValidation(nRows, nSplits, NULL, NULL) nRows : number of rows in the training data nSplits : number folds (partitions) in the cross-validation e.g, nfolds = 3 for 3-way cross-validation 20.2.4.1 Generate a random test/train split mpg describes the characteristics of several makes and models of cars from different years. The goal is to predict city fuel efficiency from highway fuel efficiency. n this exercise, you will split mpg into a training set mpg_train (75% of the data) and a test set mpg_test (25% of the data). One way to do this is to generate a column of uniform random numbers between 0 and 1, using the function runif(). Generate a vector of uniform random numbers: gp = runif(N). (N = data sample size) dframe[gp &lt; X,] will be about the right size. dframe[gp &gt;= X,] will be the complement. glimpse(mpg) ## Rows: 234 ## Columns: 11 ## $ manufacturer &lt;chr&gt; &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;audi&quot;, &quot;… ## $ model &lt;chr&gt; &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4&quot;, &quot;a4 quattro&quot;, &quot;… ## $ displ &lt;dbl&gt; 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.0, 2.0, 2.… ## $ year &lt;int&gt; 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999, 1999, 200… ## $ cyl &lt;int&gt; 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 8, 8, … ## $ trans &lt;chr&gt; &quot;auto(l5)&quot;, &quot;manual(m5)&quot;, &quot;manual(m6)&quot;, &quot;auto(av)&quot;, &quot;auto… ## $ drv &lt;chr&gt; &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4… ## $ cty &lt;int&gt; 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15, 17, 17, 1… ## $ hwy &lt;int&gt; 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25, 25, 25, 2… ## $ fl &lt;chr&gt; &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p&quot;, &quot;p… ## $ class &lt;chr&gt; &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;compact&quot;, &quot;c… # Use nrow to get the number of rows in mpg (N) and print it (N &lt;- nrow(mpg)) ## [1] 234 # Calculate how many rows 75% of N should be and print it # Hint: use round() to get an integer (target &lt;- round(N * 0.75)) ## [1] 176 # Create the vector of N uniform random variables: gp gp &lt;- runif(N) # Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data) mpg_train &lt;- mpg[gp &lt; 0.75, ] mpg_test &lt;- mpg[gp &gt;= 0.75, ] # Use nrow() to examine mpg_train and mpg_test c(train_n = nrow(mpg_train), test_n = nrow(mpg_test)) ## train_n test_n ## 180 54 A random split won’t always produce sets of exactly X% and (100-X)% of the data, but it should be close. 20.2.4.2 Train a model using test/train split You will use mpg_train to train a model to predict city fuel efficiency (cty) from highway fuel efficiency (hwy). # Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy mpg_model &lt;- lm(cty ~ hwy, mpg_train) # Use summary() to examine the model summary(mpg_model) ## ## Call: ## lm(formula = cty ~ hwy, data = mpg_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.8862 -0.7400 0.0108 0.7051 2.5889 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.7766 0.3551 2.19 0.03 * ## hwy 0.6844 0.0147 46.62 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.15 on 178 degrees of freedom ## Multiple R-squared: 0.924, Adjusted R-squared: 0.924 ## F-statistic: 2.17e+03 on 1 and 178 DF, p-value: &lt;0.0000000000000002 20.2.4.3 Evaluate model using test/train split Now you will test the model mpg_model on the test data, mpg_test. Functions rmse() and r_squared() to calculate RMSE and R-squared # function rmse, # predcol: The predicted values, ycol: The actual outcome rmse &lt;- function(predcol, ycol) { res = predcol-ycol sqrt(mean(res^2)) } # function r_squared r_squared &lt;- function(predcol, ycol) { tss = sum( (ycol - mean(ycol))^2 ) rss = sum( (predcol - ycol)^2 ) 1 - rss/tss } Evaluate RMSE for both the test and training sets. # predict cty from hwy for the training set mpg_train$pred &lt;- predict(mpg_model, mpg_train) # predict cty from hwy for the test set mpg_test$pred &lt;- predict(mpg_model, mpg_test) # Evaluate the rmse on both training and test data and print them rmse_train &lt;- rmse(mpg_train$pred, mpg_train$cty) rmse_test &lt;- rmse(mpg_test$pred, mpg_test$cty) list(rmse_train = rmse_train, rmse_test = rmse_test) ## $rmse_train ## [1] 1.14 ## ## $rmse_test ## [1] 1.55 Evaluate r-squared for both the test and training sets. # Evaluate the r-squared on both training and test data.and print them rsq_train &lt;- r_squared(mpg_train$pred, mpg_train$cty) rsq_test &lt;- r_squared(mpg_test$pred, mpg_test$cty) list(rsq_train = rsq_train, rsq_test = rsq_test) ## $rsq_train ## [1] 0.924 ## ## $rsq_test ## [1] 0.885 Plot test data. # Plot the predictions (on the x-axis) against the outcome (cty) on the test data ggplot(mpg_test, aes(x = pred, y = cty)) + geom_point() + geom_abline() Good performance on the test data is more confirmation that the model works as expected. 20.2.4.4 Create a cross validation plan n-fold cross validation plan from vtreat package: splitPlan &lt;- kWayCrossValidation(nRows, nSplits, dframe, y) nRows is the number of rows of data to be split. nSplits is the desired number of cross-validation folds. dframe and y : set them both to NULL. they are for compatibility with other vtreat data partitioning functions. The resulting splitPlan is a list of nSplits elements; each element contains two vectors: train: the indices of dframe that will form the training set app: the indices of dframe that will form the test (or application) set # Load the package vtreat library(vtreat) ## ## Attaching package: &#39;vtreat&#39; ## The following object is masked from &#39;package:infer&#39;: ## ## fit # Get the number of rows in mpg nRows &lt;- nrow(mpg) # Implement the 3-fold cross-fold plan with vtreat splitPlan &lt;- kWayCrossValidation(nRows, 3, NULL, NULL) # Examine the split plan str(splitPlan) ## List of 3 ## $ :List of 2 ## ..$ train: int [1:156] 4 5 6 7 8 9 10 11 13 14 ... ## ..$ app : int [1:78] 81 134 206 77 12 69 195 127 26 23 ... ## $ :List of 2 ## ..$ train: int [1:156] 1 2 3 5 7 9 10 11 12 14 ... ## ..$ app : int [1:78] 79 98 198 138 80 56 186 156 101 39 ... ## $ :List of 2 ## ..$ train: int [1:156] 1 2 3 4 6 8 12 13 17 19 ... ## ..$ app : int [1:78] 209 16 100 54 175 147 196 162 124 122 ... ## - attr(*, &quot;splitmethod&quot;)= chr &quot;kwaycross&quot; 20.2.4.5 Evaluate a modeling procedure using n-fold cross-validation If dframe is the training data, then one way to add a column of cross-validation predictions to the frame: # Initialize a column of the appropriate length dframe$pred.cv &lt;- 0 # k is the number of folds # splitPlan is the cross validation plan for(i in 1:k) { # Get the ith split split &lt;- splitPlan[[i]] # Build a model on the training data # from this split # (lm, in this case) model &lt;- lm(fmla, data = dframe[split$train,]) # make predictions on the # application data from this split dframe$pred.cv[split$app] &lt;- predict(model, newdata = dframe[split$app,]) } Run the 3-fold cross validation plan from splitPlan and put the predictions in the column mpg$pred.cv. # Run the 3-fold cross validation plan from splitPlan k &lt;- 3 # Number of folds mpg$pred.cv &lt;- 0 for(i in 1:k) { split &lt;- splitPlan[[i]] model &lt;- lm(cty ~ hwy, data = mpg[split$train,]) mpg$pred.cv[split$app] &lt;- predict(model, newdata = mpg[split$app,]) } head(mpg) ## # A tibble: 6 × 12 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 1.8 1999 4 auto(l5) f 18 29 p compa… ## 2 audi a4 1.8 1999 4 manual(m5) f 21 29 p compa… ## 3 audi a4 2 2008 4 manual(m6) f 20 31 p compa… ## 4 audi a4 2 2008 4 auto(av) f 21 30 p compa… ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compa… ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compa… ## # ℹ 1 more variable: pred.cv &lt;dbl&gt; Are the two values about the same? # Predict from a full model mpg$pred &lt;- predict(lm(cty ~ hwy, data = mpg)) # Get the rmse of the full model&#39;s predictions rmse(mpg$pred, mpg$cty) ## [1] 1.25 # Get the rmse of the cross-validation predictions rmse(mpg$pred.cv, mpg$cty) ## [1] 1.28 Remember, cross-validation validates the modeling process, not an actual model. 20.3 Issues to Consider 20.3.1 Categorical IV one-hot-encoding model.matrix(): Converts categorical variable with N levels into N - 1 indicator variables. 20.3.1.1 Structure of categorical inputs You will call model.matrix() to examine how R represents data with both categorical and numerical inputs for modeling. The dataset flowers following columns: Flowers: the average number of flowers on a meadowfoam plant Intensity: the intensity of a light treatment applied to the plant Time: A categorical variable - when (Late or Early) in the lifecycle the light treatment occurred The ultimate goal is to predict Flowers as a function of Time and Intensity. flowers &lt;- read_delim(&quot;data/flowers.txt&quot;) glimpse(flowers) ## Rows: 24 ## Columns: 3 ## $ Flowers &lt;dbl&gt; 62.3, 77.4, 55.3, 54.2, 49.6, 61.9, 39.4, 45.7, 31.3, 44.9, … ## $ Time &lt;chr&gt; &quot;Late&quot;, &quot;Late&quot;, &quot;Late&quot;, &quot;Late&quot;, &quot;Late&quot;, &quot;Late&quot;, &quot;Late&quot;, &quot;Lat… ## $ Intensity &lt;dbl&gt; 150, 150, 300, 300, 450, 450, 600, 600, 750, 750, 900, 900, … # Use unique() to see how many possible values Time takes unique(flowers$Time) ## [1] &quot;Late&quot; &quot;Early&quot; TimeLate = Late = 1; TimeLate = Early = 0 # Use fmla and model.matrix to see how the data is represented for modeling mmat &lt;- model.matrix(Flowers ~ Intensity + Time, flowers) # Examine the first 20 lines of mmat head(mmat, 20) ## (Intercept) Intensity TimeLate ## 1 1 150 1 ## 2 1 150 1 ## 3 1 300 1 ## 4 1 300 1 ## 5 1 450 1 ## 6 1 450 1 ## 7 1 600 1 ## 8 1 600 1 ## 9 1 750 1 ## 10 1 750 1 ## 11 1 900 1 ## 12 1 900 1 ## 13 1 150 0 ## 14 1 150 0 ## 15 1 300 0 ## 16 1 300 0 ## 17 1 450 0 ## 18 1 450 0 ## 19 1 600 0 ## 20 1 600 0 # Examine the first 20 lines of flowers head(flowers, 20) ## # A tibble: 20 × 3 ## Flowers Time Intensity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 62.3 Late 150 ## 2 77.4 Late 150 ## 3 55.3 Late 300 ## 4 54.2 Late 300 ## 5 49.6 Late 450 ## 6 61.9 Late 450 ## 7 39.4 Late 600 ## 8 45.7 Late 600 ## 9 31.3 Late 750 ## 10 44.9 Late 750 ## 11 36.8 Late 900 ## 12 41.9 Late 900 ## 13 77.8 Early 150 ## 14 75.6 Early 150 ## 15 69.1 Early 300 ## 16 78 Early 300 ## 17 57 Early 450 ## 18 71.1 Early 450 ## 19 62.9 Early 600 ## 20 52.2 Early 600 20.3.1.2 Modeling with categorical inputs You will fit a linear model to the flowers data, to predict Flowers as a function of Time and Intensity. # Fit a model to predict Flowers from Intensity and Time : flower_model flower_model &lt;- lm(Flowers ~ Intensity + Time, flowers) # Use summary to examine flower_model summary(flower_model) ## ## Call: ## lm(formula = Flowers ~ Intensity + Time, data = flowers) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.65 -4.14 -1.56 5.63 12.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 83.46417 3.27377 25.49 &lt; 0.0000000000000002 *** ## Intensity -0.04047 0.00513 -7.89 0.0000001 *** ## TimeLate -12.15833 2.62956 -4.62 0.00015 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.44 on 21 degrees of freedom ## Multiple R-squared: 0.799, Adjusted R-squared: 0.78 ## F-statistic: 41.8 on 2 and 21 DF, p-value: 0.0000000479 Intercept = Early (reference group), TimeLate = Late. # Predict the number of flowers on each plant flowers$predictions &lt;- predict(flower_model, flowers) # Plot predictions vs actual flowers (predictions on x-axis) ggplot(flowers, aes(x = predictions, y = Flowers)) + geom_point() + geom_abline(color = &quot;blue&quot;) 20.3.2 Interactions 20.3.2.1 Modeling an interaction You will use interactions to model the effect of gender and gastric activity on alcohol metabolism. The alcohol data frame has the columns: Metabol: the alcohol metabolism rate Gastric: the rate of gastric alcohol dehydrogenase activity Sex: the sex of the drinker (Male or Female) alcohol &lt;- read_tsv(&quot;data/alcohol.txt&quot;) glimpse(alcohol) ## Rows: 32 ## Columns: 5 ## $ Subject &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ Metabol &lt;dbl&gt; 0.6, 0.6, 1.5, 0.4, 0.1, 0.2, 0.3, 0.3, 0.4, 1.0, 1.1, 1.2, 1.… ## $ Gastric &lt;dbl&gt; 1.0, 1.6, 1.5, 2.2, 1.1, 1.2, 0.9, 0.8, 1.5, 0.9, 1.6, 1.7, 1.… ## $ Sex &lt;chr&gt; &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;Female&quot;, &quot;F… ## $ Alcohol &lt;chr&gt; &quot;Alcoholic&quot;, &quot;Alcoholic&quot;, &quot;Alcoholic&quot;, &quot;Non-alcoholic&quot;, &quot;Non-a… No interaction. # Fit the main effects only model model_add &lt;- lm(Metabol ~ Gastric + Sex, alcohol) summary(model_add) ## ## Call: ## lm(formula = Metabol ~ Gastric + Sex, data = alcohol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.278 -0.633 -0.097 0.578 4.570 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.947 0.520 -3.75 0.0008 *** ## Gastric 1.966 0.267 7.35 0.000000042 *** ## SexMale 1.617 0.511 3.16 0.0036 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.33 on 29 degrees of freedom ## Multiple R-squared: 0.765, Adjusted R-squared: 0.749 ## F-statistic: 47.3 on 2 and 29 DF, p-value: 0.000000000741 Add Gastric as a main effect, plus interaction. # Fit the interaction model model_interaction &lt;- lm(Metabol ~ Gastric + Gastric:Sex, alcohol) summary(model_interaction) ## ## Call: ## lm(formula = Metabol ~ Gastric + Gastric:Sex, data = alcohol) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.466 -0.509 0.014 0.566 4.067 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.750 0.531 -1.41 0.16824 ## Gastric 1.149 0.345 3.33 0.00237 ** ## Gastric:SexMale 1.042 0.241 4.32 0.00017 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.2 on 29 degrees of freedom ## Multiple R-squared: 0.808, Adjusted R-squared: 0.795 ## F-statistic: 61 on 2 and 29 DF, p-value: 0.0000000000403 An interaction appears to give a better fit to the data. cross-validation Because this dataset is small, we will use cross-validation to simulate making predictions on out-of-sample data. # Create the formula with main effects only fmla_add &lt;- as.formula(Metabol ~ Gastric + Sex) # Create the formula with interactions fmla_interaction &lt;- as.formula(Metabol ~ Gastric + Gastric:Sex) # Create the splitting plan for 3-fold cross validation set.seed(34245) # set the seed for reproducibility splitPlan &lt;- kWayCrossValidation(nrow(alcohol), 3, NULL, NULL) # Sample code: Get cross-val predictions for main-effects only model alcohol$pred_add &lt;- 0 # initialize the prediction vector for(i in 1:3) { split &lt;- splitPlan[[i]] model_add &lt;- lm(fmla_add, data = alcohol[split$train, ]) alcohol$pred_add[split$app] &lt;- predict(model_add, newdata = alcohol[split$app, ]) } # Get the cross-val predictions for the model with interactions alcohol$pred_interaction &lt;- 0 # initialize the prediction vector for(i in 1:3) { split &lt;- splitPlan[[i]] model_interaction &lt;- lm(fmla_interaction, data = alcohol[split$train, ]) alcohol$pred_interaction[split$app] &lt;- predict(model_interaction, newdata = alcohol[split$app, ]) } # see dataset head(alcohol) ## # A tibble: 6 × 7 ## Subject Metabol Gastric Sex Alcohol pred_add pred_interaction ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 0.6 1 Female Alcoholic -0.209 0.416 ## 2 2 0.6 1.6 Female Alcoholic 1.46 1.30 ## 3 3 1.5 1.5 Female Alcoholic 1.26 1.18 ## 4 4 0.4 2.2 Female Non-alcoholic 2.62 2.00 ## 5 5 0.1 1.1 Female Non-alcoholic 0.224 0.338 ## 6 6 0.2 1.2 Female Non-alcoholic 0.682 0.832 Use tidyr’s gather() which takes multiple columns and collapses them into key-value pairs. alcohol %&gt;% gather(key = modeltype, value = pred, pred_add, pred_interaction) ## # A tibble: 64 × 7 ## Subject Metabol Gastric Sex Alcohol modeltype pred ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 1 0.6 1 Female Alcoholic pred_add -0.209 ## 2 2 0.6 1.6 Female Alcoholic pred_add 1.46 ## 3 3 1.5 1.5 Female Alcoholic pred_add 1.26 ## 4 4 0.4 2.2 Female Non-alcoholic pred_add 2.62 ## 5 5 0.1 1.1 Female Non-alcoholic pred_add 0.224 ## 6 6 0.2 1.2 Female Non-alcoholic pred_add 0.682 ## 7 7 0.3 0.9 Female Non-alcoholic pred_add -0.396 ## 8 8 0.3 0.8 Female Non-alcoholic pred_add -0.582 ## 9 9 0.4 1.5 Female Non-alcoholic pred_add 1.04 ## 10 10 1 0.9 Female Non-alcoholic pred_add -0.396 ## # ℹ 54 more rows # Get RMSE alcohol %&gt;% gather(key = modeltype, value = pred, pred_add, pred_interaction) %&gt;% mutate(residuals = Metabol - pred) %&gt;% group_by(modeltype) %&gt;% summarize(rmse = sqrt(mean(residuals^2))) ## # A tibble: 2 × 2 ## modeltype rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 pred_add 1.38 ## 2 pred_interaction 1.26 Cross-validation confirms that a model with interaction will likely give better predictions. 20.3.3 Transforming DV before modeling Root Mean Squared Relative Error RMS-relative error = \\(\\sqrt{\\overline{(\\frac{pred - y}{y})^2}}\\) Predicting log-outcome reduces RMS-relative error But the model will o/en have larger RMSE 20.3.3.1 Relative error You will compare relative error to absolute error. The example (toy) dataset fdata includes the columns: y: the true output to be predicted by some model; imagine it is the amount of money a customer will spend on a visit to your store. pred: the predictions of a model that predicts y. label: categorical: whether y comes from a population that makes small purchases, or large ones. You want to know which model does “better”: the one predicting the small purchases, or the one predicting large ones. fdata &lt;- read_tsv(&quot;data/fdata.txt&quot;) glimpse(fdata) ## Rows: 100 ## Columns: 3 ## $ y &lt;dbl&gt; 9.15, 1.90, -3.86, 2.39, 1.54, 13.56, 10.20, 1.10, 3.94, 9.04, 1… ## $ pred &lt;dbl&gt; 6.43, 3.47, 1.59, 3.76, 9.51, 6.93, 8.19, 1.51, 8.99, 6.15, 8.50… ## $ label &lt;chr&gt; &quot;small purchases&quot;, &quot;small purchases&quot;, &quot;small purchases&quot;, &quot;small … # Examine the data: generate the summaries for the groups large and small: fdata %&gt;% # group by small/large purchases group_by(label) %&gt;% summarize(min = min(y), # min of y mean = mean(y), # mean of y max = max(y)) # max of y ## # A tibble: 2 × 4 ## label min mean max ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 large purchases 96.1 606. 1102. ## 2 small purchases -5.89 6.48 18.6 # Fill in the blanks to add error columns fdata2 &lt;- fdata %&gt;% # group by label group_by(label) %&gt;% mutate(residual = y - pred, # Residual relerr = residual / y) # Relative error # Compare the rmse and rmse.rel of the large and small groups: fdata2 %&gt;% group_by(label) %&gt;% summarize(rmse = sqrt(mean(residual^2)), # RMSE rmse.rel = sqrt(mean(relerr^2))) # Root mean squared relative error ## # A tibble: 2 × 3 ## label rmse rmse.rel ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 large purchases 5.54 0.0147 ## 2 small purchases 4.01 1.25 Notice from this example how a model with larger RMSE might still be better, if relative errors are more important than absolute errors. # Plot the predictions for both groups of purchases ggplot(fdata2, aes(x = pred, y = y, color = label)) + geom_point() + geom_abline() + facet_wrap(~ label, ncol = 1, scales = &quot;free&quot;) + ggtitle(&quot;Outcome vs prediction&quot;) 20.3.3.2 Modeling log-transformed output You will practice modeling on log-transformed monetary output, and then transforming the “log-money” predictions back into monetary units. Income2005 records subjects’ incomes in 2005, as well as the results of several aptitude tests taken by the subjects in 1981. You will build a model of log(income) from the inputs, and then convert log(income) back into income. # A vector contain &quot;incometrain&quot; and &quot;incometest&quot; Income2005 &lt;- load(&quot;data/Income.RData&quot;) # Set up dataset income_train &lt;- incometrain income_test &lt;- incometest # See data structure glimpse(income_train) ## Rows: 2,069 ## Columns: 7 ## $ Subject &lt;int&gt; 2, 6, 8, 9, 16, 17, 18, 20, 21, 22, 29, 34, 37, 38, 39, 41,… ## $ Arith &lt;int&gt; 8, 30, 13, 21, 17, 29, 30, 17, 29, 27, 12, 8, 21, 9, 16, 15… ## $ Word &lt;int&gt; 15, 35, 35, 28, 30, 33, 35, 28, 33, 31, 9, 20, 31, 17, 23, … ## $ Parag &lt;int&gt; 6, 15, 12, 10, 12, 13, 14, 14, 13, 14, 11, 8, 13, 9, 10, 11… ## $ Math &lt;int&gt; 6, 23, 4, 13, 17, 21, 23, 20, 25, 22, 9, 3, 12, 5, 8, 18, 8… ## $ AFQT &lt;dbl&gt; 6.84, 99.39, 44.02, 59.68, 50.28, 89.67, 95.98, 67.02, 88.4… ## $ Income2005 &lt;int&gt; 5500, 65000, 36000, 65000, 71000, 43000, 120000, 64000, 253… glimpse(income_test) ## Rows: 515 ## Columns: 7 ## $ Subject &lt;int&gt; 7, 13, 47, 62, 73, 78, 89, 105, 106, 107, 129, 152, 160, 16… ## $ Arith &lt;int&gt; 14, 30, 26, 12, 18, 25, 24, 28, 30, 19, 28, 30, 28, 16, 21,… ## $ Word &lt;int&gt; 27, 29, 33, 25, 34, 35, 34, 32, 35, 31, 33, 33, 31, 25, 28,… ## $ Parag &lt;int&gt; 8, 13, 13, 10, 13, 14, 15, 14, 12, 11, 14, 14, 13, 13, 13, … ## $ Math &lt;int&gt; 11, 24, 16, 10, 18, 21, 17, 20, 23, 16, 23, 22, 19, 12, 16,… ## $ AFQT &lt;dbl&gt; 47.41, 72.31, 75.47, 36.38, 81.53, 85.35, 84.98, 81.79, 87.… ## $ Income2005 &lt;int&gt; 19000, 8000, 66309, 30000, 186135, 14657, 62000, 40000, 105… # Examine Income2005 in the training set summary(income_train$Income2005) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 63 23000 39000 49894 61500 703637 Plot outcome variable. ggplot(income_train, aes(Income2005)) + geom_histogram() ## `stat_bin()` using `bins = 30`. Pick better ## value with `binwidth`. Fit log transform model and predict on testing data. # Fit the linear model model.log &lt;- lm(log(Income2005) ~ Arith + Word + Parag + Math + AFQT, income_train) # Make predictions on income_test income_test$logpred &lt;- predict(model.log, income_test) summary(income_test$logpred) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9.77 10.13 10.42 10.42 10.70 11.01 Reverse the log transformation to put the predictions into “monetary units”. # Convert the predictions to monetary units income_test$pred.income &lt;- exp(income_test$logpred) summary(income_test$pred.income) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 17432 25167 33615 35363 44566 60217 Plot a scatter plot of predicted income vs income on the test set. # Plot predicted income (x axis) vs income ggplot(income_test, aes(x = pred.income, y = Income2005)) + geom_point() + geom_abline(color = &quot;blue&quot;) Remember that when you transform the output before modeling, you have to ‘reverse transform’ the resulting predictions after applying the model. 20.3.3.3 Comparing RMSE and RMS Relative Error In this exercise, you will show that log-transforming a monetary output before modeling improves mean relative error (but increases RMSE) compared to modeling the monetary output directly. Compare the results of model.log from the previous exercise to a model (model.abs) that directly fits income. # a model that directly fits income to the inputs model.abs &lt;- lm(formula = Income2005 ~ Arith + Word + Parag + Math + AFQT, data = income_train) summary(model.abs) ## ## Call: ## lm(formula = Income2005 ~ Arith + Word + Parag + Math + AFQT, ## data = income_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -78728 -24137 -6979 11964 648573 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17517 6420 2.73 0.0064 ** ## Arith 1552 303 5.12 0.00000034 *** ## Word -132 265 -0.50 0.6175 ## Parag -1155 618 -1.87 0.0619 . ## Math 726 372 1.95 0.0513 . ## AFQT 178 144 1.23 0.2173 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 45500 on 2063 degrees of freedom ## Multiple R-squared: 0.116, Adjusted R-squared: 0.114 ## F-statistic: 54.4 on 5 and 2063 DF, p-value: &lt;0.0000000000000002 # Add predictions to the test set income_test &lt;- income_test %&gt;% # predictions from model.abs mutate(pred.absmodel = predict(model.abs, income_test), # predictions from model.log pred.logmodel = exp(predict(model.log, income_test))) head(income_test) ## Subject Arith Word Parag Math AFQT Income2005 logpred pred.income ## 3 7 14 27 8 11 47.4 19000 10.3 28644 ## 6 13 30 29 13 24 72.3 8000 10.9 56646 ## 22 47 26 33 13 16 75.5 66309 10.7 44468 ## 27 62 12 25 10 10 36.4 30000 10.1 25462 ## 30 73 18 34 13 18 81.5 186135 10.5 36356 ## 31 78 25 35 14 21 85.3 14657 10.8 46948 ## pred.absmodel pred.logmodel ## 3 42844 28644 ## 6 75499 56646 ## 22 63519 44468 ## 27 35008 25462 ## 30 53495 36356 ## 31 65929 46948 # Gather the predictions and calculate residuals and relative error income_long &lt;- income_test %&gt;% gather(key = modeltype, value = pred, pred.absmodel, pred.logmodel) %&gt;% mutate(residual = Income2005 - pred, # residuals relerr = residual / Income2005) # relative error head(income_long) ## Subject Arith Word Parag Math AFQT Income2005 logpred pred.income ## 1 7 14 27 8 11 47.4 19000 10.3 28644 ## 2 13 30 29 13 24 72.3 8000 10.9 56646 ## 3 47 26 33 13 16 75.5 66309 10.7 44468 ## 4 62 12 25 10 10 36.4 30000 10.1 25462 ## 5 73 18 34 13 18 81.5 186135 10.5 36356 ## 6 78 25 35 14 21 85.3 14657 10.8 46948 ## modeltype pred residual relerr ## 1 pred.absmodel 42844 -23844 -1.2550 ## 2 pred.absmodel 75499 -67499 -8.4374 ## 3 pred.absmodel 63519 2790 0.0421 ## 4 pred.absmodel 35008 -5008 -0.1669 ## 5 pred.absmodel 53495 132640 0.7126 ## 6 pred.absmodel 65929 -51272 -3.4981 # Calculate RMSE and relative RMSE and compare income_long %&gt;% # group by modeltype group_by(modeltype) %&gt;% # RMSE summarize(rmse = sqrt(mean(residual^2)), # Root mean squared relative error rmse.rel = sqrt(mean(relerr^2))) ## # A tibble: 2 × 3 ## modeltype rmse rmse.rel ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 pred.absmodel 37448. 3.18 ## 2 pred.logmodel 39235. 2.22 You’ve seen how modeling log(income) can reduce the relative error of the fit, at the cost of increased RMSE. Which tradeoff to make depends on the goals of your project. 20.3.4 Transforming IV before modeling 20.3.4.1 Input transforms: the “hockey stick” In this exercise, we will build a model to predict price from a measure of the house’s size. The houseprice dataset, has the columns: price: house price in units of $1000 size: surface area houseprice &lt;- read_rds(&quot;data/houseprice.rds&quot;) glimpse(houseprice) ## Rows: 40 ## Columns: 2 ## $ size &lt;int&gt; 72, 98, 92, 90, 44, 46, 90, 150, 94, 90, 90, 66, 142, 74, 86, 46… ## $ price &lt;int&gt; 156, 153, 230, 152, 42, 157, 113, 573, 202, 261, 175, 212, 486, … A scatterplot of the data shows that the data is quite non-linear: a sort of “hockey-stick” where price is fairly flat for smaller houses, but rises steeply as the house gets larger. Quadratics and tritics are often good functional forms to express hockey-stick like relationships. ggplot(houseprice, aes(size, price)) + geom_point() + geom_smooth(se = F) ## `geom_smooth()` using method = &#39;loess&#39; and ## formula = &#39;y ~ x&#39; You will fit a model to predict price as a function of the squared size, and look at its fit on the training data. # Fit a model of price as a function of squared size # Because ^ is also a symbol to express interactions, use the function I() to treat the expression x^2 “as is”: that is, as the square of x rather than the interaction of x with itself. model_sqr &lt;- lm(price ~ I(size^2), houseprice) # Fit a model of price as a linear function of size model_lin &lt;- lm(price ~ size, houseprice) # Make predictions and compare houseprice %&gt;% # predictions from linear model mutate(pred_lin = predict(model_lin, houseprice), # predictions from quadratic model pred_sqr = predict(model_sqr, houseprice)) %&gt;% gather(key = modeltype, value = pred, pred_lin, pred_sqr) ## size price modeltype pred ## 1 72 156 pred_lin 160.8 ## 2 98 153 pred_lin 263.9 ## 3 92 230 pred_lin 240.1 ## 4 90 152 pred_lin 232.2 ## 5 44 42 pred_lin 49.8 ## 6 46 157 pred_lin 57.7 ## 7 90 113 pred_lin 232.2 ## 8 150 573 pred_lin 470.1 ## 9 94 202 pred_lin 248.0 ## 10 90 261 pred_lin 232.2 ## 11 90 175 pred_lin 232.2 ## 12 66 212 pred_lin 137.0 ## 13 142 486 pred_lin 438.4 ## 14 74 109 pred_lin 168.7 ## 15 86 220 pred_lin 216.3 ## 16 46 186 pred_lin 57.7 ## 17 54 133 pred_lin 89.4 ## 18 130 360 pred_lin 390.8 ## 19 122 283 pred_lin 359.1 ## 20 118 380 pred_lin 343.2 ## 21 100 185 pred_lin 271.8 ## 22 74 186 pred_lin 168.7 ## 23 146 459 pred_lin 454.2 ## 24 92 167 pred_lin 240.1 ## 25 100 171 pred_lin 271.8 ## 26 140 547 pred_lin 430.4 ## 27 94 170 pred_lin 248.0 ## 28 90 286 pred_lin 232.2 ## 29 120 293 pred_lin 351.1 ## 30 70 109 pred_lin 152.9 ## 31 100 205 pred_lin 271.8 ## 32 132 514 pred_lin 398.7 ## 33 58 175 pred_lin 105.3 ## 34 92 249 pred_lin 240.1 ## 35 76 234 pred_lin 176.7 ## 36 90 242 pred_lin 232.2 ## 37 66 177 pred_lin 137.0 ## 38 134 399 pred_lin 406.7 ## 39 140 511 pred_lin 430.4 ## 40 64 107 pred_lin 129.1 ## 41 72 156 pred_sqr 153.8 ## 42 98 153 pred_sqr 246.5 ## 43 92 230 pred_sqr 222.6 ## 44 90 152 pred_sqr 214.9 ## 45 44 42 pred_sqr 85.6 ## 46 46 157 pred_sqr 89.4 ## 47 90 113 pred_sqr 214.9 ## 48 150 573 pred_sqr 517.0 ## 49 94 202 pred_sqr 230.4 ## 50 90 261 pred_sqr 214.9 ## 51 90 175 pred_sqr 214.9 ## 52 66 212 pred_sqr 136.4 ## 53 142 486 pred_sqr 468.0 ## 54 74 109 pred_sqr 159.9 ## 55 86 220 pred_sqr 200.2 ## 56 46 186 pred_sqr 89.4 ## 57 54 133 pred_sqr 106.2 ## 58 130 360 pred_sqr 399.5 ## 59 122 283 pred_sqr 357.2 ## 60 118 380 pred_sqr 337.1 ## 61 100 185 pred_sqr 254.8 ## 62 74 186 pred_sqr 159.9 ## 63 146 459 pred_sqr 492.1 ## 64 92 167 pred_sqr 222.6 ## 65 100 171 pred_sqr 254.8 ## 66 140 547 pred_sqr 456.1 ## 67 94 170 pred_sqr 230.4 ## 68 90 286 pred_sqr 214.9 ## 69 120 293 pred_sqr 347.1 ## 70 70 109 pred_sqr 147.8 ## 71 100 205 pred_sqr 254.8 ## 72 132 514 pred_sqr 410.5 ## 73 58 175 pred_sqr 115.6 ## 74 92 249 pred_sqr 222.6 ## 75 76 234 pred_sqr 166.2 ## 76 90 242 pred_sqr 214.9 ## 77 66 177 pred_sqr 136.4 ## 78 134 399 pred_sqr 421.7 ## 79 140 511 pred_sqr 456.1 ## 80 64 107 pred_sqr 130.9 Graphically compare the predictions of the two models to the data. houseprice %&gt;% # predictions from linear model mutate(pred_lin = predict(model_lin, houseprice), # predictions from quadratic model pred_sqr = predict(model_sqr, houseprice)) %&gt;% gather(key = modeltype, value = pred, pred_lin, pred_sqr) %&gt;% ggplot(aes(x = size)) + # actual prices geom_point(aes(y = price)) + # the predictions geom_line(aes(y = pred, color = modeltype)) + scale_color_brewer(palette = &quot;Dark2&quot;) In this exercise, you will confirm whether the quadratic model would perform better on out-of-sample data. Since this dataset is small, you will use cross-validation. # Create a splitting plan for 3-fold cross validation set.seed(34245) # set the seed for reproducibility splitPlan &lt;- kWayCrossValidation(nrow(houseprice), 3, NULL, NULL) # Sample code: get cross-val predictions for price ~ size houseprice$pred_lin &lt;- 0 # initialize the prediction vector for(i in 1:3) { split &lt;- splitPlan[[i]] model_lin &lt;- lm(price ~ size, data = houseprice[split$train,]) houseprice$pred_lin[split$app] &lt;- predict(model_lin, newdata = houseprice[split$app,]) } # Get cross-val predictions for price as a function of size^2 (use fmla_sqr) houseprice$pred_sqr &lt;- 0 # initialize the prediction vector for(i in 1:3) { split &lt;- splitPlan[[i]] model_sqr &lt;- lm(price ~ I(size^2), data = houseprice[split$train, ]) houseprice$pred_sqr[split$app] &lt;- predict(model_sqr, newdata = houseprice[split$app, ]) } # Gather the predictions and calculate the residuals houseprice_long &lt;- houseprice %&gt;% gather(key = modeltype, value = pred, pred_lin, pred_sqr) %&gt;% mutate(residuals = price - pred) # Compare the cross-validated RMSE for the two models houseprice_long %&gt;% group_by(modeltype) %&gt;% # group by modeltype summarize(rmse = sqrt(mean(residuals^2))) ## # A tibble: 2 × 2 ## modeltype rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 pred_lin 71.8 ## 2 pred_sqr 60.5 You’ve confirmed that the quadratic input tranformation improved the model. 20.4 Dealing with Non-Linear Responses 20.4.1 Logistic regression to predict probabilities Evaluating a logistic regression model \\(pseudoR^2 = 1 - \\frac{deviance}{null.deviance}\\) Deviance: analogous to variance (RSS) Null deviance: Similar to \\(SS_{Tot}\\) \\(pseudoR^2\\) : Deviance explained, close to 1 is the better 20.4.1.1 Fit a logistic regression model You will estimate the probability that a sparrow survives a severe winter storm, based on physical characteristics of the sparrow. sparrow dataset has columns: status: outcome variable, “Survived” or “Perished” total_length: length of the bird from tip of beak to tip of tail (mm) weight: in grams humerus : length of humerus (“upper arm bone” that connects the wing to the body) (inches) sparrow &lt;- read_rds(&quot;data/sparrow.rds&quot;) glimpse(sparrow) ## Rows: 87 ## Columns: 11 ## $ status &lt;fct&gt; Survived, Survived, Survived, Survived, Survived, Survive… ## $ age &lt;chr&gt; &quot;adult&quot;, &quot;adult&quot;, &quot;adult&quot;, &quot;adult&quot;, &quot;adult&quot;, &quot;adult&quot;, &quot;ad… ## $ total_length &lt;int&gt; 154, 160, 155, 154, 156, 161, 157, 159, 158, 158, 160, 16… ## $ wingspan &lt;int&gt; 241, 252, 243, 245, 247, 253, 251, 247, 247, 252, 252, 25… ## $ weight &lt;dbl&gt; 24.5, 26.9, 26.9, 24.3, 24.1, 26.5, 24.6, 24.2, 23.6, 26.… ## $ beak_head &lt;dbl&gt; 31.2, 30.8, 30.6, 31.7, 31.5, 31.8, 31.1, 31.4, 29.8, 32.… ## $ humerus &lt;dbl&gt; 0.69, 0.74, 0.73, 0.74, 0.71, 0.78, 0.74, 0.73, 0.70, 0.7… ## $ femur &lt;dbl&gt; 0.67, 0.71, 0.70, 0.69, 0.71, 0.74, 0.74, 0.72, 0.67, 0.7… ## $ legbone &lt;dbl&gt; 1.02, 1.18, 1.15, 1.15, 1.13, 1.14, 1.15, 1.13, 1.08, 1.1… ## $ skull &lt;dbl&gt; 0.59, 0.60, 0.60, 0.58, 0.57, 0.61, 0.61, 0.61, 0.60, 0.6… ## $ sternum &lt;dbl&gt; 0.83, 0.84, 0.85, 0.84, 0.82, 0.89, 0.86, 0.79, 0.82, 0.8… # Create the survived column sparrow$survived &lt;- ifelse(sparrow$status == &quot;Survived&quot;, TRUE, FALSE) head(sparrow) ## status age total_length wingspan weight beak_head humerus femur legbone ## 1 Survived adult 154 241 24.5 31.2 0.69 0.67 1.02 ## 2 Survived adult 160 252 26.9 30.8 0.74 0.71 1.18 ## 3 Survived adult 155 243 26.9 30.6 0.73 0.70 1.15 ## 4 Survived adult 154 245 24.3 31.7 0.74 0.69 1.15 ## 5 Survived adult 156 247 24.1 31.5 0.71 0.71 1.13 ## 6 Survived adult 161 253 26.5 31.8 0.78 0.74 1.14 ## skull sternum survived ## 1 0.59 0.83 TRUE ## 2 0.60 0.84 TRUE ## 3 0.60 0.85 TRUE ## 4 0.58 0.84 TRUE ## 5 0.57 0.82 TRUE ## 6 0.61 0.89 TRUE # Fit the logistic regression model sparrow_model &lt;- glm(survived ~ total_length + weight + humerus, data = sparrow, family = &quot;binomial&quot;) # Call summary summary(sparrow_model) ## ## Call: ## glm(formula = survived ~ total_length + weight + humerus, family = &quot;binomial&quot;, ## data = sparrow) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 46.881 16.963 2.76 0.00571 ** ## total_length -0.543 0.141 -3.86 0.00011 *** ## weight -0.569 0.277 -2.05 0.04006 * ## humerus 75.461 19.159 3.94 0.000082 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 118.008 on 86 degrees of freedom ## Residual deviance: 75.094 on 83 degrees of freedom ## AIC: 83.09 ## ## Number of Fisher Scoring iterations: 5 # Call glance perf &lt;- glance(sparrow_model) perf ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 118. 86 -37.5 83.1 93.0 75.1 83 87 # Calculate pseudo-R-squared (pseudoR2 &lt;- 1 - (perf$deviance / perf$null.deviance)) ## [1] 0.364 20.4.1.2 Predict Recall that when calling predict() to get the predicted probabilities from a glm() model, you must specify that you want the response. predict(model, type = \"response\") You will also use the GainCurvePlot() function to plot the gain curve from the model predictions. If the model’s gain curve is close to the ideal (“wizard”) gain curve, then the model sorted the sparrows well. # Make predictions sparrow$pred &lt;- predict(sparrow_model, type = &quot;response&quot;) # Look at gain curve GainCurvePlot(sparrow, &quot;pred&quot;, &quot;survived&quot;, &quot;sparrow survival model&quot;) You see from the gain curve that the model follows the wizard curve for about the first 30% of the data, identifying about 45% of the surviving sparrows with only a few false positives. 20.4.2 Poisson and quasipoisson regression to predict counts Predicting Counts: counts, integers in range [0, ∞] Poisson/Quasipoisson Regression glm(formula, data, family = \"poisson\" / \"quasipoisson\") outcome: integer counts: e.g. number of traffic tickets a driver gets rates: e.g. number of website hits/day prediction: expected rate or intensity (not integral) expected : traffic tickets; expected hits/day Poisson vs. Quasipoisson Poisson assumes that mean(y) = var(y) If var(y) much different from mean(y) ⟶ use quasipoisson If var(y) much close to mean(y) ⟶ use poisson Evaluate the model \\(pseudoR^2\\) RMSE 20.4.2.1 Fit a model to predict counts You will build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July. The data frame has the columns: cnt: the number of bikes rented in that hour (the outcome) hr: the hour of the day (0-23, as a factor) holiday: TRUE/FALSE workingday: TRUE if neither a holiday nor a weekend, else FALSE weathersit: categorical, “Clear to partly cloudy”/“Light Precipitation”/“Misty” temp: normalized temperature in Celsius atemp: normalized “feeling” temperature in Celsius hum: normalized humidity windspeed: normalized windspeed instant: the time index -- number of hours since beginning of dataset (not a variable) mnth and yr: month and year indices (not variables) bikes &lt;- load(&quot;data/Bikes.RData&quot;) str(bikesJuly) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 ... ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ... ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... Should you use poisson or quasipoisson regression? # Calculate the mean and variance of the outcome c(mean_bike = mean(bikesJuly$cnt), var_bike = var(bikesJuly$cnt)) ## mean_bike var_bike ## 274 45864 Since mean and var are much different, use quasipoisson. # Fit the model bike_model &lt;- glm(cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed, bikesJuly, family = &quot;quasipoisson&quot;) # Call glance (perf &lt;- glance(bike_model)) ## # A tibble: 1 × 8 ## null.deviance df.null logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 133365. 743 NA NA NA 28775. 712 744 # Calculate pseudo-R-squared (pseudoR2 &lt;- 1 - (perf$deviance / perf$null.deviance)) ## [1] 0.784 As with a logistic model, you hope for a \\(pseudoR^2\\) near 1. 20.4.2.2 Predict on new data You will use the model you built in the previous exercise to make predictions for the month of August. The dataset bikesAugust has the same columns as bikesJuly. str(bikesAugust) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 ... ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ... ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... Recall that you must specify type = \"response\" with predict() when predicting counts from a glm poisson or quasipoisson model. # Make predictions on August data bikesAugust$pred &lt;- predict(bike_model, newdata = bikesAugust, type = &quot;response&quot;) # Calculate the RMSE bikesAugust %&gt;% mutate(residual = cnt - pred) %&gt;% summarize(rmse = sqrt(mean(residual^2))) ## rmse ## 1 113 # Plot predictions vs cnt (pred on x-axis) ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline(color = &quot;darkblue&quot;) (Quasi)poisson models predict non-negative rates, making them useful for count or frequency data. 20.4.2.3 Visualize the predictions Since the bike rental data is time series data, you might be interested in how the model performs as a function of time. In this exercise, you will compare the predictions and actual rentals on an hourly basis, for the first 14 days of August. head(bikesAugust) ## hr holiday workingday weathersit temp atemp hum windspeed cnt ## 1 0 FALSE TRUE Clear to partly cloudy 0.68 0.636 0.79 0.1642 47 ## 2 1 FALSE TRUE Clear to partly cloudy 0.66 0.606 0.83 0.0896 33 ## 3 2 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 13 ## 4 3 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 7 ## 5 4 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 4 ## 6 5 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 49 ## instant mnth yr pred ## 1 13748 8 1 94.96 ## 2 13749 8 1 51.74 ## 3 13750 8 1 37.98 ## 4 13751 8 1 17.58 ## 5 13752 8 1 9.36 ## 6 13753 8 1 33.20 The time index, instant counts the number of observations since the beginning of data collection. The sample code converts the instants to daily units, starting from 0. bikesAugust %&gt;% # set start to 0, convert unit to days mutate(instant = (instant - min(instant))/24) %&gt;% # gather cnt and pred into a value column gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% select(instant, valuetype, value) %&gt;% head() ## instant valuetype value ## 1 0.0000 cnt 47 ## 2 0.0417 cnt 33 ## 3 0.0833 cnt 13 ## 4 0.1250 cnt 7 ## 5 0.1667 cnt 4 ## 6 0.2083 cnt 49 # Plot predictions and cnt by date/time quasipoisson_plot &lt;- bikesAugust %&gt;% mutate(instant = (instant - min(instant))/24) %&gt;% gather(key = valuetype, value = value, cnt, pred) %&gt;% # restric to first 14 days filter(instant &lt; 14) %&gt;% # plot value by instant ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Quasipoisson model&quot;) quasipoisson_plot This model mostly identifies the slow and busy hours of the day, although it often underestimates peak demand. 20.4.3 GAM to learn non-linear transforms Generalized Additive Models (GAMs) With GAM, the outcome depends additively on unknown smooth functions of the input variables. Automatically learn input variable transformations. Using GAM to learn the transformations is useful when you don’t have the domain knowledge to tell you the correct transform. mgcv package: gam(y ~ s(x1) + x2..., family, data) family gaussian (default): “regular” regression binomial: probabilities poisson/quasipoisson: counts s() designates that variable should be non-linear Use s() with continuous variables Best for larger datasets 20.4.3.1 Model with GAM You will model the average leaf weight on a soybean plant as a function of time (after planting). As you will see, the soybean plant doesn’t grow at a steady rate, but rather has a “growth spurt” that eventually tapers off. Hence, leaf weight is not well described by a linear model. soybean &lt;- load(&quot;data/Soybean.RData&quot;) glimpse(soybean_train) ## Rows: 330 ## Columns: 5 ## $ Plot &lt;ord&gt; 1988F1, 1988F1, 1988F1, 1988F1, 1988F1, 1988F1, 1988F1, 1988F2… ## $ Variety &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F,… ## $ Year &lt;fct&gt; 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 19… ## $ Time &lt;dbl&gt; 14, 21, 28, 35, 49, 63, 77, 21, 28, 35, 49, 56, 70, 14, 21, 28… ## $ weight &lt;dbl&gt; 0.106, 0.261, 0.666, 2.110, 6.230, 13.350, 17.751, 0.269, 0.77… Does the relationship look linear? # Plot weight vs Time (Time on x axis) ggplot(soybean_train, aes(x = Time, y = weight)) + geom_point() Fit a generalized additive model. # Load the package mgcv library(mgcv) # Fit the GAM Model model.gam &lt;- gam(weight ~ s(Time), data = soybean_train, family = gaussian) # Call summary() on model.gam and look for R-squared summary(model.gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## weight ~ s(Time) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 6.164 0.114 53.9 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(Time) 8.5 8.93 338 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.902 Deviance explained = 90.4% ## GCV = 4.4395 Scale est. = 4.3117 n = 330 The “deviance explained” reports the model’s unadjusted \\(R^2\\) # linear model model.lin &lt;- lm(formula = weight ~ Time, soybean_train) # Call summary() on model.lin and look for R-squared summary(model.lin) ## ## Call: ## lm(formula = weight ~ Time, data = soybean_train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.393 -1.710 -0.391 1.906 11.438 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.55928 0.35853 -18.3 &lt;0.0000000000000002 *** ## Time 0.29209 0.00744 39.2 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.78 on 328 degrees of freedom ## Multiple R-squared: 0.824, Adjusted R-squared: 0.824 ## F-statistic: 1.54e+03 on 1 and 328 DF, p-value: &lt;0.0000000000000002 For this data, the GAM appears to fit the data better than a linear model, as measured by the R-squared. See the derived relationship between Time and weight. # Call plot() on model.gam plot(model.gam) 20.4.3.2 Predict with on test data glimpse(soybean_test) ## Rows: 82 ## Columns: 5 ## $ Plot &lt;ord&gt; 1988F1, 1988F1, 1988F1, 1988F2, 1988F2, 1988F2, 1988F3, 1988F3… ## $ Variety &lt;fct&gt; F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, F, P, P, P, P, P,… ## $ Year &lt;fct&gt; 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 1988, 19… ## $ Time &lt;dbl&gt; 42, 56, 70, 14, 42, 77, 49, 63, 14, 35, 63, 42, 14, 21, 28, 70… ## $ weight &lt;dbl&gt; 3.560, 8.710, 16.342, 0.104, 2.930, 17.747, 6.130, 18.080, 0.1… For GAM models, the predict() method returns a matrix, so use as.numeric() to convert the matrix to a vector. # Get predictions from linear model soybean_test$pred.lin &lt;- predict(model.lin, newdata = soybean_test) # Get predictions from gam model soybean_test$pred.gam &lt;- as.numeric(predict(model.gam, newdata = soybean_test)) head(soybean_test) ## Grouped Data: weight ~ Time | Plot ## Plot Variety Year Time weight pred.lin pred.gam ## 5 1988F1 F 1988 42 3.560 5.71 3.94 ## 7 1988F1 F 1988 56 8.710 9.80 9.96 ## 9 1988F1 F 1988 70 16.342 13.89 16.55 ## 11 1988F2 F 1988 14 0.104 -2.47 0.13 ## 15 1988F2 F 1988 42 2.930 5.71 3.94 ## 19 1988F2 F 1988 77 17.747 15.93 18.68 # Gather the predictions into a &quot;long&quot; dataset soybean_long &lt;- soybean_test %&gt;% gather(key = modeltype, value = pred, pred.lin, pred.gam) head(soybean_long) ## Plot Variety Year Time weight modeltype pred ## 1 1988F1 F 1988 42 3.560 pred.lin 5.71 ## 2 1988F1 F 1988 56 8.710 pred.lin 9.80 ## 3 1988F1 F 1988 70 16.342 pred.lin 13.89 ## 4 1988F2 F 1988 14 0.104 pred.lin -2.47 ## 5 1988F2 F 1988 42 2.930 pred.lin 5.71 ## 6 1988F2 F 1988 77 17.747 pred.lin 15.93 Calculate and compare the RMSE of both models. # Calculate the rmse soybean_long %&gt;% mutate(residual = weight - pred) %&gt;% # residuals group_by(modeltype) %&gt;% # group by modeltype summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE ## # A tibble: 2 × 2 ## modeltype rmse ## &lt;chr&gt; &lt;dbl&gt; ## 1 pred.gam 2.29 ## 2 pred.lin 3.19 Compare the predictions of each model against the actual average leaf weights. # Compare the predictions against actual weights on the test data soybean_long %&gt;% # the column for the x axis ggplot(aes(x = Time)) + # the y-column for the scatterplot geom_point(aes(y = weight)) + # the y-column for the point-and-line plot geom_point(aes(y = pred, color = modeltype)) + # the y-column for the point-and-line plot geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + scale_color_brewer(palette = &quot;Dark2&quot;) Notice that the linear model sometimes predicts negative weights! But GAM doesn’t. The GAM learns the non-linear growth function of the soybean plants, including the fact that weight is never negative. 20.5 Tree-Based Methods 20.5.1 Random forests You will again build a model to predict the number of bikes rented in an hour as a function of the weather, the type of day (holiday, working day, or weekend), and the time of day. You will train the model on data from the month of July. You will use the ranger package to fit the random forest model. ranger(fmla, data, num.trees, respect.unordered.factors = \"order\") respect.unordered.factors: specifies how to treat unordered factor variables. str(bikesJuly) ## &#39;data.frame&#39;: 744 obs. of 12 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ cnt : int 149 93 90 33 4 10 27 50 142 219 ... ## $ instant : int 13004 13005 13006 13007 13008 13009 13010 13011 13012 13013 ... ## $ mnth : int 7 7 7 7 7 7 7 7 7 7 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... Set up # Random seed to reproduce results seed &lt;- 423563 # The outcome column outcome &lt;- &quot;cnt&quot; # The input variables vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;) # Create the formula string for bikes rented as a function of the inputs (fmla &lt;- paste(outcome, &quot;~&quot;, paste(vars, collapse = &quot; + &quot;))) ## [1] &quot;cnt ~ hr + holiday + workingday + weathersit + temp + atemp + hum + windspeed&quot; # Load the package ranger library(ranger) ## ## Attaching package: &#39;ranger&#39; ## The following object is masked from &#39;package:randomForest&#39;: ## ## importance # Fit and print the random forest model (bike_model_rf &lt;- ranger(fmla, # formula bikesJuly, # data num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed)) ## Ranger result ## ## Call: ## ranger(fmla, bikesJuly, num.trees = 500, respect.unordered.factors = &quot;order&quot;, seed = seed) ## ## Type: Regression ## Number of trees: 500 ## Sample size: 744 ## Number of independent variables: 8 ## Mtry: 2 ## Target node size: 5 ## Variable importance mode: none ## Splitrule: variance ## OOB prediction error (MSE): 8231 ## R squared (OOB): 0.821 Now, predict bike rentals for the month of August. The predict() function for a ranger model produces a list. One of the elements of this list is predictions, a vector of predicted values. Access predictions with the $ notation. # Make predictions on the August data bikesAugust$pred &lt;- predict(bike_model_rf, bikesAugust)$predictions # Calculate the RMSE of the predictions bikesAugust %&gt;% mutate(residual = cnt - pred) %&gt;% # calculate the residual summarize(rmse = sqrt(mean(residual^2))) # calculate rmse ## rmse ## 1 97.2 The poisson model you built for this data gave an RMSE of about 112.6. How does this model compare? # Plot actual outcome vs predictions (predictions on x-axis) ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() This random forest model outperforms the poisson count model on the same data; it is discovering more complex non-linear or non-additive relationships in the data. Visualize random forest bike model predictions Recall that the quasipoisson model mostly identified the pattern of slow and busy hours in the day, but it somewhat underestimated peak demands. You would like to see how the random forest model compares. head(bikesAugust) ## hr holiday workingday weathersit temp atemp hum windspeed cnt ## 1 0 FALSE TRUE Clear to partly cloudy 0.68 0.636 0.79 0.1642 47 ## 2 1 FALSE TRUE Clear to partly cloudy 0.66 0.606 0.83 0.0896 33 ## 3 2 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 13 ## 4 3 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 7 ## 5 4 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 4 ## 6 5 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 49 ## instant mnth yr pred ## 1 13748 8 1 77.4 ## 2 13749 8 1 36.2 ## 3 13750 8 1 38.3 ## 4 13751 8 1 28.2 ## 5 13752 8 1 42.2 ## 6 13753 8 1 52.4 first_two_weeks &lt;- bikesAugust %&gt;% # Set start to 0, convert unit to days mutate(instant = (instant - min(instant)) / 24) %&gt;% # Gather cnt and pred into a column named value with key valuetype gather(key = valuetype, value = value, cnt, pred) %&gt;% # Filter for rows in the first two filter(instant &lt; 14) head(first_two_weeks) ## hr holiday workingday weathersit temp atemp hum windspeed ## 1 0 FALSE TRUE Clear to partly cloudy 0.68 0.636 0.79 0.1642 ## 2 1 FALSE TRUE Clear to partly cloudy 0.66 0.606 0.83 0.0896 ## 3 2 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 ## 4 3 FALSE TRUE Clear to partly cloudy 0.64 0.576 0.83 0.1045 ## 5 4 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 ## 6 5 FALSE TRUE Misty 0.64 0.591 0.78 0.1343 ## instant mnth yr valuetype value ## 1 0.0000 8 1 cnt 47 ## 2 0.0417 8 1 cnt 33 ## 3 0.0833 8 1 cnt 13 ## 4 0.1250 8 1 cnt 7 ## 5 0.1667 8 1 cnt 4 ## 6 0.2083 8 1 cnt 49 Plot the predictions and actual counts by hour for the first 14 days of August. # Plot predictions and cnt by date/time randomforest_plot &lt;- ggplot(first_two_weeks, aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Random Forest model&quot;) randomforest_plot The random forest model captured the day-to-day variations in peak demand better than the quasipoisson model, but it still underestmates peak demand, and also overestimates minimum demand. So there is still room for improvement. 20.5.2 One-Hot-Encoding 20.5.2.1 vtreat vtreat creates a treatment plan to transform categorical variables into indicator variables (coded \"lev\"), and to clean bad values out of numerical variables (coded \"clean\"). To design a treatment plan: treatplan &lt;- designTreatmentsZ(data, varlist) data: the original training data frame varlist: a vector of input variables to be treated (as strings). designTreatmentsZ() returns a list with an element scoreFrame: a data frame that includes the names and types of the new variables: scoreFrame &lt;- treatplan %&gt;% magrittr::use_series(scoreFrame) %&gt;% select(varName, origName, code) varName: the name of the new treated variable origName: the name of the original variable that the treated variable comes from code: the type of the new variable. \"clean\": a numerical variable with no NAs or NaNs \"lev\": an indicator variable for a specific level of the original categorical variable. For these exercises, we want varName where code is either \"clean\" or \"lev\": newvarlist &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;) %&gt;% magrittr::use_series(varName) To transform the dataset into all numerical and one-hot-encoded variables: data.treat &lt;- prepare(treatplan, data, varRestrictions = newvarlist) treatplan: the treatment plan data: the data frame to be treated varRestrictions: the variables desired in the treated data Assume that color and size are input variables, and popularity is the outcome to be predicted. library(magrittr) dframe &lt;- read_tsv(&quot;data/dframe_vtreat.txt&quot;) dframe ## # A tibble: 10 × 3 ## color size popularity ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 r 11 1.40 ## 2 b 15 0.922 ## 3 g 14 1.20 ## 4 b 13 1.08 ## 5 r 11 0.804 ## 6 r 9 1.10 ## 7 g 12 0.875 ## 8 b 7 0.695 ## 9 g 12 0.883 ## 10 g 11 1.02 # Create a vector of variable names vars &lt;- c(&quot;color&quot;, &quot;size&quot;) # Create the treatment plan treatplan &lt;- designTreatmentsZ(dframe, vars) ## [1] &quot;vtreat 1.6.4 inspecting inputs Tue Jan 16 13:43:25 2024&quot; ## [1] &quot;designing treatments Tue Jan 16 13:43:25 2024&quot; ## [1] &quot; have initial level statistics Tue Jan 16 13:43:25 2024&quot; ## [1] &quot; scoring treatments Tue Jan 16 13:43:25 2024&quot; ## [1] &quot;have treatment plan Tue Jan 16 13:43:25 2024&quot; treatplan ## [1] &quot;treatmentplan&quot; ## origName varName code rsq sig extraModelDegrees ## 1 color color_catP catP 0 1 2 ## 2 size size clean 0 1 0 ## 3 color color_lev_x_b lev 0 1 0 ## 4 color color_lev_x_g lev 0 1 0 ## 5 color color_lev_x_r lev 0 1 0 # Examine the scoreFrame (scoreFrame &lt;- treatplan %&gt;% use_series(scoreFrame) %&gt;% select(varName, origName, code)) ## varName origName code ## 1 color_catP color catP ## 2 size size clean ## 3 color_lev_x_b color lev ## 4 color_lev_x_g color lev ## 5 color_lev_x_r color lev # We only want the rows with codes &quot;clean&quot; or &quot;lev&quot; (newvars &lt;- scoreFrame %&gt;% filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% use_series(varName)) ## [1] &quot;size&quot; &quot;color_lev_x_b&quot; &quot;color_lev_x_g&quot; &quot;color_lev_x_r&quot; # Create the treated training data (dframe.treat &lt;- prepare(treatplan, dframe, varRestriction = newvars)) ## size color_lev_x_b color_lev_x_g color_lev_x_r ## 1 11 0 0 1 ## 2 15 1 0 0 ## 3 14 0 1 0 ## 4 13 1 0 0 ## 5 11 0 0 1 ## 6 9 0 0 1 ## 7 12 0 1 0 ## 8 7 1 0 0 ## 9 12 0 1 0 ## 10 11 0 1 0 20.5.2.2 Novel levels When a level of a categorical variable is rare, sometimes it will fail to show up in training data. If that rare level then appears in future data, downstream models may not know what to do with it. When such novel levels appear, using model.matrix or caret::dummyVars to one-hot-encode will not work correctly. vtreat is a “safer” alternative to model.matrix for one-hot-encoding, because it can manage novel levels safely. vtreat also manages missing values in the data (both categorical and continuous). In this exercise, you will see how vtreat handles categorical values that did not appear in the training set. Are there colors in testframe that didn’t appear in dframe? testframe &lt;- read_tsv(&quot;data/testframe_vtreat.txt&quot;) list(testframe = unique(testframe$color), dframe = unique(dframe$color)) ## $testframe ## [1] &quot;g&quot; &quot;y&quot; &quot;b&quot; &quot;r&quot; ## ## $dframe ## [1] &quot;r&quot; &quot;b&quot; &quot;g&quot; # Use prepare() to one-hot-encode testframe (testframe.treat &lt;- prepare(treatplan, testframe, varRestriction = newvars)) ## size color_lev_x_b color_lev_x_g color_lev_x_r ## 1 7 0 1 0 ## 2 8 0 1 0 ## 3 10 0 0 0 ## 4 12 1 0 0 ## 5 6 0 0 0 ## 6 8 0 0 1 ## 7 12 0 1 0 ## 8 12 1 0 0 ## 9 12 0 0 0 ## 10 8 1 0 0 As you saw, vtreat encodes novel colors like yellow that were not present in the data as all zeros: ‘none of the known colors’. This allows downstream models to accept these novel values without crashing. 20.5.2.3 vtreat the bike rental data In this exercise, you will create one-hot-encoded data frames of the July/August bike data, for use with xgboost later on. Set the flag verbose=FALSE to prevent the function from printing too many messages. # The outcome column outcome &lt;- &quot;cnt&quot; # The input columns vars &lt;- c(&quot;hr&quot;, &quot;holiday&quot;, &quot;workingday&quot;, &quot;weathersit&quot;, &quot;temp&quot;, &quot;atemp&quot;, &quot;hum&quot;, &quot;windspeed&quot;) # Create the treatment plan from bikesJuly (the training data) treatplan &lt;- designTreatmentsZ(bikesJuly, vars, verbose = FALSE) summary(treatplan) ## Length Class Mode ## treatments 9 -none- list ## scoreFrame 8 data.frame list ## outcomename 1 -none- character ## vtreatVersion 1 package_version list ## outcomeType 1 -none- character ## outcomeTarget 1 -none- character ## meanY 1 -none- logical ## splitmethod 1 -none- character # Get the &quot;clean&quot; and &quot;lev&quot; variables from the scoreFrame (newvars &lt;- treatplan %&gt;% use_series(scoreFrame) %&gt;% # get the rows you care about filter(code %in% c(&quot;clean&quot;, &quot;lev&quot;)) %&gt;% # get the varName column use_series(varName)) ## [1] &quot;holiday&quot; ## [2] &quot;workingday&quot; ## [3] &quot;temp&quot; ## [4] &quot;atemp&quot; ## [5] &quot;hum&quot; ## [6] &quot;windspeed&quot; ## [7] &quot;hr_lev_x_0&quot; ## [8] &quot;hr_lev_x_1&quot; ## [9] &quot;hr_lev_x_10&quot; ## [10] &quot;hr_lev_x_11&quot; ## [11] &quot;hr_lev_x_12&quot; ## [12] &quot;hr_lev_x_13&quot; ## [13] &quot;hr_lev_x_14&quot; ## [14] &quot;hr_lev_x_15&quot; ## [15] &quot;hr_lev_x_16&quot; ## [16] &quot;hr_lev_x_17&quot; ## [17] &quot;hr_lev_x_18&quot; ## [18] &quot;hr_lev_x_19&quot; ## [19] &quot;hr_lev_x_2&quot; ## [20] &quot;hr_lev_x_20&quot; ## [21] &quot;hr_lev_x_21&quot; ## [22] &quot;hr_lev_x_22&quot; ## [23] &quot;hr_lev_x_23&quot; ## [24] &quot;hr_lev_x_3&quot; ## [25] &quot;hr_lev_x_4&quot; ## [26] &quot;hr_lev_x_5&quot; ## [27] &quot;hr_lev_x_6&quot; ## [28] &quot;hr_lev_x_7&quot; ## [29] &quot;hr_lev_x_8&quot; ## [30] &quot;hr_lev_x_9&quot; ## [31] &quot;weathersit_lev_x_Clear_to_partly_cloudy&quot; ## [32] &quot;weathersit_lev_x_Light_Precipitation&quot; ## [33] &quot;weathersit_lev_x_Misty&quot; # Prepare the training data bikesJuly.treat &lt;- prepare(treatplan, bikesJuly, varRestriction = newvars) # Prepare the test data bikesAugust.treat &lt;- prepare(treatplan, bikesAugust, varRestriction = newvars) # Call str() on the treated data str(bikesJuly.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ holiday : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday : num 0 0 0 0 0 0 0 0 0 0 ... ## $ temp : num 0.76 0.74 0.72 0.72 0.7 0.68 0.7 0.74 0.78 0.82 ... ## $ atemp : num 0.727 0.697 0.697 0.712 0.667 ... ## $ hum : num 0.66 0.7 0.74 0.84 0.79 0.79 0.79 0.7 0.62 0.56 ... ## $ windspeed : num 0 0.1343 0.0896 0.1343 0.194 ... ## $ hr_lev_x_0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x_20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x_4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x_5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x_6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x_7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x_8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x_9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ weathersit_lev_x_Clear_to_partly_cloudy: num 1 1 1 1 1 1 1 1 1 1 ... ## $ weathersit_lev_x_Light_Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x_Misty : num 0 0 0 0 0 0 0 0 0 0 ... str(bikesAugust.treat) ## &#39;data.frame&#39;: 744 obs. of 33 variables: ## $ holiday : num 0 0 0 0 0 0 0 0 0 0 ... ## $ workingday : num 1 1 1 1 1 1 1 1 1 1 ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ hr_lev_x_0 : num 1 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_1 : num 0 1 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_10 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_11 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_12 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_13 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_14 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_15 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_16 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_17 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_18 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_19 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_2 : num 0 0 1 0 0 0 0 0 0 0 ... ## $ hr_lev_x_20 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_21 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_22 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_23 : num 0 0 0 0 0 0 0 0 0 0 ... ## $ hr_lev_x_3 : num 0 0 0 1 0 0 0 0 0 0 ... ## $ hr_lev_x_4 : num 0 0 0 0 1 0 0 0 0 0 ... ## $ hr_lev_x_5 : num 0 0 0 0 0 1 0 0 0 0 ... ## $ hr_lev_x_6 : num 0 0 0 0 0 0 1 0 0 0 ... ## $ hr_lev_x_7 : num 0 0 0 0 0 0 0 1 0 0 ... ## $ hr_lev_x_8 : num 0 0 0 0 0 0 0 0 1 0 ... ## $ hr_lev_x_9 : num 0 0 0 0 0 0 0 0 0 1 ... ## $ weathersit_lev_x_Clear_to_partly_cloudy: num 1 1 1 1 0 0 1 0 0 0 ... ## $ weathersit_lev_x_Light_Precipitation : num 0 0 0 0 0 0 0 0 0 0 ... ## $ weathersit_lev_x_Misty : num 0 0 0 0 1 1 0 1 1 1 ... The bike data is now in completely numeric form, ready to use with xgboost. Note that the treated data does not include the outcome column. 20.5.3 Gradient boosting Gradient boosting is an ensemble method that builds up a model by incrementally improving the existing one. Repeat until either the residuals are small enough, or the maximum number of iterations is reached. Regularization: learning rate η ∈ (0, 1) Larger η : faster learning Smaller η : less risk of over xgboost package: Run xgb.cv() with a large number of rounds (trees). xgb.cv()$evaluation_log : records estimated RMSE for each round. Find the number of trees that minimizes estimated RMSE: \\(n_{best}\\) Run xgboost(), setting nrounds = \\(n_{best}\\) 20.5.3.1 Find the right number of trees In this exercise, you will get ready to build a gradient boosting model to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July. Remember that bikesJuly.treat no longer has the outcome column, so you must get it from the untreated data: bikesJuly$cnt. You will use the xgboost package to fit the random forest model. xgb.cv() uses cross-validation to estimate the out-of-sample learning error as each new tree is added to the model. The appropriate number of trees to use in the final model is the number that minimizes the holdout RMSE. The key arguments to the xgb.cv() call are: data: a numeric matrix. label: vector of outcomes (also numeric). nrounds: the maximum number of rounds (trees to build). nfold: the number of folds for the cross-validation. 5 is a good number. objective: \"reg:squarederror\" for continuous outcomes. eta: the learning rate. max_depth: maximum depth of trees. early_stopping_rounds: after this many rounds without improvement, stop. verbose: FALSE to stay silent. set.seed(1234) # Load the package xgboost library(xgboost) ## ## Attaching package: &#39;xgboost&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## slice # Run xgb.cv cv &lt;- xgb.cv(data = as.matrix(bikesJuly.treat), label = bikesJuly$cnt, nrounds = 50, nfold = 5, objective = &quot;reg:squarederror&quot;, eta = 0.75, max_depth = 5, early_stopping_rounds = 5, verbose = FALSE # silent ); cv ## ##### xgb.cv 5-folds ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1 161.5 1.255 168.8 6.00 ## 2 121.2 1.156 134.6 4.69 ## 3 105.4 2.290 120.9 8.65 ## 4 92.3 1.555 107.1 6.42 ## 5 81.0 4.094 101.4 3.83 ## 6 70.4 4.588 96.1 4.49 ## 7 65.1 4.510 92.9 3.76 ## 8 59.8 4.005 90.1 2.85 ## 9 56.6 4.103 89.3 3.80 ## 10 52.7 4.536 88.2 4.51 ## 11 49.1 2.629 87.4 2.48 ## 12 46.1 2.230 87.0 2.87 ## 13 43.3 2.285 86.1 3.00 ## 14 41.5 2.421 85.7 2.98 ## 15 39.3 2.870 84.7 2.67 ## 16 37.2 2.554 84.4 2.72 ## 17 35.7 2.733 84.7 3.29 ## 18 33.8 2.237 84.1 3.43 ## 19 32.1 1.887 84.4 3.93 ## 20 30.9 2.122 84.2 3.63 ## 21 29.6 2.054 83.8 3.28 ## 22 28.1 1.892 83.7 3.65 ## 23 27.2 1.968 83.6 3.81 ## 24 26.4 2.126 83.5 3.84 ## 25 25.5 2.208 83.3 3.97 ## 26 24.5 2.143 83.5 4.10 ## 27 23.2 2.056 83.3 3.76 ## 28 22.4 1.767 83.2 3.82 ## 29 21.4 1.618 83.2 3.96 ## 30 20.7 1.415 83.4 4.08 ## 31 20.0 1.348 83.5 4.07 ## 32 19.2 1.164 83.4 4.24 ## 33 18.6 0.933 83.1 4.50 ## 34 18.1 0.766 83.4 4.56 ## 35 17.6 0.839 83.6 4.61 ## 36 17.1 0.800 83.6 4.44 ## 37 16.6 0.843 83.7 4.27 ## 38 16.0 0.717 83.6 4.06 ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## Best iteration: ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 33 18.6 0.933 83.1 4.5 summary(cv) ## Length Class Mode ## call 10 -none- call ## params 4 -none- list ## callbacks 2 -none- list ## evaluation_log 5 data.table list ## niter 1 -none- numeric ## nfeatures 1 -none- numeric ## folds 5 -none- list ## best_iteration 1 -none- numeric ## best_ntreelimit 1 -none- numeric Each row of the evaluation_log corresponds to an additional tree, so the row number tells you the number of trees in the model. # Get the evaluation log elog &lt;- cv$evaluation_log elog ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std ## 1: 1 161.5 1.255 168.8 6.00 ## 2: 2 121.2 1.156 134.6 4.69 ## 3: 3 105.4 2.290 120.9 8.65 ## 4: 4 92.3 1.555 107.1 6.42 ## 5: 5 81.0 4.094 101.4 3.83 ## 6: 6 70.4 4.588 96.1 4.49 ## 7: 7 65.1 4.510 92.9 3.76 ## 8: 8 59.8 4.005 90.1 2.85 ## 9: 9 56.6 4.103 89.3 3.80 ## 10: 10 52.7 4.536 88.2 4.51 ## 11: 11 49.1 2.629 87.4 2.48 ## 12: 12 46.1 2.230 87.0 2.87 ## 13: 13 43.3 2.285 86.1 3.00 ## 14: 14 41.5 2.421 85.7 2.98 ## 15: 15 39.3 2.870 84.7 2.67 ## 16: 16 37.2 2.554 84.4 2.72 ## 17: 17 35.7 2.733 84.7 3.29 ## 18: 18 33.8 2.237 84.1 3.43 ## 19: 19 32.1 1.887 84.4 3.93 ## 20: 20 30.9 2.122 84.2 3.63 ## 21: 21 29.6 2.054 83.8 3.28 ## 22: 22 28.1 1.892 83.7 3.65 ## 23: 23 27.2 1.968 83.6 3.81 ## 24: 24 26.4 2.126 83.5 3.84 ## 25: 25 25.5 2.208 83.3 3.97 ## 26: 26 24.5 2.143 83.5 4.10 ## 27: 27 23.2 2.056 83.3 3.76 ## 28: 28 22.4 1.767 83.2 3.82 ## 29: 29 21.4 1.618 83.2 3.96 ## 30: 30 20.7 1.415 83.4 4.08 ## 31: 31 20.0 1.348 83.5 4.07 ## 32: 32 19.2 1.164 83.4 4.24 ## 33: 33 18.6 0.933 83.1 4.50 ## 34: 34 18.1 0.766 83.4 4.56 ## 35: 35 17.6 0.839 83.6 4.61 ## 36: 36 17.1 0.800 83.6 4.44 ## 37: 37 16.6 0.843 83.7 4.27 ## 38: 38 16.0 0.717 83.6 4.06 ## iter train_rmse_mean train_rmse_std test_rmse_mean test_rmse_std # Determine and print how many trees minimize training and test error elog %&gt;% # find the index of min(train_rmse_mean) summarize(ntrees.train = which.min(train_rmse_mean), # find the index of min(test_rmse_mean) ntrees.test = which.min(test_rmse_mean)) ## ntrees.train ntrees.test ## 1 38 33 In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test error starts to increase. It’s important to use cross-validation to find the right number of trees (as determined by ntrees.test) and avoid an overfit model. 20.5.3.2 Fit xgboost model and predict You will fit a gradient boosting model using xgboost() to predict the number of bikes rented in an hour as a function of the weather and the type and time of day. You will train the model on data from the month of July and predict on data for the month of August. # best number of trees ntrees &lt;- 30 set.seed(1234) # Run xgboost on training data bike_model_xgb &lt;- xgboost( data = as.matrix(bikesJuly.treat), # training data as matrix label = bikesJuly$cnt, # column of outcomes nrounds = ntrees, # number of trees to build objective = &quot;reg:squarederror&quot;, # objective eta = 0.75, # learning rate max_depth = 5, verbose = FALSE # silent ) # Make predictions on testing data bikesAugust$pred &lt;- predict(bike_model_xgb, as.matrix(bikesAugust.treat)) # Plot predictions (on x axis) vs actual bike rental count ggplot(bikesAugust, aes(x = pred, y = cnt)) + geom_point() + geom_abline() Overall, the scatterplot looked pretty good, but did you notice that the model made some negative predictions? 20.5.3.3 Evaluate the xgboost model You will evaluate the gradient boosting model bike_model_xgb that you fit in the last exercise, using data from the month of August. You’ll compare this model’s RMSE for August to the RMSE of previous models that you’ve built. str(bikesAugust) ## &#39;data.frame&#39;: 744 obs. of 13 variables: ## $ hr : Factor w/ 24 levels &quot;0&quot;,&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ holiday : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ workingday: logi TRUE TRUE TRUE TRUE TRUE TRUE ... ## $ weathersit: chr &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; &quot;Clear to partly cloudy&quot; ... ## $ temp : num 0.68 0.66 0.64 0.64 0.64 0.64 0.64 0.64 0.66 0.68 ... ## $ atemp : num 0.636 0.606 0.576 0.576 0.591 ... ## $ hum : num 0.79 0.83 0.83 0.83 0.78 0.78 0.78 0.83 0.78 0.74 ... ## $ windspeed : num 0.1642 0.0896 0.1045 0.1045 0.1343 ... ## $ cnt : int 47 33 13 7 4 49 185 487 681 350 ... ## $ instant : int 13748 13749 13750 13751 13752 13753 13754 13755 13756 13757 ... ## $ mnth : int 8 8 8 8 8 8 8 8 8 8 ... ## $ yr : int 1 1 1 1 1 1 1 1 1 1 ... ## $ pred : num 52.38 40.3 6.55 12.72 -22.96 ... Compare to the RMSE from the: poisson model (approx. 112.6) &amp; random forest model (approx. 96.7). # Calculate RMSE bikesAugust %&gt;% mutate(residuals = cnt - pred) %&gt;% summarize(rmse = sqrt(mean(residuals^2))) ## rmse ## 1 84.4 Even though this gradient boosting made some negative predictions, overall it makes smaller errors than the previous two models. Perhaps rounding negative predictions up to zero is a reasonable tradeoff. 20.5.3.4 Visualize the xgboost model You’ve now seen three different ways to model the bike rental data. Let’s compare the gradient boosting model’s predictions to the other two models as a function of time. # Plot predictions and actual bike rentals as a function of time (days) bikesAugust %&gt;% # set start to 0, convert unit to days mutate(instant = (instant - min(instant))/24) %&gt;% gather(key = valuetype, value = value, cnt, pred) %&gt;% filter(instant &lt; 14) %&gt;% # first two weeks ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + geom_point() + geom_line() + scale_x_continuous(&quot;Day&quot;, breaks = 0:14, labels = 0:14) + scale_color_brewer(palette = &quot;Dark2&quot;) + ggtitle(&quot;Predicted August bike rentals, Gradient Boosting model&quot;) The gradient boosting pattern captures rental variations due to time of day and other factors better than the previous models. gridExtra::grid.arrange(quasipoisson_plot, randomforest_plot, nrow = 2) "],["unsupervised-learning.html", "Chapter 21 Unsupervised Learning 21.1 k-means clustering 21.2 Hierarchical clustering 21.3 Dimensionality reduction - PCA 21.4 Case study", " Chapter 21 Unsupervised Learning 21.1 k-means clustering 21.1.1 Types of machine learning Unsupervised learning Finding structure in unlabeled data. Clustering Finding homogeneous subgroups within larger group. Dimensionality reduction Finding patterns in the features of the data. Visualization of high dimensional data. Pre-processing before supervised learning. Supervised learning Making predictions based on labeled data. Regression: continous DV Classification: categorical DV Reinforcement learning 21.1.2 Introduction to k-means k-means in R kmeans(x, centers = num, nstart = num) x : data centers : the number of predetermined groups or clusters nstart : run algorithm multiple times to improve odds of the best model. (kmeans algorithm has a random component. A single run of kmeans may not find the optimal solution to kmeans.) 21.1.2.1 k-means clustering library(tidyverse) # str(x): num [1:300, 1:2] x &lt;- read_tsv(&quot;data/x_k_means.txt&quot;, col_select = -1) %&gt;% as.matrix() head(x) ## [,1] [,2] ## [1,] 3.37 2.00 ## [2,] 1.44 2.76 ## [3,] 2.36 2.04 ## [4,] 2.63 2.74 ## [5,] 2.40 1.85 ## [6,] 1.89 1.94 Plot to see possible centers. plot(x[,1], x[,2]) # Create the k-means model: km.out km.out &lt;- kmeans(x, centers = 3, nstart = 20) # Inspect the result summary(km.out) ## Length Class Mode ## cluster 300 -none- numeric ## centers 6 -none- numeric ## totss 1 -none- numeric ## withinss 3 -none- numeric ## tot.withinss 1 -none- numeric ## betweenss 1 -none- numeric ## size 3 -none- numeric ## iter 1 -none- numeric ## ifault 1 -none- numeric Access the cluster component directly. This is useful anytime you need the cluster membership for each observation of the data used to build the clustering model. # Print the cluster membership component of the model km.out$cluster ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 ## [38] 3 3 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 ## [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 ## [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 3 3 3 3 ## [260] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 ## [297] 3 1 3 3 Human friendly output of basic modeling results. # Print the km.out object km.out ## K-means clustering with 3 clusters of sizes 98, 150, 52 ## ## Cluster means: ## [,1] [,2] ## 1 2.217 2.0511 ## 2 -5.056 1.9699 ## 3 0.664 -0.0913 ## ## Clustering vector: ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 ## [38] 3 3 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 ## [75] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 ## [112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [186] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 ## [223] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 1 3 3 3 3 ## [260] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 3 3 1 3 3 3 3 3 3 1 3 3 3 3 3 3 1 3 3 ## [297] 3 1 3 3 ## ## Within cluster sum of squares by cluster: ## [1] 148.6 295.2 95.5 ## (between_SS / total_SS = 87.2 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; 21.1.2.2 Visualizing and interpreting One of the more intuitive ways to interpret the results of k-means models is by plotting the data as a scatter plot and using color to label the samples’ cluster membership (col = km.out$cluster). # Scatter plot of x plot(x, col = km.out$cluster, main = &quot;k-means with 3 clusters&quot;, xlab = &quot;&quot;, ylab = &quot;&quot;) 21.1.3 Model selection Best outcome is based on total within cluster sum of squares: For each cluster For each observation in the cluster Determine squared distance from observation to cluster center Sum all of them together Running algorithm multiple times helps find the global minimum total within cluster sum of squares. 21.1.3.1 Handling random algorithms This random initialization can result in assigning observations to different cluster labels. Also, the random initialization can result in finding different local minima for the k-means algorithm. At the top of each plot, the measure of model quality—total within cluster sum of squares error—will be plotted. Look for the model(s) with the lowest error to find models with the better model results. Your task is to generate six kmeans() models on the data, plotting the results of each, in order to see the impact of random initializations on model results. # Set up 2 x 3 plotting grid par(mfrow = c(2, 3)) # Set seed set.seed(1) for(i in 1:6) { # Run kmeans() on x with three clusters and one start km.out &lt;- kmeans(x, centers = 3, nstart = 1) # Plot clusters plot(x, col = km.out$cluster, main = km.out$tot.withinss, # total within cluster sum of squares error xlab = &quot;&quot;, ylab = &quot;&quot;) } Because of the random initialization of the k-means algorithm, there’s quite some variation in cluster assignments among the six models. 21.1.3.2 Selecting number of clusters If you do not know the number of clusters and need to determine it, you will need to run the algorithm multiple times, each time with a different number of clusters. From this, you can observe how a measure of model quality changes with the number of clusters. Plots displaying this information help to determine the number of clusters and are often referred to as scree plots. The elbow indicates the number of clusters inherent in the data. # Initialize total within sum of squares error: wss wss &lt;- 0 # For 1 to 15 cluster centers for (i in 1:15) { km.out &lt;- kmeans(x, centers = i, nstart = 20) # Save total within sum of squares to wss variable wss[i] &lt;- km.out$tot.withinss } wss ## [1] 4211 777 539 432 369 309 263 230 202 180 161 150 139 129 121 # Plot total within sum of squares vs. number of clusters plot(1:15, wss, type = &quot;b&quot;, xlab = &quot;Number of Clusters&quot;, ylab = &quot;Within groups sum of squares&quot;) # Set k equal to the number of clusters corresponding to the elbow location k &lt;- 2 21.1.4 Pokemon data Data challenges Selecting the variables to cluster upon Scaling the data Determining the number of clusters Often no clean “elbow” in scree plot This will be a core part of the exercises Visualize the results for interpretation 21.1.4.1 Real world data The first challenge with the Pokemon data is that there is no pre-determined number of clusters. You will determine the appropriate number of clusters, keeping in mind that in real data the elbow in the scree plot might be less of a sharp elbow. Use your judgment on making the determination of the number of clusters. # convert df to matrix, dim: 800*6 pokemon &lt;- read_csv(&quot;data/Pokemon.csv&quot;) %&gt;% select(6:11) %&gt;% as.matrix() head(pokemon) ## HitPoints Attack Defense SpecialAttack SpecialDefense Speed ## [1,] 45 49 49 65 65 45 ## [2,] 60 62 63 80 80 60 ## [3,] 80 82 83 100 100 80 ## [4,] 80 100 123 122 120 80 ## [5,] 39 52 43 60 50 65 ## [6,] 58 64 58 80 65 80 Find appropriate number of clusters. # Initialize total within sum of squares error: wss wss &lt;- 0 # Look over 1 to 15 possible clusters for (i in 1:15) { # Fit the model: km.out km.out &lt;- kmeans(pokemon, centers = i, nstart = 20, iter.max = 50) # Save the within cluster sum of squares wss[i] &lt;- km.out$tot.withinss } # Produce a scree plot plot(1:15, wss, type = &quot;b&quot;, xlab = &quot;Number of Clusters&quot;, ylab = &quot;Within groups sum of squares&quot;) # Select number of clusters k &lt;- 3 # Build model with k clusters: km.out km.out &lt;- kmeans(pokemon, centers = k, nstart = 20, iter.max = 50) # View the resulting model km.out ## K-means clustering with 3 clusters of sizes 270, 175, 355 ## ## Cluster means: ## HitPoints Attack Defense SpecialAttack SpecialDefense Speed ## 1 81.9 96.2 77.7 104.1 86.9 94.7 ## 2 79.3 97.3 108.9 66.7 87.0 57.3 ## 3 54.7 56.9 53.6 52.0 53.0 53.6 ## ## Clustering vector: ## [1] 3 3 1 1 3 3 1 1 1 3 3 2 1 3 3 3 3 3 3 1 3 3 1 1 3 3 3 1 3 1 3 1 3 2 3 3 2 ## [38] 3 3 1 3 1 3 1 3 3 3 1 3 3 1 3 2 3 1 3 3 3 1 3 1 3 1 3 1 3 3 2 3 1 1 1 3 2 ## [75] 2 3 3 1 3 1 3 2 2 3 1 3 2 2 3 1 3 3 1 3 2 3 2 3 2 3 1 1 1 2 3 2 3 2 3 1 3 ## [112] 1 3 2 2 2 3 3 2 3 2 3 2 2 2 3 1 3 2 3 1 1 1 1 1 1 2 2 2 3 2 2 2 3 3 1 1 1 ## [149] 3 3 2 3 2 1 1 2 1 1 1 3 3 1 1 1 1 1 3 3 2 3 3 1 3 3 2 3 3 3 1 3 3 3 3 1 3 ## [186] 1 3 3 3 3 3 3 1 3 3 1 1 2 3 3 2 1 3 3 1 3 3 3 3 3 2 1 2 3 2 1 3 3 1 3 2 3 ## [223] 2 2 2 3 2 3 2 2 2 2 2 3 3 2 3 2 3 2 3 3 1 3 1 2 3 1 1 1 3 2 1 1 3 3 2 3 3 ## [260] 3 2 1 1 1 2 3 3 2 2 1 1 1 3 3 1 1 3 3 1 1 3 3 2 2 3 3 3 3 3 3 3 3 3 3 3 1 ## [297] 3 3 1 3 3 3 2 3 3 1 1 3 3 3 2 3 3 1 3 1 3 3 3 1 3 2 3 2 3 3 3 2 3 2 3 2 2 ## [334] 2 3 3 1 3 1 1 3 3 3 3 3 3 2 3 1 1 3 1 3 1 2 2 3 1 3 3 3 1 3 1 3 2 1 1 1 1 ## [371] 2 3 2 3 2 3 2 3 2 3 2 3 1 3 2 3 1 1 3 2 2 3 1 1 3 3 1 1 3 3 1 3 2 2 2 3 3 ## [408] 2 1 1 3 2 2 1 2 2 2 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 3 2 2 3 3 1 3 3 1 3 3 1 ## [445] 3 3 3 3 3 3 1 3 1 3 2 3 2 3 2 2 2 1 3 2 3 3 1 3 1 3 2 1 3 1 3 1 1 1 1 3 1 ## [482] 3 3 1 3 2 3 3 3 3 2 3 3 1 1 3 3 1 1 3 2 3 2 3 1 2 3 1 3 3 1 2 1 1 2 2 2 1 ## [519] 1 1 1 2 1 2 1 1 1 1 2 2 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 3 ## [556] 3 1 3 3 1 3 3 1 3 3 3 3 2 3 1 3 1 3 1 3 1 3 2 3 3 1 3 1 3 2 2 3 1 3 1 2 2 ## [593] 3 2 2 3 3 1 2 2 3 3 1 3 3 1 3 1 3 1 1 3 3 1 3 2 1 1 3 2 3 2 1 3 2 3 2 3 1 ## [630] 3 2 3 1 3 1 3 3 2 3 3 1 3 1 3 3 1 3 1 1 3 2 3 2 3 1 2 3 1 3 2 3 2 2 3 3 1 ## [667] 3 1 3 3 1 3 2 2 3 2 1 3 1 2 3 1 2 3 2 3 2 2 3 2 3 2 1 2 3 3 1 3 1 1 1 1 1 ## [704] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 2 2 3 3 1 3 3 1 3 3 3 3 1 3 3 3 3 1 3 3 1 ## [741] 3 1 3 2 1 3 1 1 3 2 1 2 3 2 3 1 3 2 3 2 3 2 3 1 3 1 3 2 3 1 1 1 1 2 3 1 1 ## [778] 2 3 2 3 3 3 3 2 2 2 2 3 2 3 1 1 1 2 2 1 1 1 1 ## ## Within cluster sum of squares by cluster: ## [1] 1018348 709021 812080 ## (between_SS / total_SS = 40.8 %) ## ## Available components: ## ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; # Plot of Defense vs. Speed by cluster membership plot(pokemon[, c(&quot;Defense&quot;, &quot;Speed&quot;)], col = km.out$cluster, main = paste(&quot;k-means clustering of Pokemon with&quot;, k, &quot;clusters&quot;), xlab = &quot;Defense&quot;, ylab = &quot;Speed&quot;) 21.2 Hierarchical clustering 21.2.1 Introduction Hierarchical clustering Number of clusters is not known ahead of time Two kinds: bottom-up: starts by assigning each point to its own cluster. Then find the closest two clusters and to join them together into a single cluster. This process continues iteratively. Until there is only one cluster. top-down Hierarchical clustering in R # Calculates similarity as Euclidean distance # between observations dist_matrix &lt;- dist(x) # Returns hierarchical clustering model hclust(d = dist_matrix) 21.2.1.1 Hierarchical clustering results In this exercise, you will create your first hierarchical clustering model using the hclust() function. Your task is to create a hierarchical clustering model of x. Remember the first step to hierarchical clustering is determining the similarity between observations, which you will do with the dist() function. # str: num [1:50, 1:2] x &lt;- x[1:50, ] head(x) ## [,1] [,2] ## [1,] 3.37 2.00 ## [2,] 1.44 2.76 ## [3,] 2.36 2.04 ## [4,] 2.63 2.74 ## [5,] 2.40 1.85 ## [6,] 1.89 1.94 # Create hierarchical clustering model: hclust.out hclust.out &lt;- hclust(d = dist(x)) # Inspect the result summary(hclust.out) ## Length Class Mode ## merge 98 -none- numeric ## height 49 -none- numeric ## order 50 -none- numeric ## labels 0 -none- NULL ## method 1 -none- character ## call 2 -none- call ## dist.method 1 -none- character 21.2.2 Selecting number of clusters Dendrogram Tree shaped structure used to interpret hierarchical clustering models. The distance between the clusters is represented as the height of the horizontal line on the dendrogram. Dendrogram plotting in R: # Draws a dendrogram plot(hclust.out) # determine the number of clusters # drawing a cut line at a particular &#39;height&#39; or &#39;distance&#39; abline(h = 6, col = &quot;red&quot;) # Tree &quot;cutting&quot; # Cut by height h cutree(hclust.out, h = 6) # Cut by number of clusters k cutree(hclust.out, k = 2) Specifying height of the line, h, is the equivalent of specifying that you want clusters that are no further apart than that height. 21.2.2.1 Cutting the tree cutree() is the R function that cuts a hierarchical model. The h and k arguments to cutree() allow you to cut the tree based on a certain height h or a certain number of clusters k. # draw dendrogram plot(hclust.out) # cut 3 cluster abline(h = 3.4, col = &quot;red&quot;) # Cut by height cutree(hclust.out, h = 3.4) ## [1] 1 2 3 1 3 2 1 2 3 2 1 3 2 2 2 3 2 2 2 1 2 2 2 1 1 2 2 2 3 2 1 3 3 2 1 2 2 2 ## [39] 2 2 3 2 3 2 2 3 2 3 2 1 # Cut by number of clusters cutree(hclust.out, k = 3) ## [1] 1 2 3 1 3 2 1 2 3 2 1 3 2 2 2 3 2 2 2 1 2 2 2 1 1 2 2 2 3 2 1 3 3 2 1 2 2 2 ## [39] 2 2 3 2 3 2 2 3 2 3 2 1 The output of each cutree() call represents the cluster assignments for each observation in the original dataset. 21.2.3 Clustering linkage Linking clusters in hierarchical clustering How is distance between clusters determined? Rules? Four methods to determine which cluster should be linked: Complete: pairwise similarity between all observations in cluster 1 and cluster 2, and uses largest of similarities hclust(d, method = \"complete\") Single: same as above but uses smallest of similarities hclust(d, method = \"single\") Average: same as above but uses average of similarities hclust(d, method = \"average\") Centroid: finds centroid of cluster 1 and centroid of cluster 2, and uses similarity between two centroids 21.2.3.1 Linkage methods You will produce hierarchical clustering models using different linkages and plot the dendrogram for each, observing the overall structure of the trees. # Cluster using complete linkage: hclust.complete hclust.complete &lt;- hclust(dist(x), method = &quot;complete&quot;) # Cluster using average linkage: hclust.average hclust.average &lt;- hclust(dist(x), method = &quot;average&quot;) # Cluster using single linkage: hclust.single hclust.single &lt;- hclust(dist(x), method = &quot;single&quot;) # Plot dendrogram of hclust.complete plot(hclust.complete, main = &quot;Complete&quot;) # Plot dendrogram of hclust.average plot(hclust.average, main = &quot;Average&quot;) # Plot dendrogram of hclust.single plot(hclust.single, main = &quot;Single&quot;) Whether you want balanced or unbalanced trees for your hierarchical clustering model depends on the context of the problem you’re trying to solve. Balanced trees (complete, average) are essential if you want an even number of observations assigned to each cluster. On the other hand, if you want to detect outliers, for example, an unbalanced tree is more desirable because pruning an unbalanced tree can result in most observations assigned to one cluster and only a few observations assigned to other clusters. 21.2.3.2 Scaling Clustering real data may require scaling the features if they have different distributions. Normalized features: mean = 0, sd = 1 You will observe the pokemon dataset, its distribution (mean and standard deviation) of each feature, scale the data accordingly, then produce a hierarchical clustering model using the complete linkage method. # View column means colMeans(pokemon) ## HitPoints Attack Defense SpecialAttack SpecialDefense ## 69.3 79.0 73.8 72.8 71.9 ## Speed ## 68.3 Since the variables are the columns of your matrix, use apply(X, MARGIN, FUN) to calculate sd. Where, x: array or matrix; MARGIN: for a matrix 1 indicates rows, 2 indicates columns. # View column standard deviations apply(pokemon, 2, sd) ## HitPoints Attack Defense SpecialAttack SpecialDefense ## 25.5 32.5 31.2 32.7 27.8 ## Speed ## 29.1 Use scale() to normalized. # Scale the data pokemon.scaled &lt;- scale(pokemon) apply(pokemon.scaled, 2, sd) ## HitPoints Attack Defense SpecialAttack SpecialDefense ## 1 1 1 1 1 ## Speed ## 1 # Create hierarchical clustering model: hclust.pokemon hclust.pokemon &lt;- hclust(dist(pokemon.scaled), method = &quot;complete&quot;) summary(hclust.pokemon) ## Length Class Mode ## merge 1598 -none- numeric ## height 799 -none- numeric ## order 800 -none- numeric ## labels 0 -none- NULL ## method 1 -none- character ## call 3 -none- call ## dist.method 1 -none- character 21.2.3.3 Compare kmeans() and hclust() Comparing k-means and hierarchical clustering, you’ll see the two methods produce different cluster memberships. This is because the two algorithms make different assumptions about how the data is generated. # Apply cutree() to hclust.pokemon: cut.pokemon cut.pokemon &lt;- cutree(hclust.pokemon, k = 3) # Compare methods table(km.out$cluster, cut.pokemon) ## cut.pokemon ## 1 2 3 ## 1 267 3 0 ## 2 171 3 1 ## 3 350 5 0 It looks like the hierarchical clustering model assigns most of the observations to cluster 1, while the k-means algorithm distributes the observations relatively evenly among all clusters. It’s important to note that there’s no consensus on which method produces better clusters. The job of the analyst in unsupervised clustering is to observe the cluster assignments and make a judgment call as to which method provides more insights into the data. 21.3 Dimensionality reduction - PCA 21.3.1 Introduction Dimensionality reduction A popular method is principal component analysis (PCA) Three goals when finding lower dimensional representation of features: Find linear combination of variables to create principal components Regression line of x &amp; y represent the principal component Maintain most variance in the data Principal components are uncorrelated (i.e., orthogonal to each other) 21.3.1.1 PCA using prcomp() prcomp(x = data, scale = FALSE / TRUE, center = TRUE) Your task is to create a PCA model of the pokemon, which has four dimensions, then to inspect the resulting model. pokemon &lt;- read_delim(&quot;data/pokemon_pca.txt&quot;, delim = &quot;,&quot;) rowname &lt;- as.matrix(pokemon)[,1] pokemon &lt;- data.matrix(pokemon)[,-1] row.names(pokemon) &lt;- rowname head(pokemon) ## HitPoints Attack Defense Speed ## Stunfisk 109 66 84 32 ## Mewtwo 106 110 90 130 ## Charmander 39 52 43 65 ## Grimer 80 80 50 25 ## Roggenrola 55 75 85 15 ## Larvesta 55 85 55 60 # Perform scaled PCA: pr.out pr.out &lt;- prcomp(pokemon, scale = TRUE, center = TRUE) # Inspect model output summary(pr.out) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.445 0.994 0.845 0.4566 ## Proportion of Variance 0.522 0.247 0.179 0.0521 ## Cumulative Proportion 0.522 0.769 0.948 1.0000 The first two principal components describe around 77% of the variance. 21.3.1.2 Additional results of PCA PCA models in R produce additional diagnostic and output components: center: the column means used to center to the data, or FALSE if the data weren’t centered scale: the column standard deviations used to scale the data, or FALSE if the data weren’t scaled rotation: the directions of the principal component vectors in terms of the original features/variables. This information allows you to define new data in terms of the original principal components x: the value of each observation in the original dataset projected to the principal components You can access these the same as other model components. For example, use pr.out$rotation to access the rotation component. pr.out$rotation ## PC1 PC2 PC3 PC4 ## HitPoints -0.487 -0.3198 -0.7256 -0.366 ## Attack -0.648 0.0529 0.0276 0.760 ## Defense -0.522 -0.2343 0.6829 -0.454 ## Speed -0.266 0.9165 -0.0802 -0.288 pr.out$center ## HitPoints Attack Defense Speed ## 68.4 76.5 73.1 64.2 pr.out$scale ## HitPoints Attack Defense Speed ## 25.7 28.0 30.1 33.7 list(dim = dim(pr.out$x), head = head(pr.out$x)) ## $dim ## [1] 50 4 ## ## $head ## PC1 PC2 PC3 PC4 ## Stunfisk -0.461 -1.487 -0.8340 -0.756 ## Mewtwo -2.302 1.256 -0.8035 -0.445 ## Charmander 1.640 0.576 0.1223 0.198 ## Grimer 0.409 -1.025 -0.7545 0.611 ## Roggenrola 0.471 -1.268 0.7638 0.390 ## Larvesta 0.404 0.210 -0.0134 0.729 21.3.2 Visualizing and interpreting Biplots biplot(pca_model) Scree plots # Getting proportion of variance for a scree plot pr.var &lt;- pr.iris$sdev^2 pve &lt;- pr.var / sum(pr.var) # Plot variance explained for each principal component plot(pve, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot;) 21.3.2.1 Interpreting biplots HitPoints and Defense have approximately the same loadings in the first two principal components. Aerodactyl and Avalugg, this two Pokemon are the least similar in terms of the second principal component. biplot(pr.out) 21.3.2.2 Variance explained The second common plot type for understanding PCA models is a scree plot. A scree plot shows the variance explained as the number of principal components increases. Sometimes the cumulative variance explained is plotted as well. Square of the standard deviations of the principal components (i.e., the variance), is available in the sdev component of the PCA model object. # Variability of each principal component: pr.var pr.var &lt;- pr.out$sdev^2 pr.var ## [1] 2.089 0.988 0.714 0.209 The proportion of the variance explained, calculated by dividing pr.var by the total variance explained by all principal components. # Variance explained by each principal component: pve pve &lt;- pr.var / sum(pr.var) pve ## [1] 0.5222 0.2471 0.1785 0.0521 21.3.2.3 Visualize variance explained Now you will create a scree plot showing the proportion of variance explained by each principal component, as well as the cumulative proportion of variance explained. One way to determine the number of principal components to retain is by looking for an elbow in the scree plot showing that as the number of principal components increases, the rate at which variance is explained decreases substantially. # Plot variance explained for each principal component plot(pve, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot;) cumsum() for cumulative proportion of variance. # Plot cumulative proportion of variance explained plot(cumsum(pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot;) Notice that when the number of principal components is equal to the number of original features in the data, the cumulative proportion of variance explained is 1. 21.3.3 Practical issues 21.3.3.1 Scaling Scaling your data before doing PCA changes the results of the PCA modeling. Here, you will perform PCA with and without scaling, then visualize the results using biplots. Sometimes scaling is appropriate when the variances of the variables are substantially different. This is commonly the case when variables have different units of measurement. # add new variable &quot;Total&quot; for pokemon data pokemon_total &lt;- read_delim(&quot;data/pokemon_add_total.txt&quot;) pokemon_total ## # A tibble: 50 × 6 ## ...1 Total HitPoints Attack Defense Speed ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Stunfisk 471 109 66 84 32 ## 2 Mewtwo 680 106 110 90 130 ## 3 Charmander 309 39 52 43 65 ## 4 Grimer 325 80 80 50 25 ## 5 Roggenrola 280 55 75 85 15 ## 6 Larvesta 360 55 85 55 60 ## 7 Metagross 600 80 135 130 70 ## 8 Delcatty 380 70 65 65 70 ## 9 Avalugg 514 95 117 184 28 ## 10 Marowak 425 60 80 110 45 ## # ℹ 40 more rows # set up pokemon pokemon &lt;- pokemon_total %&gt;% select(-1) %&gt;% as.matrix() row.names(pokemon) &lt;- pokemon_total$...1 head(pokemon) ## Total HitPoints Attack Defense Speed ## Stunfisk 471 109 66 84 32 ## Mewtwo 680 106 110 90 130 ## Charmander 309 39 52 43 65 ## Grimer 325 80 80 50 25 ## Roggenrola 280 55 75 85 15 ## Larvesta 360 55 85 55 60 See how the scale of the variables differs. You can see that Total column has larger mean and sd. # Mean of each variable colMeans(pokemon) ## Total HitPoints Attack Defense Speed ## 419.3 68.4 76.5 73.1 64.2 # Standard deviation of each variable apply(pokemon, 2, sd) ## Total HitPoints Attack Defense Speed ## 119.0 25.7 28.0 30.1 33.7 # PCA model with scaling: pr.with.scaling pr.with.scaling &lt;- prcomp(pokemon, scale = TRUE) # PCA model without scaling: pr.without.scaling pr.without.scaling &lt;- prcomp(pokemon, scale = FALSE) # Create biplots of both for comparison biplot(pr.with.scaling) biplot(pr.without.scaling) The new Total column contains much more variation, on average, than the other four columns, so it has a disproportionate effect on the PCA model when scaling is not performed. After scaling the data, there’s a much more even distribution of the loading vectors. 21.4 Case study 21.4.1 Introduction Example use case Human breast mass data: Ten features measured of each cell nuclei Summary information is provided for each group of cells Includes diagnosis: benign (not cancerous) and malignant (cancerous) Analysis Download data and prepare data for modeling Exploratory data analysis (# observations, # features, etc.) Perform PCA and interpret results Complete two types of clustering Understand and compare the two types Combine PCA and clustering 21.4.1.1 Preparing the data # load dataset wisc.df &lt;- read.csv(&quot;data/WisconsinCancer.csv&quot;) glimpse(wisc.df) ## Rows: 569 ## Columns: 33 ## $ id &lt;int&gt; 842302, 842517, 84300903, 84348301, 84358402, … ## $ diagnosis &lt;chr&gt; &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;M&quot;, &quot;… ## $ radius_mean &lt;dbl&gt; 18.0, 20.6, 19.7, 11.4, 20.3, 12.4, 18.2, 13.7… ## $ texture_mean &lt;dbl&gt; 10.4, 17.8, 21.2, 20.4, 14.3, 15.7, 20.0, 20.8… ## $ perimeter_mean &lt;dbl&gt; 122.8, 132.9, 130.0, 77.6, 135.1, 82.6, 119.6,… ## $ area_mean &lt;dbl&gt; 1001, 1326, 1203, 386, 1297, 477, 1040, 578, 5… ## $ smoothness_mean &lt;dbl&gt; 0.1184, 0.0847, 0.1096, 0.1425, 0.1003, 0.1278… ## $ compactness_mean &lt;dbl&gt; 0.2776, 0.0786, 0.1599, 0.2839, 0.1328, 0.1700… ## $ concavity_mean &lt;dbl&gt; 0.3001, 0.0869, 0.1974, 0.2414, 0.1980, 0.1578… ## $ concave.points_mean &lt;dbl&gt; 0.1471, 0.0702, 0.1279, 0.1052, 0.1043, 0.0809… ## $ symmetry_mean &lt;dbl&gt; 0.242, 0.181, 0.207, 0.260, 0.181, 0.209, 0.17… ## $ fractal_dimension_mean &lt;dbl&gt; 0.0787, 0.0567, 0.0600, 0.0974, 0.0588, 0.0761… ## $ radius_se &lt;dbl&gt; 1.095, 0.543, 0.746, 0.496, 0.757, 0.335, 0.44… ## $ texture_se &lt;dbl&gt; 0.905, 0.734, 0.787, 1.156, 0.781, 0.890, 0.77… ## $ perimeter_se &lt;dbl&gt; 8.59, 3.40, 4.58, 3.44, 5.44, 2.22, 3.18, 3.86… ## $ area_se &lt;dbl&gt; 153.4, 74.1, 94.0, 27.2, 94.4, 27.2, 53.9, 51.… ## $ smoothness_se &lt;dbl&gt; 0.00640, 0.00522, 0.00615, 0.00911, 0.01149, 0… ## $ compactness_se &lt;dbl&gt; 0.04904, 0.01308, 0.04006, 0.07458, 0.02461, 0… ## $ concavity_se &lt;dbl&gt; 0.0537, 0.0186, 0.0383, 0.0566, 0.0569, 0.0367… ## $ concave.points_se &lt;dbl&gt; 0.01587, 0.01340, 0.02058, 0.01867, 0.01885, 0… ## $ symmetry_se &lt;dbl&gt; 0.0300, 0.0139, 0.0225, 0.0596, 0.0176, 0.0216… ## $ fractal_dimension_se &lt;dbl&gt; 0.00619, 0.00353, 0.00457, 0.00921, 0.00511, 0… ## $ radius_worst &lt;dbl&gt; 25.4, 25.0, 23.6, 14.9, 22.5, 15.5, 22.9, 17.1… ## $ texture_worst &lt;dbl&gt; 17.3, 23.4, 25.5, 26.5, 16.7, 23.8, 27.7, 28.1… ## $ perimeter_worst &lt;dbl&gt; 184.6, 158.8, 152.5, 98.9, 152.2, 103.4, 153.2… ## $ area_worst &lt;dbl&gt; 2019, 1956, 1709, 568, 1575, 742, 1606, 897, 7… ## $ smoothness_worst &lt;dbl&gt; 0.162, 0.124, 0.144, 0.210, 0.137, 0.179, 0.14… ## $ compactness_worst &lt;dbl&gt; 0.666, 0.187, 0.424, 0.866, 0.205, 0.525, 0.25… ## $ concavity_worst &lt;dbl&gt; 0.7119, 0.2416, 0.4504, 0.6869, 0.4000, 0.5355… ## $ concave.points_worst &lt;dbl&gt; 0.2654, 0.1860, 0.2430, 0.2575, 0.1625, 0.1741… ## $ symmetry_worst &lt;dbl&gt; 0.460, 0.275, 0.361, 0.664, 0.236, 0.399, 0.30… ## $ fractal_dimension_worst &lt;dbl&gt; 0.1189, 0.0890, 0.0876, 0.1730, 0.0768, 0.1244… ## $ X &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… # Convert the features of the data: wisc.data wisc.data &lt;- as.matrix(wisc.df[, 3:32]) head(wisc.data, 2) ## radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## [1,] 18.0 10.4 123 1001 0.1184 ## [2,] 20.6 17.8 133 1326 0.0847 ## compactness_mean concavity_mean concave.points_mean symmetry_mean ## [1,] 0.2776 0.3001 0.1471 0.242 ## [2,] 0.0786 0.0869 0.0702 0.181 ## fractal_dimension_mean radius_se texture_se perimeter_se area_se ## [1,] 0.0787 1.095 0.905 8.59 153.4 ## [2,] 0.0567 0.543 0.734 3.40 74.1 ## smoothness_se compactness_se concavity_se concave.points_se symmetry_se ## [1,] 0.00640 0.0490 0.0537 0.0159 0.0300 ## [2,] 0.00522 0.0131 0.0186 0.0134 0.0139 ## fractal_dimension_se radius_worst texture_worst perimeter_worst area_worst ## [1,] 0.00619 25.4 17.3 185 2019 ## [2,] 0.00353 25.0 23.4 159 1956 ## smoothness_worst compactness_worst concavity_worst concave.points_worst ## [1,] 0.162 0.666 0.712 0.265 ## [2,] 0.124 0.187 0.242 0.186 ## symmetry_worst fractal_dimension_worst ## [1,] 0.460 0.119 ## [2,] 0.275 0.089 # Set the row names of wisc.data row.names(wisc.data) &lt;- wisc.df$id head(wisc.data, 2) ## radius_mean texture_mean perimeter_mean area_mean smoothness_mean ## 842302 18.0 10.4 123 1001 0.1184 ## 842517 20.6 17.8 133 1326 0.0847 ## compactness_mean concavity_mean concave.points_mean symmetry_mean ## 842302 0.2776 0.3001 0.1471 0.242 ## 842517 0.0786 0.0869 0.0702 0.181 ## fractal_dimension_mean radius_se texture_se perimeter_se area_se ## 842302 0.0787 1.095 0.905 8.59 153.4 ## 842517 0.0567 0.543 0.734 3.40 74.1 ## smoothness_se compactness_se concavity_se concave.points_se symmetry_se ## 842302 0.00640 0.0490 0.0537 0.0159 0.0300 ## 842517 0.00522 0.0131 0.0186 0.0134 0.0139 ## fractal_dimension_se radius_worst texture_worst perimeter_worst ## 842302 0.00619 25.4 17.3 185 ## 842517 0.00353 25.0 23.4 159 ## area_worst smoothness_worst compactness_worst concavity_worst ## 842302 2019 0.162 0.666 0.712 ## 842517 1956 0.124 0.187 0.242 ## concave.points_worst symmetry_worst fractal_dimension_worst ## 842302 0.265 0.460 0.119 ## 842517 0.186 0.275 0.089 # Create diagnosis vector, 1 = malignant (&quot;M&quot;), 0 diagnosis &lt;- as.numeric(wisc.df$diagnosis == &quot;M&quot;) str(diagnosis) ## num [1:569] 1 1 1 1 1 1 1 1 1 1 ... 21.4.1.2 Exploratory data analysis How many observations are in this dataset? # ans: 569 dim(wisc.data) ## [1] 569 30 How many variables/features in the data are suffixed with _mean? # detect string pattern str_detect(colnames(wisc.data), &quot;_mean&quot;) %&gt;% sum() ## [1] 10 How many of the observations have a malignant diagnosis? # malignant diagnosis = 1 sum(diagnosis) ## [1] 212 21.4.1.3 Performing PCA The next step in your analysis is to perform PCA on wisc.data. It’s important to check if the data need to be scaled before performing PCA. Recall two common reasons for scaling data: The input variables use different units of measurement. The input variables have significantly different variances. # Check column means colMeans(wisc.data) ## radius_mean texture_mean perimeter_mean ## 14.12729 19.28965 91.96903 ## area_mean smoothness_mean compactness_mean ## 654.88910 0.09636 0.10434 ## concavity_mean concave.points_mean symmetry_mean ## 0.08880 0.04892 0.18116 ## fractal_dimension_mean radius_se texture_se ## 0.06280 0.40517 1.21685 ## perimeter_se area_se smoothness_se ## 2.86606 40.33708 0.00704 ## compactness_se concavity_se concave.points_se ## 0.02548 0.03189 0.01180 ## symmetry_se fractal_dimension_se radius_worst ## 0.02054 0.00379 16.26919 ## texture_worst perimeter_worst area_worst ## 25.67722 107.26121 880.58313 ## smoothness_worst compactness_worst concavity_worst ## 0.13237 0.25427 0.27219 ## concave.points_worst symmetry_worst fractal_dimension_worst ## 0.11461 0.29008 0.08395 # Check standard deviations apply(wisc.data, 2, sd) ## radius_mean texture_mean perimeter_mean ## 3.52405 4.30104 24.29898 ## area_mean smoothness_mean compactness_mean ## 351.91413 0.01406 0.05281 ## concavity_mean concave.points_mean symmetry_mean ## 0.07972 0.03880 0.02741 ## fractal_dimension_mean radius_se texture_se ## 0.00706 0.27731 0.55165 ## perimeter_se area_se smoothness_se ## 2.02185 45.49101 0.00300 ## compactness_se concavity_se concave.points_se ## 0.01791 0.03019 0.00617 ## symmetry_se fractal_dimension_se radius_worst ## 0.00827 0.00265 4.83324 ## texture_worst perimeter_worst area_worst ## 6.14626 33.60254 569.35699 ## smoothness_worst compactness_worst concavity_worst ## 0.02283 0.15734 0.20862 ## concave.points_worst symmetry_worst fractal_dimension_worst ## 0.06573 0.06187 0.01806 # Execute PCA, scaling if appropriate: wisc.pr wisc.pr &lt;- prcomp(wisc.data, scale = TRUE) # Look at summary of results summary(wisc.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## Standard deviation 3.644 2.386 1.6787 1.407 1.284 1.0988 0.8217 0.6904 ## Proportion of Variance 0.443 0.190 0.0939 0.066 0.055 0.0403 0.0225 0.0159 ## Cumulative Proportion 0.443 0.632 0.7264 0.792 0.847 0.8876 0.9101 0.9260 ## PC9 PC10 PC11 PC12 PC13 PC14 PC15 ## Standard deviation 0.6457 0.5922 0.5421 0.51104 0.49128 0.39624 0.30681 ## Proportion of Variance 0.0139 0.0117 0.0098 0.00871 0.00805 0.00523 0.00314 ## Cumulative Proportion 0.9399 0.9516 0.9614 0.97007 0.97812 0.98335 0.98649 ## PC16 PC17 PC18 PC19 PC20 PC21 PC22 ## Standard deviation 0.28260 0.24372 0.22939 0.22244 0.17652 0.173 0.16565 ## Proportion of Variance 0.00266 0.00198 0.00175 0.00165 0.00104 0.001 0.00091 ## Cumulative Proportion 0.98915 0.99113 0.99288 0.99453 0.99557 0.997 0.99749 ## PC23 PC24 PC25 PC26 PC27 PC28 PC29 ## Standard deviation 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987 0.02736 ## Proportion of Variance 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005 0.00002 ## Cumulative Proportion 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997 1.00000 ## PC30 ## Standard deviation 0.0115 ## Proportion of Variance 0.0000 ## Cumulative Proportion 1.0000 21.4.1.4 Interpreting PCA results Now you’ll use some visualizations to better understand your PCA model. What stands out to you about this plot? Is it easy or difficult to understand? # Create a biplot of wisc.pr biplot(wisc.pr) Scatter plot each observation by principal components 1 and 2, coloring the points by the diagnosis. # Scatter plot observations by components 1 and 2 plot(wisc.pr$x[, c(1, 2)], col = (diagnosis + 1), xlab = &quot;PC1&quot;, ylab = &quot;PC2&quot;) Repeat the same for principal components 1 and 3. What do you notice about these plots? # Repeat for components 1 and 3 plot(wisc.pr$x[, c(1, 3)], col = (diagnosis + 1), xlab = &quot;PC1&quot;, ylab = &quot;PC3&quot;) Because principal component 2 explains more variance in the original data than principal component 3, you can see that the first plot has a cleaner cut separating the two subgroups. 21.4.1.5 Variance explained You will produce scree plots showing the proportion of variance explained as the number of principal components increases. # Set up 1 x 2 plotting grid par(mfrow = c(1, 2)) # Calculate variability of each component pr.var &lt;- wisc.pr$sdev^2 # Variance explained by each principal component: pve pve &lt;- pr.var / sum(pr.var) # Plot variance explained for each principal component plot(pve, xlab = &quot;Principal Component&quot;, ylab = &quot;Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot;) # Plot cumulative proportion of variance explained plot(cumsum(pve), xlab = &quot;Principal Component&quot;, ylab = &quot;Cumulative Proportion of Variance Explained&quot;, ylim = c(0, 1), type = &quot;b&quot;) What is the minimum number of principal components needed to explain 80% of the variance in the data? 21.4.2 Next steps Complete hierarchical clustering Complete k-means clustering Combine PCA and clustering Contrast results of hierarchical clustering with diagnosis Compare hierarchical and k-means clustering results PCA as a pre-processing step for clustering 21.4.2.1 Communicating PCA results The loadings, represented as vectors, explain the mapping from the original features to the principal components. The principal components are naturally ordered from the most variance explained to the least variance explained. For the first principal component, what is the component of the loading vector for the feature concave.points_mean? # -0.26085376 wisc.pr$rotation[,&quot;PC1&quot;] ## radius_mean texture_mean perimeter_mean ## -0.2189 -0.1037 -0.2275 ## area_mean smoothness_mean compactness_mean ## -0.2210 -0.1426 -0.2393 ## concavity_mean concave.points_mean symmetry_mean ## -0.2584 -0.2609 -0.1382 ## fractal_dimension_mean radius_se texture_se ## -0.0644 -0.2060 -0.0174 ## perimeter_se area_se smoothness_se ## -0.2113 -0.2029 -0.0145 ## compactness_se concavity_se concave.points_se ## -0.1704 -0.1536 -0.1834 ## symmetry_se fractal_dimension_se radius_worst ## -0.0425 -0.1026 -0.2280 ## texture_worst perimeter_worst area_worst ## -0.1045 -0.2366 -0.2249 ## smoothness_worst compactness_worst concavity_worst ## -0.1280 -0.2101 -0.2288 ## concave.points_worst symmetry_worst fractal_dimension_worst ## -0.2509 -0.1229 -0.1318 What is the minimum number of principal components required to explain 80% of the variance of the data? # PC5 Cumulative Proportion = 0.84734 &amp; screen plot summary(wisc.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## Standard deviation 3.644 2.386 1.6787 1.407 1.284 1.0988 0.8217 0.6904 ## Proportion of Variance 0.443 0.190 0.0939 0.066 0.055 0.0403 0.0225 0.0159 ## Cumulative Proportion 0.443 0.632 0.7264 0.792 0.847 0.8876 0.9101 0.9260 ## PC9 PC10 PC11 PC12 PC13 PC14 PC15 ## Standard deviation 0.6457 0.5922 0.5421 0.51104 0.49128 0.39624 0.30681 ## Proportion of Variance 0.0139 0.0117 0.0098 0.00871 0.00805 0.00523 0.00314 ## Cumulative Proportion 0.9399 0.9516 0.9614 0.97007 0.97812 0.98335 0.98649 ## PC16 PC17 PC18 PC19 PC20 PC21 PC22 ## Standard deviation 0.28260 0.24372 0.22939 0.22244 0.17652 0.173 0.16565 ## Proportion of Variance 0.00266 0.00198 0.00175 0.00165 0.00104 0.001 0.00091 ## Cumulative Proportion 0.98915 0.99113 0.99288 0.99453 0.99557 0.997 0.99749 ## PC23 PC24 PC25 PC26 PC27 PC28 PC29 ## Standard deviation 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987 0.02736 ## Proportion of Variance 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005 0.00002 ## Cumulative Proportion 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997 1.00000 ## PC30 ## Standard deviation 0.0115 ## Proportion of Variance 0.0000 ## Cumulative Proportion 1.0000 21.4.2.2 Hierarchical clustering # Scale the wisc.data data: data.scaled data.scaled &lt;- scale(wisc.data) # Calculate the (Euclidean) distances: data.dist data.dist &lt;- dist(data.scaled) # Create a hierarchical clustering model: wisc.hclust wisc.hclust &lt;- hclust(data.dist, method = &quot;complete&quot;) # plot plot(wisc.hclust) What is the height at which the clustering model has 4 clusters? ANS: 20 21.4.2.3 Selecting number of clusters You will compare the outputs from your hierarchical clustering model to the actual diagnoses. Normally when performing unsupervised learning like this, a target variable isn’t available. We do have it with this dataset, however, so it can be used to check the performance of the clustering model. When performing supervised learning, using clustering to create new features may or may not improve the performance of the final model. This exercise will help you determine if, in this case, hierarchical clustering provides a promising new feature. # Cut tree so that it has 4 clusters: wisc.hclust.clusters wisc.hclust.clusters &lt;- cutree(wisc.hclust, k = 4) # Compare cluster membership to actual diagnoses table(wisc.hclust.clusters, diagnosis) ## diagnosis ## wisc.hclust.clusters 0 1 ## 1 12 165 ## 2 2 5 ## 3 343 40 ## 4 0 2 21.4.2.4 k-means clustering &amp; compare You will create a k-means clustering model on the Wisconsin breast cancer data and compare the results to the actual diagnoses and the results of your hierarchical clustering model. # Create a k-means model on wisc.data: wisc.km wisc.km &lt;- kmeans(scale(wisc.data), centers = 2, nstart = 20) # Compare k-means to actual diagnoses table(wisc.km$cluster, diagnosis) ## diagnosis ## 0 1 ## 1 343 37 ## 2 14 175 # Compare k-means to hierarchical clustering table(wisc.km$cluster, wisc.hclust.clusters) ## wisc.hclust.clusters ## 1 2 3 4 ## 1 17 0 363 0 ## 2 160 7 20 2 Looking at the second table you generated, it looks like clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 equivalent from the k-means algorithm, and cluster 3 can be interpreted as the cluster 2 equivalent. 21.4.2.5 Clustering on PCA results You will put together several steps you used earlier and, in doing so, you will experience some of the creativity that is typical in unsupervised learning. Recall from earlier exercises that the PCA model required significantly fewer features to describe 80% and 95% of the variability of the data. In addition to normalizing data and potentially avoiding overfitting, PCA also uncorrelates the variables, sometimes improving the performance of other modeling techniques. Let’s see if PCA improves or degrades the performance of hierarchical clustering. # Using the minimum number of principal components required to describe at least 90% of the variability in the data # PC7 = 0.91 summary(wisc.pr) ## Importance of components: ## PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 ## Standard deviation 3.644 2.386 1.6787 1.407 1.284 1.0988 0.8217 0.6904 ## Proportion of Variance 0.443 0.190 0.0939 0.066 0.055 0.0403 0.0225 0.0159 ## Cumulative Proportion 0.443 0.632 0.7264 0.792 0.847 0.8876 0.9101 0.9260 ## PC9 PC10 PC11 PC12 PC13 PC14 PC15 ## Standard deviation 0.6457 0.5922 0.5421 0.51104 0.49128 0.39624 0.30681 ## Proportion of Variance 0.0139 0.0117 0.0098 0.00871 0.00805 0.00523 0.00314 ## Cumulative Proportion 0.9399 0.9516 0.9614 0.97007 0.97812 0.98335 0.98649 ## PC16 PC17 PC18 PC19 PC20 PC21 PC22 ## Standard deviation 0.28260 0.24372 0.22939 0.22244 0.17652 0.173 0.16565 ## Proportion of Variance 0.00266 0.00198 0.00175 0.00165 0.00104 0.001 0.00091 ## Cumulative Proportion 0.98915 0.99113 0.99288 0.99453 0.99557 0.997 0.99749 ## PC23 PC24 PC25 PC26 PC27 PC28 PC29 ## Standard deviation 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987 0.02736 ## Proportion of Variance 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005 0.00002 ## Cumulative Proportion 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997 1.00000 ## PC30 ## Standard deviation 0.0115 ## Proportion of Variance 0.0000 ## Cumulative Proportion 1.0000 Create a hierarchical clustering model with complete linkage. # Create a hierarchical clustering model: wisc.pr.hclust wisc.pr.hclust &lt;- hclust(dist(wisc.pr$x[, 1:7]), method = &quot;complete&quot;) # Cut model into 4 clusters: wisc.pr.hclust.clusters wisc.pr.hclust.clusters &lt;- cutree(wisc.pr.hclust, k = 4) # Compare to actual diagnoses table(wisc.pr.hclust.clusters, diagnosis) ## diagnosis ## wisc.pr.hclust.clusters 0 1 ## 1 5 113 ## 2 350 97 ## 3 2 0 ## 4 0 2 # Compare previous k-means and actual diagnosis table(wisc.km$cluster, diagnosis) ## diagnosis ## 0 1 ## 1 343 37 ## 2 14 175 # Compare previous hierarchical and actual diagnosis table(wisc.hclust.clusters, diagnosis) ## diagnosis ## wisc.hclust.clusters 0 1 ## 1 12 165 ## 2 2 5 ## 3 343 40 ## 4 0 2 "],["intermediate-importing-data-in-r.html", "Chapter 22 Intermediate Importing Data in R 22.1 Import from databases-1 22.2 Import from databases-2 22.3 Import from the web-1 22.4 Import from the web-2 22.5 Import from statistical software", " Chapter 22 Intermediate Importing Data in R 22.1 Import from databases-1 22.1.1 Connect to a database Database Management System DBMS Open source MySQL, PostgreSQL, SQLite Proprietary Oracle Database, Microsoft/ SQL Server SQL = Structured Query Language all of above implementations use SQL Databases in R Different R packages, e.g, MySQL: RMySQL PostgresSQL: RPostgresSQL Conventions specified in DBI to interact with the database DBI is an interface, and RMySQL is the implementation 22.1.1.1 Establish a connection The first step to import data from a SQL database is creating a connection to it. As Filip explained, you need different packages depending on the database you want to connect to. All of these packages do this in a uniform way, as specified in the DBI package. dbConnect() creates a connection between your R session and a SQL database. The first argument has to be a DBIdriver object, that specifies how connections are made and how data is mapped between R and the database. Specifically for MySQL databases, you can build such a driver with RMySQL::MySQL(). If the MySQL database is a remote database hosted on a server, you’ll also have to specify the following arguments in dbConnect(): dbname, host, port, user and password. # Load the DBI package library(DBI) # Edit dbConnect() call con &lt;- dbConnect(RMySQL::MySQL(), # specifies the driver # database name dbname = &quot;tweater&quot;, # where the database is hosted host = &quot;courses.csrrinzqubik.us-east-1.rds.amazonaws.com&quot;, # through which port you want to connect port = 3306, # credentials to authenticate yourself user = &quot;student&quot;, password = &quot;datacamp&quot;) # Inspect the connection con ## &lt;MySQLConnection:0,0&gt; You are now connected to the MySQL database tweater. dbDisconnect(con) is to disconnected after finish work. 22.1.2 Import table data 22.1.2.1 List the database tables dbListTables() requires the connection object as an input, and outputs a character vector with the table names. # Build a vector of table names: tables tables &lt;- dbListTables(con) # Display structure of tables str(tables) ## chr [1:3] &quot;comments&quot; &quot;tweats&quot; &quot;users&quot; 22.1.2.2 Import table dbReadTable() simply pass it the connection object, followed by the name of the table you want to import. The resulting object is a standard R data frame. # Import the users table from tweater: users users &lt;- dbReadTable(con, &quot;users&quot;) # Print users users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja 22.1.2.3 Import all tables Use lapply # Get table names table_names &lt;- dbListTables(con) # Import all tables, conn is the first argument of dbReadTable tables &lt;- lapply(table_names, dbReadTable, conn = con) # Print out tables names(tables) &lt;- table_names tables ## $comments ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1012 87 1 awesome! thanks! ## 5 1010 88 6 yuck! ## 6 1026 77 4 not my thing! ## 7 1004 49 1 this is fabulous! ## 8 1030 75 6 so easy! ## 9 1025 88 2 oh yes ## 10 1007 49 3 serious? ## 11 1020 77 1 couldn&#39;t be better ## 12 1014 77 1 saved my day ## ## $tweats ## id user_id ## 1 75 3 ## 2 88 4 ## 3 77 6 ## 4 87 5 ## 5 49 1 ## 6 24 7 ## post ## 1 break egg. bake egg. eat egg. ## 2 wash strawberries. add ice. blend. enjoy. ## 3 2 slices of bread. add cheese. grill. heaven. ## 4 open and crush avocado. add shrimps. perfect starter. ## 5 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 6 just eat an apple. simply and healthy. ## date ## 1 2015-09-05 ## 2 2015-09-14 ## 3 2015-09-21 ## 4 2015-09-22 ## 5 2015-09-22 ## 6 2015-09-24 ## ## $users ## id name login ## 1 1 elisabeth elismith ## 2 2 mike mikey ## 3 3 thea teatime ## 4 4 thomas tomatotom ## 5 5 oliver olivander ## 6 6 kate katebenn ## 7 7 anjali lianja Who posted the tweat on which somebody commented “awesome! thanks!” (comment 1012)? ANS: oliver. 22.2 Import from databases-2 22.2.1 SQL Queries from inside R As a data scientist, you’ll often be working with huge databases that contain tables with millions of rows. If you want to do some analyses on this data, it’s possible that you only need a fraction of this data. In this case, it’s a good idea to send SQL queries to your database, and only import the data you actually need into R. dbGetQuery(con, SQLquery), the second argument is an SQL query in the form of a character string. # Import tweat_id column of comments where user_id is 1: elisabeth elisabeth &lt;- dbGetQuery(con, &quot;SELECT tweat_id FROM comments WHERE user_id = 1&quot;) # Print elisabeth elisabeth ## tweat_id ## 1 87 ## 2 49 ## 3 77 ## 4 77 # Import post column of tweats where date is higher than &#39;2015-09-21&#39;: latest latest &lt;- dbGetQuery(con, &quot;SELECT post FROM tweats WHERE date &gt; &#39;2015-09-21&#39;&quot;) # Print latest latest ## post ## 1 open and crush avocado. add shrimps. perfect starter. ## 2 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## 3 just eat an apple. simply and healthy. # Create data frame specific specific &lt;- dbGetQuery(con, &quot;SELECT message FROM comments WHERE tweat_id = 77 AND user_id &gt; 4&quot;) # Print specific specific ## message ## 1 great! # Create data frame short # CHAR_LENGTH() returns the number of characters in a string short &lt;- dbGetQuery(con, &quot;SELECT id, name FROM users WHERE CHAR_LENGTH(name) &lt; 5&quot;) # Print short short ## id name ## 1 2 mike ## 2 3 thea ## 3 6 kate # Inner join by key dbGetQuery(con, &quot;SELECT post, message FROM tweats INNER JOIN comments on tweats.id = tweat_id WHERE tweat_id = 77&quot;) ## post message ## 1 2 slices of bread. add cheese. grill. heaven. great! ## 2 2 slices of bread. add cheese. grill. heaven. not my thing! ## 3 2 slices of bread. add cheese. grill. heaven. couldn&#39;t be better ## 4 2 slices of bread. add cheese. grill. heaven. saved my day 22.2.2 DBI internals The combination of dbSendQuery, dbFetch and, dbClearResult gives the exact same result as dbGetQuery did before, so why do this? dbFetch query calls allow you to specify a maximum number of records to retrieve per fetch. This can be useful when you need to load in tons of records, but want to do this chunk by chunk. (If you’re working on a super complicated algorithm that involves millions of database records, you might want to consider a treatment of data in chunks.) 22.2.2.1 Send - Fetch - Clear Behind the dbGetQueryscenes, the following steps are performed: Sending the specified query with dbSendQuery(); Fetching the result of executing the query on the database with dbFetch(); Clearing the result with dbClearResult(). It gives you the ability to fetch the query’s result in chunks rather than all at once. You can do this by specifying the n argument inside dbFetch(). # Send query to the database # Selects comments for the users with an id above 4 res &lt;- dbSendQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) # Use dbFetch() twice dbFetch(res, n = 2) # import only two records of the query ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! dbFetch(res) # import all remaining queries (don&#39;t specify n) ## id tweat_id user_id message ## 1 1011 49 5 love it ## 2 1010 88 6 yuck! ## 3 1030 75 6 so easy! # Clear res dbClearResult(res) ## [1] TRUE dbGetQuery will get all 5 records at once. So above using dbFetch, you first get 2 records by setting n = 2 (first chunk), next get 5-2=3 remaining records (second chunk). # try what dbGetQuery get dbGetQuery(con, &quot;SELECT * FROM comments WHERE user_id &gt; 4&quot;) ## id tweat_id user_id message ## 1 1022 87 7 nice! ## 2 1000 77 7 great! ## 3 1011 49 5 love it ## 4 1010 88 6 yuck! ## 5 1030 75 6 so easy! 22.2.2.2 Disconnect It’s always polite to manually disconnect from the database afterwards. You do this with the dbDisconnect() function. # Create the data frame long_tweats long_tweats &lt;- dbGetQuery(con, &quot;SELECT post, date FROM tweats WHERE CHAR_LENGTH(post) &gt; 40&quot;) # Print long_tweats print(long_tweats) ## post ## 1 wash strawberries. add ice. blend. enjoy. ## 2 2 slices of bread. add cheese. grill. heaven. ## 3 open and crush avocado. add shrimps. perfect starter. ## 4 nachos. add tomato sauce, minced meat and cheese. oven for 10 mins. ## date ## 1 2015-09-14 ## 2 2015-09-21 ## 3 2015-09-22 ## 4 2015-09-22 Disconnect from the database. # Disconnect from the database dbDisconnect(con) ## [1] TRUE 22.3 Import from the web-1 22.3.1 HTTP HyperText Transfer Protocol Rules about data exchange between computers Language of the web 22.3.1.1 Import flat files from the web R sees it’s a URL, does GET request, and reads in the specific type’s file. # Load the readr package library(readr) # Import the csv file: pools url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; pools &lt;- read_csv(url_csv) ## Rows: 20 Columns: 4 ## ── Column specification ────────────────────── ## Delimiter: &quot;,&quot; ## chr (2): Name, Address ## dbl (2): Latitude, Longitude ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Import the txt file: potatoes url_delim &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/potatoes.txt&quot; potatoes &lt;- read_tsv(url_delim) ## Rows: 160 Columns: 8 ## ── Column specification ────────────────────── ## Delimiter: &quot;\\t&quot; ## dbl (8): area, temp, size, storage, method, texture, flavor, moistness ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. # Print pools and potatoes pools ## # A tibble: 20 × 4 ## Name Address Latitude Longitude ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Acacia Ridge Leisure Centre 1391 Beaudesert… -27.6 153. ## 2 Bellbowrie Pool Sugarwood Stree… -27.6 153. ## 3 Carole Park Cnr Boundary Ro… -27.6 153. ## 4 Centenary Pool (inner City) 400 Gregory Ter… -27.5 153. ## 5 Chermside Pool 375 Hamilton Ro… -27.4 153. ## 6 Colmslie Pool (Morningside) 400 Lytton Road… -27.5 153. ## 7 Spring Hill Baths (inner City) 14 Torrington S… -27.5 153. ## 8 Dunlop Park Pool (Corinda) 794 Oxley Road,… -27.5 153. ## 9 Fortitude Valley Pool 432 Wickham Str… -27.5 153. ## 10 Hibiscus Sports Complex (upper MtGravatt) 90 Klumpp Road,… -27.6 153. ## 11 Ithaca Pool ( Paddington) 131 Caxton Stre… -27.5 153. ## 12 Jindalee Pool 11 Yallambee Ro… -27.5 153. ## 13 Manly Pool 1 Fairlead Cres… -27.5 153. ## 14 Mt Gravatt East Aquatic Centre Cnr wecker Road… -27.5 153. ## 15 Musgrave Park Pool (South Brisbane) 100 Edmonstone … -27.5 153. ## 16 Newmarket Pool 71 Alderson Str… -27.4 153. ## 17 Runcorn Pool 37 Bonemill Roa… -27.6 153. ## 18 Sandgate Pool 231 Flinders Pa… -27.3 153. ## 19 Langlands Parks Pool (Stones Corner) 5 Panitya Stree… -27.5 153. ## 20 Yeronga Park Pool 81 School Road,… -27.5 153. potatoes ## # A tibble: 160 × 8 ## area temp size storage method texture flavor moistness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 1 1 1 1 2.9 3.2 3 ## 2 1 1 1 1 2 2.3 2.5 2.6 ## 3 1 1 1 1 3 2.5 2.8 2.8 ## 4 1 1 1 1 4 2.1 2.9 2.4 ## 5 1 1 1 1 5 1.9 2.8 2.2 ## 6 1 1 1 2 1 1.8 3 1.7 ## 7 1 1 1 2 2 2.6 3.1 2.4 ## 8 1 1 1 2 3 3 3 2.9 ## 9 1 1 1 2 4 2.2 3.2 2.5 ## 10 1 1 1 2 5 2 2.8 1.9 ## # ℹ 150 more rows 22.3.1.2 Secure importing In the previous exercises, you have been working with URLs that all start with http://. There is, however, a safer alternative to HTTP, namely HTTPS, which stands for HyperText Transfer Protocol Secure. Just remember this: HTTPS is relatively safe, HTTP is not. # https URL to the swimming_pools csv file. url_csv &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/swimming_pools.csv&quot; # Import the file using read.csv(): pools1 # .csv referring to contains column names in the first row pools1 &lt;- read.csv(url_csv) # Print the structure of pools1 str(pools1) ## &#39;data.frame&#39;: 20 obs. of 4 variables: ## $ Name : chr &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... ## $ Address : chr &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... ## $ Latitude : num -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num 153 153 153 153 153 ... # Import the file using read_csv(): pools2 pools2 &lt;- read_csv(url_csv) # Print the structure of pools2 str(pools2) ## spc_tbl_ [20 × 4] (S3: spec_tbl_df/tbl_df/tbl/data.frame) ## $ Name : chr [1:20] &quot;Acacia Ridge Leisure Centre&quot; &quot;Bellbowrie Pool&quot; &quot;Carole Park&quot; &quot;Centenary Pool (inner City)&quot; ... ## $ Address : chr [1:20] &quot;1391 Beaudesert Road, Acacia Ridge&quot; &quot;Sugarwood Street, Bellbowrie&quot; &quot;Cnr Boundary Road and Waterford Road Wacol&quot; &quot;400 Gregory Terrace, Spring Hill&quot; ... ## $ Latitude : num [1:20] -27.6 -27.6 -27.6 -27.5 -27.4 ... ## $ Longitude: num [1:20] 153 153 153 153 153 ... ## - attr(*, &quot;spec&quot;)= ## .. cols( ## .. Name = col_character(), ## .. Address = col_character(), ## .. Latitude = col_double(), ## .. Longitude = col_double() ## .. ) ## - attr(*, &quot;problems&quot;)=&lt;externalptr&gt; 22.3.2 Downloading files 22.3.2.1 Import Excel files from the web readxl can’t handle .xls files that are on the internet, so you can first download to local file: download.file(url, file.path()), then import it. # Load the readxl package library(readxl) # Specification of url: url_xls url_xls &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/latitude.xls&quot; # Download file behind URL, name it local_latitude.xls dest_path &lt;- file.path(&quot;data&quot;, &quot;local_latitude.xls&quot;) download.file(url_xls, dest_path) # Import the local .xls file with readxl: excel_readxl excel_readxl &lt;- read_excel(dest_path) 22.3.2.2 Downloading any file securely With download.file() you can download any kind of file from the web, using HTTP and HTTPS: images, executable files, but also .RData files. An RData file is very efficient format to store R data. You can load data from an RData file using the load() function, but this function does not accept a URL string as an argument. In this exercise, you’ll first download the RData file securely, and then import the local data file. # https URL to the wine RData file. url_rdata &lt;- &quot;https://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/wine.RData&quot; # Download the wine file to your working directory download.file(url_rdata, file.path(&quot;data&quot;, &quot;wine_local.RData&quot;)) # Load the wine data into your workspace using load() load(&quot;data/wine_local.RData&quot;) # Print out the summary of the wine data summary(wine) ## Alcohol Malic acid Ash Alcalinity of ash Magnesium ## Min. :11.0 Min. :0.74 Min. :1.36 Min. :10.6 Min. : 70.0 ## 1st Qu.:12.4 1st Qu.:1.60 1st Qu.:2.21 1st Qu.:17.2 1st Qu.: 88.0 ## Median :13.1 Median :1.87 Median :2.36 Median :19.5 Median : 98.0 ## Mean :13.0 Mean :2.34 Mean :2.37 Mean :19.5 Mean : 99.6 ## 3rd Qu.:13.7 3rd Qu.:3.10 3rd Qu.:2.56 3rd Qu.:21.5 3rd Qu.:107.0 ## Max. :14.8 Max. :5.80 Max. :3.23 Max. :30.0 Max. :162.0 ## Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins ## Min. :0.98 Min. :0.34 Min. :0.130 Min. :0.41 ## 1st Qu.:1.74 1st Qu.:1.20 1st Qu.:0.270 1st Qu.:1.25 ## Median :2.35 Median :2.13 Median :0.340 Median :1.55 ## Mean :2.29 Mean :2.02 Mean :0.362 Mean :1.59 ## 3rd Qu.:2.80 3rd Qu.:2.86 3rd Qu.:0.440 3rd Qu.:1.95 ## Max. :3.88 Max. :5.08 Max. :0.660 Max. :3.58 ## Color intensity Hue Proline ## Min. : 1.28 Min. :1.27 Min. : 278 ## 1st Qu.: 3.21 1st Qu.:1.93 1st Qu.: 500 ## Median : 4.68 Median :2.78 Median : 672 ## Mean : 5.05 Mean :2.60 Mean : 745 ## 3rd Qu.: 6.20 3rd Qu.:3.17 3rd Qu.: 985 ## Max. :13.00 Max. :4.00 Max. :1680 22.3.2.3 httr Downloading a file from the Internet means sending a GET request and receiving the file you asked for. Internally, all the previously discussed functions use a GET request to download files. httr provides a convenient function, GET() to execute this GET request. The result is a response object, that provides easy access to the status code, content-type and, of course, the actual content. You can extract the content from the request using the content() function. At the time of writing, there are three ways to retrieve this content: as a raw object, as a character vector, or an R object, such as a list. # Load the httr package library(httr) # Get the url, save response to resp url &lt;- &quot;http://www.example.com/&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.example.com/] ## Date: 2024-01-16 05:44 ## Status: 200 ## Content-Type: text/html; charset=UTF-8 ## Size: 1.26 kB ## &lt;!doctype html&gt; ## &lt;html&gt; ## &lt;head&gt; ## &lt;title&gt;Example Domain&lt;/title&gt; ## ## &lt;meta charset=&quot;utf-8&quot; /&gt; ## &lt;meta http-equiv=&quot;Content-type&quot; content=&quot;text/html; charset=utf-8&quot; /&gt; ## &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1&quot; /&gt; ## &lt;style type=&quot;text/css&quot;&gt; ## body { ## ... # Get the raw content of resp: raw_content raw_content &lt;- content(resp, as = &quot;raw&quot;) # Print the head of raw_content head(raw_content) ## [1] 3c 21 64 6f 63 74 If you don’t tell content() how to retrieve the content through the as argument, it’ll try its best to figure out which type is most appropriate based on the content-type. content(resp) ## {html_document} ## &lt;html&gt; ## [1] &lt;head&gt;\\n&lt;title&gt;Example Domain&lt;/title&gt;\\n&lt;meta charset=&quot;utf-8&quot;&gt;\\n&lt;meta http ... ## [2] &lt;body&gt;\\n&lt;div&gt;\\n &lt;h1&gt;Example Domain&lt;/h1&gt;\\n &lt;p&gt;This domain is for use ... Web content does not limit itself to HTML pages and files stored on remote servers. There are many other data formats out there. A very common one is JSON. This format is very often used by so-called Web APIs, interfaces to web servers with which you as a client can communicate to get or store information in more complicated ways. See the content-type is json. # Get the url url &lt;- &quot;http://www.omdbapi.com/?apikey=72bc447a&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json&quot; resp &lt;- GET(url) # Print resp resp ## Response [http://www.omdbapi.com/?apikey=72bc447a&amp;t=Annie+Hall&amp;y=&amp;plot=short&amp;r=json] ## Date: 2024-01-16 05:44 ## Status: 200 ## Content-Type: application/json; charset=utf-8 ## Size: 1.02 kB # Print content of resp as text content(resp, as = &quot;text&quot;) ## [1] &quot;{\\&quot;Title\\&quot;:\\&quot;Annie Hall\\&quot;,\\&quot;Year\\&quot;:\\&quot;1977\\&quot;,\\&quot;Rated\\&quot;:\\&quot;PG\\&quot;,\\&quot;Released\\&quot;:\\&quot;20 Apr 1977\\&quot;,\\&quot;Runtime\\&quot;:\\&quot;93 min\\&quot;,\\&quot;Genre\\&quot;:\\&quot;Comedy, Romance\\&quot;,\\&quot;Director\\&quot;:\\&quot;Woody Allen\\&quot;,\\&quot;Writer\\&quot;:\\&quot;Woody Allen, Marshall Brickman\\&quot;,\\&quot;Actors\\&quot;:\\&quot;Woody Allen, Diane Keaton, Tony Roberts\\&quot;,\\&quot;Plot\\&quot;:\\&quot;Alvy Singer, a divorced Jewish comedian, reflects on his relationship with ex-lover Annie Hall, an aspiring nightclub singer, which ended abruptly just like his previous marriages.\\&quot;,\\&quot;Language\\&quot;:\\&quot;English, German\\&quot;,\\&quot;Country\\&quot;:\\&quot;United States\\&quot;,\\&quot;Awards\\&quot;:\\&quot;Won 4 Oscars. 32 wins &amp; 9 nominations total\\&quot;,\\&quot;Poster\\&quot;:\\&quot;https://m.media-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg\\&quot;,\\&quot;Ratings\\&quot;:[{\\&quot;Source\\&quot;:\\&quot;Internet Movie Database\\&quot;,\\&quot;Value\\&quot;:\\&quot;8.0/10\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Rotten Tomatoes\\&quot;,\\&quot;Value\\&quot;:\\&quot;97%\\&quot;},{\\&quot;Source\\&quot;:\\&quot;Metacritic\\&quot;,\\&quot;Value\\&quot;:\\&quot;92/100\\&quot;}],\\&quot;Metascore\\&quot;:\\&quot;92\\&quot;,\\&quot;imdbRating\\&quot;:\\&quot;8.0\\&quot;,\\&quot;imdbVotes\\&quot;:\\&quot;274,689\\&quot;,\\&quot;imdbID\\&quot;:\\&quot;tt0075686\\&quot;,\\&quot;Type\\&quot;:\\&quot;movie\\&quot;,\\&quot;DVD\\&quot;:\\&quot;03 Sep 2015\\&quot;,\\&quot;BoxOffice\\&quot;:\\&quot;$38,251,425\\&quot;,\\&quot;Production\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Website\\&quot;:\\&quot;N/A\\&quot;,\\&quot;Response\\&quot;:\\&quot;True\\&quot;}&quot; This time do not specify as argument. R figures out automatically that you’re dealing with a JSON, and converts the JSON to a named R list. # Print content of resp content(resp) ## $Title ## [1] &quot;Annie Hall&quot; ## ## $Year ## [1] &quot;1977&quot; ## ## $Rated ## [1] &quot;PG&quot; ## ## $Released ## [1] &quot;20 Apr 1977&quot; ## ## $Runtime ## [1] &quot;93 min&quot; ## ## $Genre ## [1] &quot;Comedy, Romance&quot; ## ## $Director ## [1] &quot;Woody Allen&quot; ## ## $Writer ## [1] &quot;Woody Allen, Marshall Brickman&quot; ## ## $Actors ## [1] &quot;Woody Allen, Diane Keaton, Tony Roberts&quot; ## ## $Plot ## [1] &quot;Alvy Singer, a divorced Jewish comedian, reflects on his relationship with ex-lover Annie Hall, an aspiring nightclub singer, which ended abruptly just like his previous marriages.&quot; ## ## $Language ## [1] &quot;English, German&quot; ## ## $Country ## [1] &quot;United States&quot; ## ## $Awards ## [1] &quot;Won 4 Oscars. 32 wins &amp; 9 nominations total&quot; ## ## $Poster ## [1] &quot;https://m.media-amazon.com/images/M/MV5BZDg1OGQ4YzgtM2Y2NS00NjA3LWFjYTctMDRlMDI3NWE1OTUyXkEyXkFqcGdeQXVyMjUzOTY1NTc@._V1_SX300.jpg&quot; ## ## $Ratings ## $Ratings[[1]] ## $Ratings[[1]]$Source ## [1] &quot;Internet Movie Database&quot; ## ## $Ratings[[1]]$Value ## [1] &quot;8.0/10&quot; ## ## ## $Ratings[[2]] ## $Ratings[[2]]$Source ## [1] &quot;Rotten Tomatoes&quot; ## ## $Ratings[[2]]$Value ## [1] &quot;97%&quot; ## ## ## $Ratings[[3]] ## $Ratings[[3]]$Source ## [1] &quot;Metacritic&quot; ## ## $Ratings[[3]]$Value ## [1] &quot;92/100&quot; ## ## ## ## $Metascore ## [1] &quot;92&quot; ## ## $imdbRating ## [1] &quot;8.0&quot; ## ## $imdbVotes ## [1] &quot;274,689&quot; ## ## $imdbID ## [1] &quot;tt0075686&quot; ## ## $Type ## [1] &quot;movie&quot; ## ## $DVD ## [1] &quot;03 Sep 2015&quot; ## ## $BoxOffice ## [1] &quot;$38,251,425&quot; ## ## $Production ## [1] &quot;N/A&quot; ## ## $Website ## [1] &quot;N/A&quot; ## ## $Response ## [1] &quot;True&quot; httr converts the JSON response body automatically to an R list. 22.4 Import from the web-2 22.4.1 APIs &amp; JSON JSON Simple, concise, well-structured Human-readable Easy to parse and generate for computers For communication with Web APIs API Application Programming Interface Set of routines and protocols for building software How different components interact Web API interface to get or add data to server HTTP verbs (GET and others) jsonlite package Download the JSON data and convert it to a named R list, fromJSON(url) Consistent, robust Support all use-cases 22.4.1.1 From JSON to R fromJSON() can convert character strings that represent JSON data into a nicely structured R list. # Load the jsonlite package library(jsonlite) ## ## Attaching package: &#39;jsonlite&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## flatten # wine_json is a JSON wine_json &lt;- &#39;{&quot;name&quot;:&quot;Chateau Migraine&quot;, &quot;year&quot;:1997, &quot;alcohol_pct&quot;:12.4, &quot;color&quot;:&quot;red&quot;, &quot;awarded&quot;:false}&#39; # Convert wine_json into a list: wine wine &lt;- fromJSON(wine_json) # Print structure of wine str(wine) ## List of 5 ## $ name : chr &quot;Chateau Migraine&quot; ## $ year : int 1997 ## $ alcohol_pct: num 12.4 ## $ color : chr &quot;red&quot; ## $ awarded : logi FALSE 22.4.1.2 Quandl API fromJSON() also works if you pass a URL as a character string or the path to a local file that contains JSON data. Let’s try this out on the Quandl API, where you can fetch all sorts of financial and economical data. # Definition of quandl_url quandl_url &lt;- &quot;https://www.quandl.com/api/v3/datasets/WIKI/FB/data.json?auth_token=i83asDsiWUUyfoypkgMz&quot; # Import Quandl data: quandl_data quandl_data &lt;- fromJSON(quandl_url) # Print structure of quandl_data str(quandl_data) ## List of 1 ## $ dataset_data:List of 10 ## ..$ limit : NULL ## ..$ transform : NULL ## ..$ column_index: NULL ## ..$ column_names: chr [1:13] &quot;Date&quot; &quot;Open&quot; &quot;High&quot; &quot;Low&quot; ... ## ..$ start_date : chr &quot;2012-05-18&quot; ## ..$ end_date : chr &quot;2018-03-27&quot; ## ..$ frequency : chr &quot;daily&quot; ## ..$ data : chr [1:1472, 1:13] &quot;2018-03-27&quot; &quot;2018-03-26&quot; &quot;2018-03-23&quot; &quot;2018-03-22&quot; ... ## ..$ collapse : NULL ## ..$ order : NULL Notice that the data element is a matrix [1:1472, 1:13]. 22.4.1.3 OMDb API Let’s compare the release year of two movies in the Open Movie Database. # Definition of the URLs url_sw4 &lt;- &quot;http://www.omdbapi.com/?apikey=72bc447a&amp;i=tt0076759&amp;r=json&quot; url_sw3 &lt;- &quot;http://www.omdbapi.com/?apikey=72bc447a&amp;i=tt0121766&amp;r=json&quot; # Import two URLs with fromJSON(): sw4 and sw3 sw4 &lt;- fromJSON(url_sw4) sw3 &lt;- fromJSON(url_sw3) # Print structure of sw4 str(sw4) ## List of 25 ## $ Title : chr &quot;Star Wars: Episode IV - A New Hope&quot; ## $ Year : chr &quot;1977&quot; ## $ Rated : chr &quot;PG&quot; ## $ Released : chr &quot;25 May 1977&quot; ## $ Runtime : chr &quot;121 min&quot; ## $ Genre : chr &quot;Action, Adventure, Fantasy&quot; ## $ Director : chr &quot;George Lucas&quot; ## $ Writer : chr &quot;George Lucas&quot; ## $ Actors : chr &quot;Mark Hamill, Harrison Ford, Carrie Fisher&quot; ## $ Plot : chr &quot;Luke Skywalker joins forces with a Jedi Knight, a cocky pilot, a Wookiee and two droids to save the galaxy from&quot;| __truncated__ ## $ Language : chr &quot;English&quot; ## $ Country : chr &quot;United States&quot; ## $ Awards : chr &quot;Won 6 Oscars. 65 wins &amp; 31 nominations total&quot; ## $ Poster : chr &quot;https://m.media-amazon.com/images/M/MV5BOTA5NjhiOTAtZWM0ZC00MWNhLThiMzEtZDFkOTk2OTU1ZDJkXkEyXkFqcGdeQXVyMTA4NDI&quot;| __truncated__ ## $ Ratings :&#39;data.frame&#39;: 3 obs. of 2 variables: ## ..$ Source: chr [1:3] &quot;Internet Movie Database&quot; &quot;Rotten Tomatoes&quot; &quot;Metacritic&quot; ## ..$ Value : chr [1:3] &quot;8.6/10&quot; &quot;93%&quot; &quot;90/100&quot; ## $ Metascore : chr &quot;90&quot; ## $ imdbRating: chr &quot;8.6&quot; ## $ imdbVotes : chr &quot;1,429,914&quot; ## $ imdbID : chr &quot;tt0076759&quot; ## $ Type : chr &quot;movie&quot; ## $ DVD : chr &quot;10 Oct 2016&quot; ## $ BoxOffice : chr &quot;$460,998,507&quot; ## $ Production: chr &quot;N/A&quot; ## $ Website : chr &quot;N/A&quot; ## $ Response : chr &quot;True&quot; Title names # Print out the Title element of both lists list(sw4 = sw4$Title, sw3 = sw3$Title) ## $sw4 ## [1] &quot;Star Wars: Episode IV - A New Hope&quot; ## ## $sw3 ## [1] &quot;Star Wars: Episode III - Revenge of the Sith&quot; # Is the release year of sw4 later than sw3? ifelse(sw4$Year &gt; sw3$Year, TRUE, FALSE) ## [1] FALSE The fourth episode of the Star Wars saga was released before the third one! 22.4.2 JSON &amp; jsonlite JSON object name value string string number boolean null JSON object JSON array Other jsonlite functions toJSON() prettify() minify() 22.4.2.1 JSON object &amp; array JSON is built on two structures: objects and arrays. To help you experiment with these, two JSON strings are included in the sample code. # array json1 &lt;- &#39;[1, 2, 3, 4, 5, 6]&#39; fromJSON(json1) ## [1] 1 2 3 4 5 6 # object:array json2 &lt;- &#39;{&quot;a&quot;: [1, 2, 3], &quot;b&quot;: [4, 5, 6]}&#39; fromJSON(json2) ## $a ## [1] 1 2 3 ## ## $b ## [1] 4 5 6 # 2 by 2 matrix json1 &lt;- &#39;[[1, 2], [3, 4]]&#39; fromJSON(json1) ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 4 # data frame json2 &lt;- &#39;[{&quot;a&quot;: 1, &quot;b&quot;: 2}, {&quot;a&quot;: 3, &quot;b&quot;: 4}, {&quot;a&quot;: 5, &quot;b&quot;: 6}]&#39; fromJSON(json2) ## a b ## 1 1 2 ## 2 3 4 ## 3 5 6 # nesting nest &lt;- fromJSON(&#39;{&quot;id&quot;:1, &quot;name&quot;:&quot;Frank&quot;, &quot;age&quot;:23, &quot;married&quot;:false, &quot;partner&quot;:{&quot;id&quot;:4,&quot;name&quot;:&quot;Julie&quot;} }&#39;) str(nest) ## List of 5 ## $ id : int 1 ## $ name : chr &quot;Frank&quot; ## $ age : int 23 ## $ married: logi FALSE ## $ partner:List of 2 ## ..$ id : int 4 ## ..$ name: chr &quot;Julie&quot; # JSON Array of JSON Objects fromJSON(&#39;[{&quot;id&quot;:1, &quot;name&quot;:&quot;Frank&quot;}, {&quot;id&quot;:4, &quot;name&quot;:&quot;Julie&quot;}, {&quot;id&quot;:12, &quot;name&quot;:&quot;Zach&quot;}]&#39;) ## id name ## 1 1 Frank ## 2 4 Julie ## 3 12 Zach 22.4.2.2 toJSON() toJSON() to convert R data to a JSON format. In its most basic use, you simply pass this function an R object to convert to a JSON. The result is an R object of the class json, which is basically a character string representing that JSON. # URL pointing to the .csv file url_csv &lt;- &quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/water.csv&quot; # Import the .csv file located at url_csv water &lt;- read.csv(url_csv) # Convert the data file according to the requirements water_json &lt;- toJSON(water) # Print out water_json water_json ## [{&quot;water&quot;:&quot;Algeria&quot;,&quot;X1992&quot;:0.064,&quot;X2002&quot;:0.017},{&quot;water&quot;:&quot;American Samoa&quot;},{&quot;water&quot;:&quot;Angola&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Antigua and Barbuda&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Argentina&quot;,&quot;X1992&quot;:0.0007,&quot;X1997&quot;:0.0007,&quot;X2002&quot;:0.0007},{&quot;water&quot;:&quot;Australia&quot;,&quot;X1992&quot;:0.0298,&quot;X2002&quot;:0.0298},{&quot;water&quot;:&quot;Austria&quot;,&quot;X1992&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Bahamas&quot;,&quot;X1992&quot;:0.0013,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Bahrain&quot;,&quot;X1992&quot;:0.0441,&quot;X2002&quot;:0.0441,&quot;X2007&quot;:0.1024},{&quot;water&quot;:&quot;Barbados&quot;,&quot;X2007&quot;:0.0146},{&quot;water&quot;:&quot;British Virgin Islands&quot;,&quot;X2007&quot;:0.0042},{&quot;water&quot;:&quot;Canada&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cape Verde&quot;,&quot;X1992&quot;:0.002,&quot;X1997&quot;:0.0017},{&quot;water&quot;:&quot;Cayman Islands&quot;,&quot;X1992&quot;:0.0033},{&quot;water&quot;:&quot;Central African Rep.&quot;},{&quot;water&quot;:&quot;Chile&quot;,&quot;X1992&quot;:0.0048,&quot;X2002&quot;:0.0048},{&quot;water&quot;:&quot;Colombia&quot;,&quot;X1992&quot;:0.0027,&quot;X2002&quot;:0.0027},{&quot;water&quot;:&quot;Cuba&quot;,&quot;X1992&quot;:0.0069,&quot;X1997&quot;:0.0069,&quot;X2002&quot;:0.0069},{&quot;water&quot;:&quot;Cyprus&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.0335},{&quot;water&quot;:&quot;Czech Rep.&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Denmark&quot;,&quot;X1992&quot;:0.015,&quot;X2002&quot;:0.015},{&quot;water&quot;:&quot;Djibouti&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Ecuador&quot;,&quot;X1992&quot;:0.0022,&quot;X1997&quot;:0.0022,&quot;X2002&quot;:0.0022},{&quot;water&quot;:&quot;Egypt&quot;,&quot;X1992&quot;:0.025,&quot;X1997&quot;:0.025,&quot;X2002&quot;:0.1},{&quot;water&quot;:&quot;El Salvador&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Finland&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;France&quot;,&quot;X1992&quot;:0.0117,&quot;X2002&quot;:0.0117},{&quot;water&quot;:&quot;Gibraltar&quot;,&quot;X1992&quot;:0.0077},{&quot;water&quot;:&quot;Greece&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01},{&quot;water&quot;:&quot;Honduras&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Hungary&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;India&quot;,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Indonesia&quot;,&quot;X1992&quot;:0.0187,&quot;X2002&quot;:0.0187},{&quot;water&quot;:&quot;Iran&quot;,&quot;X1992&quot;:0.003,&quot;X1997&quot;:0.003,&quot;X2002&quot;:0.003,&quot;X2007&quot;:0.2},{&quot;water&quot;:&quot;Iraq&quot;,&quot;X1997&quot;:0.0074,&quot;X2002&quot;:0.0074},{&quot;water&quot;:&quot;Ireland&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Israel&quot;,&quot;X1992&quot;:0.0256,&quot;X2002&quot;:0.0256,&quot;X2007&quot;:0.14},{&quot;water&quot;:&quot;Italy&quot;,&quot;X1992&quot;:0.0973,&quot;X2002&quot;:0.0973},{&quot;water&quot;:&quot;Jamaica&quot;,&quot;X1992&quot;:0.0005,&quot;X1997&quot;:0.0005,&quot;X2002&quot;:0.0005},{&quot;water&quot;:&quot;Japan&quot;,&quot;X1997&quot;:0.04,&quot;X2002&quot;:0.04},{&quot;water&quot;:&quot;Jordan&quot;,&quot;X1997&quot;:0.002,&quot;X2007&quot;:0.0098},{&quot;water&quot;:&quot;Kazakhstan&quot;,&quot;X1997&quot;:1.328,&quot;X2002&quot;:1.328},{&quot;water&quot;:&quot;Kuwait&quot;,&quot;X1992&quot;:0.507,&quot;X1997&quot;:0.231,&quot;X2002&quot;:0.4202},{&quot;water&quot;:&quot;Lebanon&quot;,&quot;X2007&quot;:0.0473},{&quot;water&quot;:&quot;Libya&quot;,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Malaysia&quot;,&quot;X1992&quot;:0.0043,&quot;X2002&quot;:0.0043},{&quot;water&quot;:&quot;Maldives&quot;,&quot;X1992&quot;:0.0004},{&quot;water&quot;:&quot;Malta&quot;,&quot;X1992&quot;:0.024,&quot;X1997&quot;:0.031,&quot;X2002&quot;:0.031},{&quot;water&quot;:&quot;Marshall Islands&quot;,&quot;X1992&quot;:0.0007},{&quot;water&quot;:&quot;Mauritania&quot;,&quot;X1992&quot;:0.002,&quot;X2002&quot;:0.002},{&quot;water&quot;:&quot;Mexico&quot;,&quot;X1992&quot;:0.0307,&quot;X2002&quot;:0.0307},{&quot;water&quot;:&quot;Morocco&quot;,&quot;X1992&quot;:0.0034,&quot;X1997&quot;:0.0034,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Namibia&quot;,&quot;X1992&quot;:0.0003,&quot;X2002&quot;:0.0003},{&quot;water&quot;:&quot;Netherlands Antilles&quot;,&quot;X1992&quot;:0.063},{&quot;water&quot;:&quot;Nicaragua&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Nigeria&quot;,&quot;X1992&quot;:0.003,&quot;X2002&quot;:0.003},{&quot;water&quot;:&quot;Norway&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;Oman&quot;,&quot;X1997&quot;:0.034,&quot;X2002&quot;:0.034,&quot;X2007&quot;:0.109},{&quot;water&quot;:&quot;Peru&quot;,&quot;X1992&quot;:0.0054,&quot;X2002&quot;:0.0054},{&quot;water&quot;:&quot;Poland&quot;,&quot;X1992&quot;:0.007,&quot;X2002&quot;:0.007},{&quot;water&quot;:&quot;Portugal&quot;,&quot;X1992&quot;:0.0016,&quot;X2002&quot;:0.0016},{&quot;water&quot;:&quot;Qatar&quot;,&quot;X1992&quot;:0.065,&quot;X1997&quot;:0.099,&quot;X2002&quot;:0.099,&quot;X2007&quot;:0.18},{&quot;water&quot;:&quot;Saudi Arabia&quot;,&quot;X1992&quot;:0.683,&quot;X1997&quot;:0.727,&quot;X2002&quot;:0.863,&quot;X2007&quot;:1.033},{&quot;water&quot;:&quot;Senegal&quot;,&quot;X1992&quot;:0,&quot;X2002&quot;:0},{&quot;water&quot;:&quot;Somalia&quot;,&quot;X1992&quot;:0.0001,&quot;X2002&quot;:0.0001},{&quot;water&quot;:&quot;South Africa&quot;,&quot;X1992&quot;:0.018,&quot;X2002&quot;:0.018},{&quot;water&quot;:&quot;Spain&quot;,&quot;X1992&quot;:0.1002,&quot;X2002&quot;:0.1002},{&quot;water&quot;:&quot;Sudan&quot;,&quot;X1992&quot;:0.0004,&quot;X1997&quot;:0.0004,&quot;X2002&quot;:0.0004},{&quot;water&quot;:&quot;Sweden&quot;,&quot;X1992&quot;:0.0002,&quot;X2002&quot;:0.0002},{&quot;water&quot;:&quot;Trinidad and Tobago&quot;,&quot;X2007&quot;:0.036},{&quot;water&quot;:&quot;Tunisia&quot;,&quot;X1992&quot;:0.008,&quot;X2002&quot;:0.013},{&quot;water&quot;:&quot;Turkey&quot;,&quot;X1992&quot;:0.0005,&quot;X2002&quot;:0.0005,&quot;X2007&quot;:0.0005},{&quot;water&quot;:&quot;United Arab Emirates&quot;,&quot;X1992&quot;:0.163,&quot;X1997&quot;:0.385,&quot;X2007&quot;:0.95},{&quot;water&quot;:&quot;United Kingdom&quot;,&quot;X1992&quot;:0.0333,&quot;X2002&quot;:0.0333},{&quot;water&quot;:&quot;United States&quot;,&quot;X1992&quot;:0.58,&quot;X2002&quot;:0.58},{&quot;water&quot;:&quot;Venezuela&quot;,&quot;X1992&quot;:0.0052,&quot;X2002&quot;:0.0052},{&quot;water&quot;:&quot;Yemen, Rep.&quot;,&quot;X1992&quot;:0.01,&quot;X2002&quot;:0.01}] As you can see, the JSON you printed out isn’t easy to read. 22.4.2.3 Minify &amp; prettify JSONs can come in different formats: minified format, pretty format: with indentation, whitespace and new lines. # Mini {&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:{&quot;x&quot;:5,&quot;y&quot;:6}} # Pretty { &quot;a&quot;: 1, &quot;b&quot;: 2, &quot;c&quot;: { &quot;x&quot;: 5, &quot;y&quot;: 6 } } The standard form that toJSON() returns, is the minified version. You can adapt this behavior by : toJSON(object, pretty = TRUE) prettify(json_string), minify(json_string) # Convert mtcars to a pretty JSON: pretty_json pretty_json &lt;- toJSON(mtcars, pretty = TRUE) # Print pretty_json pretty_json ## [ ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.62, ## &quot;qsec&quot;: 16.46, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Mazda RX4&quot;, ## &quot;fcyl_fam&quot;: &quot;6:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Mazda RX4&quot; ## }, ## { ## &quot;mpg&quot;: 21, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 160, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.9, ## &quot;wt&quot;: 2.875, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Mazda RX4 Wag&quot;, ## &quot;fcyl_fam&quot;: &quot;6:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Mazda RX4 Wag&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 108, ## &quot;hp&quot;: 93, ## &quot;drat&quot;: 3.85, ## &quot;wt&quot;: 2.32, ## &quot;qsec&quot;: 18.61, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Datsun 710&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Datsun 710&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 258, ## &quot;hp&quot;: 110, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.215, ## &quot;qsec&quot;: 19.44, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Hornet 4 Drive&quot;, ## &quot;fcyl_fam&quot;: &quot;6:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Hornet 4 Drive&quot; ## }, ## { ## &quot;mpg&quot;: 18.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 17.02, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Hornet Sportabout&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Hornet Sportabout&quot; ## }, ## { ## &quot;mpg&quot;: 18.1, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 225, ## &quot;hp&quot;: 105, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.46, ## &quot;qsec&quot;: 20.22, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Valiant&quot;, ## &quot;fcyl_fam&quot;: &quot;6:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Valiant&quot; ## }, ## { ## &quot;mpg&quot;: 14.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 360, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.21, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 15.84, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Duster 360&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Duster 360&quot; ## }, ## { ## &quot;mpg&quot;: 24.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 146.7, ## &quot;hp&quot;: 62, ## &quot;drat&quot;: 3.69, ## &quot;wt&quot;: 3.19, ## &quot;qsec&quot;: 20, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 240D&quot;, ## &quot;fcyl_fam&quot;: &quot;4:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Merc 240D&quot; ## }, ## { ## &quot;mpg&quot;: 22.8, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 140.8, ## &quot;hp&quot;: 95, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.15, ## &quot;qsec&quot;: 22.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 230&quot;, ## &quot;fcyl_fam&quot;: &quot;4:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Merc 230&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.3, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 280&quot;, ## &quot;fcyl_fam&quot;: &quot;6:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Merc 280&quot; ## }, ## { ## &quot;mpg&quot;: 17.8, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 167.6, ## &quot;hp&quot;: 123, ## &quot;drat&quot;: 3.92, ## &quot;wt&quot;: 3.44, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 280C&quot;, ## &quot;fcyl_fam&quot;: &quot;6:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Merc 280C&quot; ## }, ## { ## &quot;mpg&quot;: 16.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 4.07, ## &quot;qsec&quot;: 17.4, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 450SE&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Merc 450SE&quot; ## }, ## { ## &quot;mpg&quot;: 17.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.73, ## &quot;qsec&quot;: 17.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 450SL&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Merc 450SL&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 275.8, ## &quot;hp&quot;: 180, ## &quot;drat&quot;: 3.07, ## &quot;wt&quot;: 3.78, ## &quot;qsec&quot;: 18, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 3, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Merc 450SLC&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Merc 450SLC&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 472, ## &quot;hp&quot;: 205, ## &quot;drat&quot;: 2.93, ## &quot;wt&quot;: 5.25, ## &quot;qsec&quot;: 17.98, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Cadillac Fleetwood&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Cadillac Fleetwood&quot; ## }, ## { ## &quot;mpg&quot;: 10.4, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 460, ## &quot;hp&quot;: 215, ## &quot;drat&quot;: 3, ## &quot;wt&quot;: 5.424, ## &quot;qsec&quot;: 17.82, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Lincoln Continental&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Lincoln Continental&quot; ## }, ## { ## &quot;mpg&quot;: 14.7, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 440, ## &quot;hp&quot;: 230, ## &quot;drat&quot;: 3.23, ## &quot;wt&quot;: 5.345, ## &quot;qsec&quot;: 17.42, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Chrysler Imperial&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Chrysler Imperial&quot; ## }, ## { ## &quot;mpg&quot;: 32.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 78.7, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 2.2, ## &quot;qsec&quot;: 19.47, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Fiat 128&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Fiat 128&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 75.7, ## &quot;hp&quot;: 52, ## &quot;drat&quot;: 4.93, ## &quot;wt&quot;: 1.615, ## &quot;qsec&quot;: 18.52, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Honda Civic&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Honda Civic&quot; ## }, ## { ## &quot;mpg&quot;: 33.9, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 71.1, ## &quot;hp&quot;: 65, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 1.835, ## &quot;qsec&quot;: 19.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Toyota Corolla&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Toyota Corolla&quot; ## }, ## { ## &quot;mpg&quot;: 21.5, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.1, ## &quot;hp&quot;: 97, ## &quot;drat&quot;: 3.7, ## &quot;wt&quot;: 2.465, ## &quot;qsec&quot;: 20.01, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Toyota Corona&quot;, ## &quot;fcyl_fam&quot;: &quot;4:0&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Toyota Corona&quot; ## }, ## { ## &quot;mpg&quot;: 15.5, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 318, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 2.76, ## &quot;wt&quot;: 3.52, ## &quot;qsec&quot;: 16.87, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Dodge Challenger&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Dodge Challenger&quot; ## }, ## { ## &quot;mpg&quot;: 15.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 304, ## &quot;hp&quot;: 150, ## &quot;drat&quot;: 3.15, ## &quot;wt&quot;: 3.435, ## &quot;qsec&quot;: 17.3, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;AMC Javelin&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;AMC Javelin&quot; ## }, ## { ## &quot;mpg&quot;: 13.3, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 350, ## &quot;hp&quot;: 245, ## &quot;drat&quot;: 3.73, ## &quot;wt&quot;: 3.84, ## &quot;qsec&quot;: 15.41, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Camaro Z28&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Camaro Z28&quot; ## }, ## { ## &quot;mpg&quot;: 19.2, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 400, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.08, ## &quot;wt&quot;: 3.845, ## &quot;qsec&quot;: 17.05, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 0, ## &quot;gear&quot;: 3, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;automatic&quot;, ## &quot;car&quot;: &quot;Pontiac Firebird&quot;, ## &quot;fcyl_fam&quot;: &quot;8:0&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Pontiac Firebird&quot; ## }, ## { ## &quot;mpg&quot;: 27.3, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 79, ## &quot;hp&quot;: 66, ## &quot;drat&quot;: 4.08, ## &quot;wt&quot;: 1.935, ## &quot;qsec&quot;: 18.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 1, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Fiat X1-9&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Fiat X1-9&quot; ## }, ## { ## &quot;mpg&quot;: 26, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 120.3, ## &quot;hp&quot;: 91, ## &quot;drat&quot;: 4.43, ## &quot;wt&quot;: 2.14, ## &quot;qsec&quot;: 16.7, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Porsche 914-2&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Porsche 914-2&quot; ## }, ## { ## &quot;mpg&quot;: 30.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 95.1, ## &quot;hp&quot;: 113, ## &quot;drat&quot;: 3.77, ## &quot;wt&quot;: 1.513, ## &quot;qsec&quot;: 16.9, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Lotus Europa&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Lotus Europa&quot; ## }, ## { ## &quot;mpg&quot;: 15.8, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 351, ## &quot;hp&quot;: 264, ## &quot;drat&quot;: 4.22, ## &quot;wt&quot;: 3.17, ## &quot;qsec&quot;: 14.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 4, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Ford Pantera L&quot;, ## &quot;fcyl_fam&quot;: &quot;8:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Ford Pantera L&quot; ## }, ## { ## &quot;mpg&quot;: 19.7, ## &quot;cyl&quot;: 6, ## &quot;disp&quot;: 145, ## &quot;hp&quot;: 175, ## &quot;drat&quot;: 3.62, ## &quot;wt&quot;: 2.77, ## &quot;qsec&quot;: 15.5, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 6, ## &quot;fcyl&quot;: &quot;6&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Ferrari Dino&quot;, ## &quot;fcyl_fam&quot;: &quot;6:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Ferrari Dino&quot; ## }, ## { ## &quot;mpg&quot;: 15, ## &quot;cyl&quot;: 8, ## &quot;disp&quot;: 301, ## &quot;hp&quot;: 335, ## &quot;drat&quot;: 3.54, ## &quot;wt&quot;: 3.57, ## &quot;qsec&quot;: 14.6, ## &quot;vs&quot;: 0, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 5, ## &quot;carb&quot;: 8, ## &quot;fcyl&quot;: &quot;8&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Maserati Bora&quot;, ## &quot;fcyl_fam&quot;: &quot;8:1&quot;, ## &quot;fvs&quot;: &quot;V-shaped&quot;, ## &quot;_row&quot;: &quot;Maserati Bora&quot; ## }, ## { ## &quot;mpg&quot;: 21.4, ## &quot;cyl&quot;: 4, ## &quot;disp&quot;: 121, ## &quot;hp&quot;: 109, ## &quot;drat&quot;: 4.11, ## &quot;wt&quot;: 2.78, ## &quot;qsec&quot;: 18.6, ## &quot;vs&quot;: 1, ## &quot;am&quot;: 1, ## &quot;gear&quot;: 4, ## &quot;carb&quot;: 2, ## &quot;fcyl&quot;: &quot;4&quot;, ## &quot;fam&quot;: &quot;manual&quot;, ## &quot;car&quot;: &quot;Volvo 142E&quot;, ## &quot;fcyl_fam&quot;: &quot;4:1&quot;, ## &quot;fvs&quot;: &quot;straight&quot;, ## &quot;_row&quot;: &quot;Volvo 142E&quot; ## } ## ] Convert pretty_json to a minimal version. # Minify pretty_json: mini_json mini_json &lt;- minify(pretty_json) # Print mini_json mini_json ## [{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.62,&quot;qsec&quot;:16.46,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Mazda RX4&quot;,&quot;fcyl_fam&quot;:&quot;6:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Mazda RX4&quot;},{&quot;mpg&quot;:21,&quot;cyl&quot;:6,&quot;disp&quot;:160,&quot;hp&quot;:110,&quot;drat&quot;:3.9,&quot;wt&quot;:2.875,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Mazda RX4 Wag&quot;,&quot;fcyl_fam&quot;:&quot;6:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Mazda RX4 Wag&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:108,&quot;hp&quot;:93,&quot;drat&quot;:3.85,&quot;wt&quot;:2.32,&quot;qsec&quot;:18.61,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Datsun 710&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Datsun 710&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:6,&quot;disp&quot;:258,&quot;hp&quot;:110,&quot;drat&quot;:3.08,&quot;wt&quot;:3.215,&quot;qsec&quot;:19.44,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Hornet 4 Drive&quot;,&quot;fcyl_fam&quot;:&quot;6:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Hornet 4 Drive&quot;},{&quot;mpg&quot;:18.7,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:175,&quot;drat&quot;:3.15,&quot;wt&quot;:3.44,&quot;qsec&quot;:17.02,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Hornet Sportabout&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Hornet Sportabout&quot;},{&quot;mpg&quot;:18.1,&quot;cyl&quot;:6,&quot;disp&quot;:225,&quot;hp&quot;:105,&quot;drat&quot;:2.76,&quot;wt&quot;:3.46,&quot;qsec&quot;:20.22,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Valiant&quot;,&quot;fcyl_fam&quot;:&quot;6:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Valiant&quot;},{&quot;mpg&quot;:14.3,&quot;cyl&quot;:8,&quot;disp&quot;:360,&quot;hp&quot;:245,&quot;drat&quot;:3.21,&quot;wt&quot;:3.57,&quot;qsec&quot;:15.84,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Duster 360&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Duster 360&quot;},{&quot;mpg&quot;:24.4,&quot;cyl&quot;:4,&quot;disp&quot;:146.7,&quot;hp&quot;:62,&quot;drat&quot;:3.69,&quot;wt&quot;:3.19,&quot;qsec&quot;:20,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 240D&quot;,&quot;fcyl_fam&quot;:&quot;4:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Merc 240D&quot;},{&quot;mpg&quot;:22.8,&quot;cyl&quot;:4,&quot;disp&quot;:140.8,&quot;hp&quot;:95,&quot;drat&quot;:3.92,&quot;wt&quot;:3.15,&quot;qsec&quot;:22.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 230&quot;,&quot;fcyl_fam&quot;:&quot;4:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Merc 230&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.3,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 280&quot;,&quot;fcyl_fam&quot;:&quot;6:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Merc 280&quot;},{&quot;mpg&quot;:17.8,&quot;cyl&quot;:6,&quot;disp&quot;:167.6,&quot;hp&quot;:123,&quot;drat&quot;:3.92,&quot;wt&quot;:3.44,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:4,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 280C&quot;,&quot;fcyl_fam&quot;:&quot;6:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Merc 280C&quot;},{&quot;mpg&quot;:16.4,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:4.07,&quot;qsec&quot;:17.4,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 450SE&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Merc 450SE&quot;},{&quot;mpg&quot;:17.3,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.73,&quot;qsec&quot;:17.6,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 450SL&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Merc 450SL&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:275.8,&quot;hp&quot;:180,&quot;drat&quot;:3.07,&quot;wt&quot;:3.78,&quot;qsec&quot;:18,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:3,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Merc 450SLC&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Merc 450SLC&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:472,&quot;hp&quot;:205,&quot;drat&quot;:2.93,&quot;wt&quot;:5.25,&quot;qsec&quot;:17.98,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Cadillac Fleetwood&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Cadillac Fleetwood&quot;},{&quot;mpg&quot;:10.4,&quot;cyl&quot;:8,&quot;disp&quot;:460,&quot;hp&quot;:215,&quot;drat&quot;:3,&quot;wt&quot;:5.424,&quot;qsec&quot;:17.82,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Lincoln Continental&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Lincoln Continental&quot;},{&quot;mpg&quot;:14.7,&quot;cyl&quot;:8,&quot;disp&quot;:440,&quot;hp&quot;:230,&quot;drat&quot;:3.23,&quot;wt&quot;:5.345,&quot;qsec&quot;:17.42,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Chrysler Imperial&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Chrysler Imperial&quot;},{&quot;mpg&quot;:32.4,&quot;cyl&quot;:4,&quot;disp&quot;:78.7,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:2.2,&quot;qsec&quot;:19.47,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Fiat 128&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Fiat 128&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:75.7,&quot;hp&quot;:52,&quot;drat&quot;:4.93,&quot;wt&quot;:1.615,&quot;qsec&quot;:18.52,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Honda Civic&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Honda Civic&quot;},{&quot;mpg&quot;:33.9,&quot;cyl&quot;:4,&quot;disp&quot;:71.1,&quot;hp&quot;:65,&quot;drat&quot;:4.22,&quot;wt&quot;:1.835,&quot;qsec&quot;:19.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Toyota Corolla&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Toyota Corolla&quot;},{&quot;mpg&quot;:21.5,&quot;cyl&quot;:4,&quot;disp&quot;:120.1,&quot;hp&quot;:97,&quot;drat&quot;:3.7,&quot;wt&quot;:2.465,&quot;qsec&quot;:20.01,&quot;vs&quot;:1,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Toyota Corona&quot;,&quot;fcyl_fam&quot;:&quot;4:0&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Toyota Corona&quot;},{&quot;mpg&quot;:15.5,&quot;cyl&quot;:8,&quot;disp&quot;:318,&quot;hp&quot;:150,&quot;drat&quot;:2.76,&quot;wt&quot;:3.52,&quot;qsec&quot;:16.87,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Dodge Challenger&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Dodge Challenger&quot;},{&quot;mpg&quot;:15.2,&quot;cyl&quot;:8,&quot;disp&quot;:304,&quot;hp&quot;:150,&quot;drat&quot;:3.15,&quot;wt&quot;:3.435,&quot;qsec&quot;:17.3,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;AMC Javelin&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;AMC Javelin&quot;},{&quot;mpg&quot;:13.3,&quot;cyl&quot;:8,&quot;disp&quot;:350,&quot;hp&quot;:245,&quot;drat&quot;:3.73,&quot;wt&quot;:3.84,&quot;qsec&quot;:15.41,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Camaro Z28&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Camaro Z28&quot;},{&quot;mpg&quot;:19.2,&quot;cyl&quot;:8,&quot;disp&quot;:400,&quot;hp&quot;:175,&quot;drat&quot;:3.08,&quot;wt&quot;:3.845,&quot;qsec&quot;:17.05,&quot;vs&quot;:0,&quot;am&quot;:0,&quot;gear&quot;:3,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;automatic&quot;,&quot;car&quot;:&quot;Pontiac Firebird&quot;,&quot;fcyl_fam&quot;:&quot;8:0&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Pontiac Firebird&quot;},{&quot;mpg&quot;:27.3,&quot;cyl&quot;:4,&quot;disp&quot;:79,&quot;hp&quot;:66,&quot;drat&quot;:4.08,&quot;wt&quot;:1.935,&quot;qsec&quot;:18.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:1,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Fiat X1-9&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Fiat X1-9&quot;},{&quot;mpg&quot;:26,&quot;cyl&quot;:4,&quot;disp&quot;:120.3,&quot;hp&quot;:91,&quot;drat&quot;:4.43,&quot;wt&quot;:2.14,&quot;qsec&quot;:16.7,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Porsche 914-2&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Porsche 914-2&quot;},{&quot;mpg&quot;:30.4,&quot;cyl&quot;:4,&quot;disp&quot;:95.1,&quot;hp&quot;:113,&quot;drat&quot;:3.77,&quot;wt&quot;:1.513,&quot;qsec&quot;:16.9,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Lotus Europa&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Lotus Europa&quot;},{&quot;mpg&quot;:15.8,&quot;cyl&quot;:8,&quot;disp&quot;:351,&quot;hp&quot;:264,&quot;drat&quot;:4.22,&quot;wt&quot;:3.17,&quot;qsec&quot;:14.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:4,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Ford Pantera L&quot;,&quot;fcyl_fam&quot;:&quot;8:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Ford Pantera L&quot;},{&quot;mpg&quot;:19.7,&quot;cyl&quot;:6,&quot;disp&quot;:145,&quot;hp&quot;:175,&quot;drat&quot;:3.62,&quot;wt&quot;:2.77,&quot;qsec&quot;:15.5,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:6,&quot;fcyl&quot;:&quot;6&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Ferrari Dino&quot;,&quot;fcyl_fam&quot;:&quot;6:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Ferrari Dino&quot;},{&quot;mpg&quot;:15,&quot;cyl&quot;:8,&quot;disp&quot;:301,&quot;hp&quot;:335,&quot;drat&quot;:3.54,&quot;wt&quot;:3.57,&quot;qsec&quot;:14.6,&quot;vs&quot;:0,&quot;am&quot;:1,&quot;gear&quot;:5,&quot;carb&quot;:8,&quot;fcyl&quot;:&quot;8&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Maserati Bora&quot;,&quot;fcyl_fam&quot;:&quot;8:1&quot;,&quot;fvs&quot;:&quot;V-shaped&quot;,&quot;_row&quot;:&quot;Maserati Bora&quot;},{&quot;mpg&quot;:21.4,&quot;cyl&quot;:4,&quot;disp&quot;:121,&quot;hp&quot;:109,&quot;drat&quot;:4.11,&quot;wt&quot;:2.78,&quot;qsec&quot;:18.6,&quot;vs&quot;:1,&quot;am&quot;:1,&quot;gear&quot;:4,&quot;carb&quot;:2,&quot;fcyl&quot;:&quot;4&quot;,&quot;fam&quot;:&quot;manual&quot;,&quot;car&quot;:&quot;Volvo 142E&quot;,&quot;fcyl_fam&quot;:&quot;4:1&quot;,&quot;fvs&quot;:&quot;straight&quot;,&quot;_row&quot;:&quot;Volvo 142E&quot;}] Pretty format is way easier to read and understand. 22.5 Import from statistical software 22.5.1 haven package Statistical Software Packages All these functions take one key argument: the path to your local file. In fact, you can even pass a URL; haven will then automatically download the file for you before importing it. 22.5.1.1 Import SAS data # Load the haven package library(haven) # Import sales.sas7bdat: sales sales &lt;- read_sas(&quot;data/sales.sas7bdat&quot;) # Display the structure of sales str(sales) ## tibble [431 × 4] (S3: tbl_df/tbl/data.frame) ## $ purchase: num [1:431] 0 0 1 1 0 0 0 0 0 0 ... ## $ age : num [1:431] 41 47 41 39 32 32 33 45 43 40 ... ## $ gender : chr [1:431] &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; &quot;Female&quot; ... ## $ income : chr [1:431] &quot;Low&quot; &quot;Low&quot; &quot;Low&quot; &quot;Low&quot; ... 22.5.1.2 Import STATA data When inspecting the result of the read_dta() call, you will notice that one column will be imported as a labelled vector, an R equivalent for the common data structure in other statistical environments. In order to effectively continue working on the data in R, it’s best to change this data into a standard R class. To convert a variable of the class labelled to a factor, you’ll need haven’s as_factor() function. The Date column has class labelled. # Import the data from the URL: sugar sugar &lt;- read_dta(&quot;http://assets.datacamp.com/production/course_1478/datasets/trade.dta&quot;) # Structure of sugar str(sugar) ## tibble [10 × 5] (S3: tbl_df/tbl/data.frame) ## $ Date : dbl+lbl [1:10] 10, 9, 8, 7, 6, 5, 4, 3, 2, 1 ## ..@ label : chr &quot;Date&quot; ## ..@ format.stata: chr &quot;%9.0g&quot; ## ..@ labels : Named num [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;2004-12-31&quot; &quot;2005-12-31&quot; &quot;2006-12-31&quot; &quot;2007-12-31&quot; ... ## $ Import : num [1:10] 37664782 16316512 11082246 35677943 9879878 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Import&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Weight_I: num [1:10] 54029106 21584365 14526089 55034932 14806865 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Weight_I&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Export : num [1:10] 54505513 102700010 37935000 48515008 71486545 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Export&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Weight_E: num [1:10] 93350013 158000010 88000000 112000005 131800000 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Weight_E&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## - attr(*, &quot;label&quot;)= chr &quot;Written by R.&quot; # Convert values in Date column to dates sugar$Date &lt;- as.Date(as_factor(sugar$Date)) # Structure of sugar again str(sugar) ## tibble [10 × 5] (S3: tbl_df/tbl/data.frame) ## $ Date : Date[1:10], format: &quot;2013-12-31&quot; &quot;2012-12-31&quot; ... ## $ Import : num [1:10] 37664782 16316512 11082246 35677943 9879878 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Import&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Weight_I: num [1:10] 54029106 21584365 14526089 55034932 14806865 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Weight_I&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Export : num [1:10] 54505513 102700010 37935000 48515008 71486545 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Export&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## $ Weight_E: num [1:10] 93350013 158000010 88000000 112000005 131800000 ... ## ..- attr(*, &quot;label&quot;)= chr &quot;Weight_E&quot; ## ..- attr(*, &quot;format.stata&quot;)= chr &quot;%9.0g&quot; ## - attr(*, &quot;label&quot;)= chr &quot;Written by R.&quot; The more sugar is traded, the higher the weight that’s traded. plot(sugar$Import, sugar$Weight_I) 22.5.1.3 Import SPSS data Depending on the SPSS data file you’re working with, you’ll need either read_sav() - for .sav files - or read_por() - for .por files. read_spss() will choose dependently. # Import person.sav: traits traits &lt;- read_sav(&quot;data/person.sav&quot;) # Summarize traits summary(traits) ## Neurotic Extroversion Agreeableness Conscientiousness ## Min. : 0.0 Min. : 5.0 Min. :15.0 Min. : 7.0 ## 1st Qu.:18.0 1st Qu.:26.0 1st Qu.:39.0 1st Qu.:25.0 ## Median :24.0 Median :31.0 Median :45.0 Median :30.0 ## Mean :23.6 Mean :30.2 Mean :44.5 Mean :30.9 ## 3rd Qu.:29.0 3rd Qu.:34.0 3rd Qu.:50.0 3rd Qu.:36.0 ## Max. :44.0 Max. :65.0 Max. :73.0 Max. :58.0 ## NA&#39;s :14 NA&#39;s :16 NA&#39;s :19 NA&#39;s :14 subset of those individuals that scored high on Extroversion and on Agreeableness, i.e. scoring higher than 40 on each of these two categories. # Print out a subset subset(traits, Extroversion &gt; 40 &amp; Agreeableness &gt; 40) ## # A tibble: 8 × 4 ## Neurotic Extroversion Agreeableness Conscientiousness ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 38 43 49 29 ## 2 20 42 46 31 ## 3 18 42 49 31 ## 4 42 43 44 29 ## 5 30 42 51 24 ## 6 18 42 50 25 ## 7 27 45 55 23 ## 8 18 43 57 34 With SPSS data files, it can also happen that some of the variables you import have the labelled class. This is done to keep all the labelling information that was originally present in the .sav and .por files. It’s advised to coerce (or change) these variables to factors or other standard R classes. # Import SPSS data from the URL: work work &lt;- read_sav(&quot;http://s3.amazonaws.com/assets.datacamp.com/production/course_1478/datasets/employee.sav&quot;) # Display summary of work$GENDER summary(work$GENDER) ## Length Class Mode ## 474 character character This information doesn’t give you a lot of useful information. Use as_factor() to convert to categorical variables. # Convert work$GENDER to a factor work$GENDER &lt;- as_factor(work$GENDER) # Display summary of work$GENDER again summary(work$GENDER) ## Female Male ## 216 258 22.5.2 foreign package Less consistent Very comprehensive All kinds of foreign data formats 22.5.2.1 Import STATA data read.dta(file, convert.factors = TRUE, convert.dates = TRUE, missing.type = FALSE) convert.factors : convert labelled STATA values to R factors convert.dates : convert STATA dates and times to Date and POSIXct missing.type : if FALSE , convert all types of missing values to NA if TRUE , store how values are missing in attributes # Load the foreign package library(foreign) # Import florida.dta and name the resulting data frame florida florida &lt;- read.dta(&quot;data/florida.dta&quot;) # Check tail() of florida tail(florida) ## gore bush buchanan nader total ## 62 2647 4051 27 59 6784 ## 63 1399 2326 26 29 3780 ## 64 97063 82214 396 2436 182109 ## 65 3835 4511 46 149 8541 ## 66 5637 12176 120 265 18198 ## 67 2796 4983 88 93 7960 The arguments you will use most often are convert.dates, convert.factors, missing.type and convert.underscore. Specify the path to the file using file.path(). Use the path variable to import the data file in three different ways; each time show its structure. Default # Specify the file path using file.path(): path path &lt;- file.path(&quot;data&quot;, &quot;edequality.dta&quot;) # Create and print structure of edu_equal_1 edu_equal_1 &lt;- read.dta(path) str(edu_equal_1) ## &#39;data.frame&#39;: 12214 obs. of 27 variables: ## $ hhid : num 1 1 1 2 2 3 4 4 5 6 ... ## $ hhweight : num 627 627 627 627 627 ... ## $ location : Factor w/ 2 levels &quot;urban location&quot;,..: 1 1 1 1 1 2 2 2 1 1 ... ## $ region : Factor w/ 9 levels &quot;Sofia city&quot;,&quot;Bourgass&quot;,..: 8 8 8 9 9 4 4 4 8 8 ... ## $ ethnicity_head : Factor w/ 4 levels &quot;Bulgaria&quot;,&quot;Turks&quot;,..: 2 2 2 1 1 1 1 1 1 1 ... ## $ age : num 37 11 8 73 70 75 79 80 82 83 ... ## $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 2 1 1 2 2 2 ... ## $ relation : Factor w/ 9 levels &quot;head &quot;,..: 1 3 3 1 2 1 1 2 1 1 ... ## $ literate : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 2 2 2 2 2 2 2 ... ## $ income_mnt : num 13.3 13.3 13.3 142.5 142.5 ... ## $ income : num 160 160 160 1710 1710 ... ## $ aggregate : num 1042 1042 1042 3271 3271 ... ## $ aggr_ind_annual : num 347 347 347 1635 1635 ... ## $ educ_completed : int 2 4 4 4 3 3 3 3 4 4 ... ## $ grade_complete : num 4 3 0 3 4 4 4 4 5 5 ... ## $ grade_all : num 4 11 8 11 8 8 8 8 13 13 ... ## $ unemployed : int 2 1 1 1 1 1 1 1 1 1 ... ## $ reason_OLF : int NA NA NA 3 3 3 9 9 3 3 ... ## $ sector : int NA NA NA NA NA NA 1 1 NA NA ... ## $ occupation : int NA NA NA NA NA NA 5 5 NA NA ... ## $ earn_mont : num 0 0 0 0 0 0 20 20 0 0 ... ## $ earn_ann : num 0 0 0 0 0 0 240 240 0 0 ... ## $ hours_week : num NA NA NA NA NA NA 30 35 NA NA ... ## $ hours_mnt : num NA NA NA NA NA ... ## $ fulltime : int NA NA NA NA NA NA 1 1 NA NA ... ## $ hhexp : num 100 100 100 343 343 ... ## $ legacy_pension_amt: num NA NA NA NA NA NA NA NA NA NA ... ## - attr(*, &quot;datalabel&quot;)= chr &quot;&quot; ## - attr(*, &quot;time.stamp&quot;)= chr &quot;&quot; ## - attr(*, &quot;formats&quot;)= chr [1:27] &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; ... ## - attr(*, &quot;types&quot;)= int [1:27] 100 100 108 108 108 100 108 108 108 100 ... ## - attr(*, &quot;val.labels&quot;)= chr [1:27] &quot;&quot; &quot;&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;var.labels&quot;)= chr [1:27] &quot;hhid&quot; &quot;hhweight&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;expansion.fields&quot;)=List of 12 ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_su1&quot; &quot;cluster&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_strata1&quot; &quot;strata&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_stages&quot; &quot;1&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_version&quot; &quot;2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;__XijVarLabcons&quot; &quot;(sum) cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_Xij&quot; &quot;cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_str&quot; &quot;0&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_j&quot; &quot;group&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_ver&quot; &quot;v.2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_i&quot; &quot;hhid dur&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note1&quot; &quot;variables g1pc, g2pc, g3pc, g4pc, g5pc, g7pc, g8pc, g9pc, g10pc, g11pc, g12pc, gall, health, rent, durables we&quot;| __truncated__ ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note0&quot; &quot;1&quot; ## - attr(*, &quot;version&quot;)= int 7 ## - attr(*, &quot;label.table&quot;)=List of 12 ## ..$ location: Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;urban location&quot; &quot;rural location&quot; ## ..$ region : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Sofia city&quot; &quot;Bourgass&quot; &quot;Varna&quot; &quot;Lovetch&quot; ... ## ..$ ethnic : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Bulgaria&quot; &quot;Turks&quot; &quot;Roma&quot; &quot;Other&quot; ## ..$ s2_q2 : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ s2_q3 : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;head &quot; &quot;spouse/partner &quot; &quot;child &quot; &quot;son/daughter-in-law &quot; ... ## ..$ lit : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; ## ..$ : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;never attanded&quot; &quot;primary&quot; &quot;secondary&quot; &quot;postsecondary&quot; ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not unemployed&quot; &quot;Unemployed&quot; ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;student&quot; &quot;housewife/childcare&quot; &quot;in retirement&quot; &quot;illness, disability&quot; ... ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;agriculture&quot; &quot;mining&quot; &quot;manufacturing&quot; &quot;utilities&quot; ... ## ..$ : Named int [1:5] 1 2 3 4 5 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;private company&quot; &quot;public works program&quot; &quot;government,public sector, army&quot; &quot;private individual&quot; ... ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; Setting convert.factors to FALSE. # Create and print structure of edu_equal_2 edu_equal_2 &lt;- read.dta(path, convert.factors = FALSE) str(edu_equal_2) ## &#39;data.frame&#39;: 12214 obs. of 27 variables: ## $ hhid : num 1 1 1 2 2 3 4 4 5 6 ... ## $ hhweight : num 627 627 627 627 627 ... ## $ location : int 1 1 1 1 1 2 2 2 1 1 ... ## $ region : int 8 8 8 9 9 4 4 4 8 8 ... ## $ ethnicity_head : int 2 2 2 1 1 1 1 1 1 1 ... ## $ age : num 37 11 8 73 70 75 79 80 82 83 ... ## $ gender : int 2 2 1 1 2 1 1 2 2 2 ... ## $ relation : int 1 3 3 1 2 1 1 2 1 1 ... ## $ literate : int 1 2 2 2 2 2 2 2 2 2 ... ## $ income_mnt : num 13.3 13.3 13.3 142.5 142.5 ... ## $ income : num 160 160 160 1710 1710 ... ## $ aggregate : num 1042 1042 1042 3271 3271 ... ## $ aggr_ind_annual : num 347 347 347 1635 1635 ... ## $ educ_completed : int 2 4 4 4 3 3 3 3 4 4 ... ## $ grade_complete : num 4 3 0 3 4 4 4 4 5 5 ... ## $ grade_all : num 4 11 8 11 8 8 8 8 13 13 ... ## $ unemployed : int 2 1 1 1 1 1 1 1 1 1 ... ## $ reason_OLF : int NA NA NA 3 3 3 9 9 3 3 ... ## $ sector : int NA NA NA NA NA NA 1 1 NA NA ... ## $ occupation : int NA NA NA NA NA NA 5 5 NA NA ... ## $ earn_mont : num 0 0 0 0 0 0 20 20 0 0 ... ## $ earn_ann : num 0 0 0 0 0 0 240 240 0 0 ... ## $ hours_week : num NA NA NA NA NA NA 30 35 NA NA ... ## $ hours_mnt : num NA NA NA NA NA ... ## $ fulltime : int NA NA NA NA NA NA 1 1 NA NA ... ## $ hhexp : num 100 100 100 343 343 ... ## $ legacy_pension_amt: num NA NA NA NA NA NA NA NA NA NA ... ## - attr(*, &quot;datalabel&quot;)= chr &quot;&quot; ## - attr(*, &quot;time.stamp&quot;)= chr &quot;&quot; ## - attr(*, &quot;formats&quot;)= chr [1:27] &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; ... ## - attr(*, &quot;types&quot;)= int [1:27] 100 100 108 108 108 100 108 108 108 100 ... ## - attr(*, &quot;val.labels&quot;)= chr [1:27] &quot;&quot; &quot;&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;var.labels&quot;)= chr [1:27] &quot;hhid&quot; &quot;hhweight&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;expansion.fields&quot;)=List of 12 ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_su1&quot; &quot;cluster&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_strata1&quot; &quot;strata&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_stages&quot; &quot;1&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_version&quot; &quot;2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;__XijVarLabcons&quot; &quot;(sum) cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_Xij&quot; &quot;cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_str&quot; &quot;0&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_j&quot; &quot;group&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_ver&quot; &quot;v.2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_i&quot; &quot;hhid dur&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note1&quot; &quot;variables g1pc, g2pc, g3pc, g4pc, g5pc, g7pc, g8pc, g9pc, g10pc, g11pc, g12pc, gall, health, rent, durables we&quot;| __truncated__ ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note0&quot; &quot;1&quot; ## - attr(*, &quot;version&quot;)= int 7 ## - attr(*, &quot;label.table&quot;)=List of 12 ## ..$ location: Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;urban location&quot; &quot;rural location&quot; ## ..$ region : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Sofia city&quot; &quot;Bourgass&quot; &quot;Varna&quot; &quot;Lovetch&quot; ... ## ..$ ethnic : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Bulgaria&quot; &quot;Turks&quot; &quot;Roma&quot; &quot;Other&quot; ## ..$ s2_q2 : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ s2_q3 : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;head &quot; &quot;spouse/partner &quot; &quot;child &quot; &quot;son/daughter-in-law &quot; ... ## ..$ lit : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; ## ..$ : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;never attanded&quot; &quot;primary&quot; &quot;secondary&quot; &quot;postsecondary&quot; ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not unemployed&quot; &quot;Unemployed&quot; ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;student&quot; &quot;housewife/childcare&quot; &quot;in retirement&quot; &quot;illness, disability&quot; ... ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;agriculture&quot; &quot;mining&quot; &quot;manufacturing&quot; &quot;utilities&quot; ... ## ..$ : Named int [1:5] 1 2 3 4 5 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;private company&quot; &quot;public works program&quot; &quot;government,public sector, army&quot; &quot;private individual&quot; ... ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; Setting convert.underscore to TRUE. # Create and print structure of edu_equal_3 edu_equal_3 &lt;- read.dta(path, convert.underscore = TRUE) str(edu_equal_3) ## &#39;data.frame&#39;: 12214 obs. of 27 variables: ## $ hhid : num 1 1 1 2 2 3 4 4 5 6 ... ## $ hhweight : num 627 627 627 627 627 ... ## $ location : Factor w/ 2 levels &quot;urban location&quot;,..: 1 1 1 1 1 2 2 2 1 1 ... ## $ region : Factor w/ 9 levels &quot;Sofia city&quot;,&quot;Bourgass&quot;,..: 8 8 8 9 9 4 4 4 8 8 ... ## $ ethnicity.head : Factor w/ 4 levels &quot;Bulgaria&quot;,&quot;Turks&quot;,..: 2 2 2 1 1 1 1 1 1 1 ... ## $ age : num 37 11 8 73 70 75 79 80 82 83 ... ## $ gender : Factor w/ 2 levels &quot;male&quot;,&quot;female&quot;: 2 2 1 1 2 1 1 2 2 2 ... ## $ relation : Factor w/ 9 levels &quot;head &quot;,..: 1 3 3 1 2 1 1 2 1 1 ... ## $ literate : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 2 2 2 2 2 2 2 2 2 ... ## $ income.mnt : num 13.3 13.3 13.3 142.5 142.5 ... ## $ income : num 160 160 160 1710 1710 ... ## $ aggregate : num 1042 1042 1042 3271 3271 ... ## $ aggr.ind.annual : num 347 347 347 1635 1635 ... ## $ educ.completed : int 2 4 4 4 3 3 3 3 4 4 ... ## $ grade.complete : num 4 3 0 3 4 4 4 4 5 5 ... ## $ grade.all : num 4 11 8 11 8 8 8 8 13 13 ... ## $ unemployed : int 2 1 1 1 1 1 1 1 1 1 ... ## $ reason.OLF : int NA NA NA 3 3 3 9 9 3 3 ... ## $ sector : int NA NA NA NA NA NA 1 1 NA NA ... ## $ occupation : int NA NA NA NA NA NA 5 5 NA NA ... ## $ earn.mont : num 0 0 0 0 0 0 20 20 0 0 ... ## $ earn.ann : num 0 0 0 0 0 0 240 240 0 0 ... ## $ hours.week : num NA NA NA NA NA NA 30 35 NA NA ... ## $ hours.mnt : num NA NA NA NA NA ... ## $ fulltime : int NA NA NA NA NA NA 1 1 NA NA ... ## $ hhexp : num 100 100 100 343 343 ... ## $ legacy.pension.amt: num NA NA NA NA NA NA NA NA NA NA ... ## - attr(*, &quot;datalabel&quot;)= chr &quot;&quot; ## - attr(*, &quot;time.stamp&quot;)= chr &quot;&quot; ## - attr(*, &quot;formats&quot;)= chr [1:27] &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; &quot;%9.0g&quot; ... ## - attr(*, &quot;types&quot;)= int [1:27] 100 100 108 108 108 100 108 108 108 100 ... ## - attr(*, &quot;val.labels&quot;)= chr [1:27] &quot;&quot; &quot;&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;var.labels&quot;)= chr [1:27] &quot;hhid&quot; &quot;hhweight&quot; &quot;location&quot; &quot;region&quot; ... ## - attr(*, &quot;expansion.fields&quot;)=List of 12 ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_su1&quot; &quot;cluster&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_strata1&quot; &quot;strata&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_stages&quot; &quot;1&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;_svy_version&quot; &quot;2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;__XijVarLabcons&quot; &quot;(sum) cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_Xij&quot; &quot;cons&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_str&quot; &quot;0&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_j&quot; &quot;group&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_ver&quot; &quot;v.2&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;ReS_i&quot; &quot;hhid dur&quot; ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note1&quot; &quot;variables g1pc, g2pc, g3pc, g4pc, g5pc, g7pc, g8pc, g9pc, g10pc, g11pc, g12pc, gall, health, rent, durables we&quot;| __truncated__ ## ..$ : chr [1:3] &quot;_dta&quot; &quot;note0&quot; &quot;1&quot; ## - attr(*, &quot;version&quot;)= int 7 ## - attr(*, &quot;label.table&quot;)=List of 12 ## ..$ location: Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;urban location&quot; &quot;rural location&quot; ## ..$ region : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;Sofia city&quot; &quot;Bourgass&quot; &quot;Varna&quot; &quot;Lovetch&quot; ... ## ..$ ethnic : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;Bulgaria&quot; &quot;Turks&quot; &quot;Roma&quot; &quot;Other&quot; ## ..$ s2_q2 : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;male&quot; &quot;female&quot; ## ..$ s2_q3 : Named int [1:9] 1 2 3 4 5 6 7 8 9 ## .. ..- attr(*, &quot;names&quot;)= chr [1:9] &quot;head &quot; &quot;spouse/partner &quot; &quot;child &quot; &quot;son/daughter-in-law &quot; ... ## ..$ lit : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; ## ..$ : Named int [1:4] 1 2 3 4 ## .. ..- attr(*, &quot;names&quot;)= chr [1:4] &quot;never attanded&quot; &quot;primary&quot; &quot;secondary&quot; &quot;postsecondary&quot; ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;Not unemployed&quot; &quot;Unemployed&quot; ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;student&quot; &quot;housewife/childcare&quot; &quot;in retirement&quot; &quot;illness, disability&quot; ... ## ..$ : Named int [1:10] 1 2 3 4 5 6 7 8 9 10 ## .. ..- attr(*, &quot;names&quot;)= chr [1:10] &quot;agriculture&quot; &quot;mining&quot; &quot;manufacturing&quot; &quot;utilities&quot; ... ## ..$ : Named int [1:5] 1 2 3 4 5 ## .. ..- attr(*, &quot;names&quot;)= chr [1:5] &quot;private company&quot; &quot;public works program&quot; &quot;government,public sector, army&quot; &quot;private individual&quot; ... ## ..$ : Named int [1:2] 1 2 ## .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;no&quot; &quot;yes&quot; How many observations/individuals of Bulgarian ethnicity have an income above 1000? nrow(subset(edu_equal_1, ethnicity_head == &quot;Bulgaria&quot; &amp; income &gt; 1000)) ## [1] 8997 22.5.2.2 Import SPSS data read.spss(file, use.value.labels = TRUE, to.data.frame = FALSE) use.value.labels : convert labelled SPSS values to R factors to.data.frame : return data frame instead of a list trim.factor.names trim_values use.missings # Import international.sav as a data frame: demo demo &lt;- read.spss(&quot;data/international.sav&quot;, to.data.frame = TRUE) ## re-encoding from CP1252 # Create boxplot of gdp variable of demo boxplot(demo$gdp) What is the correlation coefficient for the two numerical variables gdp and f_illit (female illiteracy rate)? cor(demo$gdp, demo$f_illit) ## [1] -0.448 Indicates a negative association among GDP and female illiteracy. You will experiment with another argument, use.value.labels. It specifies whether variables with value labels should be converted into R factors with levels that are named accordingly. # Import international.sav as demo_1 demo_1 &lt;- read.spss(&quot;data/international.sav&quot;, to.data.frame = TRUE) ## re-encoding from CP1252 # Print out the head of demo_1 head(demo_1) ## id country contint m_illit f_illit lifeexpt gdp ## 1 1 Argentina Americas 3.0 3.0 16 3375 ## 2 2 Benin Africa 45.2 74.5 7 521 ## 3 3 Burundi Africa 33.2 48.1 5 86 ## 4 4 Chile Americas 4.2 4.4 14 4523 ## 5 5 Dominican Republic Americas 12.0 12.7 12 2408 ## 6 6 El Salvador Americas 17.6 22.9 11 2302 This time, variables with value labels are not converted to R factors. # Import international.sav as demo_2 demo_2 &lt;- read.spss(&quot;data/international.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) ## re-encoding from CP1252 # Print out the head of demo_2 head(demo_2) ## id country contint m_illit f_illit lifeexpt gdp ## 1 1 Argentina 2 3.0 3.0 16 3375 ## 2 2 Benin 1 45.2 74.5 7 521 ## 3 3 Burundi 1 33.2 48.1 5 86 ## 4 4 Chile 2 4.2 4.4 14 4523 ## 5 5 Dominican Republic 2 12.0 12.7 12 2408 ## 6 6 El Salvador 2 17.6 22.9 11 2302 "],["introduction-to-sql.html", "Chapter 23 Introduction to SQL 23.1 Relational Databases 23.2 Querying", " Chapter 23 Introduction to SQL 23.1 Relational Databases 23.1.1 Databases Relational databases Define relationships between tables of data inside the database Database advantages More storage than spreadsheet applications Storage is more secure SQL Short for Structured Query Language The most widely used programming language for databases 23.1.2 Tables Tables Table rows and columns are referred to as records and fields Fields are set at database creation; there is no limit to the number of records Table names: lowercase no spaces — use underscores instead refer to a collective group or be plural Table: fields A field is a column that holds one piece of information about all records Field names: lowercase no spaces — use underscores instead singular different from other field names different from the table name key Unique identifiers are used to identify records in a table They are unique and often numbers 23.1.3 Data SQL data types Strings: VARCHAR Integers: INT Floats: NUMERIC Schemas Shows a database’s design, such as what tables are included in the database and any relationships between its tables. Shows data types 23.2 Querying 23.2.1 Introducing 23.2.1.1 Querying table Practice selecting fields from books table. -- Return all titles from the books table SELECT title FROM books; -- Select title and author from the books table SELECT title, author FROM books; -- Select all fields from the books table SELECT * FROM books; 23.2.2 Writing queries 23.2.2.1 DISTINCT DISTINCT keyword can be used to return unique values in a field. There are 350 books in the books table, representing all of the books that our local library has available for checkout. But how many different authors are represented in these 350 books? The answer is surely less than 350. For example, J.K. Rowling wrote all seven Harry Potter books, so if our library has all Harry Potter books, seven books will be written by J.K Rowling. -- Select unique authors from the books table SELECT DISTINCT author FROM books -- Select unique authors and genre combinations from the books table SELECT DISTINCT author, genre FROM books; You found 247 unique authors in the books table overall but 249 unique combinations of authors and genres. This means there are one or two authors who have written books in multiple genres! 23.2.2.2 AS Use aliasing to rename columns. -- Alias author so that it becomes unique_author SELECT DISTINCT author AS unique_author FROM books; 23.2.2.3 VIEW A view is a virtual table that is the result of a saved SQL SELECT statement The query code is stored for later use. -- Save the results of this query as a view called library_authors CREATE VIEW library_authors AS SELECT DISTINCT author AS unique_author FROM books; -- Select all columns from library_authors SELECT * FROM library_authors As your SQL queries become long and complex, you’ll want to be able to save your queries for referencing later. Views can also be useful when the information contained in a database table isn’t quite what you need. You can create your own custom view with exactly the information you are looking for, without needing to edit the database itself 23.2.3 SQL flavors Two popular SQL flavors PostgreSQL Free and open-source relational database system “PostgreSQL” refers to both the PostgreSQL database system and its associated SQL flavor SQL Server Has free and paid versions Created by Microsoft T-SQL is Microsoft’s SQL flavor, used with SQL Server databases Comparing PostgreSQL and SQL Server PostgreSQL: LIMIT 2 SQL Server: TOP(2) 23.2.3.1 LIMIT Queries are often written with a LIMIT of just a few records to test out code before selecting thousands of results from the database. -- Select the first 10 genres from books using PostgreSQL SELECT genre FROM books LIMIT 10; "],["intermediate-sql.html", "Chapter 24 Intermediate SQL 24.1 Selecting Data 24.2 Filtering Records 24.3 Aggregate Functions 24.4 Sorting and Grouping", " Chapter 24 Intermediate SQL 24.1 Selecting Data 24.1.1 Querying a database Setup films database # Creating a new database # Connect to the default postgres database library(DBI) con &lt;- dbConnect(RSQLite::SQLite(), &quot;&quot;) knitr::opts_chunk$set(connection = &quot;con&quot;) # Loading data films &lt;- read.csv(&quot;data/films.csv&quot;, header = FALSE, na.strings = &quot;&quot;) colnames(films) &lt;- c(&quot;id&quot;, &quot;title&quot;, &quot;release_year&quot;, &quot;country&quot;, &quot;duration&quot;, &quot;language&quot;, &quot;certification&quot;, &quot;gross&quot;, &quot;budget&quot;) people &lt;- read.csv(&quot;data/people.csv&quot;, header = FALSE, col.names = c(&quot;id&quot;, &quot;name&quot;, &quot;birthdate&quot;, &quot;deathdate&quot;), na.strings = &quot;&quot;) reviews &lt;- read.csv(&quot;data/reviews.csv&quot;, header = FALSE, col.names = c(&quot;id&quot;, &quot;film_id&quot;,&quot;num_user&quot;, &quot;num_critic&quot;, &quot;imdb_score&quot;, &quot;num_votes&quot;, &quot;facebook_likes&quot;), na.strings = &quot;&quot;) roles &lt;- read.csv(&quot;data/roles.csv&quot;, header = FALSE, col.names = c(&quot;id&quot;, &quot;film_id&quot;, &quot;person_id&quot;, &quot;role&quot;), na.strings = &quot;&quot;) # Create database tables dbWriteTable(con, &quot;films&quot;, films) dbWriteTable(con, &quot;people&quot;, people) dbWriteTable(con, &quot;reviews&quot;, reviews) dbWriteTable(con, &quot;roles&quot;, roles) # List database tables dbListTables(con) ## [1] &quot;films&quot; &quot;people&quot; &quot;reviews&quot; &quot;roles&quot; 24.1.1.1 COUNT &amp; DISTINCT COUNT Counts the number of records with a value in a field COUNT(field_name) counts values in a field, multiple fields: separate by , COUNT(*) counts records in a table: total observation Use an AS for clarity Includes duplicates DISTINCT DISTINCT removes duplicates to return only unique values Combine COUNT() with DISTINCT to count unique values -- Count the number of records in the people table SELECT COUNT(*) AS count_records FROM people; Table 24.1: 1 records count_records 8397 -- Count the number of birthdates in the people table SELECT COUNT(birthdate) AS count_birthdate FROM people; Table 24.2: 1 records count_birthdate 6152 -- Count the records for languages and countries represented in the films table SELECT COUNT(language) AS count_languages, COUNT(country) AS count_countries FROM films; Table 24.3: 1 records count_languages count_countries 4957 4966 SELECT DISTINCT -- Return the unique countries from the films table SELECT DISTINCT country FROM films; Table 24.4: Displaying records 1 - 10 country USA Germany Japan Denmark UK Italy France West Germany Sweden Soviet Union -- Count the distinct countries from the films table SELECT COUNT(DISTINCT country) AS count_distinct_countries FROM films Table 24.5: 1 records count_distinct_countries 64 24.1.2 Query execution Order of execution SQL is not processed in its written order e.g., SQL needs to know where to SELECT data FROM before it can LIMIT the results. Good to know processing order for debugging and aliasing Most common errors Misspelling Incorrect capitalization Incorrect or missing punctuation, especially commas 24.1.3 SQL style Holywell’s style guide Other notices: Semicolon ; Indicates the end of a query Easier to translate between SQL flavors Dealing with non-standard field names Put non-standard field names in double-quotes: \"release year\" SELECT title, \"release year\", country 24.2 Filtering Records 24.2.1 Filtering numbers WHERE Comparison operators &gt; Greater than or after &lt; Less than or before = Equal to &gt;= Greater than or equal to &lt;= Less than or equal to &lt;&gt; Not equal to  WHERE with strings Use single-quotes around strings we want to filter: WHERE country = 'Japan' 24.2.1.1 WHERE Using WHERE with numbers In this case, you’ll want to filter your data to a specific budget range. -- Select film_ids and imdb_score with an imdb_score over 7.0 SELECT film_id, imdb_score FROM reviews WHERE imdb_score &gt; 7; Table 24.6: Displaying records 1 - 10 film_id imdb_score 3934 7.1 74 7.6 1254 8.0 4841 8.1 3252 7.2 1181 7.3 3929 7.1 3298 7.4 2744 7.4 4707 7.4 -- Select imdb_score with an imdb_score over 7.0 SELECT COUNT(imdb_score) FROM reviews WHERE imdb_score &gt; 7; Table 24.7: 1 records COUNT(imdb_score) 1536 -- Select film_ids and facebook_likes for ten records with less than 1000 likes SELECT film_id, facebook_likes FROM reviews WHERE facebook_likes &lt; 1000 LIMIT 10; Table 24.8: Displaying records 1 - 10 film_id facebook_likes 3405 0 478 491 74 930 740 0 2869 689 1181 0 2020 0 2312 912 1820 872 831 975 -- Count the records with at least 100,000 votes SELECT COUNT(num_votes) AS films_over_100K_votes FROM reviews WHERE num_votes &gt;= 100000; Table 24.9: 1 records films_over_100K_votes 1211 Using WHERE with text WHERE can also filter string values. -- Count the Spanish-language films SELECT COUNT(language) AS count_spanish FROM films WHERE language = &#39;Spanish&#39;; Table 24.10: 1 records count_spanish 40 There are 40 Spanish-language films in this table. 24.2.2 Multiple criteria Multiple criteria OR , AND , BETWEEN -- OR SELECT * FROM coats WHERE color = &#39;yellow&#39; OR length = &#39;short&#39;; -- AND SELECT * FROM coats WHERE color = &#39;yellow&#39; AND length = &#39;short&#39;; -- BETWEEN, AND: BETWEEN 上下界都包含(1,5也算)  SELECT * FROM coats WHERE buttons BETWEEN 1 AND 5; -- AND, OR: Enclose individual clauses in parentheses SELECT title FROM films WHERE (release_year = 1994 OR release_year = 1995) AND (certification = &#39;PG&#39; OR certification = &#39;R&#39;); -- BETWEEN, AND, OR SELECT title FROM films WHERE release_year BETWEEN 1994 AND 2000 AND country=&#39;UK&#39;; 24.2.2.1 AND The following exercises combine AND and OR with the WHERE clause. Combining conditions with AND will prove to be very useful when we want our query to return a specific subset of records. -- Select the title and release_year for all German-language films released before 2000 SELECT title, release_year FROM films WHERE (language = &#39;German&#39;) AND (release_year &lt; 2000); Table 24.11: 6 records title release_year Metropolis 1927 Pandora’s Box 1929 The Torture Chamber of Dr. Sadism 1967 Das Boot 1981 Run Lola Run 1998 Aimee &amp; Jaguar 1999 -- Update the query to see all German-language films released after 2000 SELECT title, release_year FROM films WHERE (release_year &gt; 2000) AND (language = &#39;German&#39;); Table 24.12: Displaying records 1 - 10 title release_year Good Bye Lenin! 2003 Downfall 2004 Summer Storm 2004 The Lives of Others 2006 The Baader Meinhof Complex 2008 The Wave 2008 Cargo 2009 Soul Kitchen 2009 The White Ribbon 2009 3 2010 Select all details for German-language films released after 2000 but before 2010 using only WHERE and AND. -- Select all records for German-language films released after 2000 and before 2010 SELECT * FROM films WHERE (language = &#39;German&#39;) AND (release_year &gt; 2000 AND release_year &lt; 2010); Table 24.13: 9 records id title release_year country duration language certification gross budget 1952 Good Bye Lenin! 2003 Germany 121 German R 4063859 4800000 2130 Downfall 2004 Germany 178 German R 5501940 13500000 2224 Summer Storm 2004 Germany 98 German R 95016 2700000 2709 The Lives of Others 2006 Germany 137 German R 11284657 2000000 3100 The Baader Meinhof Complex 2008 Germany 184 German R 476270 20000000 3143 The Wave 2008 Germany 107 German NA NA 5000000 3220 Cargo 2009 Switzerland 112 German NA NA 4500000 3346 Soul Kitchen 2009 Germany 99 German NA 274385 4000000 3412 The White Ribbon 2009 Germany 144 German R 2222647 12000000 24.2.2.2 OR This time you’ll write a query to get the title and release_year of films released in 1990 or 1999, which were in English or Spanish and took in more than $2,000,000 gross. -- Find the title and year of films from the 1990 or 1999 SELECT title, release_year FROM films WHERE (release_year = 1990 OR release_year = 1999) -- Add a filter to see only English or Spanish-language films AND (language = &#39;English&#39; OR language = &#39;Spanish&#39;) -- Filter films with more than $2,000,000 gross AND (gross &gt; 2000000); Table 24.14: Displaying records 1 - 10 title release_year Arachnophobia 1990 Back to the Future Part III 1990 Child’s Play 2 1990 Dances with Wolves 1990 Days of Thunder 1990 Dick Tracy 1990 Die Hard 2 1990 Edward Scissorhands 1990 Flatliners 1990 Ghost 1990 24.2.2.3 BETWEEN Let’s use BETWEEN with AND on the films database to get the title and release_year of all Spanish-language films released between 1990 and 2000 (inclusive) with budgets over $100 million. -- Select the title and release_year for films released between 1990 and 2000 SELECT title, release_year FROM films WHERE (release_year BETWEEN 1990 AND 2000) -- Narrow down your query to films with budgets &gt; $100 million AND (budget &gt; 100000000) -- Restrict the query to only Spanish-language films AND (language = &#39;Spanish&#39;); Table 24.15: 1 records title release_year Tango 1998 SELECT title, release_year FROM films WHERE release_year BETWEEN 1990 AND 2000 AND budget &gt; 100000000 -- Amend the query to include Spanish or French-language films AND (language = &#39;Spanish&#39; OR language = &#39;French&#39;); Table 24.16: 2 records title release_year Les couloirs du temps: Les visiteurs II 1998 Tango 1998 24.2.3 Filtering text Filter a pattern rather than specific text. LIKE eg., WHERE name LIKE '%r', WHERE name LIKE '___t%' NOT LIKE eg., WHERE name NOT LIKE 'A.%' IN eg., WHERE release_year IN (1920, 1930, 1940), WHERE country IN ('Germany', 'France') Pattern % match zero, one, or many characters _ match a single character 24.2.3.1 LIKE &amp; NOT LIKE The LIKE and NOT LIKE operators can be used to find records that either match or do not match a specified pattern, respectively. They can be coupled with the wildcards % and _. This is useful when you want to filter text, but not to an exact word. -- Select the names that start with B SELECT name FROM people WHERE name LIKE &#39;B%&#39;; Table 24.17: Displaying records 1 - 10 name B.J. Novak Babak Najafi Babar Ahmed Bahare Seddiqi Bai Ling Bailee Madison Balinese Tari Legong Dancers BÃ¡lint PÃ©ntek Baltasar KormÃ¡kur Balthazar Getty SELECT name FROM people -- Select the names that have r as the second letter WHERE name LIKE &#39;_r%&#39; Table 24.18: Displaying records 1 - 10 name Ara Celi Aramis Knight Arben Bajraktaraj Arcelia RamÃ­rez Archie Kao Archie Panjabi Aretha Franklin Ari Folman Ari Gold Ari Graynor SELECT name FROM people -- Select names that don&#39;t start with A WHERE name NOT LIKE &#39;A%&#39; Table 24.19: Displaying records 1 - 10 name 50 Cent Ãlex Angulo Ãlex de la Iglesia Ãngela Molina B.J. Novak Babak Najafi Babar Ahmed Bahare Seddiqi Bai Ling Bailee Madison SELECT COUNT(name) AS count_name_beginA FROM people -- Select names that don&#39;t start with A WHERE name NOT LIKE &#39;A%&#39; Table 24.20: 1 records count_name_beginA 7768 24.2.3.2 WHERE IN You can query multiple conditions using the IN operator and a set of parentheses. It is a valuable piece of code that helps us keep our queries clean and concise. -- Find the title and release_year for all films over two hours in length released in 1990 and 2000 SELECT title, release_year FROM films WHERE (duration &gt; 120) AND (release_year IN (1990, 2000)); Table 24.21: Displaying records 1 - 10 title release_year Dances with Wolves 1990 Die Hard 2 1990 Ghost 1990 Goodfellas 1990 Mo’ Better Blues 1990 Pretty Woman 1990 The Godfather: Part III 1990 The Hunt for Red October 1990 All the Pretty Horses 2000 Almost Famous 2000 -- Find the title and language of all films in English, Spanish, and French SELECT title, language FROM films WHERE language IN (&#39;English&#39;, &#39;Spanish&#39;, &#39;French&#39;); Table 24.22: Displaying records 1 - 10 title language The Broadway Melody English Hell’s Angels English A Farewell to Arms English 42nd Street English She Done Him Wrong English It Happened One Night English Top Hat English Modern Times English The Charge of the Light Brigade English Snow White and the Seven Dwarfs English -- Find the title, certification, and language all films certified NC-17 or R that are in English, Italian, or Greek SELECT title, certification, language FROM films WHERE (certification IN (&#39;NC-17&#39;, &#39;R&#39;)) AND (language IN (&#39;English&#39;, &#39;Italian&#39;, &#39;Greek&#39;)); Table 24.23: Displaying records 1 - 10 title certification language Psycho R English A Fistful of Dollars R Italian Rosemary’s Baby R English The Wild Bunch R English Catch-22 R English Cotton Comes to Harlem R English The Ballad of Cable Hogue R English The Conformist R Italian Woodstock R English Sweet Sweetback’s Baadasssss Song R English 24.2.3.3 Combine filtering &amp; selecting How many 90’s films we have in our dataset that would be suitable for English-speaking teens? (You will be using DISTINCT here too because, surprise, there are two movies named ‘Hamlet’ in this dataset.) -- Count the unique titles SELECT COUNT(DISTINCT title) AS nineties_english_films_for_teens FROM films -- Filter to release_years to between 1990 and 1999 WHERE (release_year BETWEEN 1990 AND 1999) -- Filter to English-language films AND (language = &#39;English&#39;) -- Narrow it down to G, PG, and PG-13 certifications AND (certification IN (&#39;G&#39;, &#39;PG&#39;, &#39;PG-13&#39;)); Table 24.24: 1 records nineties_english_films_for_teens 310 24.2.4 NULL values null : Missing values COUNT(field_name) includes only non-missing values COUNT(*) includes missing values Use IS NULL or IS NOT NULL to: Identify missing values Select missing values Exclude missing values -- List all film titles with missing budgets SELECT title AS no_budget_info FROM films WHERE budget IS NULL Table 24.25: Displaying records 1 - 10 no_budget_info Pandora’s Box The Prisoner of Zenda The Blue Bird Bambi State Fair Open Secret Deadline - U.S.A. Ordet The Party’s Over The Torture Chamber of Dr. Sadism -- Count the number of films we have language data for SELECT COUNT(title) AS count_language_known FROM films WHERE language IS NOT NULL; Table 24.26: 1 records count_language_known 4957 24.3 Aggregate Functions 24.3.1 Summarizing data Aggregate functions AVG() , SUM() , MIN() , MAX() , COUNT() Numerical fields only AVG(), SUM() Various data types MIN() , MAX() , COUNT() MIN() &lt;-&gt; MAX() in non-numerical data A &lt;-&gt; Z 1715 &lt;-&gt; 2022 Aliasing when summarizing Perhaps you’d like to know how old the oldest film in the films table is, what the most expensive film is, or how many films you have listed. -- Query the sum of film durations SELECT SUM(duration) AS total_duration FROM films; Table 24.27: 1 records total_duration 534882 -- Calculate the average duration of all films SELECT AVG(duration) AS average_duration FROM films; Table 24.28: 1 records average_duration 108 -- Find the latest release_year SELECT MAX(release_year) AS latest_year FROM films; Table 24.29: 1 records latest_year 2016 -- Find the duration of the shortest film SELECT MIN(duration) AS shortest_film FROM films; Table 24.30: 1 records shortest_film 7 24.3.2 Summarizing subsets Using WHERE with aggregate functions ROUND() : Round a number to a specified decimal ROUND(number_to_round, decimal_places) whole number: ROUND(number_to_round, 0) 小數點第二位: ROUND(number_to_round, 2) to thousands: ROUND(number_to_round, -3) 123456 ⟶ 12000, negative parameter 24.3.2.1 Aggregate functions &amp; WHERE In your film-industry role, as an example, you may like to summarize each certification category to compare how they each perform or if one certification has a higher average budget than another. -- Calculate the sum of gross from the year 2000 or later SELECT SUM(gross) AS total_gross FROM films WHERE release_year &gt;= 2000; Table 24.31: 1 records total_gross 150900926358 -- Calculate the average gross of films that start with A SELECT AVG(gross) AS avg_gross_A FROM films WHERE title LIKE &#39;A%&#39;; Table 24.32: 1 records avg_gross_A 47893236 -- Calculate the lowest gross film in 1994 SELECT MIN(gross) AS lowest_gross FROM films WHERE release_year = 1994; Table 24.33: 1 records lowest_gross 125169 -- Calculate the highest gross film released between 2000-2012 SELECT MAX(gross) AS highest_gross FROM films WHERE release_year BETWEEN 2000 AND 2012; Table 24.34: 1 records highest_gross 760505847 24.3.2.2 ROUND() Aggregate functions work great with numerical values; however, these results can sometimes get unwieldy when dealing with long decimal values. SQL provides you with the ROUND() function to tame these long decimals. -- Round the average number of facebook_likes to one decimal place SELECT ROUND(AVG(facebook_likes), 1) AS avg_facebook_likes FROM reviews; Table 24.35: 1 records avg_facebook_likes 7803 A useful thing you can do with ROUND() is have a negative number as the decimal place parameter. This can come in handy if your manager only needs to know the average number of facebook_likes to the hundreds since granularity below one hundred likes won’t impact decision making. -- Calculate the average budget rounded to the thousands, 39902000 SELECT ROUND(AVG(budget), -3) AS avg_budget_thousands FROM films; Table 24.36: 1 records avg_budget_thousands 39902826 The ROUND() function is very handy when making financial calculations to get a top-level view or specify to the penny or cent. 24.3.3 Aliasing and arithmetic Arithmetic + , - , * , and / When dividing, SQL assumes that we want to get an integer back if we divide an integer by an integer.We can add decimal places to our numbers if we want more precision SELECT (4 / 3); ⟶ 1 SELECT (4.0 / 3.0); ⟶ 1.333… Aliasing with arithmetic &amp; function Aggregate functions vs. arithmetic aggregate functions: perform their operations on the fields vertically arithmetic: adds up the records horizontally -- default SELECT (2 / 10); Table 24.37: 1 records (2 / 10) 0 -- add decimal SELECT (2.0 / 10.0); Table 24.38: 1 records (2.0 / 10.0) 0.2 24.3.3.1 Aliasing with functions Aliasing can be a lifesaver, especially as we start to do more complex SQL queries with multiple criteria. Aliases help you keep your code clean and readable. For example, if you want to find the MAX() value of several fields without aliasing, you’ll end up with the result with several columns called max and no idea which is which. You can fix this with aliasing. -- Calculate the title and duration_hours from films SELECT title, (duration / 60.0) AS duration_hours FROM films; Table 24.39: Displaying records 1 - 10 title duration_hours Intolerance: Love’s Struggle Throughout the Ages 2.05 Over the Hill to the Poorhouse 1.83 The Big Parade 2.52 Metropolis 2.42 Pandora’s Box 1.83 The Broadway Melody 1.67 Hell’s Angels 1.60 A Farewell to Arms 1.32 42nd Street 1.48 She Done Him Wrong 1.10 -- Calculate the percentage of people who are no longer alive SELECT COUNT(deathdate) * 100.0 / COUNT(*) AS percentage_dead FROM people; Table 24.40: 1 records percentage_dead 9.37 -- Find the number of decades in the films table SELECT (MAX(release_year) - MIN(release_year)) / 10.0 AS number_of_decades FROM films; Table 24.41: 1 records number_of_decades 10 films table covers films released over one hundred years! 24.3.3.2 Rounding results In the previous exercise, many of the results were inconveniently long. Update the query by adding ROUND()` around the calculation. -- Round duration_hours to two decimal places SELECT title, ROUND((duration / 60.0), 2) AS duration_hours FROM films; Table 24.42: Displaying records 1 - 10 title duration_hours Intolerance: Love’s Struggle Throughout the Ages 2.05 Over the Hill to the Poorhouse 1.83 The Big Parade 2.52 Metropolis 2.42 Pandora’s Box 1.83 The Broadway Melody 1.67 Hell’s Angels 1.60 A Farewell to Arms 1.32 42nd Street 1.48 She Done Him Wrong 1.10 24.4 Sorting and Grouping 24.4.1 Sorting results ORDER BY : ASC, DESC Text ASC : Alphabetically (A-Z) ORDER BY multiple fields: ORDER BY field_one, field_two Different orders: ORDER BY birthdate, name DESC; 24.4.1.1 ORDER BY Sorting single fields -- Select name from people and sort alphabetically SELECT name FROM people ORDER BY name ASC; Table 24.43: Displaying records 1 - 10 name 50 Cent A. Michael Baldwin A. Raven Cruz A.J. Buckley A.J. DeLucia A.J. Langer AJ Michalka Aaliyah Aaron Ashmore Aaron Hann -- Select the title and duration from longest to shortest film SELECT title, duration FROM films ORDER BY duration DESC; Table 24.44: Displaying records 1 - 10 title duration Carlos 334 Blood In, Blood Out 330 Heaven’s Gate 325 The Legend of Suriyothai 300 Das Boot 293 Apocalypse Now 289 The Company 286 Gods and Generals 280 Gettysburg 271 Arn: The Knight Templar 270 Sorting multiple fields It will sort by the first field specified, then sort by the next, and so on. -- Select the release year, duration, and title sorted by release year and duration SELECT release_year, duration, title FROM films WHERE release_year IS NOT NULL ORDER BY release_year, duration; Table 24.45: Displaying records 1 - 10 release_year duration title 1916 123 Intolerance: Love’s Struggle Throughout the Ages 1920 110 Over the Hill to the Poorhouse 1925 151 The Big Parade 1927 145 Metropolis 1929 100 The Broadway Melody 1929 110 Pandora’s Box 1930 96 Hell’s Angels 1932 79 A Farewell to Arms 1933 66 She Done Him Wrong 1933 89 42nd Street -- Select the certification, release year, and title sorted by certification and release year -- Ordered first by certification (alphabetically) and second by release year, starting with the most recent year SELECT certification, release_year, title FROM films WHERE certification IS NOT NULL ORDER BY certification ASC, release_year DESC; Table 24.46: Displaying records 1 - 10 certification release_year title Approved 1967 In Cold Blood Approved 1967 Point Blank Approved 1967 You Only Live Twice Approved 1966 A Funny Thing Happened on the Way to the Forum Approved 1966 A Man for All Seasons Approved 1966 Batman: The Movie Approved 1966 The Good, the Bad and the Ugly Approved 1966 Torn Curtain Approved 1965 Major Dundee Approved 1965 Thunderball 24.4.2 Grouping data 24.4.2.1 GROUP BY GROUP BY multiple fields: GROUP BY certification, language GROUP BY single fields -- Find the release_year and film_count of each year SELECT release_year, COUNT(*) AS film_count FROM films WHERE release_year IS NOT NULL GROUP BY release_year; Table 24.47: Displaying records 1 - 10 release_year film_count 1916 1 1920 1 1925 1 1927 1 1929 2 1930 1 1932 1 1933 2 1934 1 1935 1 -- Find the release_year and average duration of films for each year SELECT release_year, AVG(duration) AS avg_duration FROM films WHERE release_year IS NOT NULL GROUP BY release_year; Table 24.48: Displaying records 1 - 10 release_year avg_duration 1916 123.0 1920 110.0 1925 151.0 1927 145.0 1929 105.0 1930 96.0 1932 79.0 1933 77.5 1934 65.0 1935 81.0 Using GROUP BY with a time or date field such as release_year can help us identify trends such as a period of time where movies were really short! GROUP BY multiple fields Look at the maximum budget for each country in each year -- Find the release_year, country, and max_budget, then group and order by release_year and country SELECT release_year, country, MAX(budget) AS max_budget FROM films WHERE release_year IS NOT NULL GROUP BY release_year, country ORDER BY release_year, country; Table 24.49: Displaying records 1 - 10 release_year country max_budget 1916 USA 385907 1920 USA 100000 1925 USA 245000 1927 Germany 6000000 1929 Germany NA 1929 USA 379000 1930 USA 3950000 1932 USA 800000 1933 USA 439000 1934 USA 325000 Answering business questions Which release_year had the most language diversity? SELECT release_year, COUNT(DISTINCT language) AS num_language FROM films GROUP BY release_year ORDER BY num_language DESC; Table 24.50: Displaying records 1 - 10 release_year num_language 2006 16 2015 15 2005 14 2013 13 2008 13 2009 12 2004 12 2007 11 2011 10 2010 10 24.4.3 Filtering grouped data HAVING vs WHERE HAVING WHERE HAVING filters grouped records WHERE filters individual records In what years was the average film duration over two hours? What films were released in the year 2000? SELECT release_year FROM films GROUP BY release_year HAVING AVG(duration) &gt; 120; SELECT title FROM films WHERE release_year = 2000; 24.4.3.1 HAVING Filtering grouped data can be especially handy when working with a large dataset. When working with thousands or even millions of rows, HAVING will allow you to filter for just the group of data you want Find out which countries (or country) have the most varied film certifications. -- Select the country and distinct count of certification as certification_count SELECT country, COUNT(DISTINCT certification) AS certification_count FROM films -- Group by country GROUP BY country -- Filter results to countries with more than 10 different certifications HAVING COUNT(DISTINCT certification) &gt; 10; Table 24.51: 1 records country certification_count USA 12 24.4.3.2 HAVING and sorting Writing a query showing what countries have the highest average film budgets. -- Select the country and average_budget from films SELECT country, AVG(budget) AS average_budget FROM films -- Group by country GROUP BY country -- Filter to countries with an average_budget of more than one billion HAVING AVG(budget) &gt; 1000000000 -- Order by descending order of the aggregated budget ORDER BY average_budget DESC; Table 24.52: 2 records country average_budget South Korea 1383960000 Hungary 1260000000 South Korea and Hungary seem to have pricey films… or do they? Actually, these budgets are pretty standard for their local currency. 24.4.3.3 All together You’ll write a query that returns the average budget and gross earnings for films each year after 1990 if the average budget is greater than 60 million. -- Select the release_year, average budget and average gross -- for films released after 1990 grouped by year SELECT release_year, AVG(budget) AS avg_budget, AVG(gross) AS avg_gross FROM films WHERE release_year &gt; 1990 GROUP BY release_year -- Only years with an avg_budget of more than 60 million HAVING AVG(budget) &gt; 60000000 -- Order the results from highest to lowest average gross and limit to one ORDER BY avg_gross DESC LIMIT 1; Table 24.53: 1 records release_year avg_budget avg_gross 2005 70323938 41159143 # disconnect the sql connection dbDisconnect(con) "],["joining-data-in-sql.html", "Chapter 25 Joining Data in SQL 25.1 Inner Joins 25.2 Outer, Cross &amp; Self Joins 25.3 Set Theory for SQL Joins 25.4 Subqueries", " Chapter 25 Joining Data in SQL 25.1 Inner Joins 25.1.1 ins &amp; outs of INNER JOIN The table.column_name format must be used when selecting columns that exist in both tables. -- Inner join of presidents and prime_ministers, joining on country SELECT prime_ministers.country, prime_ministers.continent, prime_minister, president FROM prime_ministers INNER JOIN presidents ON prime_ministers.country = presidents.country;  Aliasing tables, then can use in SELECT and ON clauses. --Inner join of presidents and prime_ministers, joining on country SELECT p1.country, p1.continent, prime_minister, president FROM prime_ministers AS p1  INNER JOIN presidents AS p2  ON p1.country = p2.country; USING: when joining on two identical column names, we can employ USING(shared_column_name) --Inner join of presidents and prime_ministers, joining on country SELECT p1.country, p1.continent, prime_minister, president FROM prime_ministers AS p1 INNER JOIN presidents AS p2 USING(country);  Setup films database # Creating a new database # Connect to the default postgres database library(DBI) con &lt;- dbConnect(RSQLite::SQLite(), &quot;&quot;) knitr::opts_chunk$set(connection = &quot;con&quot;) # List database tables dbListTables(con) ## character(0) -- Setup database schema, cities CREATE TABLE cities ( name VARCHAR PRIMARY KEY, country_code VARCHAR, city_proper_pop REAL, metroarea_pop REAL, urbanarea_pop REAL ); -- Setup database schema, countries CREATE TABLE countries ( code VARCHAR PRIMARY KEY, name VARCHAR, continent VARCHAR, region VARCHAR, surface_area REAL, indep_year INTEGER, local_name VARCHAR, gov_form VARCHAR, capital VARCHAR, cap_long REAL, cap_lat REAL ); -- Setup database schema, languages CREATE TABLE languages ( lang_id INTEGER PRIMARY KEY, code VARCHAR, name VARCHAR, percent REAL, official BOOLEAN ); -- Setup database schema, economies CREATE TABLE economies ( econ_id INTEGER PRIMARY KEY, code VARCHAR, year INTEGER, income_group VARCHAR, gdp_percapita REAL, gross_savings REAL, inflation_rate REAL, total_investment REAL, unemployment_rate REAL, exports REAL, imports REAL ); -- Setup database schema, currencies CREATE TABLE currencies ( curr_id INTEGER PRIMARY KEY, code VARCHAR, basic_unit VARCHAR, curr_code VARCHAR, frac_unit VARCHAR, frac_perbasic REAL ); -- Setup database schema, populations CREATE TABLE populations ( pop_id INTEGER PRIMARY KEY, country_code VARCHAR, year INTEGER, fertility_rate REAL, life_expectancy REAL, size REAL ); -- Setup database schema, economies2015 CREATE TABLE economies2015 ( code VARCHAR PRIMARY KEY, year INTEGER, income_group VARCHAR, gross_savings REAL ); -- Setup database schema, economies2019 CREATE TABLE economies2019 ( code VARCHAR PRIMARY KEY, year INTEGER, income_group VARCHAR, gross_savings REAL ); -- Setup database schema, eu_countries CREATE TABLE eu_countries ( code VARCHAR PRIMARY KEY, name VARCHAR ); # List database tables dbListTables(con) ## [1] &quot;cities&quot; &quot;countries&quot; &quot;currencies&quot; &quot;economies&quot; ## [5] &quot;economies2015&quot; &quot;economies2019&quot; &quot;eu_countries&quot; &quot;languages&quot; ## [9] &quot;populations&quot; # Loading data cities &lt;- read.csv(&quot;data/countries/countries/cities.csv&quot;, na.strings = &quot;&quot;) countries &lt;- read.csv(&quot;data/countries/countries/countries.csv&quot;, na.strings = &quot;&quot;) currencies &lt;- read.csv(&quot;data/countries/countries/currencies.csv&quot;, na.strings = &quot;&quot;) economies &lt;- read.csv(&quot;data/countries/countries/economies.csv&quot;, na.strings = &quot;&quot;) economies2015 &lt;- read.csv(&quot;data/countries/countries/economies2015.csv&quot;, na.strings = &quot;&quot;) economies2019 &lt;- read.csv(&quot;data/countries/countries/economies2019.csv&quot;, na.strings = &quot;&quot;) eu_countries &lt;- read.csv(&quot;data/countries/countries/eu_countries.csv&quot;, na.strings = &quot;&quot;) languages &lt;- read.csv(&quot;data/countries/countries/languages.csv&quot;, na.strings = &quot;&quot;) populations &lt;- read.csv(&quot;data/countries/countries/populations.csv&quot;, na.strings = &quot;&quot;) # Create database tables dbWriteTable(con, &quot;cities&quot;, cities, overwrite = TRUE) dbWriteTable(con, &quot;countries&quot;, countries, overwrite = TRUE) dbWriteTable(con, &quot;currencies&quot;, currencies, overwrite = TRUE) dbWriteTable(con, &quot;economies&quot;, economies, overwrite = TRUE) dbWriteTable(con, &quot;economies2015&quot;, economies2015, overwrite = TRUE) dbWriteTable(con, &quot;economies2019&quot;, economies2019, overwrite = TRUE) dbWriteTable(con, &quot;eu_countries&quot;, eu_countries, overwrite = TRUE) dbWriteTable(con, &quot;languages&quot;, languages, overwrite = TRUE) dbWriteTable(con, &quot;populations&quot;, populations, overwrite = TRUE) # List database tables dbListTables(con) ## [1] &quot;cities&quot; &quot;countries&quot; &quot;currencies&quot; &quot;economies&quot; ## [5] &quot;economies2015&quot; &quot;economies2019&quot; &quot;eu_countries&quot; &quot;languages&quot; ## [9] &quot;populations&quot; 25.1.1.1 INNER JOIN You’ll be working with the countries database, which contains information about the most populous world cities in the world, along with country-level economic, population, and geographic data. The database also contains information on languages spoken in each country. Use the cities and countries tables to build your first inner join. -- Select all columns from cities SELECT * FROM cities; Table 25.1: Displaying records 1 - 10 name country_code city_proper_pop metroarea_pop urbanarea_pop Abidjan CIV 4765000 NA 4765000 Abu Dhabi ARE 1145000 NA 1145000 Abuja NGA 1235880 6000000 1235880 Accra GHA 2070463 4010054 2070463 Addis Ababa ETH 3103673 4567857 3103673 Ahmedabad IND 5570585 NA 5570585 Alexandria EGY 4616625 NA 4616625 Algiers DZA 3415811 5000000 3415811 Almaty KAZ 1703481 NA 1703481 Ankara TUR 5271000 4585000 5271000 Perform an inner join with the cities table on the left and the countries table on the right. SELECT * FROM cities -- Inner join to countries INNER JOIN countries -- Match on country codes ON cities.country_code = countries.code; Table 25.2: Displaying records 1 - 10 name country_code city_proper_pop metroarea_pop urbanarea_pop code name continent region surface_area indep_year local_name gov_form capital cap_long cap_lat Abidjan CIV 4765000 NA 4765000 CIV Cote d’Ivoire Africa Western Africa 322463 1960 Cote d?Ivoire Republic Yamoussoukro -4.030 5.33 Abu Dhabi ARE 1145000 NA 1145000 ARE United Arab Emirates Asia Middle East 83600 1971 Al-Imarat al-´Arabiya al-Muttahida Emirate Federation Abu Dhabi 54.370 24.48 Abuja NGA 1235880 6000000 1235880 NGA Nigeria Africa Western Africa 923768 1960 Nigeria Federal Republic Abuja 7.489 9.06 Accra GHA 2070463 4010054 2070463 GHA Ghana Africa Western Africa 238533 1957 Ghana Republic Accra -0.208 5.57 Addis Ababa ETH 3103673 4567857 3103673 ETH Ethiopia Africa Eastern Africa 1104300 -1000 YeItyop´iya Republic Addis Ababa 38.747 9.02 Ahmedabad IND 5570585 NA 5570585 IND India Asia Southern and Central Asia 3287260 1947 Bharat/India Federal Republic New Delhi 77.225 28.64 Alexandria EGY 4616625 NA 4616625 EGY Egypt Africa Northern Africa 1001450 1922 Misr Republic Cairo 31.246 30.10 Algiers DZA 3415811 5000000 3415811 DZA Algeria Africa Northern Africa 2381740 1962 Al-Jaza?ir/Algerie Republic Algiers 3.051 36.74 Almaty KAZ 1703481 NA 1703481 KAZ Kazakhstan Asia Southern and Central Asia 2724900 1991 Qazaqstan Republic Astana 71.438 51.19 Ankara TUR 5271000 4585000 5271000 TUR Turkey Asia Middle East 774815 1923 Turkiye Republic Ankara 32.361 39.72 Keep only the name of the city, the name of the country, and the region the country is located in. -- Select name fields (with alias) and region SELECT cities.name AS city, countries.name AS country, countries.region FROM cities INNER JOIN countries ON cities.country_code = countries.code; Table 25.3: Displaying records 1 - 10 city country region Abidjan Cote d’Ivoire Western Africa Abu Dhabi United Arab Emirates Middle East Abuja Nigeria Western Africa Accra Ghana Western Africa Addis Ababa Ethiopia Eastern Africa Ahmedabad India Southern and Central Asia Alexandria Egypt Northern Africa Algiers Algeria Northern Africa Almaty Kazakhstan Southern and Central Asia Ankara Turkey Middle East 25.1.1.2 Joining with aliased tables You’ll use data from both the countries and economies tables to examine the inflation rate in 2010 and 2015. -- Select fields with aliases SELECT c.code AS country_code, c.name, e.year, e.inflation_rate FROM countries AS c -- Join to economies (alias e) INNER JOIN economies AS e -- Match on code field using table aliases ON c.code = e.code; Table 25.4: Displaying records 1 - 10 country_code name year inflation_rate AFG Afghanistan 2010 2.179 AFG Afghanistan 2015 -1.549 NLD Netherlands 2010 0.932 NLD Netherlands 2015 0.220 ALB Albania 2010 3.605 ALB Albania 2015 1.896 DZA Algeria 2010 3.913 DZA Algeria 2015 4.784 AGO Angola 2010 14.480 AGO Angola 2015 10.287 Notice that only the code field is ambiguous, so it requires a table name or alias before it. All the other fields (name, year, and inflation_rate) do not occur in more than one table name, so do not require table names or aliasing in the SELECT statement. 25.1.1.3 USING When both the field names being joined on are the same, you can take advantage of the USING clause. SELECT c.name AS country, l.name AS language, official FROM countries AS c INNER JOIN languages AS l -- Match using the code column USING(code); Table 25.5: Displaying records 1 - 10 country language official Afghanistan Dari 1 Afghanistan Other 0 Afghanistan Pashto 1 Afghanistan Turkic 0 Netherlands Dutch 1 Albania Albanian 1 Albania Greek 0 Albania Other 0 Albania unspecified 0 Algeria Arabic 1 It looks like Afghanistan has multiple official and unofficial languages. 25.1.2 Defining relationships One-to-many relationships One-to-one relationships Many-to-many relationships 25.1.2.1 Relationships in our database What best describes the relationship between code in the countries table and country_code in the cities table? ANS: one-to-many Which of these options best describes the relationship between the countries table and the languages table? ANS: many-to-many 25.1.2.2 Inspecting a relationship You’ve just identified that the countries table has a many-to-many relationship with the languages table. That is, many languages can be spoken in a country, and a language can be spoken in many countries. First, what is the best way to query all the different languages spoken in a country? Second, how is this different from the best way to query all the countries that speak each language? Recall that when writing joins, many users prefer to write SQL code out of order by writing the join first (along with any table aliases), and writing the SELECT statement at the end. -- Select country and language names, aliased SELECT c.name AS country, l.name AS language -- From countries (aliased) FROM countries AS c -- Join to languages (aliased) INNER JOIN languages AS l -- Use code as the joining field with the USING keyword USING(code) ORDER BY country; Table 25.6: Displaying records 1 - 10 country language Afghanistan Dari Afghanistan Other Afghanistan Pashto Afghanistan Turkic Albania Albanian Albania Greek Albania Other Albania unspecified Algeria Arabic Algeria Berber or Tamazight -- Rearrange SELECT statement, keeping aliases SELECT l.name AS language, c.name AS country FROM countries AS c INNER JOIN languages AS l USING(code) -- Order the results by language ORDER BY language; Table 25.7: Displaying records 1 - 10 language country Afar Djibouti Afar Eritrea Afar Ethiopia Afrikaans South Africa Afrikaans Namibia Akyem Ghana Albanian Albania Albanian Macedonia Albanian Switzerland Alsatian France When we read SQL results, we expect the most important column to be on the far left, and it’s helpful if results are ordered by relevance to the question at hand. By default, results are ordered by the column from the left table, but you can change this using ORDER BY. 25.1.3 Multiple joins Chaining joins SELECT * FROM left_table INNER JOIN right_table ON left_table.id = right_table.id INNER JOIN another_table ON left/right_table.id = another_table.id; Joining on multiple keys SELECT * FROM left_table INNER JOIN right_table -- INNER JOIN ON the id AND date field ON left_table.id = right_table.id AND left_table.date = right_table.date; 25.1.3.1 Joining multiple tables Suppose you are interested in the relationship between fertility and unemployment rates. Your task in this exercise is to join tables to return the country name, year, fertility rate, and unemployment rate in a single result from the countries, populations and economies tables. -- Select relevant fields SELECT name, year, fertility_rate -- Inner join countries and populations, aliased, on code FROM countries AS c INNER JOIN populations AS p ON c.code = p.country_code; Table 25.8: Displaying records 1 - 10 name year fertility_rate Afghanistan 2010 5.75 Afghanistan 2015 4.65 Netherlands 2010 1.79 Netherlands 2015 1.71 Albania 2010 1.66 Albania 2015 1.79 Algeria 2010 2.87 Algeria 2015 2.81 American Samoa 2010 NA American Samoa 2015 NA Chain another inner join to your query with the economies table. Select name, and select year and unemployment_rate from economies. -- Select fields SELECT name, e.year, fertility_rate, e.unemployment_rate FROM countries AS c INNER JOIN populations AS p ON c.code = p.country_code -- Join to economies (as e) INNER JOIN economies AS e -- Match on country code USING(code); Table 25.9: Displaying records 1 - 10 name year fertility_rate unemployment_rate Afghanistan 2010 4.65 NA Afghanistan 2015 4.65 NA Afghanistan 2010 5.75 NA Afghanistan 2015 5.75 NA Netherlands 2010 1.71 5.00 Netherlands 2015 1.71 6.89 Netherlands 2010 1.79 5.00 Netherlands 2015 1.79 6.89 Albania 2010 1.66 14.00 Albania 2015 1.66 17.10 25.1.3.2 Checking multi-table joins Have a look at the results for Albania from the previous query below. You can see that the 2015 fertility_rate has been paired with 2010 unemployment_rate, and vice versa. SELECT name, e.year, fertility_rate, e.unemployment_rate FROM countries AS c INNER JOIN populations AS p ON c.code = p.country_code -- Join to economies (as e) INNER JOIN economies AS e -- Match on country code USING(code) WHERE name = &#39;Albania&#39;; Table 25.10: 4 records name year fertility_rate unemployment_rate Albania 2010 1.66 14.0 Albania 2015 1.66 17.1 Albania 2010 1.79 14.0 Albania 2015 1.79 17.1 Instead of four records, the query should return two: one for each year. Fix the query by explicitly stating that both the country code and year should match! SELECT name, e.year, fertility_rate, unemployment_rate FROM countries AS c INNER JOIN populations AS p ON c.code = p.country_code INNER JOIN economies AS e ON c.code = e.code -- Add an additional joining condition such that you are also joining on year AND p.year = e.year; Table 25.11: Displaying records 1 - 10 name year fertility_rate unemployment_rate Afghanistan 2010 5.75 NA Afghanistan 2015 4.65 NA Netherlands 2010 1.79 5.00 Netherlands 2015 1.71 6.89 Albania 2010 1.66 14.00 Albania 2015 1.79 17.10 Algeria 2010 2.87 9.96 Algeria 2015 2.81 11.21 Angola 2010 6.42 NA Angola 2015 6.00 NA There are only two lines of Albania results now: one for 2010 and one for 2015. 25.2 Outer, Cross &amp; Self Joins 25.2.1 LEFT &amp; RIGHT JOINs LEFT JOIN Return all records in the left table, and those records in the right table that match on the joining field provided. More intuitive to users when typing from left to right. RIGHT JOIN Less commonly used than LEFT JOIN Any RIGHT JOIN can be re-written as a LEFT JOIN 25.2.1.1 LEFT JOIN As before, you will be using the cities and countries tables. You’ll begin with an INNER JOIN with the cities table (left) and countries table (right). This helps if you are interested only in records where a country is present in both tables. You’ll then change to a LEFT JOIN. This helps if you’re interested in returning all countries in the cities table, whether or not they have a match in the countries table. SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- Perform an inner join with cities as c1 and countries as c2 on country code INNER JOIN countries AS c2 ON c1.country_code = c2.code ORDER BY code DESC; Table 25.12: Displaying records 1 - 10 city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742979 Cape Town ZAF South Africa Southern Africa 3740026 Durban ZAF South Africa Southern Africa 3442361 Ekurhuleni ZAF South Africa Southern Africa 3178470 Johannesburg ZAF South Africa Southern Africa 4434827 Sana’a YEM Yemen Middle East 1937451 Hanoi VNM Vietnam Southeast Asia 6844100 Ho Chi Minh City VNM Vietnam Southeast Asia 7681700 Caracas VEN Venezuela South America 1943901 SELECT c1.name AS city, code, c2.name AS country, region, city_proper_pop FROM cities AS c1 -- Join right table (with alias) LEFT JOIN countries AS c2 ON c1.country_code = c2.code ORDER BY code DESC; Table 25.13: Displaying records 1 - 10 city code country region city_proper_pop Harare ZWE Zimbabwe Eastern Africa 1606000 Lusaka ZMB Zambia Eastern Africa 1742979 Cape Town ZAF South Africa Southern Africa 3740026 Durban ZAF South Africa Southern Africa 3442361 Ekurhuleni ZAF South Africa Southern Africa 3178470 Johannesburg ZAF South Africa Southern Africa 4434827 Sana’a YEM Yemen Middle East 1937451 Hanoi VNM Vietnam Southeast Asia 6844100 Ho Chi Minh City VNM Vietnam Southeast Asia 7681700 Caracas VEN Venezuela South America 1943901 Notice that the INNER JOIN resulted in 230 records, whereas the LEFT JOIN returned 236 records. Remember that the LEFT JOIN is a type of outer join: its result is not limited to only those records that have matches for both tables on the joining field. Building on your LEFT JOIN ou will use AVG() in combination with a LEFT JOIN to determine the average gross domestic product (GDP) per capita by region in 2010. SELECT name, region, gdp_percapita FROM countries AS c LEFT JOIN economies AS e -- Match on code fields USING(code) -- Filter for the year 2010 WHERE year = 2010; Table 25.14: Displaying records 1 - 10 name region gdp_percapita Afghanistan Southern and Central Asia 540 Angola Central Africa 3599 Albania Southern Europe 4098 United Arab Emirates Middle East 34629 Argentina South America 10413 Armenia Middle East 3122 Antigua and Barbuda Caribbean 13532 Australia Australia and New Zealand 56363 Austria Western Europe 46757 Azerbaijan Middle East 5847 -- Select region, and average gdp_percapita as avg_gdp SELECT region, AVG(gdp_percapita) AS avg_gdp FROM countries AS c LEFT JOIN economies AS e USING(code) WHERE year = 2010 -- Group by region GROUP BY region; Table 25.15: Displaying records 1 - 10 region avg_gdp Australia and New Zealand 44792 Baltic Countries 12631 British Islands 43588 Caribbean 11413 Central Africa 4797 Central America 4970 Eastern Africa 1757 Eastern Asia 24963 Eastern Europe 10095 Melanesia 2533 SELECT region, AVG(gdp_percapita) AS avg_gdp FROM countries AS c LEFT JOIN economies AS e USING(code) WHERE year = 2010 GROUP BY region -- Order by descending avg_gdp ORDER BY avg_gdp DESC -- Return only first 10 records LIMIT 10; Table 25.16: Displaying records 1 - 10 region avg_gdp Western Europe 58131 Nordic Countries 57074 North America 47912 Australia and New Zealand 44792 British Islands 43588 Eastern Asia 24963 Southern Europe 22926 Middle East 18205 Baltic Countries 12631 Caribbean 11413 25.2.1.2 RIGHT JOIN -- Modify this query to use RIGHT JOIN instead of LEFT JOIN SELECT countries.name AS country, languages.name AS language, percent FROM languages RIGHT JOIN countries USING(code) WHERE language IS NOT NULL ORDER BY language; Table 25.17: Displaying records 1 - 10 country language percent Djibouti Afar NA Eritrea Afar NA Ethiopia Afar 1.7 Namibia Afrikaans 10.4 South Africa Afrikaans 13.5 Ghana Akyem 3.2 Albania Albanian 98.8 Macedonia Albanian 25.1 Switzerland Albanian 3.0 France Alsatian NA This is same as below left join. -- Modify this query to use LEFT JOIN SELECT countries.name AS country, languages.name AS language, percent FROM countries LEFT JOIN languages USING(code) WHERE language IS NOT NULL ORDER BY language; Table 25.18: Displaying records 1 - 10 country language percent Djibouti Afar NA Eritrea Afar NA Ethiopia Afar 1.7 Namibia Afrikaans 10.4 South Africa Afrikaans 13.5 Ghana Akyem 3.2 Albania Albanian 98.8 Macedonia Albanian 25.1 Switzerland Albanian 3.0 France Alsatian NA 25.2.2 FULL JOIN FULL JOIN A FULL JOIN combines a LEFT JOIN and a RIGHT JOIN. 25.2.2.1 Comparing joins You’ll examine how results can differ when performing a full join compared to a left join and inner join by joining the countries and currencies tables. You’ll be focusing on the North American region and records where the name of the country is missing. Begin with a full join with countries on the left and currencies on the right. Then complete a similar left join and conclude with an inner join, observing the results you see along the way. FUll JOIN SELECT name AS country, code, region, basic_unit FROM countries -- Join to currencies FULL JOIN currencies USING (code) -- Where region is North America or name is null WHERE (region = &#39;North America&#39;) OR (name IS NULL) ORDER BY region DESC; Table 25.19: Displaying records 1 - 10 country code region basic_unit Bermuda BMU North America Bermudian dollar Greenland GRL North America NA Canada CAN North America Canadian dollar United States USA North America United States dollar NA AIA NA East Caribbean dollar NA IOT NA United States dollar NA CCK NA Australian dollar NA COK NA New Zealand dollar NA TMP NA United States dollar NA FLK NA Falkland Islands pound LEFT JOIN SELECT name AS country, code, region, basic_unit FROM countries -- Join to currencies LEFT JOIN currencies USING (code) WHERE region = &#39;North America&#39; OR name IS NULL ORDER BY region; Table 25.20: 4 records country code region basic_unit Bermuda BMU North America Bermudian dollar Greenland GRL North America NA Canada CAN North America Canadian dollar United States USA North America United States dollar INNER JOIN SELECT name AS country, code, region, basic_unit FROM countries -- Join to currencies INNER JOIN currencies USING (code) WHERE region = &#39;North America&#39; OR name IS NULL ORDER BY region; Table 25.21: 3 records country code region basic_unit Bermuda BMU North America Bermudian dollar Canada CAN North America Canadian dollar United States USA North America United States dollar The FULL JOIN query returned 18 records, the LEFT JOIN returned 4 records, and the INNER JOIN only returned 3 records. 25.2.2.2 Chaining FULL JOINs uppose you are doing some research on Melanesia and Micronesia, and are interested in pulling information about languages and currencies into the data we see for these regions in the countries table. Since languages and currencies exist in separate tables, this will require two consecutive full joins involving the countries, languages and currencies tables. SELECT c1.name AS country, region, l.name AS language, basic_unit, frac_unit FROM countries as c1 -- Full join with languages (alias as l) FULL JOIN languages AS l USING(code) -- Full join with currencies (alias as c2) FULL JOIN currencies AS c2 USING(code) WHERE region LIKE &#39;M%esia&#39;; Table 25.22: Displaying records 1 - 10 country region language basic_unit frac_unit Fiji Islands Melanesia NA NA NA Guam Micronesia English NA NA Guam Micronesia Filipino NA NA Guam Micronesia Chamorro NA NA Guam Micronesia Other Pacific Islander NA NA Guam Micronesia Asian NA NA Guam Micronesia Other NA NA Kiribati Micronesia Kiribati Australian dollar Cent Kiribati Micronesia English Australian dollar Cent Marshall Islands Micronesia Marshallese United States dollar Cent The first FULL JOIN in the query pulled countries and languages, and the second FULL JOIN added in currency data for each record in the result of the first FULL JOIN. 25.2.3 CROSS JOIN creates all possible combinations of two tables. SELECT id1, id2 FROM table1 CROSS JOIN table2; Histories and languages CROSS JOIN can be incredibly helpful when asking questions that involve looking at all possible combinations or pairings between two sets of data. Imagine you are a researcher interested in the languages spoken in two countries: Pakistan and India. You are interested in asking: What are the languages presently spoken in the two countries? Given the shared history between the two countries, what languages could potentially have been spoken in either country over the course of their history? Explore how INNER JOIN and CROSS JOIN can help answer these two questions, respectively. SELECT c.name AS country, l.name AS language -- Inner join countries as c with languages as l on code FROM countries AS c INNER JOIN languages AS l USING(code) WHERE c.code IN (&#39;PAK&#39;,&#39;IND&#39;) AND l.code in (&#39;PAK&#39;,&#39;IND&#39;); Table 25.23: Displaying records 1 - 10 country language India Assamese India Bengali India Gujarati India Hindi India Kannada India Maithili India Malayalam India Marathi India Oriya India Other Look at possible combinations of languages that could have been spoken in the two countries given their history. SELECT c.name AS country, l.name AS language FROM countries AS c -- Perform a cross join to languages (alias as l) CROSS JOIN languages AS l WHERE c.code in (&#39;PAK&#39;,&#39;IND&#39;) AND l.code in (&#39;PAK&#39;,&#39;IND&#39;); Table 25.24: Displaying records 1 - 10 country language India Hindi India Bengali India Telugu India Marathi India Tamil India Urdu India Gujarati India Kannada India Malayalam India Oriya Notice that the INNER JOIN returned 25 records, whereas the CROSS JOIN returned 50 records, as it took all combinations of languages returned by the INNER JOIN for both countries. (25*2=50) Choosing your join Determine the names of the five countries and their respective regions with the lowest life expectancy for the year 2010. SELECT c.name AS country, region, life_expectancy AS life_exp FROM countries AS c -- Join to populations (alias as p) using an appropriate join FULL JOIN populations AS p ON c.code = p.country_code -- Filter for only results in the year 2010 WHERE year = 2010 AND life_exp IS NOT NULL -- Sort by life_exp ORDER BY life_exp -- Limit to five records LIMIT 5; Table 25.25: 5 records country region life_exp Lesotho Southern Africa 47.5 Central African Republic Central Africa 47.6 Sierra Leone Western Africa 48.2 Swaziland Southern Africa 48.3 Zimbabwe Eastern Africa 49.6 Notice that all four types of joins we have learned will return the same result. 25.2.4 Self joins Self joins are tables joined with themselves They can be used to compare parts of the same table Comparing a country to itself Suppose you are interested in finding out how much the populations for each country changed from 2010 to 2015. You can visualize this change by performing a self join. Since you’ll be joining the populations table to itself, you can alias populations first as p1 and again as p2. This is good practice whenever you are aliasing tables with the same first letter. -- Select aliased fields from populations as p1 SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 FROM populations AS p1 -- Join populations as p1 to itself, alias as p2, on country code INNER JOIN populations AS p2 ON p1.country_code = p2.country_code Table 25.26: Displaying records 1 - 10 country_code size2010 size2015 ABW 101597 101597 ABW 101597 103889 ABW 103889 101597 ABW 103889 103889 AFG 27962207 27962207 AFG 27962207 32526562 AFG 32526562 27962207 AFG 32526562 32526562 AGO 21219954 21219954 AGO 21219954 25021974 Since you want to compare records from 2010 and 2015, eliminate unwanted records by extending the WHERE statement to include only records where the p1.year matches p2.year - 5. SELECT p1.country_code, p1.size AS size2010, p2.size AS size2015 FROM populations AS p1 INNER JOIN populations AS p2 ON p1.country_code = p2.country_code WHERE p1.year = 2010 -- Filter such that p1.year is always five years before p2.year AND (p2.year = 2015); Table 25.27: Displaying records 1 - 10 country_code size2010 size2015 ABW 101597 103889 AFG 27962207 32526562 AGO 21219954 25021974 ALB 2913021 2889167 AND 84419 70473 ARE 8329453 9156963 ARG 41222875 43416755 ARM 2963496 3017712 ASM 55636 55538 ATG 87233 91818 All joins on deck 25.3 Set Theory for SQL Joins Venn diagrams and set theory 25.3.1 UNION vs. UNION ALL UNION takes two tables as input, and returns all records from both tables, excluding duplicates SELECT * FROM left_table UNION SELECT * FROM right_table; UNION ALL takes two tables and returns all records from both tables, including duplicates SELECT * FROM left_table UNION ALL SELECT * FROM right_table; UNION and UNION ALL syntax Both queries on the left and right of the set operation must have the same data types. The names of the fields do not need to be the same, as the result will always contain field names from the left query. Two tables, languages and currencies: SELECT code FROM languages UNION SELECT curr_id FROM currencies; ❌A SQL error, because languages and currencies do not have the same number of fields. SELECT code FROM languages UNION SELECT curr_id FROM currencies; ❌A SQL error, because code and curr_id are not of the same data type. SELECT code FROM languages UNION ALL SELECT code FROM currencies; Table 25.28: Displaying records 1 - 10 code AFG AFG AFG AFG ALB ALB ALB ALB DZA DZA ⭕An unordered list of each country code in languages and currencies, including duplicates. Comparing global economies In this exercise, you have two tables, economies2015 and economies2019, available to you under the tabs in the console. You’ll perform a set operation to stack all records in these two tables on top of each other, excluding duplicates. When drafting queries containing set operations, it is often helpful to write the queries on either side of the operation first, and then call the set operator. -- Select all fields from economies2015 SELECT * FROM economies2015 -- Set operation UNION -- Select all fields from economies2019 SELECT * FROM economies2019 ORDER BY code, year; Table 25.29: Displaying records 1 - 10 code year income_group gross_savings ABW 2015 High income 14.87 AGO 2015 Lower middle income 25.02 AGO 2019 Lower middle income 25.52 ALB 2015 Upper middle income 16.86 ALB 2019 Upper middle income 14.50 ARG 2015 Upper middle income 14.29 ARG 2019 Upper middle income 14.29 ARM 2015 Upper middle income 18.43 ARM 2019 Upper middle income 9.82 ATG 2015 High income 7.05 UNION can be helpful for consolidating data from multiple tables into one result, which as you have seen, can then be ordered in meaningful ways. Comparing two set operations In this exercise, you will looking at cases for when UNION is appropriate compared to UNION ALL. You will be looking at combinations of country code and year from the economies and populations tables. -- Query that determines all pairs of code and year from economies and populations, without duplicates SELECT code, year FROM economies UNION SELECT country_code, year FROM populations ORDER BY code, year; Table 25.30: Displaying records 1 - 10 code year ABW 2010 ABW 2015 AFG 2010 AFG 2015 AGO 2010 AGO 2015 ALB 2010 ALB 2015 AND 2010 AND 2015 -- Amend the query to return all combinations (including duplicates) SELECT code, year FROM economies -- Set theory clause UNION ALL SELECT country_code, year FROM populations ORDER BY code, year; Table 25.31: Displaying records 1 - 10 code year ABW 2010 ABW 2015 AFG 2010 AFG 2010 AFG 2015 AFG 2015 AGO 2010 AGO 2010 AGO 2015 AGO 2015 UNION returned 434 records, whereas UNION ALL returned 814. Are you able to spot the duplicates in the UNION ALL? 25.3.2 INTERSECT INTERSECT is a robust set operation for finding the set of identical records between two sets of records. INTERSECT vs. INNER JOIN on two columns INTERSECT INNER JOIN SELECT * FROM left_table INTERSECT SELECT * FROM right_table; SELECT * FROM left_table INNER JOIN right_table ON left.id = right.id AND left.val = right.val removes duplicates return duplicates, if id is duplicated in either table return NULL never return NULL A set-based operator that compares complete rows between two sets and can never return more rows than in the smaller table. An operator that generally matches on a limited set of columns and can return zero rows or more rows from either table. Let’s say you are interested in those countries that share names with cities. Return all city names that are also country names. -- Return all cities with the same name as a country SELECT name FROM cities INTERSECT SELECT name FROM countries; Table 25.32: 1 records name Singapore It looks as though Singapore is the only country in our database that has a city with the same name! 25.3.3 EXCEPT EXCEPT retains only records from the left table that are not present in the right table Find the names of cities that do not have the same names as their countries. -- Return all cities that do not have the same name as a country SELECT name FROM cities EXCEPT SELECT name FROM countries ORDER BY name; Table 25.33: Displaying records 1 - 10 name Abidjan Abu Dhabi Abuja Accra Addis Ababa Ahmedabad Alexandria Algiers Almaty Ankara Note that if countries had been on the left and cities on the right, you would have returned the opposite: all countries that do not have the same name as a city. SELECT name FROM countries EXCEPT SELECT name FROM cities ORDER BY name; Table 25.34: Displaying records 1 - 10 name Afghanistan Albania Algeria American Samoa Andorra Angola Antigua and Barbuda Argentina Armenia Aruba Calling all set operators 25.4 Subqueries 25.4.1 With semi joins &amp; anti joins Additive joins The six joins we’ve worked with so far are all “additive” in that they add columns to the original “left” table. INNER JOIN LEFT JOIN RIGHT JOIN FULL JOIN CROSS JOIN Self joins None additive joins Semi join Anti join Chooses records in the first table where a condition is met in the second table. Chooses records in the first table where a condition is NOT met in the second table. SELECT president, country, continent FROM presidents WHERE country IN  (SELECT country FROM states WHERE indep_year &lt; 1800); SELECT country, president FROM presidents WHERE continent LIKE &#39;%America&#39; AND country NOT IN  (SELECT country FROM states WHERE indep_year &lt; 1800); 25.4.1.1 Semi join Let’s say you are interested in identifying languages spoken in the Middle East. The languages table contains information about languages and countries, but it does not tell you what region the countries belong to. You can build up a semi join by filtering the countries table by a particular region, and then using this to further filter the languages table. -- Select country code for countries in the Middle East SELECT code FROM countries WHERE region = &#39;Middle East&#39;; Table 25.35: Displaying records 1 - 10 code ARE ARM AZE BHR GEO IRQ ISR YEM JOR KWT -- Select unique language names SELECT DISTINCT name FROM languages -- Order by the name of the language ORDER BY name; Table 25.36: Displaying records 1 - 10 name Afar Afrikaans Akyem Albanian Alsatian Amerindian Amharic Angolar Antiguan creole Arabic Create a semi join out of the two queries you’ve written, which filters unique languages returned in the first query for only those languages spoken in the 'Middle East'. SELECT DISTINCT name FROM languages -- Add syntax to use bracketed subquery below as a filter WHERE code IN (SELECT code FROM countries WHERE region = &#39;Middle East&#39;) ORDER BY name; Table 25.37: Displaying records 1 - 10 name Arabic Aramaic Armenian Azerbaijani Azeri Baluchi Bulgarian Circassian English Farsi 25.4.1.2 Anti join The anti join is a related and powerful joining tool. It can be particularly useful for identifying whether an incorrect number of records appears in a join. Say you are interested in identifying currencies of Oceanian countries. You have written the following INNER JOIN, which returns 15 records. SELECT c1.code, name, basic_unit AS currency FROM countries AS c1 INNER JOIN currencies AS c2 ON c1.code = c2.code WHERE c1.continent = &#39;Oceania&#39;; Table 25.38: Displaying records 1 - 10 code name currency AUS Australia Australian dollar KIR Kiribati Australian dollar MHL Marshall Islands United States dollar NRU Nauru Australian dollar PLW Palau United States dollar PNG Papua New Guinea Papua New Guinean kina PYF French Polynesia CFP franc SLB Solomon Islands Solomon Islands dollar WSM Samoa Samoan tala TON Tonga Tongan paʻanga Now, you want to ensure that all Oceanian countries from the countries table are included in this result. -- Select code and name of countries from Oceania SELECT code, name FROM countries WHERE continent = &#39;Oceania&#39;; Table 25.39: Displaying records 1 - 10 code name ASM American Samoa AUS Australia FJI Fiji Islands GUM Guam KIR Kiribati MHL Marshall Islands FSM Micronesia, Federated States of NRU Nauru PLW Palau PNG Papua New Guinea There’re 19 records. If there are any Oceanian countries excluded in this INNER JOIN, you want to return the names of these countries. SELECT code, name FROM countries WHERE continent = &#39;Oceania&#39; -- Filter for countries not included in the bracketed subquery AND code NOT IN (SELECT code FROM currencies); Table 25.40: 5 records code name ASM American Samoa FJI Fiji Islands GUM Guam FSM Micronesia, Federated States of MNP Northern Mariana Islands Anti join determined which five out of 19 countries that were not included in the INNER JOIN provided. 25.4.2 Inside WHERE &amp; SELECT Syntax for subqueries inside WHERE All semi joins and anti joins we have seen included a subequery in WHERE WHERE is the most common place for subqueries -- example SELECT * FROM some_table WHERE some_field IN (SELECT some_numeric_field FROM another_table WHERE field2 = some_condition); Subqueries inside SELECT -- example SELECT DISTINCT continent, (SELECT COUNT(*) FROM monarchs WHERE states.continent = monarch.continent) AS monarch_count FROM states; 25.4.2.1 Subquery inside WHERE In this exercise, you will nest a subquery from the populations table inside another query from the same table, populations. Your goal is to figure out which countries had high average life expectancies in 2015. Suppose you only want records from 2015 with life_expectancy above 1.15 * avg_life_expectancy. SELECT * FROM populations WHERE life_expectancy &gt; 1.15 * avg_life_expectancy AND year = 2015; Write a query to calculate a value for avg_life_expectancy. -- Select average life_expectancy from the populations table SELECT AVG(life_expectancy) FROM populations -- Filter for the year 2015 WHERE year = 2015; Table 25.41: 1 records AVG(life_expectancy) 71.7 Nest this calculation into another query. SELECT * FROM populations -- Filter for only those populations where life expectancy is 1.15 times higher than average WHERE life_expectancy &gt; 1.15 * (SELECT AVG(life_expectancy) FROM populations WHERE year = 2015) AND year = 2015; Table 25.42: Displaying records 1 - 10 pop_id country_code year fertility_rate life_expectancy size 21 AUS 2015 1.83 82.5 23789752 376 CHE 2015 1.54 83.2 8281430 356 ESP 2015 1.32 83.4 46443994 134 FRA 2015 2.01 82.7 66538391 170 HKG 2015 1.20 84.3 7305700 174 ISL 2015 1.93 82.9 330815 190 ITA 2015 1.37 83.5 60730582 194 JPN 2015 1.46 83.8 126958472 340 SGP 2015 1.24 82.6 5535002 374 SWE 2015 1.88 82.6 9799186 Many of these country codes as being relatively wealthy countries, which makes sense as we might expect life expectancy to be higher in wealthier nations. WHERE do people live? Identifying capital cities in order of largest to smallest population. Get the urban area population for capital cities only. -- Select relevant fields from cities table SELECT name, country_code, urbanarea_pop FROM cities -- Filter using a subquery on the countries table WHERE name IN (SELECT capital FROM countries) ORDER BY urbanarea_pop DESC; Table 25.43: Displaying records 1 - 10 name country_code urbanarea_pop Beijing CHN 21516000 Dhaka BGD 14543124 Tokyo JPN 13513734 Moscow RUS 12197596 Cairo EGY 10230350 Kinshasa COD 10130000 Jakarta IDN 10075310 Seoul KOR 9995784 Mexico City MEX 8974724 Lima PER 8852000 25.4.2.2 Subquery inside SELECT Begin with a LEFT JOIN combined with a GROUP BY to select the nine countries with the most cities appearing in the cities table, along with the counts of these cities. -- Find top nine countries with the most cities SELECT countries.name AS country, COUNT(cities.name) AS cities_num FROM countries LEFT JOIN cities ON countries.code = cities.country_code GROUP BY countries.name -- Order by count of cities as cities_num ORDER BY cities_num DESC, country ASC LIMIT 9; Table 25.44: 9 records country cities_num China 36 India 18 Japan 11 Brazil 10 Pakistan 9 United States 9 Indonesia 7 Russian Federation 7 South Korea 7 Write a query that returns the same result as the join, but leveraging a nested query instead. SELECT countries.name AS country, -- Subquery that provides the count of cities (SELECT COUNT(*) FROM cities WHERE cities.country_code = countries.code) AS cities_num FROM countries ORDER BY cities_num DESC, country LIMIT 9; Table 25.45: 9 records country cities_num China 36 India 18 Japan 11 Brazil 10 Pakistan 9 United States 9 Indonesia 7 Russian Federation 7 South Korea 7 Notice how the subquery involves only one additional step in your SELECT statement, whereas the JOIN and GROUP BY are a two-step process. 25.4.3 Inside FROM Include a subquery as a temporary table in FROM clause and then SELECT from it. 25.4.3.1 Subquery inside FROM Subqueries inside FROM can help select columns from multiple tables in a single query. Say you are interested in determining the number of languages spoken for each country. You want to present this information alongside each country’s local_name, which is a field only present in the countries table and not in the languages table. Use a subquery inside FROM to bring information from these two tables together. -- Select code, and language count as lang_num = sub SELECT code, COUNT(name) AS lang_num FROM languages GROUP BY code; Table 25.46: Displaying records 1 - 10 code lang_num ABW 7 AFG 4 AGO 12 AIA 1 ALB 4 AND 4 ARE 5 ARG 6 ARM 3 ASM 5 -- Select local_name and lang_num from appropriate tables SELECT countries.local_name, sub.lang_num FROM countries, (SELECT code, COUNT(*) AS lang_num FROM languages GROUP BY code) AS sub -- Where codes match, only present in the countries and not in the languages WHERE countries.code = sub.code ORDER BY lang_num DESC; Table 25.47: Displaying records 1 - 10 local_name lang_num Zambia 19 YeItyop´iya 16 Zimbabwe 16 Bharat/India 14 Nepal 14 South Africa 13 Mali 13 France 13 Angola 12 Malawi 12 25.4.3.2 Subquery challenge Suppose you’re interested in analyzing inflation and unemployment rate for certain countries in 2015. You are not interested in countries with \"Republic\" or \"Monarchy\" as their form of government, but are interested in all other forms of government, such as emirate federations, socialist states, and commonwealths. You will use the field gov_form to filter for these two conditions, which represents a country’s form of government. -- Select relevant fields SELECT code, inflation_rate, unemployment_rate FROM economies WHERE year = 2015 -- Anti join AND code NOT IN -- Subquery returning country codes filtered on gov_form (SELECT code FROM countries -- Filter which do not contain the words &quot;Republic&quot; or &quot;Monarchy&quot; in gov_form. WHERE gov_form LIKE &#39;%Republic%&#39; OR gov_form LIKE &#39;%Monarchy%&#39;) ORDER BY inflation_rate DESC; Table 25.48: Displaying records 1 - 10 code inflation_rate unemployment_rate SSD 52.813 NA LBY 9.839 NA MAC 4.564 1.82 ARE 4.070 NA HKG 3.037 3.30 SRB 1.392 18.20 MNE 1.204 NA TLS 0.553 NA ROU -0.596 6.81 PRI -0.751 12.00 In 2015, South Sudan (with country code SSD) had inflation above 50%! 25.4.3.3 Final challenge Your task is to determine the top 10 capital cities in Europe and the Americas by city_perc, a metric you’ll calculate. city_perc is a percentage that calculates the “proper” population in a city as a percentage of the total population in the wider metro area, as follows: city_proper_pop / metroarea_pop * 100 Do not use table aliasing in this exercise. -- Select fields from cities SELECT name, country_code, city_proper_pop, metroarea_pop, -- avoid integer dived by integer return integer, so multiple 1.0 ((city_proper_pop * 1.0) / (metroarea_pop * 1.0) * 100) AS city_perc FROM cities -- Use subquery to filter city name, capital cities in Europe and Americas WHERE name IN (SELECT capital FROM countries WHERE continent LIKE &#39;Europe&#39; OR continent LIKE &#39;%America&#39;) -- with &#39;America&#39; at the end of name -- Add filter condition such that metroarea_pop does not have null values AND (metroarea_pop IS NOT NULL) -- Sort and limit the result ORDER BY city_perc DESC LIMIT 10; Table 25.49: Displaying records 1 - 10 name country_code city_proper_pop metroarea_pop city_perc Lima PER 8852000 10750000 82.3 Bogota COL 7878783 9800000 80.4 Moscow RUS 12197596 16170000 75.4 Vienna AUT 1863881 2600000 71.7 Montevideo URY 1305082 1947604 67.0 Caracas VEN 1943901 2923959 66.5 Rome ITA 2877215 4353775 66.1 Brasilia BRA 2556149 3919864 65.2 London GBR 8673713 13879757 62.5 Budapest HUN 1759407 2927944 60.1 Lima has the highest percentage of people living in the city ‘proper’, relative to the wider metropolitan population. # disconnect the sql connection dbDisconnect(con) "],["introduction-to-git.html", "Chapter 26 Introduction to Git 26.1 Introduction 26.2 Making changes 26.3 Git workflows 26.4 Collaborating with Git", " Chapter 26 Introduction to Git 26.1 Introduction 26.1.1 Useful shell commands See location / working directory: pwd See what is in current directory: ls (list files) Changing directory: cd file_place Editing a file: nano file_name Delete, add, change contents of a file Save changes: Ctrl + O Exit the text editor: Ctrl + X Create or edit a file: echo Create a new file echo \"Review for duplicate records\" &gt; todo.txt Add content to existing file echo \"Review for duplicate records\" &gt;&gt; todo.txt Checking Git version: git --version 26.1.2 Saving files Git workflow Modify a file nano , echo Save the draft Adding a single file: git add file_name Adding all modified files: git add . Commit the updated file git commit -m \"...\" Repeat Check the status of files git status 26.1.3 Comparing files Compare an unstaged file with the last committed version: git diff filename Compare a staged file with the last committed version: git diff -r HEAD filename Compare all staged files with the last committed versions: git diff -r HEAD 26.2 Making changes 26.2.1 Storing data with Git The commit structure Git commits have three parts: Commit contains the metadata Git hash allow data sharing between repos If two files are the same, then their hashes are the same eg., the last summary_statistics.csv hash is 3f5003f Tree tracks the names and locations in the repo Blob binary large object may contain data of any kind compressed snapshot of a file’s contents Viewing a repository’s history git log Show more recent commits: press space Quit the log and return to the terminal: press q Finding a particular commit git show c27fa856 Only need the first 6-8 characters of the hash Useful for viewing changes made in a particular commit (vs git diff compare changes between commits) 26.2.2 Viewing changes The HEAD shortcut Compares staged files to the version in the last commit Use a tilde ~ to pick a specific commit to compare versions Changes per document by line git annotate file_name Summary Command Function git show HEAD~1 Show what changed in the second most recent commit git diff 35f4b4d 186398f Show changes between two commits git diff HEAD~1 HEAD~2 Show changes between two commits git annotate file Show line-by-line changes and associated metadata 26.2.3 Undoing changes before committing Staged files Unstaging a single file git reset HEAD file_name Unstaging all files git reset HEAD Unstaged files Undo changes to an unstaged file git checkout -- file_name checkout means switching to a different version, defaults to the last commit losing all changes made to the unstaged file forever Undo changes to all unstaged files git checkout . This command must be run in the main directory 26.2.4 Restoring and reverting Customizing the log output By restrict the number with - git log -3 shows the three most recent commits git log -3 file_name shows the three most recent commits of one file By restrict with date git log --since='Apr 2 2022' since particular date git log --since='Apr 2 2022' --until='Apr 11 2022 between two dates Cleaning a repository See what files are not being tracked git clean -n Delete those files git clean -f 26.3 Git workflows 26.3.1 Configuring Git Levels of settings git config --list : view the list of all customizable settings Git has three levels of settings: 1. --local : settings for one specific project 2. --global : settings for all of our projects 3. --system : settings for every users on this computer Changing our settings git config --global setting value Change email address to johnsmith@datacamp.com: git config --global user.email johnsmith@datacamp.com Change username to John Smith: git config --global user.name 'John Smith' Creating a custom alias Set up an alias through global settings Typically used to shorten a command eg., To create an alias for committing files by executing ci : git config --global alias.ci 'commit -m' We can now commit files by executing: git ci Tracking aliases: git config --global --list Ignoring specific files nano .gitignore 26.3.2 Branches There’re 3 branches, 2 merges in the picture. Source and destination When merging two branches: the commits are called parent commits source : the branch we want to merge from destination : the branch we want to merge into eg., When merging Analysis into Main, Analysis = source Main = destination Identifying branches git branch * = current branch Creating a new branch git checkout -b branch_name The difference between branches git diff branch_1 branch_2 26.3.3 Working with branches Switch branches git checkout branch_name Why do we merge branches? main = ground truth Each branch should be for a specific task Once the task is complete we should merge our changes into main to keep it up to date and accurate Merging branches git merge source destination eg., To merge summary-statistics into main git merge summary-statistics main 26.3.4 Handling conflict A conflict occurs when a file in different branches has different contents that prevent them from automatically merging into a single version. Git conflicts nano todo.txt reserve only c) line and delete others lines Another example to see how to delete lines: 26.4 Collaborating with Git 26.4.1 Creating repos Benefits of repos Systematically track versions Collaborate with colleagues Git stores everything! Don’t create a nested repos Creating a new repo git init repo_name Converting a project git init 26.4.2 Working with remotes Benefits of remote repos Everything is backed up Collaboration, regardless of location git clone is a very useful command for copying other repos onto your local computer, whether from another local directory or remote storage such as GitHub. Cloning locally git clone path-to-project-directory git clone /home/john/repo git clone /home/john/repo new_repo_name Cloning a remote Remote repos are stored in an online hosting service e.g., GitHub, Bitbucket, or Gitlab We can clone a remote repo on to our local computer git clone [URL] ed., git clone https://github.com/datacamp/project Identifying a remote git remote Git stores a remote tag in the new repo’s configuration Getting more information git remote -v Creating a remote git remote add name URL Defining remote names is useful for merging branches git remote add george https://github.com/george_datacamp/repo 26.4.3 Pulling from a remote Two ways to Synchronize local and remote repos fetch and merge Fetching from a remote git fetch remote_name local_branch Synchronizing content git merge remote_name local_branch pull Short cut of above 2 steps process git pull remote_name local_branch Important to save locally before pulling from a remote 26.4.4 Pushing to a remote git push Save changes locally first Push into remote_name from local_branch git push remote_name local_branch Resolving a conflict git pull remote_name local_branch Git will automatically open the nano text editor and ask us to add a message for the merge Leave a message that we are pulling the latest report from the remote "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
